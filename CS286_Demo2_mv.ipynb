{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This script is a single-file PyTorch experiment runner for stress prediction. It:\n",
        "\n",
        "Builds sliding windows of participant data with sizes [3,4,5,6,7,10,14] to predict next-day stress levels.\n",
        "\n",
        "Creates inputs for ANN and sequence models (CNN1D, RNN, GRU, LSTM).\n",
        "\n",
        "Splits participants train/validation/test to avoid data leakage.\n",
        "\n",
        "Trains models with class-weighted cross-entropy loss, records metrics (loss, accuracy, F1), and applies early stopping based on validation F1.\n",
        "\n",
        "Saves results summaries, confusion matrices, and plots to the outputs directory.\n",
        "\n",
        "Supports evaluation across multiple window sizes and models, providing detailed per-class metrics and sample counts."
      ],
      "metadata": {
        "id": "BN3kFknhasl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "experiment_runner.py\n",
        "\n",
        "Single-file PyTorch experiment runner that:\n",
        "- builds sliding windows for window_sizes = [3,4,5,6,7,10,14]\n",
        "- creates ANN and sequence inputs\n",
        "- trains ANN, CNN1D, SimpleRNN, GRU, LSTM\n",
        "- participant-wise split (train/val/test)\n",
        "- saves results to CSV and confusion matrix PNGs\n",
        "\n",
        "Usage:\n",
        "    python experiment_runner.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "CSV_PATH = \"stress_detection.csv\"\n",
        "OUTPUT_DIR = Path(\"outputs\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "WINDOW_SIZES = [3,4,5,6,7,10,14]\n",
        "RAW_FEATURES = [\n",
        "    'Openness','Conscientiousness','Extraversion','Agreeableness','Neuroticism',\n",
        "    'sleep_time','wake_time','sleep_duration','PSQI_score',\n",
        "    'call_duration','num_calls','num_sms',\n",
        "    'screen_on_time','skin_conductance','accelerometer',\n",
        "    'mobility_radius','mobility_distance'\n",
        "]\n",
        "STATIC_PERSONALITY = ['Openness','Conscientiousness','Extraversion','Agreeableness','Neuroticism']\n",
        "TARGET_COL = 'PSS_score'\n",
        "CLASS_COL = 'stress_class'  # we will add\n",
        "\n",
        "# train/val/test by participant fractions\n",
        "TRAIN_P = 0.7\n",
        "VAL_P = 0.15\n",
        "TEST_P = 0.15\n",
        "\n",
        "# training hyperparams (conservative)\n",
        "EPOCHS = 60\n",
        "BATCH_SIZE = 64\n",
        "LR = 1e-3\n",
        "PATIENCE = 8\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# models to run\n",
        "MODEL_NAMES = [\"ANN\",\"CNN1D\",\"RNN\",\"GRU\",\"LSTM\"]\n",
        "\n",
        "# -------------------------\n",
        "# Helpers\n",
        "# -------------------------\n",
        "def stress_to_class(score):\n",
        "    # as user specified: 0-13 low, 14-26 moderate, 27-40 high\n",
        "    s = float(score)\n",
        "    if s <= 13: return 0\n",
        "    if s <= 26: return 1\n",
        "    return 2\n",
        "\n",
        "def ensure_dir(path):\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Dataset builder\n",
        "# -------------------------\n",
        "def build_windows(df, raw_features, window_size=4):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      X_seq: list of arrays shape (window_size, F)\n",
        "      X_flat: list of 1D arrays flattened + aggregated features\n",
        "      y: list of int labels\n",
        "      participants: list of participant ids (aligned with samples)\n",
        "    \"\"\"\n",
        "    X_seq = []\n",
        "    X_flat = []\n",
        "    y = []\n",
        "    participants = []\n",
        "\n",
        "    for pid, df_p in df.groupby(\"participant_id\"):\n",
        "        df_p = df_p.sort_values(\"day\").reset_index(drop=True)\n",
        "        n = len(df_p)\n",
        "        # skip participants with insufficient rows\n",
        "        if n <= window_size:\n",
        "            continue\n",
        "        arr = df_p[raw_features].values.astype(float)\n",
        "        for i in range(n - window_size):\n",
        "            window = arr[i:i+window_size]  # shape (window_size, F)\n",
        "            # aggregated features\n",
        "            mean_vals = window.mean(axis=0)\n",
        "            std_vals = window.std(axis=0)\n",
        "            min_vals = window.min(axis=0)\n",
        "            max_vals = window.max(axis=0)\n",
        "            slope_vals = window[-1] - window[0]\n",
        "            agg = np.concatenate([mean_vals, std_vals, min_vals, max_vals, slope_vals])\n",
        "\n",
        "            flat = np.concatenate([window.flatten(), agg])\n",
        "            target_score = df_p[TARGET_COL].iloc[i+window_size]  # next day\n",
        "            cls = stress_to_class(target_score)\n",
        "\n",
        "            X_seq.append(window.copy())\n",
        "            X_flat.append(flat.copy())\n",
        "            y.append(int(cls))\n",
        "            participants.append(pid)\n",
        "    return np.array(X_seq), np.array(X_flat), np.array(y), np.array(participants)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# PyTorch Dataset\n",
        "# -------------------------\n",
        "class StressDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        # X: either (N,window,F) for seq models or (N,d) for ANN\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# -------------------------\n",
        "# Models\n",
        "# -------------------------\n",
        "class ANNModel(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=[256,128,64], num_classes=3, p_drop=0.3):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last = in_dim\n",
        "        for h in hidden:\n",
        "            layers.append(nn.Linear(last,h))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.BatchNorm1d(h))\n",
        "            layers.append(nn.Dropout(p_drop))\n",
        "            last = h\n",
        "        layers.append(nn.Linear(last,num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self,x):\n",
        "        return self.net(x)\n",
        "\n",
        "class CNN1DModel(nn.Module):\n",
        "    def __init__(self, seq_len, feat_dim, num_classes=3):\n",
        "        super().__init__()\n",
        "        # treat features as channels -> input shape (B, F, L)\n",
        "        self.conv1 = nn.Conv1d(feat_dim, 64, kernel_size=2)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.conv2 = nn.Conv1d(64,128,kernel_size=2)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        # compute flattened size after convs:\n",
        "        l = seq_len - (2-1) - (2-1)  # two convs kernel 2 valid padding\n",
        "        if l < 1: l = 1\n",
        "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128*1,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64,num_classes)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        # x: (B, L, F)\n",
        "        x = x.permute(0,2,1)  # -> (B, F, L)\n",
        "        x = self.conv1(x); x = self.bn1(x); x = torch.relu(x)\n",
        "        x = self.conv2(x); x = self.bn2(x); x = torch.relu(x)\n",
        "        x = self.gap(x)  # (B,128,1)\n",
        "        return self.fc(x)\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=64, num_layers=1, rnn_type='RNN', num_classes=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        if rnn_type=='RNN':\n",
        "            self.rnn = nn.RNN(input_dim,hidden,num_layers,batch_first=True,nonlinearity='tanh',dropout=dropout)\n",
        "        elif rnn_type=='GRU':\n",
        "            self.rnn = nn.GRU(input_dim,hidden,num_layers,batch_first=True,dropout=dropout)\n",
        "        elif rnn_type=='LSTM':\n",
        "            self.rnn = nn.LSTM(input_dim,hidden,num_layers,batch_first=True,dropout=dropout)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown rnn_type\")\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden,32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(32,num_classes)\n",
        "        )\n",
        "        self.rnn_type = rnn_type\n",
        "    def forward(self,x):\n",
        "        # x: (B,L,F)\n",
        "        out, _ = self.rnn(x)  # out: (B,L,H)\n",
        "        last = out[:, -1, :]  # last timestep\n",
        "        return self.head(last)\n",
        "\n",
        "# -------------------------\n",
        "# Training & evaluation utilities\n",
        "# -------------------------\n",
        "def compute_class_weights(y):\n",
        "    counts = Counter(y.tolist())\n",
        "    total = sum(counts.values())\n",
        "    num_classes = len(counts)\n",
        "    weights = []\n",
        "    for i in range(max(counts.keys())+1):\n",
        "        cnt = counts.get(i, 0)\n",
        "        if cnt==0:\n",
        "            weights.append(0.0)\n",
        "        else:\n",
        "            weights.append(total / (num_classes * cnt))\n",
        "    return torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "def train_one_epoch(model, loader, opt, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    preds = []\n",
        "    trues = []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        yb = yb.to(DEVICE)\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        preds.append(out.detach().argmax(dim=1).cpu().numpy())\n",
        "        trues.append(yb.cpu().numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    trues = np.concatenate(trues)\n",
        "    mean_loss = total_loss / len(trues)\n",
        "    acc = accuracy_score(trues, preds)\n",
        "    f1 = f1_score(trues, preds, average='macro', zero_division=0)\n",
        "    return mean_loss, acc, f1\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    preds = []\n",
        "    trues = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            yb = yb.to(DEVICE)\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            preds.append(out.argmax(dim=1).cpu().numpy())\n",
        "            trues.append(yb.cpu().numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    trues = np.concatenate(trues)\n",
        "    mean_loss = total_loss / len(trues)\n",
        "    acc = accuracy_score(trues, preds)\n",
        "    f1 = f1_score(trues, preds, average='macro', zero_division=0)\n",
        "    prec, rec, f1s, sup = precision_recall_fscore_support(trues, preds, average=None, zero_division=0)\n",
        "    return mean_loss, acc, f1, prec, rec, f1s, sup, preds, trues\n",
        "\n",
        "# -------------------------\n",
        "# Experiment loop\n",
        "# -------------------------\n",
        "def run_experiment_for_window(df, window_size, model_name, train_pids, val_pids, test_pids):\n",
        "    # build windows\n",
        "    X_seq, X_flat, y, pids = build_windows(df, RAW_FEATURES, window_size=window_size)\n",
        "    if len(y)==0:\n",
        "        print(f\"No samples for window {window_size}\")\n",
        "        return None\n",
        "\n",
        "    # split by given participant ids - keep only samples whose pid in sets\n",
        "    train_mask = np.isin(pids, train_pids)\n",
        "    val_mask = np.isin(pids, val_pids)\n",
        "    test_mask = np.isin(pids, test_pids)\n",
        "\n",
        "    X_seq_train, X_seq_val, X_seq_test = X_seq[train_mask], X_seq[val_mask], X_seq[test_mask]\n",
        "    X_flat_train, X_flat_val, X_flat_test = X_flat[train_mask], X_flat[val_mask], X_flat[test_mask]\n",
        "    y_train, y_val, y_test = y[train_mask], y[val_mask], y[test_mask]\n",
        "\n",
        "    # skip if any split empty\n",
        "    if len(y_train)==0 or len(y_val)==0 or len(y_test)==0:\n",
        "        print(\"One split empty for window\", window_size)\n",
        "        return None\n",
        "\n",
        "    # Choose input style based on model\n",
        "    if model_name==\"ANN\":\n",
        "        # scale flattened features\n",
        "        scaler = StandardScaler().fit(X_flat_train)\n",
        "        Xtr = scaler.transform(X_flat_train)\n",
        "        Xv = scaler.transform(X_flat_val)\n",
        "        Xt = scaler.transform(X_flat_test)\n",
        "        input_dim = Xtr.shape[1]\n",
        "        train_ds = StressDataset(Xtr, y_train)\n",
        "        val_ds = StressDataset(Xv, y_val)\n",
        "        test_ds = StressDataset(Xt, y_test)\n",
        "    else:\n",
        "        # sequence models: use X_seq shaped (N, L, F) - scale per feature across training sequence samples\n",
        "        # reshape to (N*L, F) to fit scaler\n",
        "        Ntr, L, F = X_seq_train.shape\n",
        "        scaler = StandardScaler().fit(X_seq_train.reshape(-1,F))\n",
        "        Xtr = scaler.transform(X_seq_train.reshape(-1,F)).reshape(Ntr,L,F)\n",
        "        Xv = scaler.transform(X_seq_val.reshape(-1,F)).reshape(X_seq_val.shape)\n",
        "        Xt = scaler.transform(X_seq_test.reshape(-1,F)).reshape(X_seq_test.shape)\n",
        "        train_ds = StressDataset(Xtr, y_train)\n",
        "        val_ds = StressDataset(Xv, y_val)\n",
        "        test_ds = StressDataset(Xt, y_test)\n",
        "        input_dim = F\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # prepare model\n",
        "    if model_name==\"ANN\":\n",
        "        model = ANNModel(input_dim).to(DEVICE)\n",
        "    elif model_name==\"CNN1D\":\n",
        "        # seq len and feat_dim\n",
        "        seq_len = X_seq_train.shape[1]\n",
        "        feat_dim = X_seq_train.shape[2]\n",
        "        model = CNN1DModel(seq_len, feat_dim).to(DEVICE)\n",
        "    elif model_name in (\"RNN\",\"GRU\",\"LSTM\"):\n",
        "        seq_len = X_seq_train.shape[1]\n",
        "        feat_dim = X_seq_train.shape[2]\n",
        "        model = RNNModel(feat_dim, hidden=64, num_layers=1, rnn_type=model_name, num_classes=3).to(DEVICE)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model\")\n",
        "\n",
        "    # class weights\n",
        "    class_weights = compute_class_weights(torch.tensor(y_train))\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
        "\n",
        "    # training loop with early stopping on val f1\n",
        "    best_val_f1 = -1.0\n",
        "    best_state = None\n",
        "    patience = PATIENCE\n",
        "    cur_wait = 0\n",
        "\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, opt, criterion)\n",
        "        val_loss, val_acc, val_f1, *_ = evaluate(model, val_loader, criterion)\n",
        "\n",
        "        # print progress\n",
        "        print(f\"[W{window_size}] {model_name} Epoch {epoch:02d} | train_loss {train_loss:.4f} acc {train_acc:.3f} f1 {train_f1:.3f} || val_loss {val_loss:.4f} acc {val_acc:.3f} f1 {val_f1:.3f}\")\n",
        "\n",
        "        if val_f1 > best_val_f1 + 1e-4:\n",
        "            best_val_f1 = val_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            cur_wait = 0\n",
        "        else:\n",
        "            cur_wait += 1\n",
        "            if cur_wait >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # load best and evaluate on test\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    test_loss, test_acc, test_f1, prec, rec, f1s, sup, preds, trues = evaluate(model, test_loader, criterion)\n",
        "\n",
        "    # save confusion matrix\n",
        "    cm = confusion_matrix(trues, preds)\n",
        "    cm_path = OUTPUT_DIR / f\"cm_w{window_size}_{model_name}.npy\"\n",
        "    np.save(cm_path, cm)\n",
        "\n",
        "    # also save a small png\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(cm, cmap='Blues')\n",
        "    plt.title(f\"CM W{window_size} {model_name}\")\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"pred\")\n",
        "    plt.ylabel(\"true\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUTPUT_DIR / f\"cm_w{window_size}_{model_name}.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # return summary dict\n",
        "    summary = {\n",
        "        \"window\": window_size,\n",
        "        \"model\": model_name,\n",
        "        \"train_samples\": len(y_train),\n",
        "        \"val_samples\": len(y_val),\n",
        "        \"test_samples\": len(y_test),\n",
        "        \"test_loss\": float(test_loss),\n",
        "        \"test_acc\": float(test_acc),\n",
        "        \"test_macro_f1\": float(test_f1),\n",
        "        \"per_class_prec\": prec.tolist(),\n",
        "        \"per_class_rec\": rec.tolist(),\n",
        "        \"per_class_f1\": f1s.tolist(),\n",
        "        \"support\": sup.tolist()\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "# -------------------------\n",
        "# Main orchestration\n",
        "# -------------------------\n",
        "def main():\n",
        "    print(\"Device:\", DEVICE)\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    # ensure ordering and class column\n",
        "    df = df.sort_values(['participant_id','day']).reset_index(drop=True)\n",
        "    df[CLASS_COL] = df[TARGET_COL].apply(stress_to_class)\n",
        "\n",
        "    # participants split\n",
        "    pids = df['participant_id'].unique()\n",
        "    random.shuffle(pids)\n",
        "    n = len(pids)\n",
        "    n_train = int(n * TRAIN_P)\n",
        "    n_val = int(n * VAL_P)\n",
        "    train_pids = pids[:n_train]\n",
        "    val_pids = pids[n_train:n_train+n_val]\n",
        "    test_pids = pids[n_train+n_val:]\n",
        "    print(f\"Participants: total {n} train {len(train_pids)} val {len(val_pids)} test {len(test_pids)}\")\n",
        "\n",
        "    results = []\n",
        "    for w in WINDOW_SIZES:\n",
        "        for mname in MODEL_NAMES:\n",
        "            print(\"=\"*60)\n",
        "            print(f\"Running Window={w} Model={mname}\")\n",
        "            try:\n",
        "                res = run_experiment_for_window(df, w, mname, train_pids, val_pids, test_pids)\n",
        "                if res is not None:\n",
        "                    results.append(res)\n",
        "                    # save intermediate results\n",
        "                    pd.DataFrame(results).to_csv(OUTPUT_DIR / \"results_summary.csv\", index=False)\n",
        "            except Exception as e:\n",
        "                print(\"Error running\", w, mname, e)\n",
        "\n",
        "    # final save\n",
        "    if results:\n",
        "        out_df = pd.DataFrame(results)\n",
        "        out_df.to_csv(OUTPUT_DIR / \"results_summary.csv\", index=False)\n",
        "        print(\"Saved results to\", OUTPUT_DIR / \"results_summary.csv\")\n",
        "    else:\n",
        "        print(\"No results to save.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTm1WxOYhHEQ",
        "outputId": "e3cdf896-29d4-435a-f315-cd1f9373f8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Participants: total 100 train 70 val 15 test 15\n",
            "============================================================\n",
            "Running Window=3 Model=ANN\n",
            "[W3] ANN Epoch 01 | train_loss 1.1996 acc 0.335 f1 0.307 || val_loss 1.1341 acc 0.299 f1 0.278\n",
            "[W3] ANN Epoch 02 | train_loss 1.0910 acc 0.384 f1 0.362 || val_loss 1.1551 acc 0.277 f1 0.265\n",
            "[W3] ANN Epoch 03 | train_loss 1.0485 acc 0.417 f1 0.397 || val_loss 1.1741 acc 0.314 f1 0.304\n",
            "[W3] ANN Epoch 04 | train_loss 0.9981 acc 0.450 f1 0.432 || val_loss 1.2088 acc 0.321 f1 0.305\n",
            "[W3] ANN Epoch 05 | train_loss 0.9691 acc 0.472 f1 0.455 || val_loss 1.2198 acc 0.299 f1 0.285\n",
            "[W3] ANN Epoch 06 | train_loss 0.8913 acc 0.517 f1 0.506 || val_loss 1.2316 acc 0.306 f1 0.288\n",
            "[W3] ANN Epoch 07 | train_loss 0.8612 acc 0.532 f1 0.515 || val_loss 1.2500 acc 0.343 f1 0.316\n",
            "[W3] ANN Epoch 08 | train_loss 0.8176 acc 0.564 f1 0.553 || val_loss 1.2950 acc 0.368 f1 0.331\n",
            "[W3] ANN Epoch 09 | train_loss 0.7710 acc 0.578 f1 0.569 || val_loss 1.2601 acc 0.388 f1 0.347\n",
            "[W3] ANN Epoch 10 | train_loss 0.7605 acc 0.605 f1 0.590 || val_loss 1.3291 acc 0.407 f1 0.358\n",
            "[W3] ANN Epoch 11 | train_loss 0.6965 acc 0.628 f1 0.620 || val_loss 1.3997 acc 0.388 f1 0.339\n",
            "[W3] ANN Epoch 12 | train_loss 0.6734 acc 0.652 f1 0.648 || val_loss 1.4262 acc 0.388 f1 0.340\n",
            "[W3] ANN Epoch 13 | train_loss 0.6182 acc 0.659 f1 0.653 || val_loss 1.4832 acc 0.442 f1 0.382\n",
            "[W3] ANN Epoch 14 | train_loss 0.5974 acc 0.697 f1 0.689 || val_loss 1.5854 acc 0.422 f1 0.371\n",
            "[W3] ANN Epoch 15 | train_loss 0.5721 acc 0.705 f1 0.700 || val_loss 1.6041 acc 0.440 f1 0.367\n",
            "[W3] ANN Epoch 16 | train_loss 0.5520 acc 0.719 f1 0.721 || val_loss 1.6939 acc 0.420 f1 0.361\n",
            "[W3] ANN Epoch 17 | train_loss 0.5034 acc 0.735 f1 0.739 || val_loss 1.7454 acc 0.437 f1 0.361\n",
            "[W3] ANN Epoch 18 | train_loss 0.4987 acc 0.752 f1 0.752 || val_loss 1.8387 acc 0.412 f1 0.347\n",
            "[W3] ANN Epoch 19 | train_loss 0.4590 acc 0.781 f1 0.779 || val_loss 1.8128 acc 0.469 f1 0.390\n",
            "[W3] ANN Epoch 20 | train_loss 0.4386 acc 0.785 f1 0.787 || val_loss 1.8902 acc 0.444 f1 0.372\n",
            "[W3] ANN Epoch 21 | train_loss 0.4292 acc 0.787 f1 0.790 || val_loss 1.8932 acc 0.449 f1 0.380\n",
            "[W3] ANN Epoch 22 | train_loss 0.4020 acc 0.801 f1 0.804 || val_loss 2.0383 acc 0.479 f1 0.403\n",
            "[W3] ANN Epoch 23 | train_loss 0.4221 acc 0.801 f1 0.799 || val_loss 2.0609 acc 0.452 f1 0.360\n",
            "[W3] ANN Epoch 24 | train_loss 0.4097 acc 0.799 f1 0.801 || val_loss 2.1433 acc 0.435 f1 0.359\n",
            "[W3] ANN Epoch 25 | train_loss 0.3982 acc 0.809 f1 0.810 || val_loss 2.0827 acc 0.462 f1 0.385\n",
            "[W3] ANN Epoch 26 | train_loss 0.4012 acc 0.814 f1 0.812 || val_loss 2.1116 acc 0.417 f1 0.351\n",
            "[W3] ANN Epoch 27 | train_loss 0.3665 acc 0.829 f1 0.831 || val_loss 2.1587 acc 0.440 f1 0.357\n",
            "[W3] ANN Epoch 28 | train_loss 0.3707 acc 0.828 f1 0.828 || val_loss 2.1563 acc 0.422 f1 0.354\n",
            "[W3] ANN Epoch 29 | train_loss 0.3537 acc 0.831 f1 0.832 || val_loss 2.1692 acc 0.440 f1 0.357\n",
            "[W3] ANN Epoch 30 | train_loss 0.3518 acc 0.835 f1 0.838 || val_loss 2.2789 acc 0.444 f1 0.355\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.1126 acc 0.269 f1 0.263 || val_loss 1.1028 acc 0.365 f1 0.309\n",
            "[W3] CNN1D Epoch 02 | train_loss 1.0683 acc 0.460 f1 0.422 || val_loss 1.1051 acc 0.301 f1 0.287\n",
            "[W3] CNN1D Epoch 03 | train_loss 1.0359 acc 0.456 f1 0.434 || val_loss 1.1314 acc 0.351 f1 0.298\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.9951 acc 0.505 f1 0.482 || val_loss 1.1534 acc 0.321 f1 0.271\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.9378 acc 0.540 f1 0.521 || val_loss 1.1979 acc 0.328 f1 0.292\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.8588 acc 0.588 f1 0.570 || val_loss 1.2335 acc 0.338 f1 0.306\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.7697 acc 0.641 f1 0.627 || val_loss 1.3504 acc 0.373 f1 0.328\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.6811 acc 0.672 f1 0.668 || val_loss 1.4444 acc 0.370 f1 0.327\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.6071 acc 0.692 f1 0.693 || val_loss 1.6054 acc 0.363 f1 0.300\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.5533 acc 0.726 f1 0.733 || val_loss 1.7602 acc 0.400 f1 0.328\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.4724 acc 0.759 f1 0.771 || val_loss 1.9274 acc 0.427 f1 0.348\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.4058 acc 0.805 f1 0.823 || val_loss 2.0173 acc 0.390 f1 0.312\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3689 acc 0.828 f1 0.847 || val_loss 2.1138 acc 0.417 f1 0.335\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3298 acc 0.840 f1 0.860 || val_loss 2.3138 acc 0.405 f1 0.320\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2969 acc 0.854 f1 0.869 || val_loss 2.4390 acc 0.395 f1 0.305\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2639 acc 0.877 f1 0.892 || val_loss 2.4409 acc 0.435 f1 0.347\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.2251 acc 0.895 f1 0.908 || val_loss 2.7585 acc 0.442 f1 0.345\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.2084 acc 0.905 f1 0.917 || val_loss 2.6400 acc 0.425 f1 0.350\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1816 acc 0.924 f1 0.935 || val_loss 2.6960 acc 0.430 f1 0.367\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1648 acc 0.935 f1 0.937 || val_loss 2.9906 acc 0.422 f1 0.333\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.1420 acc 0.940 f1 0.947 || val_loss 2.8507 acc 0.402 f1 0.318\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1286 acc 0.950 f1 0.957 || val_loss 3.1391 acc 0.430 f1 0.331\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.1234 acc 0.952 f1 0.955 || val_loss 2.8838 acc 0.402 f1 0.345\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.1016 acc 0.967 f1 0.968 || val_loss 3.2001 acc 0.410 f1 0.324\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.0821 acc 0.972 f1 0.973 || val_loss 3.4351 acc 0.417 f1 0.335\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.0763 acc 0.977 f1 0.978 || val_loss 3.2467 acc 0.420 f1 0.324\n",
            "[W3] CNN1D Epoch 27 | train_loss 0.0760 acc 0.973 f1 0.975 || val_loss 3.4467 acc 0.407 f1 0.327\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.1046 acc 0.265 f1 0.235 || val_loss 1.0999 acc 0.316 f1 0.279\n",
            "[W3] RNN Epoch 02 | train_loss 1.0923 acc 0.367 f1 0.327 || val_loss 1.1046 acc 0.323 f1 0.273\n",
            "[W3] RNN Epoch 03 | train_loss 1.0822 acc 0.419 f1 0.392 || val_loss 1.1124 acc 0.363 f1 0.323\n",
            "[W3] RNN Epoch 04 | train_loss 1.0748 acc 0.439 f1 0.406 || val_loss 1.1236 acc 0.321 f1 0.290\n",
            "[W3] RNN Epoch 05 | train_loss 1.0675 acc 0.421 f1 0.398 || val_loss 1.1301 acc 0.321 f1 0.295\n",
            "[W3] RNN Epoch 06 | train_loss 1.0599 acc 0.399 f1 0.383 || val_loss 1.1415 acc 0.311 f1 0.290\n",
            "[W3] RNN Epoch 07 | train_loss 1.0464 acc 0.453 f1 0.427 || val_loss 1.1456 acc 0.321 f1 0.300\n",
            "[W3] RNN Epoch 08 | train_loss 1.0444 acc 0.430 f1 0.410 || val_loss 1.1548 acc 0.316 f1 0.295\n",
            "[W3] RNN Epoch 09 | train_loss 1.0281 acc 0.414 f1 0.402 || val_loss 1.1745 acc 0.323 f1 0.299\n",
            "[W3] RNN Epoch 10 | train_loss 1.0220 acc 0.437 f1 0.420 || val_loss 1.1806 acc 0.316 f1 0.295\n",
            "[W3] RNN Epoch 11 | train_loss 1.0159 acc 0.434 f1 0.420 || val_loss 1.1875 acc 0.306 f1 0.288\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0994 acc 0.369 f1 0.317 || val_loss 1.1000 acc 0.360 f1 0.290\n",
            "[W3] GRU Epoch 02 | train_loss 1.0946 acc 0.425 f1 0.374 || val_loss 1.1006 acc 0.393 f1 0.322\n",
            "[W3] GRU Epoch 03 | train_loss 1.0912 acc 0.453 f1 0.400 || val_loss 1.1030 acc 0.402 f1 0.351\n",
            "[W3] GRU Epoch 04 | train_loss 1.0874 acc 0.447 f1 0.391 || val_loss 1.1050 acc 0.393 f1 0.353\n",
            "[W3] GRU Epoch 05 | train_loss 1.0799 acc 0.462 f1 0.420 || val_loss 1.1098 acc 0.405 f1 0.364\n",
            "[W3] GRU Epoch 06 | train_loss 1.0753 acc 0.425 f1 0.400 || val_loss 1.1118 acc 0.370 f1 0.339\n",
            "[W3] GRU Epoch 07 | train_loss 1.0636 acc 0.454 f1 0.420 || val_loss 1.1243 acc 0.393 f1 0.358\n",
            "[W3] GRU Epoch 08 | train_loss 1.0571 acc 0.442 f1 0.412 || val_loss 1.1313 acc 0.373 f1 0.343\n",
            "[W3] GRU Epoch 09 | train_loss 1.0409 acc 0.448 f1 0.424 || val_loss 1.1409 acc 0.351 f1 0.327\n",
            "[W3] GRU Epoch 10 | train_loss 1.0269 acc 0.443 f1 0.424 || val_loss 1.1567 acc 0.373 f1 0.340\n",
            "[W3] GRU Epoch 11 | train_loss 1.0151 acc 0.460 f1 0.441 || val_loss 1.1642 acc 0.368 f1 0.341\n",
            "[W3] GRU Epoch 12 | train_loss 0.9951 acc 0.503 f1 0.477 || val_loss 1.1774 acc 0.375 f1 0.334\n",
            "[W3] GRU Epoch 13 | train_loss 0.9658 acc 0.512 f1 0.491 || val_loss 1.1920 acc 0.380 f1 0.345\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1001 acc 0.429 f1 0.300 || val_loss 1.1016 acc 0.442 f1 0.315\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0972 acc 0.463 f1 0.338 || val_loss 1.1005 acc 0.432 f1 0.306\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0954 acc 0.443 f1 0.371 || val_loss 1.1002 acc 0.422 f1 0.348\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0924 acc 0.461 f1 0.385 || val_loss 1.1016 acc 0.432 f1 0.352\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0895 acc 0.455 f1 0.379 || val_loss 1.1034 acc 0.420 f1 0.343\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0867 acc 0.438 f1 0.397 || val_loss 1.1043 acc 0.388 f1 0.351\n",
            "[W3] LSTM Epoch 07 | train_loss 1.0812 acc 0.417 f1 0.382 || val_loss 1.1092 acc 0.365 f1 0.331\n",
            "[W3] LSTM Epoch 08 | train_loss 1.0711 acc 0.456 f1 0.419 || val_loss 1.1204 acc 0.383 f1 0.332\n",
            "[W3] LSTM Epoch 09 | train_loss 1.0634 acc 0.424 f1 0.396 || val_loss 1.1266 acc 0.368 f1 0.337\n",
            "[W3] LSTM Epoch 10 | train_loss 1.0473 acc 0.463 f1 0.433 || val_loss 1.1389 acc 0.360 f1 0.332\n",
            "[W3] LSTM Epoch 11 | train_loss 1.0318 acc 0.438 f1 0.420 || val_loss 1.1765 acc 0.365 f1 0.324\n",
            "[W3] LSTM Epoch 12 | train_loss 1.0114 acc 0.466 f1 0.441 || val_loss 1.1778 acc 0.356 f1 0.325\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=4 Model=ANN\n",
            "[W4] ANN Epoch 01 | train_loss 1.2047 acc 0.331 f1 0.312 || val_loss 1.1134 acc 0.295 f1 0.288\n",
            "[W4] ANN Epoch 02 | train_loss 1.0808 acc 0.412 f1 0.390 || val_loss 1.1201 acc 0.303 f1 0.295\n",
            "[W4] ANN Epoch 03 | train_loss 1.0431 acc 0.414 f1 0.399 || val_loss 1.1311 acc 0.326 f1 0.317\n",
            "[W4] ANN Epoch 04 | train_loss 0.9568 acc 0.481 f1 0.465 || val_loss 1.1628 acc 0.333 f1 0.320\n",
            "[W4] ANN Epoch 05 | train_loss 0.9560 acc 0.483 f1 0.467 || val_loss 1.2040 acc 0.341 f1 0.322\n",
            "[W4] ANN Epoch 06 | train_loss 0.8633 acc 0.524 f1 0.510 || val_loss 1.2043 acc 0.369 f1 0.348\n",
            "[W4] ANN Epoch 07 | train_loss 0.8158 acc 0.564 f1 0.549 || val_loss 1.2648 acc 0.374 f1 0.348\n",
            "[W4] ANN Epoch 08 | train_loss 0.7455 acc 0.598 f1 0.588 || val_loss 1.2970 acc 0.369 f1 0.339\n",
            "[W4] ANN Epoch 09 | train_loss 0.7012 acc 0.631 f1 0.618 || val_loss 1.3293 acc 0.385 f1 0.345\n",
            "[W4] ANN Epoch 10 | train_loss 0.6606 acc 0.652 f1 0.643 || val_loss 1.3817 acc 0.372 f1 0.334\n",
            "[W4] ANN Epoch 11 | train_loss 0.6633 acc 0.659 f1 0.653 || val_loss 1.4683 acc 0.382 f1 0.328\n",
            "[W4] ANN Epoch 12 | train_loss 0.5907 acc 0.701 f1 0.694 || val_loss 1.4436 acc 0.382 f1 0.351\n",
            "[W4] ANN Epoch 13 | train_loss 0.5587 acc 0.713 f1 0.710 || val_loss 1.5652 acc 0.415 f1 0.354\n",
            "[W4] ANN Epoch 14 | train_loss 0.5148 acc 0.740 f1 0.738 || val_loss 1.5869 acc 0.413 f1 0.349\n",
            "[W4] ANN Epoch 15 | train_loss 0.5183 acc 0.732 f1 0.730 || val_loss 1.7225 acc 0.379 f1 0.327\n",
            "[W4] ANN Epoch 16 | train_loss 0.4749 acc 0.760 f1 0.758 || val_loss 1.6807 acc 0.431 f1 0.372\n",
            "[W4] ANN Epoch 17 | train_loss 0.4497 acc 0.781 f1 0.784 || val_loss 1.6967 acc 0.428 f1 0.355\n",
            "[W4] ANN Epoch 18 | train_loss 0.4264 acc 0.798 f1 0.800 || val_loss 1.8301 acc 0.408 f1 0.330\n",
            "[W4] ANN Epoch 19 | train_loss 0.4075 acc 0.802 f1 0.808 || val_loss 1.8865 acc 0.421 f1 0.344\n",
            "[W4] ANN Epoch 20 | train_loss 0.3791 acc 0.812 f1 0.818 || val_loss 1.9114 acc 0.413 f1 0.320\n",
            "[W4] ANN Epoch 21 | train_loss 0.3722 acc 0.827 f1 0.826 || val_loss 1.9760 acc 0.423 f1 0.346\n",
            "[W4] ANN Epoch 22 | train_loss 0.3682 acc 0.823 f1 0.821 || val_loss 1.9867 acc 0.415 f1 0.346\n",
            "[W4] ANN Epoch 23 | train_loss 0.4001 acc 0.829 f1 0.822 || val_loss 1.9831 acc 0.444 f1 0.381\n",
            "[W4] ANN Epoch 24 | train_loss 0.3327 acc 0.848 f1 0.848 || val_loss 2.0140 acc 0.418 f1 0.353\n",
            "[W4] ANN Epoch 25 | train_loss 0.3480 acc 0.851 f1 0.848 || val_loss 2.0608 acc 0.431 f1 0.377\n",
            "[W4] ANN Epoch 26 | train_loss 0.3342 acc 0.853 f1 0.849 || val_loss 2.0778 acc 0.415 f1 0.351\n",
            "[W4] ANN Epoch 27 | train_loss 0.3237 acc 0.857 f1 0.861 || val_loss 2.0284 acc 0.438 f1 0.363\n",
            "[W4] ANN Epoch 28 | train_loss 0.2881 acc 0.865 f1 0.864 || val_loss 2.1086 acc 0.426 f1 0.359\n",
            "[W4] ANN Epoch 29 | train_loss 0.2935 acc 0.863 f1 0.859 || val_loss 2.2223 acc 0.400 f1 0.340\n",
            "[W4] ANN Epoch 30 | train_loss 0.2624 acc 0.879 f1 0.883 || val_loss 2.2576 acc 0.413 f1 0.344\n",
            "[W4] ANN Epoch 31 | train_loss 0.2796 acc 0.887 f1 0.880 || val_loss 2.3331 acc 0.395 f1 0.325\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=4 Model=CNN1D\n",
            "[W4] CNN1D Epoch 01 | train_loss 1.1089 acc 0.352 f1 0.323 || val_loss 1.1047 acc 0.318 f1 0.273\n",
            "[W4] CNN1D Epoch 02 | train_loss 1.0671 acc 0.428 f1 0.403 || val_loss 1.1185 acc 0.395 f1 0.328\n",
            "[W4] CNN1D Epoch 03 | train_loss 1.0394 acc 0.450 f1 0.426 || val_loss 1.1372 acc 0.397 f1 0.327\n",
            "[W4] CNN1D Epoch 04 | train_loss 1.0024 acc 0.473 f1 0.451 || val_loss 1.1666 acc 0.400 f1 0.332\n",
            "[W4] CNN1D Epoch 05 | train_loss 0.9511 acc 0.512 f1 0.494 || val_loss 1.1844 acc 0.390 f1 0.356\n",
            "[W4] CNN1D Epoch 06 | train_loss 0.8930 acc 0.536 f1 0.519 || val_loss 1.2006 acc 0.362 f1 0.319\n",
            "[W4] CNN1D Epoch 07 | train_loss 0.8207 acc 0.581 f1 0.568 || val_loss 1.2715 acc 0.346 f1 0.305\n",
            "[W4] CNN1D Epoch 08 | train_loss 0.7472 acc 0.624 f1 0.622 || val_loss 1.4565 acc 0.382 f1 0.315\n",
            "[W4] CNN1D Epoch 09 | train_loss 0.6776 acc 0.649 f1 0.648 || val_loss 1.4424 acc 0.362 f1 0.308\n",
            "[W4] CNN1D Epoch 10 | train_loss 0.6239 acc 0.680 f1 0.680 || val_loss 1.7077 acc 0.387 f1 0.308\n",
            "[W4] CNN1D Epoch 11 | train_loss 0.5685 acc 0.718 f1 0.727 || val_loss 1.7024 acc 0.395 f1 0.323\n",
            "[W4] CNN1D Epoch 12 | train_loss 0.5139 acc 0.728 f1 0.738 || val_loss 1.7424 acc 0.382 f1 0.315\n",
            "[W4] CNN1D Epoch 13 | train_loss 0.4987 acc 0.744 f1 0.752 || val_loss 1.7822 acc 0.367 f1 0.309\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=4 Model=RNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W4] RNN Epoch 01 | train_loss 1.1059 acc 0.432 f1 0.297 || val_loss 1.1098 acc 0.385 f1 0.274\n",
            "[W4] RNN Epoch 02 | train_loss 1.0924 acc 0.463 f1 0.367 || val_loss 1.1065 acc 0.344 f1 0.286\n",
            "[W4] RNN Epoch 03 | train_loss 1.0839 acc 0.466 f1 0.414 || val_loss 1.1122 acc 0.333 f1 0.295\n",
            "[W4] RNN Epoch 04 | train_loss 1.0726 acc 0.459 f1 0.422 || val_loss 1.1191 acc 0.318 f1 0.286\n",
            "[W4] RNN Epoch 05 | train_loss 1.0611 acc 0.432 f1 0.405 || val_loss 1.1299 acc 0.303 f1 0.280\n",
            "[W4] RNN Epoch 06 | train_loss 1.0471 acc 0.435 f1 0.409 || val_loss 1.1451 acc 0.303 f1 0.279\n",
            "[W4] RNN Epoch 07 | train_loss 1.0333 acc 0.445 f1 0.422 || val_loss 1.1640 acc 0.321 f1 0.296\n",
            "[W4] RNN Epoch 08 | train_loss 1.0173 acc 0.464 f1 0.436 || val_loss 1.1635 acc 0.331 f1 0.307\n",
            "[W4] RNN Epoch 09 | train_loss 1.0060 acc 0.455 f1 0.435 || val_loss 1.1757 acc 0.328 f1 0.304\n",
            "[W4] RNN Epoch 10 | train_loss 0.9906 acc 0.484 f1 0.462 || val_loss 1.1913 acc 0.341 f1 0.315\n",
            "[W4] RNN Epoch 11 | train_loss 0.9767 acc 0.483 f1 0.462 || val_loss 1.2063 acc 0.354 f1 0.329\n",
            "[W4] RNN Epoch 12 | train_loss 0.9583 acc 0.496 f1 0.477 || val_loss 1.2332 acc 0.346 f1 0.316\n",
            "[W4] RNN Epoch 13 | train_loss 0.9446 acc 0.507 f1 0.485 || val_loss 1.2239 acc 0.362 f1 0.338\n",
            "[W4] RNN Epoch 14 | train_loss 0.9260 acc 0.522 f1 0.504 || val_loss 1.2518 acc 0.341 f1 0.318\n",
            "[W4] RNN Epoch 15 | train_loss 0.9143 acc 0.512 f1 0.491 || val_loss 1.2750 acc 0.346 f1 0.325\n",
            "[W4] RNN Epoch 16 | train_loss 0.9063 acc 0.534 f1 0.517 || val_loss 1.2767 acc 0.351 f1 0.327\n",
            "[W4] RNN Epoch 17 | train_loss 0.8807 acc 0.541 f1 0.525 || val_loss 1.3191 acc 0.359 f1 0.329\n",
            "[W4] RNN Epoch 18 | train_loss 0.8696 acc 0.565 f1 0.551 || val_loss 1.2985 acc 0.351 f1 0.325\n",
            "[W4] RNN Epoch 19 | train_loss 0.8523 acc 0.554 f1 0.542 || val_loss 1.3365 acc 0.338 f1 0.312\n",
            "[W4] RNN Epoch 20 | train_loss 0.8273 acc 0.564 f1 0.550 || val_loss 1.3744 acc 0.382 f1 0.353\n",
            "[W4] RNN Epoch 21 | train_loss 0.8220 acc 0.585 f1 0.576 || val_loss 1.3725 acc 0.354 f1 0.325\n",
            "[W4] RNN Epoch 22 | train_loss 0.7990 acc 0.578 f1 0.570 || val_loss 1.3905 acc 0.372 f1 0.334\n",
            "[W4] RNN Epoch 23 | train_loss 0.7857 acc 0.584 f1 0.575 || val_loss 1.4460 acc 0.367 f1 0.324\n",
            "[W4] RNN Epoch 24 | train_loss 0.7707 acc 0.594 f1 0.588 || val_loss 1.4652 acc 0.377 f1 0.338\n",
            "[W4] RNN Epoch 25 | train_loss 0.7453 acc 0.604 f1 0.598 || val_loss 1.4923 acc 0.359 f1 0.321\n",
            "[W4] RNN Epoch 26 | train_loss 0.7371 acc 0.593 f1 0.594 || val_loss 1.5484 acc 0.356 f1 0.313\n",
            "[W4] RNN Epoch 27 | train_loss 0.7269 acc 0.613 f1 0.614 || val_loss 1.5548 acc 0.359 f1 0.317\n",
            "[W4] RNN Epoch 28 | train_loss 0.7207 acc 0.623 f1 0.623 || val_loss 1.5500 acc 0.367 f1 0.324\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=4 Model=GRU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W4] GRU Epoch 01 | train_loss 1.1024 acc 0.220 f1 0.211 || val_loss 1.0973 acc 0.328 f1 0.298\n",
            "[W4] GRU Epoch 02 | train_loss 1.0960 acc 0.348 f1 0.331 || val_loss 1.0993 acc 0.315 f1 0.291\n",
            "[W4] GRU Epoch 03 | train_loss 1.0907 acc 0.381 f1 0.361 || val_loss 1.0997 acc 0.313 f1 0.294\n",
            "[W4] GRU Epoch 04 | train_loss 1.0913 acc 0.418 f1 0.373 || val_loss 1.1036 acc 0.341 f1 0.305\n",
            "[W4] GRU Epoch 05 | train_loss 1.0831 acc 0.447 f1 0.408 || val_loss 1.1092 acc 0.326 f1 0.298\n",
            "[W4] GRU Epoch 06 | train_loss 1.0779 acc 0.380 f1 0.361 || val_loss 1.1143 acc 0.321 f1 0.298\n",
            "[W4] GRU Epoch 07 | train_loss 1.0660 acc 0.438 f1 0.410 || val_loss 1.1250 acc 0.364 f1 0.331\n",
            "[W4] GRU Epoch 08 | train_loss 1.0552 acc 0.444 f1 0.418 || val_loss 1.1402 acc 0.341 f1 0.314\n",
            "[W4] GRU Epoch 09 | train_loss 1.0413 acc 0.437 f1 0.417 || val_loss 1.1567 acc 0.344 f1 0.312\n",
            "[W4] GRU Epoch 10 | train_loss 1.0238 acc 0.471 f1 0.445 || val_loss 1.1680 acc 0.356 f1 0.327\n",
            "[W4] GRU Epoch 11 | train_loss 1.0145 acc 0.470 f1 0.449 || val_loss 1.1825 acc 0.379 f1 0.341\n",
            "[W4] GRU Epoch 12 | train_loss 0.9870 acc 0.487 f1 0.468 || val_loss 1.1957 acc 0.364 f1 0.326\n",
            "[W4] GRU Epoch 13 | train_loss 0.9658 acc 0.497 f1 0.478 || val_loss 1.2126 acc 0.362 f1 0.321\n",
            "[W4] GRU Epoch 14 | train_loss 0.9433 acc 0.514 f1 0.497 || val_loss 1.2261 acc 0.379 f1 0.338\n",
            "[W4] GRU Epoch 15 | train_loss 0.8980 acc 0.537 f1 0.526 || val_loss 1.2603 acc 0.379 f1 0.331\n",
            "[W4] GRU Epoch 16 | train_loss 0.8656 acc 0.554 f1 0.545 || val_loss 1.2913 acc 0.397 f1 0.358\n",
            "[W4] GRU Epoch 17 | train_loss 0.8313 acc 0.577 f1 0.573 || val_loss 1.3096 acc 0.382 f1 0.348\n",
            "[W4] GRU Epoch 18 | train_loss 0.7974 acc 0.573 f1 0.567 || val_loss 1.3865 acc 0.400 f1 0.343\n",
            "[W4] GRU Epoch 19 | train_loss 0.7729 acc 0.584 f1 0.584 || val_loss 1.4464 acc 0.395 f1 0.339\n",
            "[W4] GRU Epoch 20 | train_loss 0.7074 acc 0.624 f1 0.630 || val_loss 1.4857 acc 0.379 f1 0.327\n",
            "[W4] GRU Epoch 21 | train_loss 0.6764 acc 0.635 f1 0.646 || val_loss 1.6117 acc 0.408 f1 0.345\n",
            "[W4] GRU Epoch 22 | train_loss 0.6460 acc 0.637 f1 0.653 || val_loss 1.6660 acc 0.403 f1 0.341\n",
            "[W4] GRU Epoch 23 | train_loss 0.6047 acc 0.654 f1 0.677 || val_loss 1.7796 acc 0.382 f1 0.317\n",
            "[W4] GRU Epoch 24 | train_loss 0.5816 acc 0.663 f1 0.693 || val_loss 1.8861 acc 0.395 f1 0.335\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=4 Model=LSTM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W4] LSTM Epoch 01 | train_loss 1.1001 acc 0.437 f1 0.216 || val_loss 1.0998 acc 0.431 f1 0.201\n",
            "[W4] LSTM Epoch 02 | train_loss 1.0969 acc 0.437 f1 0.290 || val_loss 1.1011 acc 0.446 f1 0.288\n",
            "[W4] LSTM Epoch 03 | train_loss 1.0941 acc 0.469 f1 0.384 || val_loss 1.1018 acc 0.397 f1 0.318\n",
            "[W4] LSTM Epoch 04 | train_loss 1.0907 acc 0.439 f1 0.384 || val_loss 1.1027 acc 0.367 f1 0.327\n",
            "[W4] LSTM Epoch 05 | train_loss 1.0864 acc 0.429 f1 0.392 || val_loss 1.1093 acc 0.326 f1 0.284\n",
            "[W4] LSTM Epoch 06 | train_loss 1.0801 acc 0.380 f1 0.355 || val_loss 1.1116 acc 0.313 f1 0.290\n",
            "[W4] LSTM Epoch 07 | train_loss 1.0693 acc 0.430 f1 0.395 || val_loss 1.1241 acc 0.359 f1 0.334\n",
            "[W4] LSTM Epoch 08 | train_loss 1.0527 acc 0.415 f1 0.400 || val_loss 1.1410 acc 0.377 f1 0.352\n",
            "[W4] LSTM Epoch 09 | train_loss 1.0376 acc 0.427 f1 0.409 || val_loss 1.1635 acc 0.385 f1 0.350\n",
            "[W4] LSTM Epoch 10 | train_loss 1.0203 acc 0.479 f1 0.451 || val_loss 1.1591 acc 0.359 f1 0.344\n",
            "[W4] LSTM Epoch 11 | train_loss 0.9929 acc 0.446 f1 0.431 || val_loss 1.2016 acc 0.395 f1 0.355\n",
            "[W4] LSTM Epoch 12 | train_loss 0.9724 acc 0.498 f1 0.477 || val_loss 1.2148 acc 0.362 f1 0.335\n",
            "[W4] LSTM Epoch 13 | train_loss 0.9396 acc 0.495 f1 0.481 || val_loss 1.2390 acc 0.354 f1 0.331\n",
            "[W4] LSTM Epoch 14 | train_loss 0.9068 acc 0.527 f1 0.514 || val_loss 1.2890 acc 0.369 f1 0.342\n",
            "[W4] LSTM Epoch 15 | train_loss 0.8598 acc 0.552 f1 0.543 || val_loss 1.3116 acc 0.318 f1 0.300\n",
            "[W4] LSTM Epoch 16 | train_loss 0.8200 acc 0.561 f1 0.553 || val_loss 1.3883 acc 0.369 f1 0.336\n",
            "[W4] LSTM Epoch 17 | train_loss 0.7719 acc 0.571 f1 0.571 || val_loss 1.5269 acc 0.362 f1 0.317\n",
            "[W4] LSTM Epoch 18 | train_loss 0.7304 acc 0.606 f1 0.608 || val_loss 1.6411 acc 0.359 f1 0.315\n",
            "[W4] LSTM Epoch 19 | train_loss 0.6776 acc 0.636 f1 0.641 || val_loss 1.7382 acc 0.377 f1 0.328\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=5 Model=ANN\n",
            "[W5] ANN Epoch 01 | train_loss 1.2519 acc 0.323 f1 0.301 || val_loss 1.1613 acc 0.379 f1 0.314\n",
            "[W5] ANN Epoch 02 | train_loss 1.1024 acc 0.381 f1 0.364 || val_loss 1.1793 acc 0.341 f1 0.291\n",
            "[W5] ANN Epoch 03 | train_loss 1.0282 acc 0.433 f1 0.416 || val_loss 1.1864 acc 0.352 f1 0.309\n",
            "[W5] ANN Epoch 04 | train_loss 0.9423 acc 0.478 f1 0.464 || val_loss 1.2347 acc 0.379 f1 0.328\n",
            "[W5] ANN Epoch 05 | train_loss 0.8698 acc 0.538 f1 0.525 || val_loss 1.2726 acc 0.355 f1 0.297\n",
            "[W5] ANN Epoch 06 | train_loss 0.8523 acc 0.545 f1 0.529 || val_loss 1.3241 acc 0.384 f1 0.321\n",
            "[W5] ANN Epoch 07 | train_loss 0.7821 acc 0.588 f1 0.576 || val_loss 1.3846 acc 0.379 f1 0.308\n",
            "[W5] ANN Epoch 08 | train_loss 0.7363 acc 0.601 f1 0.593 || val_loss 1.4145 acc 0.360 f1 0.303\n",
            "[W5] ANN Epoch 09 | train_loss 0.7043 acc 0.628 f1 0.617 || val_loss 1.4547 acc 0.371 f1 0.319\n",
            "[W5] ANN Epoch 10 | train_loss 0.6484 acc 0.649 f1 0.644 || val_loss 1.5324 acc 0.376 f1 0.309\n",
            "[W5] ANN Epoch 11 | train_loss 0.6280 acc 0.670 f1 0.666 || val_loss 1.6044 acc 0.363 f1 0.290\n",
            "[W5] ANN Epoch 12 | train_loss 0.5383 acc 0.727 f1 0.727 || val_loss 1.6674 acc 0.381 f1 0.296\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=5 Model=CNN1D\n",
            "[W5] CNN1D Epoch 01 | train_loss 1.1092 acc 0.377 f1 0.310 || val_loss 1.1024 acc 0.405 f1 0.351\n",
            "[W5] CNN1D Epoch 02 | train_loss 1.0778 acc 0.455 f1 0.423 || val_loss 1.1075 acc 0.413 f1 0.355\n",
            "[W5] CNN1D Epoch 03 | train_loss 1.0619 acc 0.465 f1 0.429 || val_loss 1.1187 acc 0.392 f1 0.325\n",
            "[W5] CNN1D Epoch 04 | train_loss 1.0374 acc 0.501 f1 0.461 || val_loss 1.1284 acc 0.336 f1 0.301\n",
            "[W5] CNN1D Epoch 05 | train_loss 0.9852 acc 0.519 f1 0.496 || val_loss 1.1527 acc 0.349 f1 0.322\n",
            "[W5] CNN1D Epoch 06 | train_loss 0.9336 acc 0.542 f1 0.517 || val_loss 1.2017 acc 0.368 f1 0.319\n",
            "[W5] CNN1D Epoch 07 | train_loss 0.8827 acc 0.572 f1 0.551 || val_loss 1.2608 acc 0.363 f1 0.317\n",
            "[W5] CNN1D Epoch 08 | train_loss 0.8236 acc 0.607 f1 0.591 || val_loss 1.3241 acc 0.384 f1 0.326\n",
            "[W5] CNN1D Epoch 09 | train_loss 0.7665 acc 0.617 f1 0.604 || val_loss 1.3258 acc 0.315 f1 0.302\n",
            "[W5] CNN1D Epoch 10 | train_loss 0.7119 acc 0.641 f1 0.627 || val_loss 1.4866 acc 0.360 f1 0.320\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=5 Model=RNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W5] RNN Epoch 01 | train_loss 1.1014 acc 0.443 f1 0.321 || val_loss 1.1014 acc 0.411 f1 0.290\n",
            "[W5] RNN Epoch 02 | train_loss 1.0900 acc 0.459 f1 0.351 || val_loss 1.1026 acc 0.405 f1 0.327\n",
            "[W5] RNN Epoch 03 | train_loss 1.0835 acc 0.477 f1 0.408 || val_loss 1.1066 acc 0.400 f1 0.335\n",
            "[W5] RNN Epoch 04 | train_loss 1.0752 acc 0.450 f1 0.412 || val_loss 1.1106 acc 0.355 f1 0.326\n",
            "[W5] RNN Epoch 05 | train_loss 1.0639 acc 0.457 f1 0.422 || val_loss 1.1276 acc 0.325 f1 0.295\n",
            "[W5] RNN Epoch 06 | train_loss 1.0474 acc 0.461 f1 0.437 || val_loss 1.1560 acc 0.339 f1 0.308\n",
            "[W5] RNN Epoch 07 | train_loss 1.0304 acc 0.446 f1 0.425 || val_loss 1.1847 acc 0.312 f1 0.288\n",
            "[W5] RNN Epoch 08 | train_loss 1.0232 acc 0.441 f1 0.419 || val_loss 1.1985 acc 0.325 f1 0.303\n",
            "[W5] RNN Epoch 09 | train_loss 1.0042 acc 0.475 f1 0.454 || val_loss 1.2060 acc 0.285 f1 0.269\n",
            "[W5] RNN Epoch 10 | train_loss 0.9911 acc 0.473 f1 0.455 || val_loss 1.2481 acc 0.277 f1 0.261\n",
            "[W5] RNN Epoch 11 | train_loss 0.9700 acc 0.479 f1 0.462 || val_loss 1.2716 acc 0.304 f1 0.283\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=5 Model=GRU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W5] GRU Epoch 01 | train_loss 1.1086 acc 0.434 f1 0.202 || val_loss 1.1052 acc 0.437 f1 0.203\n",
            "[W5] GRU Epoch 02 | train_loss 1.0966 acc 0.434 f1 0.264 || val_loss 1.1015 acc 0.379 f1 0.277\n",
            "[W5] GRU Epoch 03 | train_loss 1.0929 acc 0.411 f1 0.359 || val_loss 1.1021 acc 0.387 f1 0.330\n",
            "[W5] GRU Epoch 04 | train_loss 1.0896 acc 0.426 f1 0.382 || val_loss 1.1031 acc 0.355 f1 0.316\n",
            "[W5] GRU Epoch 05 | train_loss 1.0817 acc 0.405 f1 0.369 || val_loss 1.1047 acc 0.331 f1 0.300\n",
            "[W5] GRU Epoch 06 | train_loss 1.0772 acc 0.411 f1 0.388 || val_loss 1.1091 acc 0.317 f1 0.290\n",
            "[W5] GRU Epoch 07 | train_loss 1.0681 acc 0.443 f1 0.411 || val_loss 1.1174 acc 0.320 f1 0.291\n",
            "[W5] GRU Epoch 08 | train_loss 1.0615 acc 0.443 f1 0.416 || val_loss 1.1228 acc 0.317 f1 0.282\n",
            "[W5] GRU Epoch 09 | train_loss 1.0475 acc 0.471 f1 0.438 || val_loss 1.1325 acc 0.320 f1 0.292\n",
            "[W5] GRU Epoch 10 | train_loss 1.0364 acc 0.440 f1 0.420 || val_loss 1.1460 acc 0.320 f1 0.287\n",
            "[W5] GRU Epoch 11 | train_loss 1.0172 acc 0.496 f1 0.464 || val_loss 1.1498 acc 0.315 f1 0.285\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=5 Model=LSTM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W5] LSTM Epoch 01 | train_loss 1.1021 acc 0.144 f1 0.114 || val_loss 1.0990 acc 0.144 f1 0.105\n",
            "[W5] LSTM Epoch 02 | train_loss 1.0980 acc 0.166 f1 0.141 || val_loss 1.0991 acc 0.197 f1 0.171\n",
            "[W5] LSTM Epoch 03 | train_loss 1.0956 acc 0.273 f1 0.240 || val_loss 1.1002 acc 0.291 f1 0.256\n",
            "[W5] LSTM Epoch 04 | train_loss 1.0917 acc 0.350 f1 0.333 || val_loss 1.1009 acc 0.333 f1 0.314\n",
            "[W5] LSTM Epoch 05 | train_loss 1.0888 acc 0.446 f1 0.394 || val_loss 1.1046 acc 0.331 f1 0.274\n",
            "[W5] LSTM Epoch 06 | train_loss 1.0819 acc 0.429 f1 0.385 || val_loss 1.1074 acc 0.328 f1 0.296\n",
            "[W5] LSTM Epoch 07 | train_loss 1.0710 acc 0.439 f1 0.410 || val_loss 1.1145 acc 0.349 f1 0.314\n",
            "[W5] LSTM Epoch 08 | train_loss 1.0602 acc 0.435 f1 0.412 || val_loss 1.1308 acc 0.333 f1 0.315\n",
            "[W5] LSTM Epoch 09 | train_loss 1.0453 acc 0.440 f1 0.418 || val_loss 1.1404 acc 0.355 f1 0.324\n",
            "[W5] LSTM Epoch 10 | train_loss 1.0264 acc 0.458 f1 0.436 || val_loss 1.1501 acc 0.357 f1 0.326\n",
            "[W5] LSTM Epoch 11 | train_loss 0.9962 acc 0.490 f1 0.466 || val_loss 1.1632 acc 0.336 f1 0.320\n",
            "[W5] LSTM Epoch 12 | train_loss 0.9677 acc 0.475 f1 0.458 || val_loss 1.2017 acc 0.368 f1 0.323\n",
            "[W5] LSTM Epoch 13 | train_loss 0.9370 acc 0.506 f1 0.488 || val_loss 1.2357 acc 0.365 f1 0.321\n",
            "[W5] LSTM Epoch 14 | train_loss 0.9032 acc 0.527 f1 0.512 || val_loss 1.2341 acc 0.357 f1 0.320\n",
            "[W5] LSTM Epoch 15 | train_loss 0.8541 acc 0.545 f1 0.537 || val_loss 1.2875 acc 0.365 f1 0.328\n",
            "[W5] LSTM Epoch 16 | train_loss 0.8225 acc 0.570 f1 0.560 || val_loss 1.2983 acc 0.363 f1 0.333\n",
            "[W5] LSTM Epoch 17 | train_loss 0.7753 acc 0.583 f1 0.583 || val_loss 1.4031 acc 0.413 f1 0.360\n",
            "[W5] LSTM Epoch 18 | train_loss 0.7299 acc 0.609 f1 0.612 || val_loss 1.5550 acc 0.419 f1 0.354\n",
            "[W5] LSTM Epoch 19 | train_loss 0.6828 acc 0.631 f1 0.639 || val_loss 1.6737 acc 0.413 f1 0.340\n",
            "[W5] LSTM Epoch 20 | train_loss 0.6373 acc 0.658 f1 0.674 || val_loss 1.6561 acc 0.376 f1 0.333\n",
            "[W5] LSTM Epoch 21 | train_loss 0.5913 acc 0.677 f1 0.697 || val_loss 1.9047 acc 0.368 f1 0.310\n",
            "[W5] LSTM Epoch 22 | train_loss 0.5590 acc 0.692 f1 0.713 || val_loss 2.0888 acc 0.397 f1 0.334\n",
            "[W5] LSTM Epoch 23 | train_loss 0.5237 acc 0.698 f1 0.725 || val_loss 2.1095 acc 0.403 f1 0.343\n",
            "[W5] LSTM Epoch 24 | train_loss 0.5017 acc 0.709 f1 0.741 || val_loss 2.2821 acc 0.397 f1 0.340\n",
            "[W5] LSTM Epoch 25 | train_loss 0.4680 acc 0.728 f1 0.765 || val_loss 2.4239 acc 0.387 f1 0.313\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=6 Model=ANN\n",
            "[W6] ANN Epoch 01 | train_loss 1.2566 acc 0.318 f1 0.294 || val_loss 1.1526 acc 0.336 f1 0.274\n",
            "[W6] ANN Epoch 02 | train_loss 1.0710 acc 0.395 f1 0.382 || val_loss 1.1595 acc 0.319 f1 0.283\n",
            "[W6] ANN Epoch 03 | train_loss 1.0308 acc 0.427 f1 0.408 || val_loss 1.1776 acc 0.328 f1 0.292\n",
            "[W6] ANN Epoch 04 | train_loss 0.9615 acc 0.471 f1 0.455 || val_loss 1.2192 acc 0.325 f1 0.291\n",
            "[W6] ANN Epoch 05 | train_loss 0.8894 acc 0.499 f1 0.488 || val_loss 1.2450 acc 0.322 f1 0.296\n",
            "[W6] ANN Epoch 06 | train_loss 0.8590 acc 0.525 f1 0.515 || val_loss 1.2703 acc 0.325 f1 0.292\n",
            "[W6] ANN Epoch 07 | train_loss 0.7674 acc 0.574 f1 0.566 || val_loss 1.3076 acc 0.364 f1 0.316\n",
            "[W6] ANN Epoch 08 | train_loss 0.7240 acc 0.604 f1 0.596 || val_loss 1.3954 acc 0.381 f1 0.317\n",
            "[W6] ANN Epoch 09 | train_loss 0.6478 acc 0.654 f1 0.647 || val_loss 1.4618 acc 0.361 f1 0.302\n",
            "[W6] ANN Epoch 10 | train_loss 0.6043 acc 0.673 f1 0.666 || val_loss 1.4930 acc 0.369 f1 0.310\n",
            "[W6] ANN Epoch 11 | train_loss 0.5402 acc 0.720 f1 0.717 || val_loss 1.5802 acc 0.364 f1 0.293\n",
            "[W6] ANN Epoch 12 | train_loss 0.5308 acc 0.731 f1 0.729 || val_loss 1.6609 acc 0.397 f1 0.309\n",
            "[W6] ANN Epoch 13 | train_loss 0.4759 acc 0.770 f1 0.771 || val_loss 1.7461 acc 0.392 f1 0.308\n",
            "[W6] ANN Epoch 14 | train_loss 0.4520 acc 0.772 f1 0.774 || val_loss 1.7980 acc 0.392 f1 0.291\n",
            "[W6] ANN Epoch 15 | train_loss 0.4301 acc 0.789 f1 0.786 || val_loss 1.8259 acc 0.403 f1 0.298\n",
            "[W6] ANN Epoch 16 | train_loss 0.3735 acc 0.828 f1 0.834 || val_loss 1.8401 acc 0.400 f1 0.330\n",
            "[W6] ANN Epoch 17 | train_loss 0.3767 acc 0.831 f1 0.835 || val_loss 1.9398 acc 0.425 f1 0.334\n",
            "[W6] ANN Epoch 18 | train_loss 0.3096 acc 0.867 f1 0.869 || val_loss 1.9941 acc 0.397 f1 0.306\n",
            "[W6] ANN Epoch 19 | train_loss 0.2901 acc 0.858 f1 0.864 || val_loss 2.0187 acc 0.425 f1 0.334\n",
            "[W6] ANN Epoch 20 | train_loss 0.2878 acc 0.870 f1 0.861 || val_loss 2.0639 acc 0.394 f1 0.327\n",
            "[W6] ANN Epoch 21 | train_loss 0.2938 acc 0.865 f1 0.862 || val_loss 2.2234 acc 0.392 f1 0.319\n",
            "[W6] ANN Epoch 22 | train_loss 0.2687 acc 0.887 f1 0.882 || val_loss 2.2685 acc 0.411 f1 0.317\n",
            "[W6] ANN Epoch 23 | train_loss 0.2846 acc 0.872 f1 0.870 || val_loss 2.2323 acc 0.428 f1 0.331\n",
            "[W6] ANN Epoch 24 | train_loss 0.2693 acc 0.879 f1 0.877 || val_loss 2.2668 acc 0.408 f1 0.323\n",
            "[W6] ANN Epoch 25 | train_loss 0.2057 acc 0.912 f1 0.912 || val_loss 2.2893 acc 0.419 f1 0.324\n",
            "[W6] ANN Epoch 26 | train_loss 0.2270 acc 0.901 f1 0.898 || val_loss 2.3607 acc 0.431 f1 0.340\n",
            "[W6] ANN Epoch 27 | train_loss 0.1998 acc 0.923 f1 0.917 || val_loss 2.5270 acc 0.436 f1 0.328\n",
            "[W6] ANN Epoch 28 | train_loss 0.1887 acc 0.920 f1 0.918 || val_loss 2.4748 acc 0.408 f1 0.309\n",
            "[W6] ANN Epoch 29 | train_loss 0.2128 acc 0.916 f1 0.914 || val_loss 2.5083 acc 0.411 f1 0.340\n",
            "[W6] ANN Epoch 30 | train_loss 0.1863 acc 0.926 f1 0.922 || val_loss 2.5656 acc 0.425 f1 0.326\n",
            "[W6] ANN Epoch 31 | train_loss 0.1812 acc 0.925 f1 0.923 || val_loss 2.5853 acc 0.392 f1 0.310\n",
            "[W6] ANN Epoch 32 | train_loss 0.1595 acc 0.929 f1 0.927 || val_loss 2.6505 acc 0.403 f1 0.304\n",
            "[W6] ANN Epoch 33 | train_loss 0.1716 acc 0.926 f1 0.930 || val_loss 2.7107 acc 0.403 f1 0.313\n",
            "[W6] ANN Epoch 34 | train_loss 0.1648 acc 0.929 f1 0.925 || val_loss 2.7220 acc 0.392 f1 0.286\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=6 Model=CNN1D\n",
            "[W6] CNN1D Epoch 01 | train_loss 1.1097 acc 0.279 f1 0.272 || val_loss 1.1009 acc 0.397 f1 0.315\n",
            "[W6] CNN1D Epoch 02 | train_loss 1.0829 acc 0.457 f1 0.392 || val_loss 1.1034 acc 0.356 f1 0.325\n",
            "[W6] CNN1D Epoch 03 | train_loss 1.0609 acc 0.444 f1 0.424 || val_loss 1.1081 acc 0.350 f1 0.330\n",
            "[W6] CNN1D Epoch 04 | train_loss 1.0378 acc 0.463 f1 0.441 || val_loss 1.1442 acc 0.425 f1 0.325\n",
            "[W6] CNN1D Epoch 05 | train_loss 0.9972 acc 0.483 f1 0.459 || val_loss 1.1364 acc 0.350 f1 0.313\n",
            "[W6] CNN1D Epoch 06 | train_loss 0.9647 acc 0.504 f1 0.485 || val_loss 1.1422 acc 0.336 f1 0.314\n",
            "[W6] CNN1D Epoch 07 | train_loss 0.9087 acc 0.526 f1 0.507 || val_loss 1.2569 acc 0.417 f1 0.348\n",
            "[W6] CNN1D Epoch 08 | train_loss 0.8574 acc 0.554 f1 0.540 || val_loss 1.2465 acc 0.378 f1 0.341\n",
            "[W6] CNN1D Epoch 09 | train_loss 0.7952 acc 0.592 f1 0.584 || val_loss 1.2875 acc 0.383 f1 0.336\n",
            "[W6] CNN1D Epoch 10 | train_loss 0.7658 acc 0.602 f1 0.591 || val_loss 1.3943 acc 0.386 f1 0.342\n",
            "[W6] CNN1D Epoch 11 | train_loss 0.7104 acc 0.629 f1 0.627 || val_loss 1.3984 acc 0.356 f1 0.321\n",
            "[W6] CNN1D Epoch 12 | train_loss 0.6802 acc 0.642 f1 0.641 || val_loss 1.4138 acc 0.361 f1 0.328\n",
            "[W6] CNN1D Epoch 13 | train_loss 0.6722 acc 0.658 f1 0.654 || val_loss 1.6108 acc 0.386 f1 0.342\n",
            "[W6] CNN1D Epoch 14 | train_loss 0.6260 acc 0.676 f1 0.675 || val_loss 1.5436 acc 0.389 f1 0.340\n",
            "[W6] CNN1D Epoch 15 | train_loss 0.5983 acc 0.695 f1 0.697 || val_loss 1.9970 acc 0.394 f1 0.314\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=6 Model=RNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W6] RNN Epoch 01 | train_loss 1.1021 acc 0.306 f1 0.263 || val_loss 1.1047 acc 0.336 f1 0.295\n",
            "[W6] RNN Epoch 02 | train_loss 1.0922 acc 0.399 f1 0.355 || val_loss 1.1092 acc 0.342 f1 0.296\n",
            "[W6] RNN Epoch 03 | train_loss 1.0853 acc 0.364 f1 0.347 || val_loss 1.1109 acc 0.325 f1 0.303\n",
            "[W6] RNN Epoch 04 | train_loss 1.0742 acc 0.404 f1 0.378 || val_loss 1.1202 acc 0.347 f1 0.317\n",
            "[W6] RNN Epoch 05 | train_loss 1.0619 acc 0.411 f1 0.393 || val_loss 1.1351 acc 0.331 f1 0.306\n",
            "[W6] RNN Epoch 06 | train_loss 1.0486 acc 0.457 f1 0.433 || val_loss 1.1442 acc 0.347 f1 0.320\n",
            "[W6] RNN Epoch 07 | train_loss 1.0372 acc 0.442 f1 0.419 || val_loss 1.1657 acc 0.342 f1 0.316\n",
            "[W6] RNN Epoch 08 | train_loss 1.0218 acc 0.447 f1 0.428 || val_loss 1.1871 acc 0.331 f1 0.306\n",
            "[W6] RNN Epoch 09 | train_loss 1.0047 acc 0.444 f1 0.430 || val_loss 1.2180 acc 0.358 f1 0.327\n",
            "[W6] RNN Epoch 10 | train_loss 0.9917 acc 0.496 f1 0.475 || val_loss 1.2317 acc 0.353 f1 0.321\n",
            "[W6] RNN Epoch 11 | train_loss 0.9753 acc 0.495 f1 0.476 || val_loss 1.2433 acc 0.342 f1 0.316\n",
            "[W6] RNN Epoch 12 | train_loss 0.9692 acc 0.478 f1 0.463 || val_loss 1.2782 acc 0.339 f1 0.312\n",
            "[W6] RNN Epoch 13 | train_loss 0.9464 acc 0.482 f1 0.467 || val_loss 1.2789 acc 0.347 f1 0.318\n",
            "[W6] RNN Epoch 14 | train_loss 0.9232 acc 0.496 f1 0.484 || val_loss 1.3105 acc 0.342 f1 0.314\n",
            "[W6] RNN Epoch 15 | train_loss 0.9103 acc 0.510 f1 0.496 || val_loss 1.3484 acc 0.353 f1 0.323\n",
            "[W6] RNN Epoch 16 | train_loss 0.8910 acc 0.518 f1 0.509 || val_loss 1.3929 acc 0.369 f1 0.335\n",
            "[W6] RNN Epoch 17 | train_loss 0.8690 acc 0.537 f1 0.528 || val_loss 1.4168 acc 0.350 f1 0.315\n",
            "[W6] RNN Epoch 18 | train_loss 0.8562 acc 0.536 f1 0.528 || val_loss 1.4597 acc 0.364 f1 0.323\n",
            "[W6] RNN Epoch 19 | train_loss 0.8349 acc 0.544 f1 0.536 || val_loss 1.4852 acc 0.361 f1 0.329\n",
            "[W6] RNN Epoch 20 | train_loss 0.8158 acc 0.564 f1 0.560 || val_loss 1.5368 acc 0.358 f1 0.329\n",
            "[W6] RNN Epoch 21 | train_loss 0.7952 acc 0.568 f1 0.562 || val_loss 1.5860 acc 0.375 f1 0.339\n",
            "[W6] RNN Epoch 22 | train_loss 0.7653 acc 0.565 f1 0.569 || val_loss 1.6375 acc 0.361 f1 0.325\n",
            "[W6] RNN Epoch 23 | train_loss 0.7365 acc 0.602 f1 0.603 || val_loss 1.7077 acc 0.364 f1 0.330\n",
            "[W6] RNN Epoch 24 | train_loss 0.7214 acc 0.604 f1 0.608 || val_loss 1.7546 acc 0.381 f1 0.347\n",
            "[W6] RNN Epoch 25 | train_loss 0.7107 acc 0.606 f1 0.611 || val_loss 1.8103 acc 0.378 f1 0.336\n",
            "[W6] RNN Epoch 26 | train_loss 0.6844 acc 0.607 f1 0.614 || val_loss 1.8914 acc 0.386 f1 0.346\n",
            "[W6] RNN Epoch 27 | train_loss 0.6516 acc 0.640 f1 0.654 || val_loss 1.9484 acc 0.378 f1 0.339\n",
            "[W6] RNN Epoch 28 | train_loss 0.6422 acc 0.648 f1 0.664 || val_loss 2.0489 acc 0.394 f1 0.341\n",
            "[W6] RNN Epoch 29 | train_loss 0.6221 acc 0.654 f1 0.666 || val_loss 2.1046 acc 0.378 f1 0.339\n",
            "[W6] RNN Epoch 30 | train_loss 0.5934 acc 0.651 f1 0.670 || val_loss 2.1744 acc 0.372 f1 0.320\n",
            "[W6] RNN Epoch 31 | train_loss 0.5920 acc 0.675 f1 0.693 || val_loss 2.2000 acc 0.386 f1 0.342\n",
            "[W6] RNN Epoch 32 | train_loss 0.5649 acc 0.668 f1 0.694 || val_loss 2.3077 acc 0.392 f1 0.338\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=6 Model=GRU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W6] GRU Epoch 01 | train_loss 1.1023 acc 0.327 f1 0.235 || val_loss 1.0972 acc 0.344 f1 0.252\n",
            "[W6] GRU Epoch 02 | train_loss 1.0963 acc 0.369 f1 0.312 || val_loss 1.1000 acc 0.361 f1 0.304\n",
            "[W6] GRU Epoch 03 | train_loss 1.0911 acc 0.413 f1 0.367 || val_loss 1.1012 acc 0.386 f1 0.335\n",
            "[W6] GRU Epoch 04 | train_loss 1.0857 acc 0.432 f1 0.391 || val_loss 1.1036 acc 0.394 f1 0.342\n",
            "[W6] GRU Epoch 05 | train_loss 1.0792 acc 0.436 f1 0.399 || val_loss 1.1077 acc 0.353 f1 0.306\n",
            "[W6] GRU Epoch 06 | train_loss 1.0717 acc 0.432 f1 0.406 || val_loss 1.1119 acc 0.336 f1 0.295\n",
            "[W6] GRU Epoch 07 | train_loss 1.0612 acc 0.420 f1 0.402 || val_loss 1.1201 acc 0.336 f1 0.306\n",
            "[W6] GRU Epoch 08 | train_loss 1.0499 acc 0.457 f1 0.431 || val_loss 1.1368 acc 0.314 f1 0.285\n",
            "[W6] GRU Epoch 09 | train_loss 1.0371 acc 0.468 f1 0.442 || val_loss 1.1438 acc 0.317 f1 0.283\n",
            "[W6] GRU Epoch 10 | train_loss 1.0171 acc 0.455 f1 0.438 || val_loss 1.1676 acc 0.353 f1 0.322\n",
            "[W6] GRU Epoch 11 | train_loss 1.0031 acc 0.451 f1 0.436 || val_loss 1.1749 acc 0.347 f1 0.314\n",
            "[W6] GRU Epoch 12 | train_loss 0.9820 acc 0.498 f1 0.477 || val_loss 1.2106 acc 0.336 f1 0.306\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=6 Model=LSTM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W6] LSTM Epoch 01 | train_loss 1.1024 acc 0.449 f1 0.207 || val_loss 1.1076 acc 0.422 f1 0.198\n",
            "[W6] LSTM Epoch 02 | train_loss 1.0987 acc 0.453 f1 0.221 || val_loss 1.1050 acc 0.444 f1 0.247\n",
            "[W6] LSTM Epoch 03 | train_loss 1.0967 acc 0.465 f1 0.288 || val_loss 1.1028 acc 0.422 f1 0.306\n",
            "[W6] LSTM Epoch 04 | train_loss 1.0932 acc 0.446 f1 0.344 || val_loss 1.1028 acc 0.397 f1 0.324\n",
            "[W6] LSTM Epoch 05 | train_loss 1.0876 acc 0.426 f1 0.380 || val_loss 1.1035 acc 0.328 f1 0.295\n",
            "[W6] LSTM Epoch 06 | train_loss 1.0827 acc 0.427 f1 0.390 || val_loss 1.1054 acc 0.369 f1 0.329\n",
            "[W6] LSTM Epoch 07 | train_loss 1.0707 acc 0.414 f1 0.381 || val_loss 1.1113 acc 0.339 f1 0.319\n",
            "[W6] LSTM Epoch 08 | train_loss 1.0557 acc 0.415 f1 0.394 || val_loss 1.1264 acc 0.328 f1 0.302\n",
            "[W6] LSTM Epoch 09 | train_loss 1.0395 acc 0.485 f1 0.450 || val_loss 1.1424 acc 0.336 f1 0.314\n",
            "[W6] LSTM Epoch 10 | train_loss 1.0143 acc 0.479 f1 0.454 || val_loss 1.1546 acc 0.364 f1 0.311\n",
            "[W6] LSTM Epoch 11 | train_loss 0.9970 acc 0.495 f1 0.466 || val_loss 1.1734 acc 0.333 f1 0.307\n",
            "[W6] LSTM Epoch 12 | train_loss 0.9607 acc 0.500 f1 0.482 || val_loss 1.1942 acc 0.369 f1 0.332\n",
            "[W6] LSTM Epoch 13 | train_loss 0.9287 acc 0.519 f1 0.499 || val_loss 1.2155 acc 0.386 f1 0.351\n",
            "[W6] LSTM Epoch 14 | train_loss 0.8853 acc 0.536 f1 0.522 || val_loss 1.2422 acc 0.381 f1 0.353\n",
            "[W6] LSTM Epoch 15 | train_loss 0.8413 acc 0.558 f1 0.547 || val_loss 1.2907 acc 0.364 f1 0.333\n",
            "[W6] LSTM Epoch 16 | train_loss 0.7939 acc 0.582 f1 0.576 || val_loss 1.3504 acc 0.375 f1 0.342\n",
            "[W6] LSTM Epoch 17 | train_loss 0.7438 acc 0.593 f1 0.596 || val_loss 1.4632 acc 0.394 f1 0.345\n",
            "[W6] LSTM Epoch 18 | train_loss 0.7094 acc 0.602 f1 0.606 || val_loss 1.4828 acc 0.408 f1 0.363\n",
            "[W6] LSTM Epoch 19 | train_loss 0.6809 acc 0.614 f1 0.622 || val_loss 1.6644 acc 0.386 f1 0.330\n",
            "[W6] LSTM Epoch 20 | train_loss 0.6263 acc 0.642 f1 0.659 || val_loss 1.7176 acc 0.381 f1 0.331\n",
            "[W6] LSTM Epoch 21 | train_loss 0.6012 acc 0.657 f1 0.677 || val_loss 1.7795 acc 0.367 f1 0.327\n",
            "[W6] LSTM Epoch 22 | train_loss 0.5781 acc 0.669 f1 0.688 || val_loss 1.8586 acc 0.389 f1 0.344\n",
            "[W6] LSTM Epoch 23 | train_loss 0.5385 acc 0.685 f1 0.712 || val_loss 1.9004 acc 0.389 f1 0.347\n",
            "[W6] LSTM Epoch 24 | train_loss 0.5143 acc 0.705 f1 0.737 || val_loss 2.0844 acc 0.394 f1 0.345\n",
            "[W6] LSTM Epoch 25 | train_loss 0.4911 acc 0.715 f1 0.748 || val_loss 2.1432 acc 0.406 f1 0.357\n",
            "[W6] LSTM Epoch 26 | train_loss 0.4715 acc 0.718 f1 0.758 || val_loss 2.2199 acc 0.394 f1 0.345\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=7 Model=ANN\n",
            "[W7] ANN Epoch 01 | train_loss 1.2216 acc 0.313 f1 0.300 || val_loss 1.1132 acc 0.258 f1 0.257\n",
            "[W7] ANN Epoch 02 | train_loss 1.0661 acc 0.396 f1 0.380 || val_loss 1.1359 acc 0.258 f1 0.256\n",
            "[W7] ANN Epoch 03 | train_loss 1.0183 acc 0.420 f1 0.405 || val_loss 1.1661 acc 0.299 f1 0.287\n",
            "[W7] ANN Epoch 04 | train_loss 0.9535 acc 0.480 f1 0.460 || val_loss 1.1977 acc 0.316 f1 0.302\n",
            "[W7] ANN Epoch 05 | train_loss 0.8526 acc 0.549 f1 0.535 || val_loss 1.2572 acc 0.307 f1 0.294\n",
            "[W7] ANN Epoch 06 | train_loss 0.8125 acc 0.560 f1 0.544 || val_loss 1.2972 acc 0.322 f1 0.301\n",
            "[W7] ANN Epoch 07 | train_loss 0.7410 acc 0.617 f1 0.601 || val_loss 1.3334 acc 0.325 f1 0.301\n",
            "[W7] ANN Epoch 08 | train_loss 0.6746 acc 0.650 f1 0.635 || val_loss 1.4400 acc 0.365 f1 0.314\n",
            "[W7] ANN Epoch 09 | train_loss 0.6321 acc 0.681 f1 0.675 || val_loss 1.4718 acc 0.357 f1 0.314\n",
            "[W7] ANN Epoch 10 | train_loss 0.5584 acc 0.708 f1 0.703 || val_loss 1.5157 acc 0.359 f1 0.318\n",
            "[W7] ANN Epoch 11 | train_loss 0.5210 acc 0.729 f1 0.724 || val_loss 1.6209 acc 0.386 f1 0.343\n",
            "[W7] ANN Epoch 12 | train_loss 0.5050 acc 0.750 f1 0.744 || val_loss 1.6816 acc 0.365 f1 0.316\n",
            "[W7] ANN Epoch 13 | train_loss 0.4468 acc 0.778 f1 0.778 || val_loss 1.8144 acc 0.383 f1 0.311\n",
            "[W7] ANN Epoch 14 | train_loss 0.3938 acc 0.827 f1 0.818 || val_loss 1.8408 acc 0.386 f1 0.328\n",
            "[W7] ANN Epoch 15 | train_loss 0.4338 acc 0.803 f1 0.802 || val_loss 1.9164 acc 0.377 f1 0.308\n",
            "[W7] ANN Epoch 16 | train_loss 0.3916 acc 0.827 f1 0.819 || val_loss 1.9557 acc 0.406 f1 0.329\n",
            "[W7] ANN Epoch 17 | train_loss 0.3356 acc 0.847 f1 0.835 || val_loss 1.9551 acc 0.362 f1 0.297\n",
            "[W7] ANN Epoch 18 | train_loss 0.3395 acc 0.840 f1 0.835 || val_loss 1.9504 acc 0.414 f1 0.351\n",
            "[W7] ANN Epoch 19 | train_loss 0.3469 acc 0.841 f1 0.836 || val_loss 2.1649 acc 0.374 f1 0.306\n",
            "[W7] ANN Epoch 20 | train_loss 0.2908 acc 0.871 f1 0.867 || val_loss 2.1244 acc 0.412 f1 0.341\n",
            "[W7] ANN Epoch 21 | train_loss 0.2579 acc 0.883 f1 0.882 || val_loss 2.1969 acc 0.406 f1 0.337\n",
            "[W7] ANN Epoch 22 | train_loss 0.2520 acc 0.888 f1 0.890 || val_loss 2.3118 acc 0.426 f1 0.344\n",
            "[W7] ANN Epoch 23 | train_loss 0.2681 acc 0.889 f1 0.888 || val_loss 2.4476 acc 0.412 f1 0.328\n",
            "[W7] ANN Epoch 24 | train_loss 0.2574 acc 0.886 f1 0.883 || val_loss 2.3991 acc 0.380 f1 0.297\n",
            "[W7] ANN Epoch 25 | train_loss 0.2267 acc 0.904 f1 0.909 || val_loss 2.3936 acc 0.388 f1 0.291\n",
            "[W7] ANN Epoch 26 | train_loss 0.2255 acc 0.904 f1 0.900 || val_loss 2.4808 acc 0.406 f1 0.316\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=7 Model=CNN1D\n",
            "[W7] CNN1D Epoch 01 | train_loss 1.1087 acc 0.348 f1 0.308 || val_loss 1.1018 acc 0.438 f1 0.257\n",
            "[W7] CNN1D Epoch 02 | train_loss 1.0899 acc 0.327 f1 0.322 || val_loss 1.1101 acc 0.409 f1 0.304\n",
            "[W7] CNN1D Epoch 03 | train_loss 1.0745 acc 0.447 f1 0.409 || val_loss 1.1129 acc 0.377 f1 0.320\n",
            "[W7] CNN1D Epoch 04 | train_loss 1.0519 acc 0.487 f1 0.434 || val_loss 1.1129 acc 0.325 f1 0.293\n",
            "[W7] CNN1D Epoch 05 | train_loss 1.0284 acc 0.492 f1 0.458 || val_loss 1.1317 acc 0.307 f1 0.298\n",
            "[W7] CNN1D Epoch 06 | train_loss 0.9928 acc 0.487 f1 0.464 || val_loss 1.1603 acc 0.388 f1 0.333\n",
            "[W7] CNN1D Epoch 07 | train_loss 0.9709 acc 0.501 f1 0.476 || val_loss 1.2173 acc 0.420 f1 0.342\n",
            "[W7] CNN1D Epoch 08 | train_loss 0.9195 acc 0.541 f1 0.517 || val_loss 1.2211 acc 0.362 f1 0.330\n",
            "[W7] CNN1D Epoch 09 | train_loss 0.8790 acc 0.542 f1 0.522 || val_loss 1.2133 acc 0.365 f1 0.330\n",
            "[W7] CNN1D Epoch 10 | train_loss 0.8487 acc 0.553 f1 0.539 || val_loss 1.3322 acc 0.377 f1 0.316\n",
            "[W7] CNN1D Epoch 11 | train_loss 0.7855 acc 0.588 f1 0.578 || val_loss 1.3844 acc 0.368 f1 0.316\n",
            "[W7] CNN1D Epoch 12 | train_loss 0.7856 acc 0.576 f1 0.563 || val_loss 1.3858 acc 0.377 f1 0.329\n",
            "[W7] CNN1D Epoch 13 | train_loss 0.7483 acc 0.602 f1 0.590 || val_loss 1.3641 acc 0.345 f1 0.318\n",
            "[W7] CNN1D Epoch 14 | train_loss 0.7221 acc 0.621 f1 0.609 || val_loss 1.5996 acc 0.394 f1 0.329\n",
            "[W7] CNN1D Epoch 15 | train_loss 0.7121 acc 0.626 f1 0.614 || val_loss 1.5644 acc 0.400 f1 0.338\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=7 Model=RNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W7] RNN Epoch 01 | train_loss 1.1083 acc 0.162 f1 0.141 || val_loss 1.0988 acc 0.226 f1 0.196\n",
            "[W7] RNN Epoch 02 | train_loss 1.0943 acc 0.289 f1 0.253 || val_loss 1.1049 acc 0.293 f1 0.256\n",
            "[W7] RNN Epoch 03 | train_loss 1.0810 acc 0.400 f1 0.363 || val_loss 1.1130 acc 0.301 f1 0.281\n",
            "[W7] RNN Epoch 04 | train_loss 1.0741 acc 0.369 f1 0.356 || val_loss 1.1223 acc 0.270 f1 0.264\n",
            "[W7] RNN Epoch 05 | train_loss 1.0573 acc 0.409 f1 0.395 || val_loss 1.1354 acc 0.278 f1 0.268\n",
            "[W7] RNN Epoch 06 | train_loss 1.0440 acc 0.411 f1 0.395 || val_loss 1.1523 acc 0.310 f1 0.296\n",
            "[W7] RNN Epoch 07 | train_loss 1.0310 acc 0.414 f1 0.399 || val_loss 1.1690 acc 0.310 f1 0.295\n",
            "[W7] RNN Epoch 08 | train_loss 1.0210 acc 0.435 f1 0.419 || val_loss 1.1777 acc 0.328 f1 0.310\n",
            "[W7] RNN Epoch 09 | train_loss 1.0013 acc 0.461 f1 0.444 || val_loss 1.2003 acc 0.333 f1 0.317\n",
            "[W7] RNN Epoch 10 | train_loss 0.9908 acc 0.450 f1 0.436 || val_loss 1.2235 acc 0.316 f1 0.301\n",
            "[W7] RNN Epoch 11 | train_loss 0.9686 acc 0.470 f1 0.455 || val_loss 1.2453 acc 0.307 f1 0.297\n",
            "[W7] RNN Epoch 12 | train_loss 0.9514 acc 0.471 f1 0.457 || val_loss 1.2806 acc 0.322 f1 0.310\n",
            "[W7] RNN Epoch 13 | train_loss 0.9351 acc 0.488 f1 0.473 || val_loss 1.2917 acc 0.325 f1 0.313\n",
            "[W7] RNN Epoch 14 | train_loss 0.9222 acc 0.480 f1 0.467 || val_loss 1.3343 acc 0.316 f1 0.304\n",
            "[W7] RNN Epoch 15 | train_loss 0.9015 acc 0.499 f1 0.487 || val_loss 1.3620 acc 0.333 f1 0.320\n",
            "[W7] RNN Epoch 16 | train_loss 0.8783 acc 0.512 f1 0.500 || val_loss 1.3611 acc 0.333 f1 0.321\n",
            "[W7] RNN Epoch 17 | train_loss 0.8660 acc 0.512 f1 0.502 || val_loss 1.4097 acc 0.336 f1 0.323\n",
            "[W7] RNN Epoch 18 | train_loss 0.8490 acc 0.514 f1 0.504 || val_loss 1.4642 acc 0.328 f1 0.309\n",
            "[W7] RNN Epoch 19 | train_loss 0.8257 acc 0.540 f1 0.530 || val_loss 1.5083 acc 0.333 f1 0.318\n",
            "[W7] RNN Epoch 20 | train_loss 0.8057 acc 0.554 f1 0.543 || val_loss 1.5356 acc 0.328 f1 0.314\n",
            "[W7] RNN Epoch 21 | train_loss 0.7775 acc 0.571 f1 0.560 || val_loss 1.6073 acc 0.322 f1 0.304\n",
            "[W7] RNN Epoch 22 | train_loss 0.7590 acc 0.568 f1 0.561 || val_loss 1.6926 acc 0.333 f1 0.301\n",
            "[W7] RNN Epoch 23 | train_loss 0.7420 acc 0.588 f1 0.582 || val_loss 1.7317 acc 0.345 f1 0.323\n",
            "[W7] RNN Epoch 24 | train_loss 0.7453 acc 0.567 f1 0.563 || val_loss 1.7620 acc 0.342 f1 0.314\n",
            "[W7] RNN Epoch 25 | train_loss 0.6998 acc 0.617 f1 0.620 || val_loss 1.8001 acc 0.342 f1 0.317\n",
            "[W7] RNN Epoch 26 | train_loss 0.6833 acc 0.602 f1 0.601 || val_loss 2.0001 acc 0.342 f1 0.314\n",
            "[W7] RNN Epoch 27 | train_loss 0.6624 acc 0.621 f1 0.624 || val_loss 2.0305 acc 0.371 f1 0.334\n",
            "[W7] RNN Epoch 28 | train_loss 0.6423 acc 0.642 f1 0.646 || val_loss 2.0673 acc 0.336 f1 0.306\n",
            "[W7] RNN Epoch 29 | train_loss 0.6370 acc 0.643 f1 0.645 || val_loss 2.1427 acc 0.357 f1 0.319\n",
            "[W7] RNN Epoch 30 | train_loss 0.6024 acc 0.647 f1 0.660 || val_loss 2.2329 acc 0.365 f1 0.326\n",
            "[W7] RNN Epoch 31 | train_loss 0.5922 acc 0.658 f1 0.671 || val_loss 2.2742 acc 0.357 f1 0.319\n",
            "[W7] RNN Epoch 32 | train_loss 0.5742 acc 0.665 f1 0.680 || val_loss 2.3623 acc 0.365 f1 0.326\n",
            "[W7] RNN Epoch 33 | train_loss 0.5962 acc 0.655 f1 0.668 || val_loss 2.3876 acc 0.383 f1 0.341\n",
            "[W7] RNN Epoch 34 | train_loss 0.5823 acc 0.649 f1 0.660 || val_loss 2.3571 acc 0.374 f1 0.334\n",
            "[W7] RNN Epoch 35 | train_loss 0.5489 acc 0.685 f1 0.703 || val_loss 2.4915 acc 0.377 f1 0.340\n",
            "[W7] RNN Epoch 36 | train_loss 0.5244 acc 0.689 f1 0.716 || val_loss 2.6425 acc 0.383 f1 0.333\n",
            "[W7] RNN Epoch 37 | train_loss 0.4983 acc 0.699 f1 0.727 || val_loss 2.7003 acc 0.374 f1 0.327\n",
            "[W7] RNN Epoch 38 | train_loss 0.4943 acc 0.717 f1 0.750 || val_loss 2.7670 acc 0.383 f1 0.334\n",
            "[W7] RNN Epoch 39 | train_loss 0.4689 acc 0.711 f1 0.744 || val_loss 2.8627 acc 0.403 f1 0.354\n",
            "[W7] RNN Epoch 40 | train_loss 0.4650 acc 0.718 f1 0.751 || val_loss 2.8999 acc 0.394 f1 0.353\n",
            "[W7] RNN Epoch 41 | train_loss 0.4616 acc 0.729 f1 0.763 || val_loss 2.9632 acc 0.397 f1 0.345\n",
            "[W7] RNN Epoch 42 | train_loss 0.4514 acc 0.727 f1 0.765 || val_loss 3.1425 acc 0.406 f1 0.357\n",
            "[W7] RNN Epoch 43 | train_loss 0.4239 acc 0.742 f1 0.786 || val_loss 3.1823 acc 0.409 f1 0.357\n",
            "[W7] RNN Epoch 44 | train_loss 0.4295 acc 0.735 f1 0.783 || val_loss 3.3131 acc 0.383 f1 0.334\n",
            "[W7] RNN Epoch 45 | train_loss 0.4190 acc 0.750 f1 0.789 || val_loss 3.4209 acc 0.409 f1 0.344\n",
            "[W7] RNN Epoch 46 | train_loss 0.4021 acc 0.765 f1 0.805 || val_loss 3.5286 acc 0.380 f1 0.327\n",
            "[W7] RNN Epoch 47 | train_loss 0.3909 acc 0.781 f1 0.824 || val_loss 3.4784 acc 0.400 f1 0.347\n",
            "[W7] RNN Epoch 48 | train_loss 0.3936 acc 0.769 f1 0.809 || val_loss 3.6574 acc 0.400 f1 0.348\n",
            "[W7] RNN Epoch 49 | train_loss 0.3810 acc 0.782 f1 0.816 || val_loss 3.6992 acc 0.400 f1 0.343\n",
            "[W7] RNN Epoch 50 | train_loss 0.3872 acc 0.775 f1 0.812 || val_loss 3.9351 acc 0.394 f1 0.338\n",
            "[W7] RNN Epoch 51 | train_loss 0.3726 acc 0.781 f1 0.822 || val_loss 3.8641 acc 0.403 f1 0.345\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=7 Model=GRU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W7] GRU Epoch 01 | train_loss 1.1018 acc 0.298 f1 0.285 || val_loss 1.1051 acc 0.284 f1 0.239\n",
            "[W7] GRU Epoch 02 | train_loss 1.0972 acc 0.389 f1 0.354 || val_loss 1.1061 acc 0.325 f1 0.290\n",
            "[W7] GRU Epoch 03 | train_loss 1.0913 acc 0.387 f1 0.363 || val_loss 1.1077 acc 0.304 f1 0.263\n",
            "[W7] GRU Epoch 04 | train_loss 1.0875 acc 0.402 f1 0.372 || val_loss 1.1103 acc 0.325 f1 0.281\n",
            "[W7] GRU Epoch 05 | train_loss 1.0833 acc 0.437 f1 0.382 || val_loss 1.1149 acc 0.351 f1 0.279\n",
            "[W7] GRU Epoch 06 | train_loss 1.0771 acc 0.416 f1 0.392 || val_loss 1.1134 acc 0.339 f1 0.312\n",
            "[W7] GRU Epoch 07 | train_loss 1.0693 acc 0.425 f1 0.405 || val_loss 1.1226 acc 0.330 f1 0.286\n",
            "[W7] GRU Epoch 08 | train_loss 1.0626 acc 0.443 f1 0.408 || val_loss 1.1288 acc 0.313 f1 0.278\n",
            "[W7] GRU Epoch 09 | train_loss 1.0507 acc 0.451 f1 0.426 || val_loss 1.1479 acc 0.316 f1 0.265\n",
            "[W7] GRU Epoch 10 | train_loss 1.0333 acc 0.486 f1 0.460 || val_loss 1.1675 acc 0.313 f1 0.267\n",
            "[W7] GRU Epoch 11 | train_loss 1.0177 acc 0.437 f1 0.417 || val_loss 1.1893 acc 0.325 f1 0.264\n",
            "[W7] GRU Epoch 12 | train_loss 0.9989 acc 0.493 f1 0.470 || val_loss 1.2036 acc 0.333 f1 0.287\n",
            "[W7] GRU Epoch 13 | train_loss 0.9747 acc 0.496 f1 0.480 || val_loss 1.2347 acc 0.310 f1 0.265\n",
            "[W7] GRU Epoch 14 | train_loss 0.9462 acc 0.507 f1 0.492 || val_loss 1.2932 acc 0.345 f1 0.277\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=7 Model=LSTM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W7] LSTM Epoch 01 | train_loss 1.0994 acc 0.447 f1 0.268 || val_loss 1.1022 acc 0.426 f1 0.270\n",
            "[W7] LSTM Epoch 02 | train_loss 1.0966 acc 0.474 f1 0.341 || val_loss 1.1011 acc 0.400 f1 0.307\n",
            "[W7] LSTM Epoch 03 | train_loss 1.0943 acc 0.457 f1 0.398 || val_loss 1.1012 acc 0.377 f1 0.309\n",
            "[W7] LSTM Epoch 04 | train_loss 1.0917 acc 0.418 f1 0.375 || val_loss 1.1014 acc 0.339 f1 0.301\n",
            "[W7] LSTM Epoch 05 | train_loss 1.0887 acc 0.412 f1 0.379 || val_loss 1.1020 acc 0.336 f1 0.288\n",
            "[W7] LSTM Epoch 06 | train_loss 1.0820 acc 0.457 f1 0.417 || val_loss 1.1030 acc 0.333 f1 0.283\n",
            "[W7] LSTM Epoch 07 | train_loss 1.0740 acc 0.477 f1 0.430 || val_loss 1.1104 acc 0.319 f1 0.280\n",
            "[W7] LSTM Epoch 08 | train_loss 1.0658 acc 0.443 f1 0.414 || val_loss 1.1187 acc 0.336 f1 0.283\n",
            "[W7] LSTM Epoch 09 | train_loss 1.0520 acc 0.441 f1 0.412 || val_loss 1.1331 acc 0.357 f1 0.276\n",
            "[W7] LSTM Epoch 10 | train_loss 1.0287 acc 0.488 f1 0.442 || val_loss 1.1356 acc 0.307 f1 0.297\n",
            "[W7] LSTM Epoch 11 | train_loss 1.0062 acc 0.459 f1 0.436 || val_loss 1.1636 acc 0.357 f1 0.282\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=10 Model=ANN\n",
            "[W10] ANN Epoch 01 | train_loss 1.2303 acc 0.299 f1 0.283 || val_loss 1.1192 acc 0.310 f1 0.293\n",
            "[W10] ANN Epoch 02 | train_loss 1.0553 acc 0.392 f1 0.377 || val_loss 1.1624 acc 0.330 f1 0.316\n",
            "[W10] ANN Epoch 03 | train_loss 0.9735 acc 0.441 f1 0.426 || val_loss 1.1951 acc 0.283 f1 0.263\n",
            "[W10] ANN Epoch 04 | train_loss 0.8761 acc 0.494 f1 0.480 || val_loss 1.2464 acc 0.327 f1 0.301\n",
            "[W10] ANN Epoch 05 | train_loss 0.8018 acc 0.554 f1 0.536 || val_loss 1.3165 acc 0.340 f1 0.304\n",
            "[W10] ANN Epoch 06 | train_loss 0.7104 acc 0.606 f1 0.591 || val_loss 1.3742 acc 0.360 f1 0.316\n",
            "[W10] ANN Epoch 07 | train_loss 0.6256 acc 0.674 f1 0.659 || val_loss 1.4477 acc 0.353 f1 0.283\n",
            "[W10] ANN Epoch 08 | train_loss 0.5545 acc 0.734 f1 0.724 || val_loss 1.5585 acc 0.380 f1 0.312\n",
            "[W10] ANN Epoch 09 | train_loss 0.4830 acc 0.764 f1 0.752 || val_loss 1.6569 acc 0.383 f1 0.321\n",
            "[W10] ANN Epoch 10 | train_loss 0.4339 acc 0.792 f1 0.783 || val_loss 1.7387 acc 0.387 f1 0.318\n",
            "[W10] ANN Epoch 11 | train_loss 0.3590 acc 0.851 f1 0.847 || val_loss 1.8524 acc 0.380 f1 0.306\n",
            "[W10] ANN Epoch 12 | train_loss 0.3273 acc 0.858 f1 0.854 || val_loss 2.0004 acc 0.357 f1 0.303\n",
            "[W10] ANN Epoch 13 | train_loss 0.2810 acc 0.879 f1 0.880 || val_loss 2.1092 acc 0.387 f1 0.309\n",
            "[W10] ANN Epoch 14 | train_loss 0.2379 acc 0.906 f1 0.903 || val_loss 2.2025 acc 0.413 f1 0.324\n",
            "[W10] ANN Epoch 15 | train_loss 0.2430 acc 0.906 f1 0.898 || val_loss 2.3014 acc 0.390 f1 0.307\n",
            "[W10] ANN Epoch 16 | train_loss 0.2016 acc 0.927 f1 0.925 || val_loss 2.3587 acc 0.397 f1 0.319\n",
            "[W10] ANN Epoch 17 | train_loss 0.2059 acc 0.914 f1 0.910 || val_loss 2.4203 acc 0.390 f1 0.304\n",
            "[W10] ANN Epoch 18 | train_loss 0.1876 acc 0.930 f1 0.920 || val_loss 2.5679 acc 0.400 f1 0.320\n",
            "[W10] ANN Epoch 19 | train_loss 0.1713 acc 0.929 f1 0.923 || val_loss 2.6267 acc 0.420 f1 0.345\n",
            "[W10] ANN Epoch 20 | train_loss 0.1628 acc 0.931 f1 0.926 || val_loss 2.6519 acc 0.417 f1 0.326\n",
            "[W10] ANN Epoch 21 | train_loss 0.1652 acc 0.937 f1 0.933 || val_loss 2.7088 acc 0.403 f1 0.317\n",
            "[W10] ANN Epoch 22 | train_loss 0.1399 acc 0.949 f1 0.948 || val_loss 2.7285 acc 0.410 f1 0.329\n",
            "[W10] ANN Epoch 23 | train_loss 0.1297 acc 0.946 f1 0.943 || val_loss 2.6789 acc 0.433 f1 0.339\n",
            "[W10] ANN Epoch 24 | train_loss 0.1307 acc 0.949 f1 0.940 || val_loss 2.8333 acc 0.427 f1 0.333\n",
            "[W10] ANN Epoch 25 | train_loss 0.1143 acc 0.953 f1 0.951 || val_loss 2.8492 acc 0.407 f1 0.317\n",
            "[W10] ANN Epoch 26 | train_loss 0.1068 acc 0.960 f1 0.957 || val_loss 2.8944 acc 0.453 f1 0.343\n",
            "[W10] ANN Epoch 27 | train_loss 0.1110 acc 0.958 f1 0.956 || val_loss 3.0275 acc 0.420 f1 0.317\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=10 Model=CNN1D\n",
            "[W10] CNN1D Epoch 01 | train_loss 1.1017 acc 0.395 f1 0.337 || val_loss 1.0972 acc 0.280 f1 0.261\n",
            "[W10] CNN1D Epoch 02 | train_loss 1.0870 acc 0.329 f1 0.314 || val_loss 1.1067 acc 0.390 f1 0.316\n",
            "[W10] CNN1D Epoch 03 | train_loss 1.0644 acc 0.440 f1 0.396 || val_loss 1.1120 acc 0.370 f1 0.311\n",
            "[W10] CNN1D Epoch 04 | train_loss 1.0515 acc 0.438 f1 0.411 || val_loss 1.1207 acc 0.353 f1 0.314\n",
            "[W10] CNN1D Epoch 05 | train_loss 1.0281 acc 0.445 f1 0.421 || val_loss 1.1245 acc 0.310 f1 0.284\n",
            "[W10] CNN1D Epoch 06 | train_loss 1.0083 acc 0.466 f1 0.440 || val_loss 1.1419 acc 0.280 f1 0.277\n",
            "[W10] CNN1D Epoch 07 | train_loss 0.9868 acc 0.508 f1 0.470 || val_loss 1.1453 acc 0.320 f1 0.305\n",
            "[W10] CNN1D Epoch 08 | train_loss 0.9544 acc 0.457 f1 0.445 || val_loss 1.1989 acc 0.383 f1 0.333\n",
            "[W10] CNN1D Epoch 09 | train_loss 0.9244 acc 0.504 f1 0.487 || val_loss 1.1969 acc 0.373 f1 0.333\n",
            "[W10] CNN1D Epoch 10 | train_loss 0.8962 acc 0.524 f1 0.507 || val_loss 1.2000 acc 0.387 f1 0.351\n",
            "[W10] CNN1D Epoch 11 | train_loss 0.8653 acc 0.539 f1 0.524 || val_loss 1.2422 acc 0.377 f1 0.333\n",
            "[W10] CNN1D Epoch 12 | train_loss 0.8287 acc 0.555 f1 0.539 || val_loss 1.3010 acc 0.430 f1 0.380\n",
            "[W10] CNN1D Epoch 13 | train_loss 0.8149 acc 0.551 f1 0.538 || val_loss 1.2693 acc 0.373 f1 0.345\n",
            "[W10] CNN1D Epoch 14 | train_loss 0.7798 acc 0.583 f1 0.566 || val_loss 1.4059 acc 0.460 f1 0.405\n",
            "[W10] CNN1D Epoch 15 | train_loss 0.7643 acc 0.586 f1 0.575 || val_loss 1.4359 acc 0.440 f1 0.383\n",
            "[W10] CNN1D Epoch 16 | train_loss 0.7365 acc 0.591 f1 0.582 || val_loss 1.4927 acc 0.400 f1 0.348\n",
            "[W10] CNN1D Epoch 17 | train_loss 0.7401 acc 0.606 f1 0.587 || val_loss 1.6449 acc 0.453 f1 0.352\n",
            "[W10] CNN1D Epoch 18 | train_loss 0.7192 acc 0.598 f1 0.588 || val_loss 1.5070 acc 0.437 f1 0.381\n",
            "[W10] CNN1D Epoch 19 | train_loss 0.6811 acc 0.635 f1 0.628 || val_loss 1.6558 acc 0.467 f1 0.402\n",
            "[W10] CNN1D Epoch 20 | train_loss 0.6679 acc 0.631 f1 0.622 || val_loss 1.6485 acc 0.427 f1 0.375\n",
            "[W10] CNN1D Epoch 21 | train_loss 0.6602 acc 0.641 f1 0.628 || val_loss 1.7528 acc 0.413 f1 0.347\n",
            "[W10] CNN1D Epoch 22 | train_loss 0.6458 acc 0.666 f1 0.658 || val_loss 1.6476 acc 0.390 f1 0.348\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=10 Model=RNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W10] RNN Epoch 01 | train_loss 1.1025 acc 0.394 f1 0.271 || val_loss 1.1132 acc 0.367 f1 0.263\n",
            "[W10] RNN Epoch 02 | train_loss 1.0883 acc 0.421 f1 0.348 || val_loss 1.1182 acc 0.360 f1 0.275\n",
            "[W10] RNN Epoch 03 | train_loss 1.0777 acc 0.449 f1 0.394 || val_loss 1.1245 acc 0.333 f1 0.288\n",
            "[W10] RNN Epoch 04 | train_loss 1.0651 acc 0.446 f1 0.403 || val_loss 1.1407 acc 0.333 f1 0.297\n",
            "[W10] RNN Epoch 05 | train_loss 1.0436 acc 0.468 f1 0.433 || val_loss 1.1632 acc 0.307 f1 0.276\n",
            "[W10] RNN Epoch 06 | train_loss 1.0310 acc 0.436 f1 0.416 || val_loss 1.1860 acc 0.327 f1 0.305\n",
            "[W10] RNN Epoch 07 | train_loss 1.0208 acc 0.438 f1 0.418 || val_loss 1.2177 acc 0.333 f1 0.308\n",
            "[W10] RNN Epoch 08 | train_loss 1.0036 acc 0.454 f1 0.436 || val_loss 1.2420 acc 0.333 f1 0.311\n",
            "[W10] RNN Epoch 09 | train_loss 0.9809 acc 0.454 f1 0.437 || val_loss 1.2784 acc 0.330 f1 0.309\n",
            "[W10] RNN Epoch 10 | train_loss 0.9613 acc 0.473 f1 0.458 || val_loss 1.3259 acc 0.360 f1 0.334\n",
            "[W10] RNN Epoch 11 | train_loss 0.9320 acc 0.496 f1 0.480 || val_loss 1.3564 acc 0.357 f1 0.326\n",
            "[W10] RNN Epoch 12 | train_loss 0.9201 acc 0.505 f1 0.490 || val_loss 1.3803 acc 0.357 f1 0.324\n",
            "[W10] RNN Epoch 13 | train_loss 0.9040 acc 0.497 f1 0.486 || val_loss 1.4063 acc 0.343 f1 0.312\n",
            "[W10] RNN Epoch 14 | train_loss 0.8768 acc 0.519 f1 0.506 || val_loss 1.4669 acc 0.360 f1 0.326\n",
            "[W10] RNN Epoch 15 | train_loss 0.8516 acc 0.536 f1 0.525 || val_loss 1.4990 acc 0.340 f1 0.312\n",
            "[W10] RNN Epoch 16 | train_loss 0.8293 acc 0.542 f1 0.531 || val_loss 1.5577 acc 0.370 f1 0.329\n",
            "[W10] RNN Epoch 17 | train_loss 0.8075 acc 0.537 f1 0.532 || val_loss 1.5963 acc 0.377 f1 0.337\n",
            "[W10] RNN Epoch 18 | train_loss 0.7900 acc 0.549 f1 0.548 || val_loss 1.6515 acc 0.360 f1 0.328\n",
            "[W10] RNN Epoch 19 | train_loss 0.7548 acc 0.555 f1 0.553 || val_loss 1.7532 acc 0.383 f1 0.336\n",
            "[W10] RNN Epoch 20 | train_loss 0.7459 acc 0.580 f1 0.582 || val_loss 1.7826 acc 0.380 f1 0.332\n",
            "[W10] RNN Epoch 21 | train_loss 0.7126 acc 0.594 f1 0.599 || val_loss 1.9059 acc 0.390 f1 0.324\n",
            "[W10] RNN Epoch 22 | train_loss 0.7002 acc 0.588 f1 0.595 || val_loss 1.8711 acc 0.377 f1 0.326\n",
            "[W10] RNN Epoch 23 | train_loss 0.6707 acc 0.615 f1 0.626 || val_loss 2.0158 acc 0.387 f1 0.325\n",
            "[W10] RNN Epoch 24 | train_loss 0.6484 acc 0.611 f1 0.621 || val_loss 2.0445 acc 0.393 f1 0.329\n",
            "[W10] RNN Epoch 25 | train_loss 0.6293 acc 0.631 f1 0.649 || val_loss 2.1362 acc 0.383 f1 0.326\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=10 Model=GRU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W10] GRU Epoch 01 | train_loss 1.1018 acc 0.351 f1 0.239 || val_loss 1.0970 acc 0.390 f1 0.261\n",
            "[W10] GRU Epoch 02 | train_loss 1.0940 acc 0.406 f1 0.298 || val_loss 1.0976 acc 0.373 f1 0.274\n",
            "[W10] GRU Epoch 03 | train_loss 1.0896 acc 0.404 f1 0.338 || val_loss 1.0987 acc 0.343 f1 0.298\n",
            "[W10] GRU Epoch 04 | train_loss 1.0854 acc 0.434 f1 0.378 || val_loss 1.1015 acc 0.367 f1 0.328\n",
            "[W10] GRU Epoch 05 | train_loss 1.0761 acc 0.444 f1 0.410 || val_loss 1.1013 acc 0.333 f1 0.315\n",
            "[W10] GRU Epoch 06 | train_loss 1.0658 acc 0.402 f1 0.383 || val_loss 1.1109 acc 0.330 f1 0.314\n",
            "[W10] GRU Epoch 07 | train_loss 1.0516 acc 0.430 f1 0.402 || val_loss 1.1258 acc 0.327 f1 0.313\n",
            "[W10] GRU Epoch 08 | train_loss 1.0435 acc 0.440 f1 0.413 || val_loss 1.1489 acc 0.357 f1 0.323\n",
            "[W10] GRU Epoch 09 | train_loss 1.0211 acc 0.449 f1 0.427 || val_loss 1.1609 acc 0.343 f1 0.308\n",
            "[W10] GRU Epoch 10 | train_loss 1.0084 acc 0.471 f1 0.444 || val_loss 1.1868 acc 0.333 f1 0.299\n",
            "[W10] GRU Epoch 11 | train_loss 0.9862 acc 0.484 f1 0.460 || val_loss 1.2236 acc 0.347 f1 0.297\n",
            "[W10] GRU Epoch 12 | train_loss 0.9646 acc 0.500 f1 0.474 || val_loss 1.2515 acc 0.340 f1 0.301\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=10 Model=LSTM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W10] LSTM Epoch 01 | train_loss 1.1021 acc 0.131 f1 0.102 || val_loss 1.0923 acc 0.187 f1 0.154\n",
            "[W10] LSTM Epoch 02 | train_loss 1.0968 acc 0.206 f1 0.184 || val_loss 1.0937 acc 0.250 f1 0.213\n",
            "[W10] LSTM Epoch 03 | train_loss 1.0936 acc 0.326 f1 0.281 || val_loss 1.0972 acc 0.383 f1 0.345\n",
            "[W10] LSTM Epoch 04 | train_loss 1.0872 acc 0.397 f1 0.368 || val_loss 1.0985 acc 0.330 f1 0.314\n",
            "[W10] LSTM Epoch 05 | train_loss 1.0790 acc 0.414 f1 0.389 || val_loss 1.1092 acc 0.343 f1 0.321\n",
            "[W10] LSTM Epoch 06 | train_loss 1.0712 acc 0.444 f1 0.401 || val_loss 1.1098 acc 0.287 f1 0.287\n",
            "[W10] LSTM Epoch 07 | train_loss 1.0577 acc 0.399 f1 0.386 || val_loss 1.1607 acc 0.357 f1 0.330\n",
            "[W10] LSTM Epoch 08 | train_loss 1.0373 acc 0.417 f1 0.395 || val_loss 1.1422 acc 0.310 f1 0.308\n",
            "[W10] LSTM Epoch 09 | train_loss 1.0242 acc 0.449 f1 0.425 || val_loss 1.1432 acc 0.303 f1 0.302\n",
            "[W10] LSTM Epoch 10 | train_loss 1.0130 acc 0.458 f1 0.434 || val_loss 1.1544 acc 0.337 f1 0.325\n",
            "[W10] LSTM Epoch 11 | train_loss 1.0002 acc 0.419 f1 0.409 || val_loss 1.1813 acc 0.357 f1 0.323\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=14 Model=ANN\n",
            "[W14] ANN Epoch 01 | train_loss 1.2741 acc 0.337 f1 0.315 || val_loss 1.1162 acc 0.179 f1 0.157\n",
            "[W14] ANN Epoch 02 | train_loss 1.0339 acc 0.436 f1 0.418 || val_loss 1.1585 acc 0.208 f1 0.207\n",
            "[W14] ANN Epoch 03 | train_loss 0.9110 acc 0.494 f1 0.478 || val_loss 1.1966 acc 0.271 f1 0.271\n",
            "[W14] ANN Epoch 04 | train_loss 0.8290 acc 0.545 f1 0.527 || val_loss 1.2479 acc 0.254 f1 0.255\n",
            "[W14] ANN Epoch 05 | train_loss 0.7282 acc 0.596 f1 0.578 || val_loss 1.2992 acc 0.271 f1 0.265\n",
            "[W14] ANN Epoch 06 | train_loss 0.6459 acc 0.656 f1 0.635 || val_loss 1.3119 acc 0.317 f1 0.299\n",
            "[W14] ANN Epoch 07 | train_loss 0.5377 acc 0.728 f1 0.709 || val_loss 1.3761 acc 0.350 f1 0.324\n",
            "[W14] ANN Epoch 08 | train_loss 0.4861 acc 0.782 f1 0.760 || val_loss 1.4664 acc 0.354 f1 0.321\n",
            "[W14] ANN Epoch 09 | train_loss 0.4014 acc 0.814 f1 0.797 || val_loss 1.5634 acc 0.346 f1 0.314\n",
            "[W14] ANN Epoch 10 | train_loss 0.3367 acc 0.850 f1 0.842 || val_loss 1.6533 acc 0.358 f1 0.328\n",
            "[W14] ANN Epoch 11 | train_loss 0.2727 acc 0.904 f1 0.897 || val_loss 1.7350 acc 0.396 f1 0.350\n",
            "[W14] ANN Epoch 12 | train_loss 0.2559 acc 0.893 f1 0.889 || val_loss 1.8013 acc 0.388 f1 0.337\n",
            "[W14] ANN Epoch 13 | train_loss 0.2027 acc 0.927 f1 0.914 || val_loss 1.9213 acc 0.375 f1 0.323\n",
            "[W14] ANN Epoch 14 | train_loss 0.1609 acc 0.939 f1 0.937 || val_loss 2.0139 acc 0.379 f1 0.326\n",
            "[W14] ANN Epoch 15 | train_loss 0.1405 acc 0.950 f1 0.949 || val_loss 2.0742 acc 0.388 f1 0.326\n",
            "[W14] ANN Epoch 16 | train_loss 0.1280 acc 0.959 f1 0.959 || val_loss 2.1435 acc 0.400 f1 0.342\n",
            "[W14] ANN Epoch 17 | train_loss 0.1233 acc 0.956 f1 0.946 || val_loss 2.2020 acc 0.379 f1 0.320\n",
            "[W14] ANN Epoch 18 | train_loss 0.1309 acc 0.954 f1 0.950 || val_loss 2.2380 acc 0.408 f1 0.341\n",
            "[W14] ANN Epoch 19 | train_loss 0.1225 acc 0.954 f1 0.951 || val_loss 2.3274 acc 0.400 f1 0.322\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=14 Model=CNN1D\n",
            "[W14] CNN1D Epoch 01 | train_loss 1.1061 acc 0.393 f1 0.319 || val_loss 1.1024 acc 0.412 f1 0.259\n",
            "[W14] CNN1D Epoch 02 | train_loss 1.0911 acc 0.447 f1 0.384 || val_loss 1.1043 acc 0.417 f1 0.356\n",
            "[W14] CNN1D Epoch 03 | train_loss 1.0775 acc 0.418 f1 0.383 || val_loss 1.1017 acc 0.254 f1 0.250\n",
            "[W14] CNN1D Epoch 04 | train_loss 1.0613 acc 0.366 f1 0.359 || val_loss 1.1135 acc 0.304 f1 0.271\n",
            "[W14] CNN1D Epoch 05 | train_loss 1.0514 acc 0.430 f1 0.402 || val_loss 1.1254 acc 0.283 f1 0.268\n",
            "[W14] CNN1D Epoch 06 | train_loss 1.0392 acc 0.396 f1 0.379 || val_loss 1.1366 acc 0.321 f1 0.305\n",
            "[W14] CNN1D Epoch 07 | train_loss 1.0180 acc 0.419 f1 0.397 || val_loss 1.1518 acc 0.263 f1 0.260\n",
            "[W14] CNN1D Epoch 08 | train_loss 1.0033 acc 0.432 f1 0.413 || val_loss 1.1614 acc 0.321 f1 0.298\n",
            "[W14] CNN1D Epoch 09 | train_loss 0.9847 acc 0.436 f1 0.423 || val_loss 1.2025 acc 0.367 f1 0.324\n",
            "[W14] CNN1D Epoch 10 | train_loss 0.9628 acc 0.479 f1 0.459 || val_loss 1.2172 acc 0.388 f1 0.318\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=14 Model=RNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W14] RNN Epoch 01 | train_loss 1.1051 acc 0.335 f1 0.252 || val_loss 1.0993 acc 0.283 f1 0.202\n",
            "[W14] RNN Epoch 02 | train_loss 1.0920 acc 0.382 f1 0.328 || val_loss 1.1021 acc 0.304 f1 0.247\n",
            "[W14] RNN Epoch 03 | train_loss 1.0838 acc 0.403 f1 0.362 || val_loss 1.1067 acc 0.338 f1 0.295\n",
            "[W14] RNN Epoch 04 | train_loss 1.0735 acc 0.431 f1 0.403 || val_loss 1.1074 acc 0.321 f1 0.297\n",
            "[W14] RNN Epoch 05 | train_loss 1.0655 acc 0.446 f1 0.421 || val_loss 1.1128 acc 0.329 f1 0.304\n",
            "[W14] RNN Epoch 06 | train_loss 1.0486 acc 0.438 f1 0.420 || val_loss 1.1145 acc 0.333 f1 0.306\n",
            "[W14] RNN Epoch 07 | train_loss 1.0345 acc 0.466 f1 0.442 || val_loss 1.1209 acc 0.333 f1 0.315\n",
            "[W14] RNN Epoch 08 | train_loss 1.0086 acc 0.449 f1 0.434 || val_loss 1.1370 acc 0.350 f1 0.333\n",
            "[W14] RNN Epoch 09 | train_loss 0.9885 acc 0.449 f1 0.434 || val_loss 1.1773 acc 0.375 f1 0.350\n",
            "[W14] RNN Epoch 10 | train_loss 0.9689 acc 0.468 f1 0.450 || val_loss 1.2063 acc 0.375 f1 0.354\n",
            "[W14] RNN Epoch 11 | train_loss 0.9488 acc 0.482 f1 0.467 || val_loss 1.2493 acc 0.396 f1 0.364\n",
            "[W14] RNN Epoch 12 | train_loss 0.9311 acc 0.490 f1 0.478 || val_loss 1.2798 acc 0.379 f1 0.354\n",
            "[W14] RNN Epoch 13 | train_loss 0.9111 acc 0.498 f1 0.488 || val_loss 1.3116 acc 0.383 f1 0.358\n",
            "[W14] RNN Epoch 14 | train_loss 0.9036 acc 0.512 f1 0.499 || val_loss 1.3574 acc 0.379 f1 0.344\n",
            "[W14] RNN Epoch 15 | train_loss 0.8654 acc 0.529 f1 0.515 || val_loss 1.3892 acc 0.375 f1 0.343\n",
            "[W14] RNN Epoch 16 | train_loss 0.8622 acc 0.546 f1 0.537 || val_loss 1.3747 acc 0.358 f1 0.332\n",
            "[W14] RNN Epoch 17 | train_loss 0.8342 acc 0.548 f1 0.538 || val_loss 1.4446 acc 0.367 f1 0.332\n",
            "[W14] RNN Epoch 18 | train_loss 0.8101 acc 0.556 f1 0.546 || val_loss 1.4940 acc 0.371 f1 0.335\n",
            "[W14] RNN Epoch 19 | train_loss 0.7944 acc 0.574 f1 0.564 || val_loss 1.4953 acc 0.371 f1 0.342\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=14 Model=GRU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W14] GRU Epoch 01 | train_loss 1.1014 acc 0.113 f1 0.072 || val_loss 1.0963 acc 0.138 f1 0.090\n",
            "[W14] GRU Epoch 02 | train_loss 1.0964 acc 0.197 f1 0.193 || val_loss 1.0968 acc 0.242 f1 0.234\n",
            "[W14] GRU Epoch 03 | train_loss 1.0900 acc 0.337 f1 0.331 || val_loss 1.0973 acc 0.283 f1 0.278\n",
            "[W14] GRU Epoch 04 | train_loss 1.0851 acc 0.375 f1 0.364 || val_loss 1.0978 acc 0.321 f1 0.311\n",
            "[W14] GRU Epoch 05 | train_loss 1.0791 acc 0.426 f1 0.398 || val_loss 1.0999 acc 0.383 f1 0.360\n",
            "[W14] GRU Epoch 06 | train_loss 1.0671 acc 0.465 f1 0.433 || val_loss 1.1063 acc 0.379 f1 0.353\n",
            "[W14] GRU Epoch 07 | train_loss 1.0593 acc 0.445 f1 0.418 || val_loss 1.1159 acc 0.396 f1 0.366\n",
            "[W14] GRU Epoch 08 | train_loss 1.0444 acc 0.487 f1 0.448 || val_loss 1.1231 acc 0.338 f1 0.327\n",
            "[W14] GRU Epoch 09 | train_loss 1.0285 acc 0.460 f1 0.433 || val_loss 1.1320 acc 0.358 f1 0.343\n",
            "[W14] GRU Epoch 10 | train_loss 1.0097 acc 0.495 f1 0.463 || val_loss 1.1368 acc 0.350 f1 0.339\n",
            "[W14] GRU Epoch 11 | train_loss 0.9901 acc 0.485 f1 0.461 || val_loss 1.1585 acc 0.354 f1 0.330\n",
            "[W14] GRU Epoch 12 | train_loss 0.9829 acc 0.488 f1 0.462 || val_loss 1.1620 acc 0.325 f1 0.312\n",
            "[W14] GRU Epoch 13 | train_loss 0.9624 acc 0.502 f1 0.482 || val_loss 1.1912 acc 0.346 f1 0.319\n",
            "[W14] GRU Epoch 14 | train_loss 0.9394 acc 0.523 f1 0.498 || val_loss 1.1707 acc 0.296 f1 0.287\n",
            "[W14] GRU Epoch 15 | train_loss 0.9078 acc 0.500 f1 0.485 || val_loss 1.2458 acc 0.350 f1 0.310\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=14 Model=LSTM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W14] LSTM Epoch 01 | train_loss 1.1022 acc 0.234 f1 0.197 || val_loss 1.0975 acc 0.263 f1 0.207\n",
            "[W14] LSTM Epoch 02 | train_loss 1.0974 acc 0.279 f1 0.228 || val_loss 1.0982 acc 0.279 f1 0.215\n",
            "[W14] LSTM Epoch 03 | train_loss 1.0936 acc 0.364 f1 0.296 || val_loss 1.1019 acc 0.371 f1 0.282\n",
            "[W14] LSTM Epoch 04 | train_loss 1.0905 acc 0.419 f1 0.373 || val_loss 1.1025 acc 0.346 f1 0.298\n",
            "[W14] LSTM Epoch 05 | train_loss 1.0865 acc 0.422 f1 0.372 || val_loss 1.1073 acc 0.371 f1 0.301\n",
            "[W14] LSTM Epoch 06 | train_loss 1.0806 acc 0.451 f1 0.397 || val_loss 1.1160 acc 0.350 f1 0.288\n",
            "[W14] LSTM Epoch 07 | train_loss 1.0685 acc 0.466 f1 0.415 || val_loss 1.1252 acc 0.371 f1 0.315\n",
            "[W14] LSTM Epoch 08 | train_loss 1.0560 acc 0.468 f1 0.435 || val_loss 1.1258 acc 0.338 f1 0.320\n",
            "[W14] LSTM Epoch 09 | train_loss 1.0352 acc 0.440 f1 0.415 || val_loss 1.1587 acc 0.362 f1 0.330\n",
            "[W14] LSTM Epoch 10 | train_loss 1.0176 acc 0.429 f1 0.408 || val_loss 1.1218 acc 0.279 f1 0.281\n",
            "[W14] LSTM Epoch 11 | train_loss 0.9961 acc 0.428 f1 0.416 || val_loss 1.2697 acc 0.371 f1 0.323\n",
            "[W14] LSTM Epoch 12 | train_loss 0.9791 acc 0.441 f1 0.429 || val_loss 1.1809 acc 0.317 f1 0.303\n",
            "[W14] LSTM Epoch 13 | train_loss 0.9575 acc 0.462 f1 0.447 || val_loss 1.2258 acc 0.329 f1 0.311\n",
            "[W14] LSTM Epoch 14 | train_loss 0.9384 acc 0.477 f1 0.464 || val_loss 1.2748 acc 0.342 f1 0.318\n",
            "[W14] LSTM Epoch 15 | train_loss 0.9118 acc 0.461 f1 0.447 || val_loss 1.4595 acc 0.346 f1 0.315\n",
            "[W14] LSTM Epoch 16 | train_loss 0.8713 acc 0.514 f1 0.500 || val_loss 1.4061 acc 0.317 f1 0.305\n",
            "[W14] LSTM Epoch 17 | train_loss 0.8449 acc 0.527 f1 0.514 || val_loss 1.6336 acc 0.350 f1 0.305\n",
            "Early stopping.\n",
            "Saved results to outputs/results_summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the script above, but instead this script uses SMOTE to augment the imbalanced classes so that the distribution is even."
      ],
      "metadata": {
        "id": "JCUkxv030PQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "experiment_runner_smote.py\n",
        "\n",
        "PyTorch experiment runner with SMOTE augmentation:\n",
        "\n",
        "- builds sliding windows for window_sizes = [3,4,5,6,7,10,14]\n",
        "- creates ANN and sequence inputs\n",
        "- applies SMOTE on training data to balance classes\n",
        "- trains ANN, CNN1D, RNN, GRU, LSTM\n",
        "- participant-wise split (train/val/test)\n",
        "- saves results to CSV, confusion matrix PNGs, and class distributions\n",
        "\n",
        "Usage:\n",
        "    python experiment_runner_smote.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "CSV_PATH = \"stress_detection.csv\"\n",
        "OUTPUT_DIR = Path(\"outputs2\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "WINDOW_SIZES = [2,3,4,5,6,7,10,14]\n",
        "RAW_FEATURES = [\n",
        "    'Openness','Conscientiousness','Extraversion','Agreeableness','Neuroticism',\n",
        "    'sleep_time','wake_time','sleep_duration','PSQI_score',\n",
        "    'call_duration','num_calls','num_sms',\n",
        "    'screen_on_time','skin_conductance','accelerometer',\n",
        "    'mobility_radius','mobility_distance'\n",
        "]\n",
        "STATIC_PERSONALITY = ['Openness','Conscientiousness','Extraversion','Agreeableness','Neuroticism']\n",
        "TARGET_COL = 'PSS_score'\n",
        "CLASS_COL = 'stress_class'\n",
        "\n",
        "TRAIN_P = 0.7\n",
        "VAL_P = 0.15\n",
        "TEST_P = 0.15\n",
        "\n",
        "EPOCHS = 60\n",
        "BATCH_SIZE = 64\n",
        "LR = 1e-3\n",
        "PATIENCE = 8\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "MODEL_NAMES = [\"ANN\",\"CNN1D\",\"RNN\",\"GRU\",\"LSTM\"]\n",
        "\n",
        "# -------------------------\n",
        "# Helpers\n",
        "# -------------------------\n",
        "def stress_to_class(score):\n",
        "    s = float(score)\n",
        "    if s <= 13: return 0\n",
        "    if s <= 26: return 1\n",
        "    return 2\n",
        "\n",
        "def ensure_dir(path):\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Window builder\n",
        "# -------------------------\n",
        "def build_windows(df, raw_features, window_size=4):\n",
        "    X_seq, X_flat, y, participants = [], [], [], []\n",
        "\n",
        "    for pid, df_p in df.groupby(\"participant_id\"):\n",
        "        df_p = df_p.sort_values(\"day\").reset_index(drop=True)\n",
        "        n = len(df_p)\n",
        "        if n <= window_size:\n",
        "            continue\n",
        "        arr = df_p[raw_features].values.astype(float)\n",
        "        for i in range(n - window_size):\n",
        "            window = arr[i:i+window_size]\n",
        "            mean_vals = window.mean(axis=0)\n",
        "            std_vals = window.std(axis=0)\n",
        "            min_vals = window.min(axis=0)\n",
        "            max_vals = window.max(axis=0)\n",
        "            slope_vals = window[-1] - window[0]\n",
        "            agg = np.concatenate([mean_vals, std_vals, min_vals, max_vals, slope_vals])\n",
        "\n",
        "            flat = np.concatenate([window.flatten(), agg])\n",
        "            target_score = df_p[TARGET_COL].iloc[i+window_size]\n",
        "            cls = stress_to_class(target_score)\n",
        "\n",
        "            X_seq.append(window.copy())\n",
        "            X_flat.append(flat.copy())\n",
        "            y.append(int(cls))\n",
        "            participants.append(pid)\n",
        "\n",
        "    return np.array(X_seq), np.array(X_flat), np.array(y), np.array(participants)\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class StressDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# -------------------------\n",
        "# Models\n",
        "# -------------------------\n",
        "class ANNModel(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=[256,128,64], num_classes=3, p_drop=0.3):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last = in_dim\n",
        "        for h in hidden:\n",
        "            layers.append(nn.Linear(last,h))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.BatchNorm1d(h))\n",
        "            layers.append(nn.Dropout(p_drop))\n",
        "            last = h\n",
        "        layers.append(nn.Linear(last,num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "class CNN1DModel(nn.Module):\n",
        "    def __init__(self, seq_len, feat_dim, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(feat_dim,64,2)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.conv2 = nn.Conv1d(64,128,2)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        l = seq_len - (2-1) - (2-1)\n",
        "        if l<1: l=1\n",
        "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128*1,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64,num_classes)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x = x.permute(0,2,1)\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.gap(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=64, num_layers=1, rnn_type='RNN', num_classes=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        if rnn_type=='RNN':\n",
        "            self.rnn = nn.RNN(input_dim,hidden,num_layers,batch_first=True,nonlinearity='tanh',dropout=dropout)\n",
        "        elif rnn_type=='GRU':\n",
        "            self.rnn = nn.GRU(input_dim,hidden,num_layers,batch_first=True,dropout=dropout)\n",
        "        elif rnn_type=='LSTM':\n",
        "            self.rnn = nn.LSTM(input_dim,hidden,num_layers,batch_first=True,dropout=dropout)\n",
        "        else: raise ValueError(\"Unknown rnn_type\")\n",
        "        self.head = nn.Sequential(nn.Linear(hidden,32), nn.ReLU(), nn.Dropout(0.2), nn.Linear(32,num_classes))\n",
        "        self.rnn_type = rnn_type\n",
        "    def forward(self,x):\n",
        "        out,_ = self.rnn(x)\n",
        "        return self.head(out[:,-1,:])\n",
        "\n",
        "# -------------------------\n",
        "# Training & Evaluation\n",
        "# -------------------------\n",
        "def compute_class_weights(y):\n",
        "    counts = Counter(y.tolist())\n",
        "    total = sum(counts.values())\n",
        "    num_classes = len(counts)\n",
        "    weights = []\n",
        "    for i in range(max(counts.keys())+1):\n",
        "        cnt = counts.get(i,0)\n",
        "        weights.append(total/(num_classes*cnt) if cnt>0 else 0.0)\n",
        "    return torch.tensor(weights,dtype=torch.float32,device=DEVICE)\n",
        "\n",
        "def train_one_epoch(model, loader, opt, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    preds, trues = [], []\n",
        "    for xb,yb in loader:\n",
        "        xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        out = model(xb)\n",
        "        loss = criterion(out,yb)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()*xb.size(0)\n",
        "        preds.append(out.detach().argmax(1).cpu().numpy())\n",
        "        trues.append(yb.cpu().numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    trues = np.concatenate(trues)\n",
        "    return total_loss/len(trues), accuracy_score(trues,preds), f1_score(trues,preds,average='macro',zero_division=0)\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss=0\n",
        "    preds,trues=[],[]\n",
        "    with torch.no_grad():\n",
        "        for xb,yb in loader:\n",
        "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            out = model(xb)\n",
        "            total_loss += criterion(out,yb).item()*xb.size(0)\n",
        "            preds.append(out.argmax(1).cpu().numpy())\n",
        "            trues.append(yb.cpu().numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    trues = np.concatenate(trues)\n",
        "    mean_loss = total_loss / len(trues)\n",
        "    acc = accuracy_score(trues,preds)\n",
        "    f1 = f1_score(trues,preds,average='macro',zero_division=0)\n",
        "    prec, rec, f1s, sup = precision_recall_fscore_support(trues,preds,average=None,zero_division=0)\n",
        "    return mean_loss, acc, f1, prec, rec, f1s, sup, preds, trues\n",
        "\n",
        "# -------------------------\n",
        "# Experiment for a window\n",
        "# -------------------------\n",
        "def run_experiment_for_window(df, window_size, model_name, train_pids, val_pids, test_pids):\n",
        "    X_seq, X_flat, y, pids = build_windows(df, RAW_FEATURES, window_size)\n",
        "    if len(y)==0: return None\n",
        "\n",
        "    # split by participant\n",
        "    train_mask = np.isin(pids, train_pids)\n",
        "    val_mask = np.isin(pids, val_pids)\n",
        "    test_mask = np.isin(pids, test_pids)\n",
        "\n",
        "    X_seq_train, X_seq_val, X_seq_test = X_seq[train_mask], X_seq[val_mask], X_seq[test_mask]\n",
        "    X_flat_train, X_flat_val, X_flat_test = X_flat[train_mask], X_flat[val_mask], X_flat[test_mask]\n",
        "    y_train, y_val, y_test = y[train_mask], y[val_mask], y[test_mask]\n",
        "\n",
        "    if len(y_train)==0 or len(y_val)==0 or len(y_test)==0:\n",
        "        print(\"One split empty for window\", window_size)\n",
        "        return None\n",
        "\n",
        "    # -------------------------\n",
        "    # SMOTE augmentation (training set only)\n",
        "    # -------------------------\n",
        "    print(f\"Class distribution BEFORE SMOTE: {Counter(y_train)}\")\n",
        "    smote = SMOTE(random_state=SEED)\n",
        "    if model_name==\"ANN\":\n",
        "        X_train_aug, y_train_aug = smote.fit_resample(X_flat_train, y_train)\n",
        "        X_train_aug = X_train_aug\n",
        "    else:\n",
        "        N,L,F = X_seq_train.shape\n",
        "        X_seq_flat = X_seq_train.reshape(N,L*F)\n",
        "        X_train_aug, y_train_aug = smote.fit_resample(X_seq_flat, y_train)\n",
        "        X_train_aug = X_train_aug.reshape(-1,L,F)\n",
        "    print(f\"Class distribution AFTER SMOTE: {Counter(y_train_aug)}\")\n",
        "\n",
        "    # save class distributions\n",
        "    dist_df = pd.DataFrame({'Class':[0,1,2],\n",
        "                            'Before': [Counter(y_train)[0],Counter(y_train)[1],Counter(y_train)[2]],\n",
        "                            'After': [Counter(y_train_aug)[0],Counter(y_train_aug)[1],Counter(y_train_aug)[2]]})\n",
        "    dist_df.to_csv(OUTPUT_DIR/f'class_distribution_w{window_size}_{model_name}.csv', index=False)\n",
        "\n",
        "    # -------------------------\n",
        "    # Scaling\n",
        "    # -------------------------\n",
        "    if model_name==\"ANN\":\n",
        "        scaler = StandardScaler().fit(X_train_aug)\n",
        "        Xtr = scaler.transform(X_train_aug)\n",
        "        Xv = scaler.transform(X_flat_val)\n",
        "        Xt = scaler.transform(X_flat_test)\n",
        "        input_dim = Xtr.shape[1]\n",
        "    else:\n",
        "        N,L,F = X_train_aug.shape\n",
        "        scaler = StandardScaler().fit(X_train_aug.reshape(-1,F))\n",
        "        Xtr = scaler.transform(X_train_aug.reshape(-1,F)).reshape(N,L,F)\n",
        "        Xv = scaler.transform(X_seq_val.reshape(-1,F)).reshape(X_seq_val.shape)\n",
        "        Xt = scaler.transform(X_seq_test.reshape(-1,F)).reshape(X_seq_test.shape)\n",
        "        input_dim = F\n",
        "\n",
        "    train_ds = StressDataset(Xtr, y_train_aug)\n",
        "    val_ds = StressDataset(Xv, y_val)\n",
        "    test_ds = StressDataset(Xt, y_test)\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # -------------------------\n",
        "    # Model\n",
        "    # -------------------------\n",
        "    if model_name==\"ANN\": model = ANNModel(input_dim).to(DEVICE)\n",
        "    elif model_name==\"CNN1D\": model = CNN1DModel(Xtr.shape[1], Xtr.shape[2]).to(DEVICE)\n",
        "    elif model_name in (\"RNN\",\"GRU\",\"LSTM\"): model = RNNModel(input_dim, hidden=64, num_layers=1, rnn_type=model_name).to(DEVICE)\n",
        "    else: raise ValueError(\"Unknown model\")\n",
        "\n",
        "    # loss and optimizer\n",
        "    class_weights = compute_class_weights(torch.tensor(y_train_aug))\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
        "\n",
        "    # -------------------------\n",
        "    # Training loop with early stopping\n",
        "    # -------------------------\n",
        "    best_val_f1, best_state, cur_wait = -1.0, None, 0\n",
        "    for epoch in range(1,EPOCHS+1):\n",
        "        train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, opt, criterion)\n",
        "        val_loss, val_acc, val_f1, *_ = evaluate(model, val_loader, criterion)\n",
        "        print(f\"[W{window_size}] {model_name} Epoch {epoch:02d} | train_loss {train_loss:.4f} acc {train_acc:.3f} f1 {train_f1:.3f} || val_loss {val_loss:.4f} acc {val_acc:.3f} f1 {val_f1:.3f}\")\n",
        "        if val_f1 > best_val_f1+1e-4:\n",
        "            best_val_f1 = val_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            cur_wait = 0\n",
        "        else:\n",
        "            cur_wait += 1\n",
        "            if cur_wait >= PATIENCE:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # test\n",
        "    if best_state is not None: model.load_state_dict(best_state)\n",
        "    test_loss, test_acc, test_f1, prec, rec, f1s, sup, preds, trues = evaluate(model, test_loader, criterion)\n",
        "\n",
        "    # save confusion matrix\n",
        "    cm = confusion_matrix(trues,preds)\n",
        "    np.save(OUTPUT_DIR/f\"cm_w{window_size}_{model_name}.npy\", cm)\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(cm, cmap='Blues')\n",
        "    plt.title(f\"CM W{window_size} {model_name}\")\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"pred\")\n",
        "    plt.ylabel(\"true\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUTPUT_DIR/f\"cm_w{window_size}_{model_name}.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return {\n",
        "        \"window\": window_size,\n",
        "        \"model\": model_name,\n",
        "        \"train_samples\": len(y_train_aug),\n",
        "        \"val_samples\": len(y_val),\n",
        "        \"test_samples\": len(y_test),\n",
        "        \"test_loss\": float(test_loss),\n",
        "        \"test_acc\": float(test_acc),\n",
        "        \"test_macro_f1\": float(test_f1),\n",
        "        \"per_class_prec\": prec.tolist(),\n",
        "        \"per_class_rec\": rec.tolist(),\n",
        "        \"per_class_f1\": f1s.tolist(),\n",
        "        \"support\": sup.tolist()\n",
        "    }\n",
        "\n",
        "# -------------------------\n",
        "# Main\n",
        "# -------------------------\n",
        "def main():\n",
        "    print(\"Device:\", DEVICE)\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    df = df.sort_values(['participant_id','day']).reset_index(drop=True)\n",
        "    df[CLASS_COL] = df[TARGET_COL].apply(stress_to_class)\n",
        "\n",
        "    # participant split\n",
        "    pids = df['participant_id'].unique()\n",
        "    random.shuffle(pids)\n",
        "    n = len(pids)\n",
        "    n_train = int(n*TRAIN_P)\n",
        "    n_val = int(n*VAL_P)\n",
        "    train_pids = pids[:n_train]\n",
        "    val_pids = pids[n_train:n_train+n_val]\n",
        "    test_pids = pids[n_train+n_val:]\n",
        "    print(f\"Participants: total {n} train {len(train_pids)} val {len(val_pids)} test {len(test_pids)}\")\n",
        "\n",
        "    results=[]\n",
        "    for w in WINDOW_SIZES:\n",
        "        for mname in MODEL_NAMES:\n",
        "            print(\"=\"*60)\n",
        "            print(f\"Running Window={w} Model={mname}\")\n",
        "            try:\n",
        "                res = run_experiment_for_window(df, w, mname, train_pids, val_pids, test_pids)\n",
        "                if res is not None:\n",
        "                    results.append(res)\n",
        "                    pd.DataFrame(results).to_csv(OUTPUT_DIR/\"results_summary.csv\", index=False)\n",
        "            except Exception as e:\n",
        "                print(\"Error running\", w, mname, e)\n",
        "\n",
        "    if results:\n",
        "        pd.DataFrame(results).to_csv(OUTPUT_DIR/\"results_summary.csv\", index=False)\n",
        "        print(\"Saved results to\", OUTPUT_DIR/\"results_summary.csv\")\n",
        "    else:\n",
        "        print(\"No results to save.\")\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ipg80yG6xw_a",
        "outputId": "b3d50fe2-3028-46a6-fc79-16c457230a1d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Participants: total 100 train 70 val 15 test 15\n",
            "============================================================\n",
            "Running Window=2 Model=ANN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 879, np.int64(1): 846, np.int64(0): 235})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 879, np.int64(0): 879, np.int64(2): 879})\n",
            "[W2] ANN Epoch 01 | train_loss 1.1238 acc 0.398 f1 0.396 || val_loss 1.1818 acc 0.274 f1 0.266\n",
            "[W2] ANN Epoch 02 | train_loss 0.9576 acc 0.514 f1 0.505 || val_loss 1.1786 acc 0.310 f1 0.296\n",
            "[W2] ANN Epoch 03 | train_loss 0.8963 acc 0.553 f1 0.543 || val_loss 1.1767 acc 0.350 f1 0.320\n",
            "[W2] ANN Epoch 04 | train_loss 0.8247 acc 0.601 f1 0.594 || val_loss 1.1620 acc 0.350 f1 0.314\n",
            "[W2] ANN Epoch 05 | train_loss 0.7570 acc 0.637 f1 0.632 || val_loss 1.1716 acc 0.379 f1 0.331\n",
            "[W2] ANN Epoch 06 | train_loss 0.7207 acc 0.650 f1 0.645 || val_loss 1.2087 acc 0.379 f1 0.319\n",
            "[W2] ANN Epoch 07 | train_loss 0.6841 acc 0.674 f1 0.670 || val_loss 1.2106 acc 0.388 f1 0.337\n",
            "[W2] ANN Epoch 08 | train_loss 0.6473 acc 0.688 f1 0.685 || val_loss 1.2775 acc 0.379 f1 0.320\n",
            "[W2] ANN Epoch 09 | train_loss 0.6341 acc 0.691 f1 0.689 || val_loss 1.2882 acc 0.374 f1 0.322\n",
            "[W2] ANN Epoch 10 | train_loss 0.6182 acc 0.703 f1 0.702 || val_loss 1.2601 acc 0.412 f1 0.355\n",
            "[W2] ANN Epoch 11 | train_loss 0.5887 acc 0.722 f1 0.721 || val_loss 1.2936 acc 0.386 f1 0.330\n",
            "[W2] ANN Epoch 12 | train_loss 0.5763 acc 0.725 f1 0.724 || val_loss 1.3469 acc 0.402 f1 0.343\n",
            "[W2] ANN Epoch 13 | train_loss 0.5644 acc 0.735 f1 0.734 || val_loss 1.3082 acc 0.395 f1 0.328\n",
            "[W2] ANN Epoch 14 | train_loss 0.5721 acc 0.730 f1 0.730 || val_loss 1.3536 acc 0.379 f1 0.324\n",
            "[W2] ANN Epoch 15 | train_loss 0.5420 acc 0.746 f1 0.745 || val_loss 1.3274 acc 0.390 f1 0.330\n",
            "[W2] ANN Epoch 16 | train_loss 0.5093 acc 0.768 f1 0.768 || val_loss 1.3773 acc 0.402 f1 0.329\n",
            "[W2] ANN Epoch 17 | train_loss 0.5169 acc 0.761 f1 0.761 || val_loss 1.4129 acc 0.393 f1 0.316\n",
            "[W2] ANN Epoch 18 | train_loss 0.4850 acc 0.790 f1 0.789 || val_loss 1.4454 acc 0.400 f1 0.332\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=2 Model=CNN1D\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 879, np.int64(1): 846, np.int64(0): 235})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 879, np.int64(0): 879, np.int64(2): 879})\n",
            "Error running 2 CNN1D Calculated padded input size per channel: (1). Kernel size: (2). Kernel size can't be greater than actual input size\n",
            "============================================================\n",
            "Running Window=2 Model=RNN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 879, np.int64(1): 846, np.int64(0): 235})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 879, np.int64(0): 879, np.int64(2): 879})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W2] RNN Epoch 01 | train_loss 1.0954 acc 0.353 f1 0.339 || val_loss 1.1019 acc 0.321 f1 0.304\n",
            "[W2] RNN Epoch 02 | train_loss 1.0761 acc 0.421 f1 0.416 || val_loss 1.1055 acc 0.321 f1 0.305\n",
            "[W2] RNN Epoch 03 | train_loss 1.0559 acc 0.451 f1 0.446 || val_loss 1.1059 acc 0.324 f1 0.307\n",
            "[W2] RNN Epoch 04 | train_loss 1.0421 acc 0.455 f1 0.449 || val_loss 1.1171 acc 0.329 f1 0.313\n",
            "[W2] RNN Epoch 05 | train_loss 1.0306 acc 0.462 f1 0.453 || val_loss 1.1213 acc 0.333 f1 0.311\n",
            "[W2] RNN Epoch 06 | train_loss 1.0138 acc 0.487 f1 0.481 || val_loss 1.1239 acc 0.324 f1 0.305\n",
            "[W2] RNN Epoch 07 | train_loss 0.9993 acc 0.495 f1 0.488 || val_loss 1.1134 acc 0.362 f1 0.328\n",
            "[W2] RNN Epoch 08 | train_loss 0.9857 acc 0.508 f1 0.499 || val_loss 1.1212 acc 0.331 f1 0.304\n",
            "[W2] RNN Epoch 09 | train_loss 0.9590 acc 0.528 f1 0.521 || val_loss 1.1228 acc 0.340 f1 0.312\n",
            "[W2] RNN Epoch 10 | train_loss 0.9449 acc 0.536 f1 0.528 || val_loss 1.1179 acc 0.355 f1 0.326\n",
            "[W2] RNN Epoch 11 | train_loss 0.9328 acc 0.555 f1 0.550 || val_loss 1.1344 acc 0.321 f1 0.298\n",
            "[W2] RNN Epoch 12 | train_loss 0.9125 acc 0.567 f1 0.559 || val_loss 1.1284 acc 0.355 f1 0.321\n",
            "[W2] RNN Epoch 13 | train_loss 0.8870 acc 0.592 f1 0.586 || val_loss 1.1345 acc 0.376 f1 0.335\n",
            "[W2] RNN Epoch 14 | train_loss 0.8764 acc 0.577 f1 0.569 || val_loss 1.1269 acc 0.367 f1 0.335\n",
            "[W2] RNN Epoch 15 | train_loss 0.8511 acc 0.601 f1 0.592 || val_loss 1.1493 acc 0.350 f1 0.319\n",
            "[W2] RNN Epoch 16 | train_loss 0.8315 acc 0.613 f1 0.604 || val_loss 1.1331 acc 0.393 f1 0.346\n",
            "[W2] RNN Epoch 17 | train_loss 0.8132 acc 0.609 f1 0.601 || val_loss 1.1743 acc 0.364 f1 0.331\n",
            "[W2] RNN Epoch 18 | train_loss 0.7974 acc 0.626 f1 0.619 || val_loss 1.1748 acc 0.379 f1 0.341\n",
            "[W2] RNN Epoch 19 | train_loss 0.7933 acc 0.627 f1 0.618 || val_loss 1.1655 acc 0.407 f1 0.360\n",
            "[W2] RNN Epoch 20 | train_loss 0.7634 acc 0.647 f1 0.640 || val_loss 1.1993 acc 0.400 f1 0.358\n",
            "[W2] RNN Epoch 21 | train_loss 0.7446 acc 0.652 f1 0.644 || val_loss 1.2015 acc 0.405 f1 0.360\n",
            "[W2] RNN Epoch 22 | train_loss 0.7332 acc 0.659 f1 0.653 || val_loss 1.2139 acc 0.393 f1 0.351\n",
            "[W2] RNN Epoch 23 | train_loss 0.7034 acc 0.675 f1 0.669 || val_loss 1.2254 acc 0.393 f1 0.342\n",
            "[W2] RNN Epoch 24 | train_loss 0.6896 acc 0.676 f1 0.669 || val_loss 1.2557 acc 0.398 f1 0.353\n",
            "[W2] RNN Epoch 25 | train_loss 0.6804 acc 0.684 f1 0.679 || val_loss 1.2574 acc 0.390 f1 0.341\n",
            "[W2] RNN Epoch 26 | train_loss 0.6535 acc 0.696 f1 0.690 || val_loss 1.2848 acc 0.398 f1 0.346\n",
            "[W2] RNN Epoch 27 | train_loss 0.6494 acc 0.699 f1 0.693 || val_loss 1.3008 acc 0.381 f1 0.326\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=2 Model=GRU\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 879, np.int64(1): 846, np.int64(0): 235})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 879, np.int64(0): 879, np.int64(2): 879})\n",
            "[W2] GRU Epoch 01 | train_loss 1.0971 acc 0.357 f1 0.316 || val_loss 1.0997 acc 0.317 f1 0.298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W2] GRU Epoch 02 | train_loss 1.0864 acc 0.421 f1 0.409 || val_loss 1.0872 acc 0.374 f1 0.342\n",
            "[W2] GRU Epoch 03 | train_loss 1.0707 acc 0.453 f1 0.446 || val_loss 1.0767 acc 0.395 f1 0.365\n",
            "[W2] GRU Epoch 04 | train_loss 1.0442 acc 0.479 f1 0.468 || val_loss 1.0805 acc 0.362 f1 0.335\n",
            "[W2] GRU Epoch 05 | train_loss 1.0102 acc 0.491 f1 0.478 || val_loss 1.1135 acc 0.336 f1 0.318\n",
            "[W2] GRU Epoch 06 | train_loss 0.9756 acc 0.522 f1 0.504 || val_loss 1.1266 acc 0.360 f1 0.336\n",
            "[W2] GRU Epoch 07 | train_loss 0.9385 acc 0.545 f1 0.531 || val_loss 1.1253 acc 0.371 f1 0.337\n",
            "[W2] GRU Epoch 08 | train_loss 0.8960 acc 0.567 f1 0.557 || val_loss 1.1606 acc 0.357 f1 0.330\n",
            "[W2] GRU Epoch 09 | train_loss 0.8511 acc 0.586 f1 0.574 || val_loss 1.1568 acc 0.417 f1 0.373\n",
            "[W2] GRU Epoch 10 | train_loss 0.8186 acc 0.614 f1 0.606 || val_loss 1.1796 acc 0.386 f1 0.348\n",
            "[W2] GRU Epoch 11 | train_loss 0.7826 acc 0.623 f1 0.615 || val_loss 1.1929 acc 0.412 f1 0.355\n",
            "[W2] GRU Epoch 12 | train_loss 0.7476 acc 0.640 f1 0.632 || val_loss 1.2337 acc 0.388 f1 0.333\n",
            "[W2] GRU Epoch 13 | train_loss 0.7190 acc 0.656 f1 0.649 || val_loss 1.2489 acc 0.376 f1 0.319\n",
            "[W2] GRU Epoch 14 | train_loss 0.6941 acc 0.671 f1 0.665 || val_loss 1.2832 acc 0.395 f1 0.333\n",
            "[W2] GRU Epoch 15 | train_loss 0.6682 acc 0.666 f1 0.660 || val_loss 1.2925 acc 0.400 f1 0.339\n",
            "[W2] GRU Epoch 16 | train_loss 0.6449 acc 0.684 f1 0.679 || val_loss 1.3379 acc 0.400 f1 0.344\n",
            "[W2] GRU Epoch 17 | train_loss 0.6258 acc 0.694 f1 0.688 || val_loss 1.3819 acc 0.402 f1 0.338\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=2 Model=LSTM\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 879, np.int64(1): 846, np.int64(0): 235})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 879, np.int64(0): 879, np.int64(2): 879})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W2] LSTM Epoch 01 | train_loss 1.1003 acc 0.344 f1 0.240 || val_loss 1.1031 acc 0.321 f1 0.251\n",
            "[W2] LSTM Epoch 02 | train_loss 1.0935 acc 0.394 f1 0.357 || val_loss 1.1042 acc 0.336 f1 0.313\n",
            "[W2] LSTM Epoch 03 | train_loss 1.0844 acc 0.410 f1 0.396 || val_loss 1.1025 acc 0.345 f1 0.332\n",
            "[W2] LSTM Epoch 04 | train_loss 1.0654 acc 0.443 f1 0.436 || val_loss 1.0996 acc 0.326 f1 0.315\n",
            "[W2] LSTM Epoch 05 | train_loss 1.0399 acc 0.458 f1 0.450 || val_loss 1.1203 acc 0.312 f1 0.300\n",
            "[W2] LSTM Epoch 06 | train_loss 0.9975 acc 0.495 f1 0.484 || val_loss 1.1144 acc 0.340 f1 0.315\n",
            "[W2] LSTM Epoch 07 | train_loss 0.9483 acc 0.532 f1 0.522 || val_loss 1.1308 acc 0.379 f1 0.344\n",
            "[W2] LSTM Epoch 08 | train_loss 0.8815 acc 0.568 f1 0.558 || val_loss 1.1578 acc 0.360 f1 0.325\n",
            "[W2] LSTM Epoch 09 | train_loss 0.8264 acc 0.597 f1 0.586 || val_loss 1.1892 acc 0.386 f1 0.344\n",
            "[W2] LSTM Epoch 10 | train_loss 0.7700 acc 0.623 f1 0.615 || val_loss 1.2214 acc 0.393 f1 0.337\n",
            "[W2] LSTM Epoch 11 | train_loss 0.7368 acc 0.645 f1 0.637 || val_loss 1.2801 acc 0.383 f1 0.338\n",
            "[W2] LSTM Epoch 12 | train_loss 0.7044 acc 0.651 f1 0.645 || val_loss 1.2759 acc 0.374 f1 0.326\n",
            "[W2] LSTM Epoch 13 | train_loss 0.6694 acc 0.668 f1 0.663 || val_loss 1.2959 acc 0.395 f1 0.337\n",
            "[W2] LSTM Epoch 14 | train_loss 0.6418 acc 0.684 f1 0.680 || val_loss 1.3363 acc 0.379 f1 0.327\n",
            "[W2] LSTM Epoch 15 | train_loss 0.6171 acc 0.702 f1 0.697 || val_loss 1.3727 acc 0.364 f1 0.322\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=ANN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 850, np.int64(1): 823, np.int64(0): 217})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 850, np.int64(2): 850, np.int64(1): 850})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1391 acc 0.394 f1 0.392 || val_loss 1.1358 acc 0.321 f1 0.298\n",
            "[W3] ANN Epoch 02 | train_loss 0.9546 acc 0.533 f1 0.527 || val_loss 1.1472 acc 0.348 f1 0.308\n",
            "[W3] ANN Epoch 03 | train_loss 0.8407 acc 0.602 f1 0.597 || val_loss 1.1859 acc 0.380 f1 0.317\n",
            "[W3] ANN Epoch 04 | train_loss 0.7544 acc 0.634 f1 0.629 || val_loss 1.1940 acc 0.385 f1 0.322\n",
            "[W3] ANN Epoch 05 | train_loss 0.6585 acc 0.690 f1 0.687 || val_loss 1.2191 acc 0.437 f1 0.348\n",
            "[W3] ANN Epoch 06 | train_loss 0.6130 acc 0.717 f1 0.714 || val_loss 1.2616 acc 0.415 f1 0.335\n",
            "[W3] ANN Epoch 07 | train_loss 0.5775 acc 0.727 f1 0.726 || val_loss 1.3022 acc 0.417 f1 0.340\n",
            "[W3] ANN Epoch 08 | train_loss 0.5521 acc 0.738 f1 0.737 || val_loss 1.3495 acc 0.432 f1 0.347\n",
            "[W3] ANN Epoch 09 | train_loss 0.5302 acc 0.749 f1 0.748 || val_loss 1.3586 acc 0.415 f1 0.328\n",
            "[W3] ANN Epoch 10 | train_loss 0.5038 acc 0.771 f1 0.771 || val_loss 1.4209 acc 0.385 f1 0.315\n",
            "[W3] ANN Epoch 11 | train_loss 0.4671 acc 0.796 f1 0.796 || val_loss 1.4195 acc 0.444 f1 0.366\n",
            "[W3] ANN Epoch 12 | train_loss 0.4263 acc 0.819 f1 0.818 || val_loss 1.4552 acc 0.435 f1 0.341\n",
            "[W3] ANN Epoch 13 | train_loss 0.4103 acc 0.815 f1 0.815 || val_loss 1.4549 acc 0.430 f1 0.356\n",
            "[W3] ANN Epoch 14 | train_loss 0.3864 acc 0.835 f1 0.835 || val_loss 1.5482 acc 0.435 f1 0.359\n",
            "[W3] ANN Epoch 15 | train_loss 0.3859 acc 0.835 f1 0.834 || val_loss 1.5939 acc 0.437 f1 0.349\n",
            "[W3] ANN Epoch 16 | train_loss 0.3689 acc 0.844 f1 0.844 || val_loss 1.6001 acc 0.440 f1 0.339\n",
            "[W3] ANN Epoch 17 | train_loss 0.3587 acc 0.851 f1 0.851 || val_loss 1.6366 acc 0.422 f1 0.331\n",
            "[W3] ANN Epoch 18 | train_loss 0.3312 acc 0.856 f1 0.856 || val_loss 1.6973 acc 0.422 f1 0.315\n",
            "[W3] ANN Epoch 19 | train_loss 0.3236 acc 0.865 f1 0.865 || val_loss 1.6896 acc 0.435 f1 0.342\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 850, np.int64(1): 823, np.int64(0): 217})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 850, np.int64(2): 850, np.int64(1): 850})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0681 acc 0.407 f1 0.409 || val_loss 1.0269 acc 0.444 f1 0.382\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9171 acc 0.562 f1 0.557 || val_loss 1.0575 acc 0.410 f1 0.299\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.7721 acc 0.626 f1 0.622 || val_loss 1.1619 acc 0.393 f1 0.330\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.6670 acc 0.687 f1 0.684 || val_loss 1.2194 acc 0.393 f1 0.324\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.5986 acc 0.713 f1 0.713 || val_loss 1.3018 acc 0.405 f1 0.341\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5265 acc 0.751 f1 0.750 || val_loss 1.3803 acc 0.378 f1 0.305\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.4740 acc 0.791 f1 0.790 || val_loss 1.4599 acc 0.368 f1 0.293\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4237 acc 0.803 f1 0.803 || val_loss 1.5061 acc 0.395 f1 0.316\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.3921 acc 0.820 f1 0.820 || val_loss 1.6147 acc 0.375 f1 0.294\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 850, np.int64(1): 823, np.int64(0): 217})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 850, np.int64(2): 850, np.int64(1): 850})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0985 acc 0.342 f1 0.329 || val_loss 1.0982 acc 0.348 f1 0.300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0750 acc 0.431 f1 0.422 || val_loss 1.1025 acc 0.316 f1 0.281\n",
            "[W3] RNN Epoch 03 | train_loss 1.0496 acc 0.456 f1 0.447 || val_loss 1.1293 acc 0.323 f1 0.301\n",
            "[W3] RNN Epoch 04 | train_loss 1.0324 acc 0.461 f1 0.447 || val_loss 1.1198 acc 0.346 f1 0.312\n",
            "[W3] RNN Epoch 05 | train_loss 1.0046 acc 0.501 f1 0.497 || val_loss 1.1096 acc 0.358 f1 0.312\n",
            "[W3] RNN Epoch 06 | train_loss 0.9839 acc 0.509 f1 0.503 || val_loss 1.1276 acc 0.348 f1 0.316\n",
            "[W3] RNN Epoch 07 | train_loss 0.9503 acc 0.544 f1 0.538 || val_loss 1.1285 acc 0.363 f1 0.319\n",
            "[W3] RNN Epoch 08 | train_loss 0.9190 acc 0.557 f1 0.550 || val_loss 1.1107 acc 0.373 f1 0.321\n",
            "[W3] RNN Epoch 09 | train_loss 0.8996 acc 0.561 f1 0.554 || val_loss 1.1372 acc 0.351 f1 0.308\n",
            "[W3] RNN Epoch 10 | train_loss 0.8662 acc 0.580 f1 0.572 || val_loss 1.1369 acc 0.353 f1 0.306\n",
            "[W3] RNN Epoch 11 | train_loss 0.8397 acc 0.597 f1 0.587 || val_loss 1.1494 acc 0.353 f1 0.309\n",
            "[W3] RNN Epoch 12 | train_loss 0.8092 acc 0.610 f1 0.604 || val_loss 1.1672 acc 0.365 f1 0.320\n",
            "[W3] RNN Epoch 13 | train_loss 0.7864 acc 0.622 f1 0.613 || val_loss 1.1526 acc 0.363 f1 0.319\n",
            "[W3] RNN Epoch 14 | train_loss 0.7697 acc 0.635 f1 0.629 || val_loss 1.1862 acc 0.343 f1 0.306\n",
            "[W3] RNN Epoch 15 | train_loss 0.7331 acc 0.664 f1 0.655 || val_loss 1.1937 acc 0.360 f1 0.312\n",
            "[W3] RNN Epoch 16 | train_loss 0.7155 acc 0.672 f1 0.665 || val_loss 1.2130 acc 0.328 f1 0.282\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 850, np.int64(1): 823, np.int64(0): 217})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 850, np.int64(2): 850, np.int64(1): 850})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0963 acc 0.359 f1 0.330 || val_loss 1.0947 acc 0.333 f1 0.316\n",
            "[W3] GRU Epoch 02 | train_loss 1.0852 acc 0.408 f1 0.401 || val_loss 1.0924 acc 0.360 f1 0.336\n",
            "[W3] GRU Epoch 03 | train_loss 1.0711 acc 0.441 f1 0.435 || val_loss 1.0937 acc 0.353 f1 0.334\n",
            "[W3] GRU Epoch 04 | train_loss 1.0507 acc 0.448 f1 0.441 || val_loss 1.1035 acc 0.356 f1 0.336\n",
            "[W3] GRU Epoch 05 | train_loss 1.0209 acc 0.479 f1 0.475 || val_loss 1.0878 acc 0.368 f1 0.338\n",
            "[W3] GRU Epoch 06 | train_loss 0.9740 acc 0.525 f1 0.520 || val_loss 1.0978 acc 0.356 f1 0.325\n",
            "[W3] GRU Epoch 07 | train_loss 0.9191 acc 0.555 f1 0.543 || val_loss 1.0807 acc 0.378 f1 0.328\n",
            "[W3] GRU Epoch 08 | train_loss 0.8429 acc 0.587 f1 0.579 || val_loss 1.1050 acc 0.405 f1 0.342\n",
            "[W3] GRU Epoch 09 | train_loss 0.7861 acc 0.604 f1 0.597 || val_loss 1.1249 acc 0.410 f1 0.344\n",
            "[W3] GRU Epoch 10 | train_loss 0.7302 acc 0.646 f1 0.640 || val_loss 1.1843 acc 0.405 f1 0.327\n",
            "[W3] GRU Epoch 11 | train_loss 0.6825 acc 0.669 f1 0.663 || val_loss 1.2109 acc 0.380 f1 0.304\n",
            "[W3] GRU Epoch 12 | train_loss 0.6579 acc 0.677 f1 0.673 || val_loss 1.2552 acc 0.375 f1 0.315\n",
            "[W3] GRU Epoch 13 | train_loss 0.6169 acc 0.705 f1 0.702 || val_loss 1.2726 acc 0.393 f1 0.312\n",
            "[W3] GRU Epoch 14 | train_loss 0.5870 acc 0.713 f1 0.710 || val_loss 1.3183 acc 0.407 f1 0.314\n",
            "[W3] GRU Epoch 15 | train_loss 0.5515 acc 0.735 f1 0.732 || val_loss 1.3700 acc 0.395 f1 0.314\n",
            "[W3] GRU Epoch 16 | train_loss 0.5241 acc 0.739 f1 0.737 || val_loss 1.4066 acc 0.422 f1 0.335\n",
            "[W3] GRU Epoch 17 | train_loss 0.5013 acc 0.767 f1 0.765 || val_loss 1.4695 acc 0.402 f1 0.312\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 850, np.int64(1): 823, np.int64(0): 217})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 850, np.int64(2): 850, np.int64(1): 850})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0996 acc 0.344 f1 0.216 || val_loss 1.1069 acc 0.264 f1 0.268\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0895 acc 0.393 f1 0.382 || val_loss 1.0834 acc 0.373 f1 0.346\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0784 acc 0.420 f1 0.412 || val_loss 1.0851 acc 0.351 f1 0.329\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0581 acc 0.440 f1 0.426 || val_loss 1.0965 acc 0.333 f1 0.320\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0180 acc 0.484 f1 0.474 || val_loss 1.1197 acc 0.356 f1 0.332\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9420 acc 0.525 f1 0.512 || val_loss 1.1332 acc 0.385 f1 0.346\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8649 acc 0.576 f1 0.568 || val_loss 1.1609 acc 0.390 f1 0.327\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7907 acc 0.598 f1 0.591 || val_loss 1.1996 acc 0.368 f1 0.307\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7372 acc 0.631 f1 0.623 || val_loss 1.2368 acc 0.373 f1 0.302\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7090 acc 0.644 f1 0.639 || val_loss 1.2905 acc 0.363 f1 0.301\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6629 acc 0.662 f1 0.656 || val_loss 1.3140 acc 0.388 f1 0.312\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6350 acc 0.676 f1 0.672 || val_loss 1.3801 acc 0.400 f1 0.325\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6016 acc 0.702 f1 0.699 || val_loss 1.3875 acc 0.398 f1 0.319\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5789 acc 0.716 f1 0.713 || val_loss 1.4575 acc 0.393 f1 0.307\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=4 Model=ANN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 817, np.int64(1): 796, np.int64(0): 207})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 817, np.int64(0): 817, np.int64(1): 817})\n",
            "[W4] ANN Epoch 01 | train_loss 1.1006 acc 0.425 f1 0.423 || val_loss 1.1817 acc 0.297 f1 0.286\n",
            "[W4] ANN Epoch 02 | train_loss 0.9092 acc 0.549 f1 0.533 || val_loss 1.1844 acc 0.326 f1 0.308\n",
            "[W4] ANN Epoch 03 | train_loss 0.8096 acc 0.596 f1 0.584 || val_loss 1.1691 acc 0.354 f1 0.323\n",
            "[W4] ANN Epoch 04 | train_loss 0.7170 acc 0.656 f1 0.647 || val_loss 1.1910 acc 0.367 f1 0.330\n",
            "[W4] ANN Epoch 05 | train_loss 0.6740 acc 0.672 f1 0.667 || val_loss 1.1935 acc 0.387 f1 0.340\n",
            "[W4] ANN Epoch 06 | train_loss 0.6064 acc 0.707 f1 0.704 || val_loss 1.2049 acc 0.405 f1 0.358\n",
            "[W4] ANN Epoch 07 | train_loss 0.5631 acc 0.736 f1 0.735 || val_loss 1.2878 acc 0.390 f1 0.321\n",
            "[W4] ANN Epoch 08 | train_loss 0.5009 acc 0.777 f1 0.776 || val_loss 1.3267 acc 0.413 f1 0.354\n",
            "[W4] ANN Epoch 09 | train_loss 0.5152 acc 0.758 f1 0.757 || val_loss 1.3343 acc 0.415 f1 0.352\n",
            "[W4] ANN Epoch 10 | train_loss 0.4704 acc 0.781 f1 0.780 || val_loss 1.3811 acc 0.400 f1 0.329\n",
            "[W4] ANN Epoch 11 | train_loss 0.4482 acc 0.803 f1 0.802 || val_loss 1.3718 acc 0.431 f1 0.357\n",
            "[W4] ANN Epoch 12 | train_loss 0.4166 acc 0.813 f1 0.813 || val_loss 1.4161 acc 0.423 f1 0.340\n",
            "[W4] ANN Epoch 13 | train_loss 0.3865 acc 0.836 f1 0.836 || val_loss 1.4551 acc 0.410 f1 0.340\n",
            "[W4] ANN Epoch 14 | train_loss 0.3697 acc 0.847 f1 0.847 || val_loss 1.5922 acc 0.423 f1 0.357\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=4 Model=CNN1D\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 817, np.int64(1): 796, np.int64(0): 207})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 817, np.int64(0): 817, np.int64(1): 817})\n",
            "[W4] CNN1D Epoch 01 | train_loss 1.0539 acc 0.424 f1 0.420 || val_loss 1.0142 acc 0.469 f1 0.337\n",
            "[W4] CNN1D Epoch 02 | train_loss 0.8898 acc 0.560 f1 0.558 || val_loss 1.0829 acc 0.418 f1 0.340\n",
            "[W4] CNN1D Epoch 03 | train_loss 0.7741 acc 0.619 f1 0.616 || val_loss 1.1315 acc 0.410 f1 0.323\n",
            "[W4] CNN1D Epoch 04 | train_loss 0.6952 acc 0.666 f1 0.666 || val_loss 1.2543 acc 0.369 f1 0.324\n",
            "[W4] CNN1D Epoch 05 | train_loss 0.6383 acc 0.701 f1 0.700 || val_loss 1.2368 acc 0.392 f1 0.330\n",
            "[W4] CNN1D Epoch 06 | train_loss 0.5914 acc 0.723 f1 0.721 || val_loss 1.3197 acc 0.405 f1 0.337\n",
            "[W4] CNN1D Epoch 07 | train_loss 0.5393 acc 0.761 f1 0.759 || val_loss 1.3360 acc 0.395 f1 0.326\n",
            "[W4] CNN1D Epoch 08 | train_loss 0.5076 acc 0.772 f1 0.772 || val_loss 1.3925 acc 0.400 f1 0.316\n",
            "[W4] CNN1D Epoch 09 | train_loss 0.4626 acc 0.798 f1 0.797 || val_loss 1.3960 acc 0.405 f1 0.312\n",
            "[W4] CNN1D Epoch 10 | train_loss 0.4258 acc 0.809 f1 0.809 || val_loss 1.4684 acc 0.385 f1 0.296\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=4 Model=RNN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 817, np.int64(1): 796, np.int64(0): 207})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 817, np.int64(0): 817, np.int64(1): 817})\n",
            "[W4] RNN Epoch 01 | train_loss 1.0943 acc 0.373 f1 0.338 || val_loss 1.0999 acc 0.300 f1 0.280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W4] RNN Epoch 02 | train_loss 1.0707 acc 0.439 f1 0.430 || val_loss 1.0969 acc 0.297 f1 0.262\n",
            "[W4] RNN Epoch 03 | train_loss 1.0389 acc 0.469 f1 0.457 || val_loss 1.1149 acc 0.313 f1 0.291\n",
            "[W4] RNN Epoch 04 | train_loss 1.0099 acc 0.494 f1 0.478 || val_loss 1.1124 acc 0.305 f1 0.280\n",
            "[W4] RNN Epoch 05 | train_loss 0.9878 acc 0.508 f1 0.500 || val_loss 1.1286 acc 0.351 f1 0.336\n",
            "[W4] RNN Epoch 06 | train_loss 0.9647 acc 0.532 f1 0.521 || val_loss 1.1183 acc 0.356 f1 0.329\n",
            "[W4] RNN Epoch 07 | train_loss 0.9381 acc 0.553 f1 0.544 || val_loss 1.1265 acc 0.359 f1 0.326\n",
            "[W4] RNN Epoch 08 | train_loss 0.9086 acc 0.570 f1 0.560 || val_loss 1.1337 acc 0.379 f1 0.335\n",
            "[W4] RNN Epoch 09 | train_loss 0.8815 acc 0.590 f1 0.583 || val_loss 1.1541 acc 0.364 f1 0.331\n",
            "[W4] RNN Epoch 10 | train_loss 0.8586 acc 0.591 f1 0.582 || val_loss 1.1669 acc 0.359 f1 0.321\n",
            "[W4] RNN Epoch 11 | train_loss 0.8241 acc 0.607 f1 0.596 || val_loss 1.1725 acc 0.369 f1 0.326\n",
            "[W4] RNN Epoch 12 | train_loss 0.7938 acc 0.625 f1 0.616 || val_loss 1.2379 acc 0.333 f1 0.303\n",
            "[W4] RNN Epoch 13 | train_loss 0.7680 acc 0.637 f1 0.628 || val_loss 1.2254 acc 0.379 f1 0.338\n",
            "[W4] RNN Epoch 14 | train_loss 0.7415 acc 0.649 f1 0.642 || val_loss 1.2534 acc 0.364 f1 0.328\n",
            "[W4] RNN Epoch 15 | train_loss 0.7165 acc 0.657 f1 0.648 || val_loss 1.2613 acc 0.387 f1 0.329\n",
            "[W4] RNN Epoch 16 | train_loss 0.6889 acc 0.678 f1 0.671 || val_loss 1.3208 acc 0.374 f1 0.327\n",
            "[W4] RNN Epoch 17 | train_loss 0.6656 acc 0.687 f1 0.681 || val_loss 1.3300 acc 0.403 f1 0.348\n",
            "[W4] RNN Epoch 18 | train_loss 0.6521 acc 0.684 f1 0.677 || val_loss 1.3640 acc 0.385 f1 0.335\n",
            "[W4] RNN Epoch 19 | train_loss 0.6221 acc 0.701 f1 0.696 || val_loss 1.4100 acc 0.364 f1 0.328\n",
            "[W4] RNN Epoch 20 | train_loss 0.5946 acc 0.725 f1 0.720 || val_loss 1.4207 acc 0.379 f1 0.312\n",
            "[W4] RNN Epoch 21 | train_loss 0.5820 acc 0.723 f1 0.719 || val_loss 1.4643 acc 0.385 f1 0.342\n",
            "[W4] RNN Epoch 22 | train_loss 0.5664 acc 0.729 f1 0.726 || val_loss 1.4782 acc 0.382 f1 0.329\n",
            "[W4] RNN Epoch 23 | train_loss 0.5299 acc 0.746 f1 0.742 || val_loss 1.5208 acc 0.377 f1 0.329\n",
            "[W4] RNN Epoch 24 | train_loss 0.5214 acc 0.753 f1 0.750 || val_loss 1.5599 acc 0.392 f1 0.328\n",
            "[W4] RNN Epoch 25 | train_loss 0.5158 acc 0.748 f1 0.745 || val_loss 1.5780 acc 0.387 f1 0.333\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=4 Model=GRU\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 817, np.int64(1): 796, np.int64(0): 207})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 817, np.int64(0): 817, np.int64(1): 817})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W4] GRU Epoch 01 | train_loss 1.1033 acc 0.339 f1 0.254 || val_loss 1.1080 acc 0.249 f1 0.250\n",
            "[W4] GRU Epoch 02 | train_loss 1.0908 acc 0.387 f1 0.334 || val_loss 1.0976 acc 0.279 f1 0.259\n",
            "[W4] GRU Epoch 03 | train_loss 1.0782 acc 0.415 f1 0.385 || val_loss 1.0918 acc 0.328 f1 0.293\n",
            "[W4] GRU Epoch 04 | train_loss 1.0582 acc 0.440 f1 0.415 || val_loss 1.0826 acc 0.346 f1 0.312\n",
            "[W4] GRU Epoch 05 | train_loss 1.0171 acc 0.483 f1 0.472 || val_loss 1.1028 acc 0.351 f1 0.325\n",
            "[W4] GRU Epoch 06 | train_loss 0.9525 acc 0.516 f1 0.499 || val_loss 1.0645 acc 0.418 f1 0.381\n",
            "[W4] GRU Epoch 07 | train_loss 0.8520 acc 0.576 f1 0.568 || val_loss 1.0711 acc 0.400 f1 0.343\n",
            "[W4] GRU Epoch 08 | train_loss 0.7852 acc 0.598 f1 0.593 || val_loss 1.1022 acc 0.431 f1 0.380\n",
            "[W4] GRU Epoch 09 | train_loss 0.7323 acc 0.623 f1 0.618 || val_loss 1.1410 acc 0.421 f1 0.353\n",
            "[W4] GRU Epoch 10 | train_loss 0.6968 acc 0.633 f1 0.627 || val_loss 1.1440 acc 0.408 f1 0.354\n",
            "[W4] GRU Epoch 11 | train_loss 0.6636 acc 0.651 f1 0.649 || val_loss 1.2077 acc 0.374 f1 0.333\n",
            "[W4] GRU Epoch 12 | train_loss 0.6390 acc 0.667 f1 0.665 || val_loss 1.2238 acc 0.392 f1 0.333\n",
            "[W4] GRU Epoch 13 | train_loss 0.6203 acc 0.674 f1 0.673 || val_loss 1.2455 acc 0.382 f1 0.320\n",
            "[W4] GRU Epoch 14 | train_loss 0.5941 acc 0.695 f1 0.694 || val_loss 1.2895 acc 0.397 f1 0.341\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=4 Model=LSTM\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 817, np.int64(1): 796, np.int64(0): 207})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 817, np.int64(0): 817, np.int64(1): 817})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W4] LSTM Epoch 01 | train_loss 1.0980 acc 0.359 f1 0.282 || val_loss 1.1011 acc 0.331 f1 0.252\n",
            "[W4] LSTM Epoch 02 | train_loss 1.0900 acc 0.400 f1 0.381 || val_loss 1.0986 acc 0.303 f1 0.279\n",
            "[W4] LSTM Epoch 03 | train_loss 1.0741 acc 0.424 f1 0.399 || val_loss 1.0828 acc 0.354 f1 0.304\n",
            "[W4] LSTM Epoch 04 | train_loss 1.0390 acc 0.459 f1 0.433 || val_loss 1.0989 acc 0.356 f1 0.331\n",
            "[W4] LSTM Epoch 05 | train_loss 0.9737 acc 0.515 f1 0.504 || val_loss 1.0731 acc 0.377 f1 0.334\n",
            "[W4] LSTM Epoch 06 | train_loss 0.8644 acc 0.568 f1 0.560 || val_loss 1.1022 acc 0.403 f1 0.337\n",
            "[W4] LSTM Epoch 07 | train_loss 0.7794 acc 0.603 f1 0.594 || val_loss 1.1545 acc 0.395 f1 0.334\n",
            "[W4] LSTM Epoch 08 | train_loss 0.7258 acc 0.637 f1 0.634 || val_loss 1.1981 acc 0.392 f1 0.315\n",
            "[W4] LSTM Epoch 09 | train_loss 0.6895 acc 0.643 f1 0.640 || val_loss 1.2434 acc 0.364 f1 0.322\n",
            "[W4] LSTM Epoch 10 | train_loss 0.6508 acc 0.667 f1 0.662 || val_loss 1.2542 acc 0.390 f1 0.323\n",
            "[W4] LSTM Epoch 11 | train_loss 0.6193 acc 0.678 f1 0.675 || val_loss 1.2833 acc 0.369 f1 0.326\n",
            "[W4] LSTM Epoch 12 | train_loss 0.5975 acc 0.688 f1 0.684 || val_loss 1.3162 acc 0.385 f1 0.331\n",
            "[W4] LSTM Epoch 13 | train_loss 0.5684 acc 0.707 f1 0.705 || val_loss 1.3975 acc 0.374 f1 0.330\n",
            "[W4] LSTM Epoch 14 | train_loss 0.5466 acc 0.714 f1 0.713 || val_loss 1.3795 acc 0.385 f1 0.329\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=5 Model=ANN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 786, np.int64(1): 760, np.int64(0): 204})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 786, np.int64(2): 786, np.int64(1): 786})\n",
            "[W5] ANN Epoch 01 | train_loss 1.1236 acc 0.433 f1 0.426 || val_loss 1.1890 acc 0.245 f1 0.240\n",
            "[W5] ANN Epoch 02 | train_loss 0.9175 acc 0.549 f1 0.535 || val_loss 1.2080 acc 0.296 f1 0.281\n",
            "[W5] ANN Epoch 03 | train_loss 0.7860 acc 0.618 f1 0.608 || val_loss 1.1933 acc 0.323 f1 0.287\n",
            "[W5] ANN Epoch 04 | train_loss 0.7054 acc 0.648 f1 0.640 || val_loss 1.1770 acc 0.384 f1 0.327\n",
            "[W5] ANN Epoch 05 | train_loss 0.6314 acc 0.699 f1 0.695 || val_loss 1.1916 acc 0.408 f1 0.326\n",
            "[W5] ANN Epoch 06 | train_loss 0.5606 acc 0.736 f1 0.734 || val_loss 1.2491 acc 0.389 f1 0.308\n",
            "[W5] ANN Epoch 07 | train_loss 0.5138 acc 0.762 f1 0.761 || val_loss 1.3275 acc 0.395 f1 0.324\n",
            "[W5] ANN Epoch 08 | train_loss 0.4630 acc 0.794 f1 0.793 || val_loss 1.3255 acc 0.419 f1 0.342\n",
            "[W5] ANN Epoch 09 | train_loss 0.4353 acc 0.810 f1 0.809 || val_loss 1.3632 acc 0.408 f1 0.313\n",
            "[W5] ANN Epoch 10 | train_loss 0.3955 acc 0.832 f1 0.831 || val_loss 1.3646 acc 0.408 f1 0.322\n",
            "[W5] ANN Epoch 11 | train_loss 0.3873 acc 0.837 f1 0.836 || val_loss 1.5058 acc 0.408 f1 0.322\n",
            "[W5] ANN Epoch 12 | train_loss 0.3658 acc 0.839 f1 0.840 || val_loss 1.5379 acc 0.413 f1 0.326\n",
            "[W5] ANN Epoch 13 | train_loss 0.3511 acc 0.852 f1 0.852 || val_loss 1.5831 acc 0.421 f1 0.332\n",
            "[W5] ANN Epoch 14 | train_loss 0.3071 acc 0.873 f1 0.872 || val_loss 1.6286 acc 0.416 f1 0.327\n",
            "[W5] ANN Epoch 15 | train_loss 0.3019 acc 0.875 f1 0.875 || val_loss 1.6073 acc 0.427 f1 0.335\n",
            "[W5] ANN Epoch 16 | train_loss 0.2520 acc 0.901 f1 0.901 || val_loss 1.6753 acc 0.427 f1 0.339\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=5 Model=CNN1D\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 786, np.int64(1): 760, np.int64(0): 204})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 786, np.int64(2): 786, np.int64(1): 786})\n",
            "[W5] CNN1D Epoch 01 | train_loss 1.0701 acc 0.413 f1 0.416 || val_loss 1.0279 acc 0.408 f1 0.295\n",
            "[W5] CNN1D Epoch 02 | train_loss 0.9238 acc 0.540 f1 0.537 || val_loss 1.0654 acc 0.419 f1 0.292\n",
            "[W5] CNN1D Epoch 03 | train_loss 0.8163 acc 0.588 f1 0.587 || val_loss 1.1370 acc 0.363 f1 0.302\n",
            "[W5] CNN1D Epoch 04 | train_loss 0.7477 acc 0.626 f1 0.626 || val_loss 1.1983 acc 0.376 f1 0.326\n",
            "[W5] CNN1D Epoch 05 | train_loss 0.7034 acc 0.662 f1 0.661 || val_loss 1.1752 acc 0.416 f1 0.342\n",
            "[W5] CNN1D Epoch 06 | train_loss 0.6662 acc 0.679 f1 0.680 || val_loss 1.2508 acc 0.363 f1 0.290\n",
            "[W5] CNN1D Epoch 07 | train_loss 0.6298 acc 0.703 f1 0.702 || val_loss 1.2640 acc 0.397 f1 0.329\n",
            "[W5] CNN1D Epoch 08 | train_loss 0.5949 acc 0.727 f1 0.727 || val_loss 1.2547 acc 0.405 f1 0.348\n",
            "[W5] CNN1D Epoch 09 | train_loss 0.5442 acc 0.761 f1 0.761 || val_loss 1.3135 acc 0.400 f1 0.338\n",
            "[W5] CNN1D Epoch 10 | train_loss 0.5201 acc 0.771 f1 0.771 || val_loss 1.4067 acc 0.379 f1 0.319\n",
            "[W5] CNN1D Epoch 11 | train_loss 0.4863 acc 0.776 f1 0.775 || val_loss 1.4751 acc 0.405 f1 0.301\n",
            "[W5] CNN1D Epoch 12 | train_loss 0.4577 acc 0.802 f1 0.802 || val_loss 1.4152 acc 0.376 f1 0.306\n",
            "[W5] CNN1D Epoch 13 | train_loss 0.4098 acc 0.818 f1 0.819 || val_loss 1.5325 acc 0.403 f1 0.344\n",
            "[W5] CNN1D Epoch 14 | train_loss 0.4112 acc 0.818 f1 0.818 || val_loss 1.5210 acc 0.395 f1 0.320\n",
            "[W5] CNN1D Epoch 15 | train_loss 0.3869 acc 0.832 f1 0.831 || val_loss 1.5298 acc 0.419 f1 0.341\n",
            "[W5] CNN1D Epoch 16 | train_loss 0.3657 acc 0.844 f1 0.845 || val_loss 1.5960 acc 0.397 f1 0.333\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=5 Model=RNN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 786, np.int64(1): 760, np.int64(0): 204})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 786, np.int64(2): 786, np.int64(1): 786})\n",
            "[W5] RNN Epoch 01 | train_loss 1.0950 acc 0.372 f1 0.336 || val_loss 1.1052 acc 0.299 f1 0.280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W5] RNN Epoch 02 | train_loss 1.0727 acc 0.452 f1 0.440 || val_loss 1.1067 acc 0.299 f1 0.277\n",
            "[W5] RNN Epoch 03 | train_loss 1.0409 acc 0.478 f1 0.466 || val_loss 1.1358 acc 0.299 f1 0.282\n",
            "[W5] RNN Epoch 04 | train_loss 1.0046 acc 0.487 f1 0.472 || val_loss 1.1601 acc 0.296 f1 0.283\n",
            "[W5] RNN Epoch 05 | train_loss 0.9818 acc 0.503 f1 0.484 || val_loss 1.1814 acc 0.312 f1 0.297\n",
            "[W5] RNN Epoch 06 | train_loss 0.9580 acc 0.522 f1 0.505 || val_loss 1.1773 acc 0.320 f1 0.298\n",
            "[W5] RNN Epoch 07 | train_loss 0.9293 acc 0.543 f1 0.529 || val_loss 1.1817 acc 0.357 f1 0.321\n",
            "[W5] RNN Epoch 08 | train_loss 0.8945 acc 0.569 f1 0.555 || val_loss 1.1994 acc 0.376 f1 0.336\n",
            "[W5] RNN Epoch 09 | train_loss 0.8629 acc 0.589 f1 0.577 || val_loss 1.2207 acc 0.357 f1 0.322\n",
            "[W5] RNN Epoch 10 | train_loss 0.8291 acc 0.598 f1 0.584 || val_loss 1.2239 acc 0.373 f1 0.327\n",
            "[W5] RNN Epoch 11 | train_loss 0.7976 acc 0.623 f1 0.613 || val_loss 1.2605 acc 0.360 f1 0.326\n",
            "[W5] RNN Epoch 12 | train_loss 0.7717 acc 0.627 f1 0.616 || val_loss 1.2456 acc 0.397 f1 0.350\n",
            "[W5] RNN Epoch 13 | train_loss 0.7283 acc 0.638 f1 0.628 || val_loss 1.2777 acc 0.395 f1 0.350\n",
            "[W5] RNN Epoch 14 | train_loss 0.7079 acc 0.654 f1 0.644 || val_loss 1.3006 acc 0.397 f1 0.353\n",
            "[W5] RNN Epoch 15 | train_loss 0.6772 acc 0.684 f1 0.679 || val_loss 1.2970 acc 0.405 f1 0.369\n",
            "[W5] RNN Epoch 16 | train_loss 0.6513 acc 0.676 f1 0.669 || val_loss 1.3253 acc 0.387 f1 0.329\n",
            "[W5] RNN Epoch 17 | train_loss 0.6174 acc 0.695 f1 0.688 || val_loss 1.3666 acc 0.397 f1 0.343\n",
            "[W5] RNN Epoch 18 | train_loss 0.5894 acc 0.716 f1 0.712 || val_loss 1.4255 acc 0.387 f1 0.348\n",
            "[W5] RNN Epoch 19 | train_loss 0.5769 acc 0.721 f1 0.716 || val_loss 1.4267 acc 0.411 f1 0.343\n",
            "[W5] RNN Epoch 20 | train_loss 0.5562 acc 0.715 f1 0.712 || val_loss 1.4265 acc 0.392 f1 0.354\n",
            "[W5] RNN Epoch 21 | train_loss 0.5286 acc 0.731 f1 0.727 || val_loss 1.4778 acc 0.389 f1 0.352\n",
            "[W5] RNN Epoch 22 | train_loss 0.5193 acc 0.737 f1 0.734 || val_loss 1.5385 acc 0.379 f1 0.319\n",
            "[W5] RNN Epoch 23 | train_loss 0.5036 acc 0.744 f1 0.742 || val_loss 1.6162 acc 0.395 f1 0.331\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=5 Model=GRU\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 786, np.int64(1): 760, np.int64(0): 204})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 786, np.int64(2): 786, np.int64(1): 786})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W5] GRU Epoch 01 | train_loss 1.0965 acc 0.342 f1 0.319 || val_loss 1.0972 acc 0.307 f1 0.272\n",
            "[W5] GRU Epoch 02 | train_loss 1.0846 acc 0.426 f1 0.403 || val_loss 1.0899 acc 0.357 f1 0.311\n",
            "[W5] GRU Epoch 03 | train_loss 1.0678 acc 0.450 f1 0.442 || val_loss 1.0952 acc 0.347 f1 0.315\n",
            "[W5] GRU Epoch 04 | train_loss 1.0362 acc 0.464 f1 0.452 || val_loss 1.1095 acc 0.339 f1 0.313\n",
            "[W5] GRU Epoch 05 | train_loss 0.9839 acc 0.506 f1 0.495 || val_loss 1.1101 acc 0.357 f1 0.321\n",
            "[W5] GRU Epoch 06 | train_loss 0.9039 acc 0.548 f1 0.536 || val_loss 1.0930 acc 0.389 f1 0.334\n",
            "[W5] GRU Epoch 07 | train_loss 0.8257 acc 0.572 f1 0.564 || val_loss 1.1338 acc 0.384 f1 0.323\n",
            "[W5] GRU Epoch 08 | train_loss 0.7696 acc 0.594 f1 0.592 || val_loss 1.1904 acc 0.384 f1 0.336\n",
            "[W5] GRU Epoch 09 | train_loss 0.7332 acc 0.631 f1 0.628 || val_loss 1.1837 acc 0.387 f1 0.311\n",
            "[W5] GRU Epoch 10 | train_loss 0.6935 acc 0.644 f1 0.640 || val_loss 1.1872 acc 0.405 f1 0.329\n",
            "[W5] GRU Epoch 11 | train_loss 0.6727 acc 0.650 f1 0.648 || val_loss 1.2534 acc 0.392 f1 0.306\n",
            "[W5] GRU Epoch 12 | train_loss 0.6453 acc 0.655 f1 0.652 || val_loss 1.2602 acc 0.392 f1 0.299\n",
            "[W5] GRU Epoch 13 | train_loss 0.6231 acc 0.676 f1 0.674 || val_loss 1.3377 acc 0.373 f1 0.297\n",
            "[W5] GRU Epoch 14 | train_loss 0.6010 acc 0.679 f1 0.677 || val_loss 1.3304 acc 0.381 f1 0.297\n",
            "[W5] GRU Epoch 15 | train_loss 0.5877 acc 0.697 f1 0.695 || val_loss 1.3459 acc 0.403 f1 0.305\n",
            "[W5] GRU Epoch 16 | train_loss 0.5715 acc 0.701 f1 0.700 || val_loss 1.3647 acc 0.384 f1 0.294\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=5 Model=LSTM\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 786, np.int64(1): 760, np.int64(0): 204})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 786, np.int64(2): 786, np.int64(1): 786})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W5] LSTM Epoch 01 | train_loss 1.0981 acc 0.335 f1 0.277 || val_loss 1.0925 acc 0.405 f1 0.277\n",
            "[W5] LSTM Epoch 02 | train_loss 1.0931 acc 0.375 f1 0.342 || val_loss 1.0868 acc 0.387 f1 0.300\n",
            "[W5] LSTM Epoch 03 | train_loss 1.0842 acc 0.422 f1 0.419 || val_loss 1.0863 acc 0.339 f1 0.310\n",
            "[W5] LSTM Epoch 04 | train_loss 1.0564 acc 0.464 f1 0.460 || val_loss 1.1064 acc 0.328 f1 0.312\n",
            "[W5] LSTM Epoch 05 | train_loss 0.9897 acc 0.490 f1 0.480 || val_loss 1.1199 acc 0.331 f1 0.303\n",
            "[W5] LSTM Epoch 06 | train_loss 0.8818 acc 0.555 f1 0.549 || val_loss 1.0831 acc 0.371 f1 0.295\n",
            "[W5] LSTM Epoch 07 | train_loss 0.7993 acc 0.588 f1 0.587 || val_loss 1.1058 acc 0.365 f1 0.303\n",
            "[W5] LSTM Epoch 08 | train_loss 0.7461 acc 0.614 f1 0.612 || val_loss 1.1795 acc 0.363 f1 0.298\n",
            "[W5] LSTM Epoch 09 | train_loss 0.7155 acc 0.631 f1 0.628 || val_loss 1.1920 acc 0.371 f1 0.284\n",
            "[W5] LSTM Epoch 10 | train_loss 0.6870 acc 0.656 f1 0.654 || val_loss 1.2224 acc 0.365 f1 0.303\n",
            "[W5] LSTM Epoch 11 | train_loss 0.6662 acc 0.648 f1 0.648 || val_loss 1.2267 acc 0.355 f1 0.285\n",
            "[W5] LSTM Epoch 12 | train_loss 0.6401 acc 0.674 f1 0.672 || val_loss 1.3116 acc 0.363 f1 0.304\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=6 Model=ANN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 755, np.int64(1): 731, np.int64(0): 194})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 755, np.int64(1): 755, np.int64(0): 755})\n",
            "[W6] ANN Epoch 01 | train_loss 1.1242 acc 0.395 f1 0.392 || val_loss 1.1380 acc 0.333 f1 0.320\n",
            "[W6] ANN Epoch 02 | train_loss 0.8877 acc 0.566 f1 0.556 || val_loss 1.1607 acc 0.342 f1 0.312\n",
            "[W6] ANN Epoch 03 | train_loss 0.7456 acc 0.626 f1 0.615 || val_loss 1.1616 acc 0.358 f1 0.315\n",
            "[W6] ANN Epoch 04 | train_loss 0.6632 acc 0.679 f1 0.673 || val_loss 1.2176 acc 0.364 f1 0.293\n",
            "[W6] ANN Epoch 05 | train_loss 0.5875 acc 0.711 f1 0.708 || val_loss 1.2466 acc 0.350 f1 0.274\n",
            "[W6] ANN Epoch 06 | train_loss 0.5265 acc 0.758 f1 0.756 || val_loss 1.2923 acc 0.364 f1 0.292\n",
            "[W6] ANN Epoch 07 | train_loss 0.4728 acc 0.781 f1 0.779 || val_loss 1.3289 acc 0.397 f1 0.317\n",
            "[W6] ANN Epoch 08 | train_loss 0.4467 acc 0.795 f1 0.793 || val_loss 1.3718 acc 0.394 f1 0.300\n",
            "[W6] ANN Epoch 09 | train_loss 0.3981 acc 0.820 f1 0.820 || val_loss 1.4139 acc 0.392 f1 0.299\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=6 Model=CNN1D\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 755, np.int64(1): 731, np.int64(0): 194})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 755, np.int64(1): 755, np.int64(0): 755})\n",
            "[W6] CNN1D Epoch 01 | train_loss 1.0640 acc 0.436 f1 0.428 || val_loss 1.0069 acc 0.444 f1 0.255\n",
            "[W6] CNN1D Epoch 02 | train_loss 0.9022 acc 0.527 f1 0.524 || val_loss 1.0390 acc 0.411 f1 0.260\n",
            "[W6] CNN1D Epoch 03 | train_loss 0.7944 acc 0.596 f1 0.597 || val_loss 1.0659 acc 0.428 f1 0.326\n",
            "[W6] CNN1D Epoch 04 | train_loss 0.7443 acc 0.626 f1 0.624 || val_loss 1.1199 acc 0.403 f1 0.331\n",
            "[W6] CNN1D Epoch 05 | train_loss 0.6980 acc 0.652 f1 0.651 || val_loss 1.1280 acc 0.417 f1 0.348\n",
            "[W6] CNN1D Epoch 06 | train_loss 0.6693 acc 0.674 f1 0.673 || val_loss 1.1529 acc 0.397 f1 0.324\n",
            "[W6] CNN1D Epoch 07 | train_loss 0.6379 acc 0.686 f1 0.685 || val_loss 1.2506 acc 0.361 f1 0.310\n",
            "[W6] CNN1D Epoch 08 | train_loss 0.6189 acc 0.696 f1 0.695 || val_loss 1.2047 acc 0.419 f1 0.345\n",
            "[W6] CNN1D Epoch 09 | train_loss 0.5879 acc 0.730 f1 0.729 || val_loss 1.2206 acc 0.425 f1 0.350\n",
            "[W6] CNN1D Epoch 10 | train_loss 0.5618 acc 0.732 f1 0.731 || val_loss 1.2611 acc 0.422 f1 0.348\n",
            "[W6] CNN1D Epoch 11 | train_loss 0.5537 acc 0.746 f1 0.746 || val_loss 1.3034 acc 0.408 f1 0.341\n",
            "[W6] CNN1D Epoch 12 | train_loss 0.5171 acc 0.758 f1 0.757 || val_loss 1.3308 acc 0.417 f1 0.336\n",
            "[W6] CNN1D Epoch 13 | train_loss 0.5333 acc 0.768 f1 0.768 || val_loss 1.4062 acc 0.400 f1 0.349\n",
            "[W6] CNN1D Epoch 14 | train_loss 0.4827 acc 0.783 f1 0.782 || val_loss 1.4050 acc 0.381 f1 0.308\n",
            "[W6] CNN1D Epoch 15 | train_loss 0.4621 acc 0.797 f1 0.797 || val_loss 1.4080 acc 0.408 f1 0.348\n",
            "[W6] CNN1D Epoch 16 | train_loss 0.4454 acc 0.803 f1 0.802 || val_loss 1.4303 acc 0.431 f1 0.351\n",
            "[W6] CNN1D Epoch 17 | train_loss 0.4481 acc 0.806 f1 0.806 || val_loss 1.4811 acc 0.397 f1 0.343\n",
            "[W6] CNN1D Epoch 18 | train_loss 0.4377 acc 0.809 f1 0.809 || val_loss 1.5540 acc 0.389 f1 0.354\n",
            "[W6] CNN1D Epoch 19 | train_loss 0.4180 acc 0.814 f1 0.813 || val_loss 1.5060 acc 0.425 f1 0.369\n",
            "[W6] CNN1D Epoch 20 | train_loss 0.3875 acc 0.831 f1 0.830 || val_loss 1.4942 acc 0.422 f1 0.355\n",
            "[W6] CNN1D Epoch 21 | train_loss 0.3802 acc 0.839 f1 0.839 || val_loss 1.5452 acc 0.428 f1 0.369\n",
            "[W6] CNN1D Epoch 22 | train_loss 0.3838 acc 0.830 f1 0.830 || val_loss 1.5632 acc 0.433 f1 0.355\n",
            "[W6] CNN1D Epoch 23 | train_loss 0.3599 acc 0.853 f1 0.853 || val_loss 1.6421 acc 0.406 f1 0.343\n",
            "[W6] CNN1D Epoch 24 | train_loss 0.3379 acc 0.860 f1 0.860 || val_loss 1.6563 acc 0.422 f1 0.378\n",
            "[W6] CNN1D Epoch 25 | train_loss 0.3286 acc 0.858 f1 0.858 || val_loss 1.6206 acc 0.428 f1 0.380\n",
            "[W6] CNN1D Epoch 26 | train_loss 0.3274 acc 0.868 f1 0.868 || val_loss 1.7454 acc 0.406 f1 0.352\n",
            "[W6] CNN1D Epoch 27 | train_loss 0.3272 acc 0.858 f1 0.858 || val_loss 1.7863 acc 0.439 f1 0.338\n",
            "[W6] CNN1D Epoch 28 | train_loss 0.3241 acc 0.864 f1 0.864 || val_loss 1.7459 acc 0.419 f1 0.357\n",
            "[W6] CNN1D Epoch 29 | train_loss 0.3022 acc 0.873 f1 0.873 || val_loss 1.8028 acc 0.406 f1 0.356\n",
            "[W6] CNN1D Epoch 30 | train_loss 0.2822 acc 0.881 f1 0.881 || val_loss 1.8058 acc 0.422 f1 0.365\n",
            "[W6] CNN1D Epoch 31 | train_loss 0.2596 acc 0.889 f1 0.889 || val_loss 1.9348 acc 0.433 f1 0.375\n",
            "[W6] CNN1D Epoch 32 | train_loss 0.2589 acc 0.891 f1 0.891 || val_loss 1.9454 acc 0.389 f1 0.333\n",
            "[W6] CNN1D Epoch 33 | train_loss 0.2569 acc 0.896 f1 0.896 || val_loss 2.0134 acc 0.425 f1 0.374\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=6 Model=RNN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 755, np.int64(1): 731, np.int64(0): 194})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 755, np.int64(1): 755, np.int64(0): 755})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W6] RNN Epoch 01 | train_loss 1.0990 acc 0.351 f1 0.345 || val_loss 1.0967 acc 0.381 f1 0.352\n",
            "[W6] RNN Epoch 02 | train_loss 1.0746 acc 0.445 f1 0.437 || val_loss 1.1000 acc 0.350 f1 0.329\n",
            "[W6] RNN Epoch 03 | train_loss 1.0411 acc 0.473 f1 0.457 || val_loss 1.1173 acc 0.339 f1 0.319\n",
            "[W6] RNN Epoch 04 | train_loss 0.9978 acc 0.510 f1 0.498 || val_loss 1.1362 acc 0.372 f1 0.347\n",
            "[W6] RNN Epoch 05 | train_loss 0.9721 acc 0.509 f1 0.499 || val_loss 1.1624 acc 0.336 f1 0.317\n",
            "[W6] RNN Epoch 06 | train_loss 0.9387 acc 0.531 f1 0.520 || val_loss 1.1633 acc 0.344 f1 0.322\n",
            "[W6] RNN Epoch 07 | train_loss 0.9237 acc 0.549 f1 0.539 || val_loss 1.1437 acc 0.375 f1 0.336\n",
            "[W6] RNN Epoch 08 | train_loss 0.8908 acc 0.570 f1 0.561 || val_loss 1.1493 acc 0.383 f1 0.337\n",
            "[W6] RNN Epoch 09 | train_loss 0.8620 acc 0.589 f1 0.582 || val_loss 1.1497 acc 0.394 f1 0.342\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=6 Model=GRU\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 755, np.int64(1): 731, np.int64(0): 194})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 755, np.int64(1): 755, np.int64(0): 755})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W6] GRU Epoch 01 | train_loss 1.0986 acc 0.334 f1 0.191 || val_loss 1.0873 acc 0.383 f1 0.255\n",
            "[W6] GRU Epoch 02 | train_loss 1.0921 acc 0.377 f1 0.352 || val_loss 1.0873 acc 0.400 f1 0.326\n",
            "[W6] GRU Epoch 03 | train_loss 1.0810 acc 0.423 f1 0.417 || val_loss 1.0921 acc 0.344 f1 0.306\n",
            "[W6] GRU Epoch 04 | train_loss 1.0604 acc 0.469 f1 0.466 || val_loss 1.1029 acc 0.336 f1 0.316\n",
            "[W6] GRU Epoch 05 | train_loss 1.0258 acc 0.473 f1 0.465 || val_loss 1.0737 acc 0.369 f1 0.310\n",
            "[W6] GRU Epoch 06 | train_loss 0.9564 acc 0.534 f1 0.527 || val_loss 1.0914 acc 0.372 f1 0.329\n",
            "[W6] GRU Epoch 07 | train_loss 0.8430 acc 0.597 f1 0.590 || val_loss 1.0879 acc 0.433 f1 0.327\n",
            "[W6] GRU Epoch 08 | train_loss 0.7829 acc 0.606 f1 0.601 || val_loss 1.1026 acc 0.406 f1 0.330\n",
            "[W6] GRU Epoch 09 | train_loss 0.7187 acc 0.644 f1 0.640 || val_loss 1.1346 acc 0.394 f1 0.332\n",
            "[W6] GRU Epoch 10 | train_loss 0.6910 acc 0.647 f1 0.645 || val_loss 1.1842 acc 0.408 f1 0.332\n",
            "[W6] GRU Epoch 11 | train_loss 0.6503 acc 0.669 f1 0.666 || val_loss 1.1763 acc 0.408 f1 0.348\n",
            "[W6] GRU Epoch 12 | train_loss 0.6255 acc 0.688 f1 0.686 || val_loss 1.2318 acc 0.406 f1 0.335\n",
            "[W6] GRU Epoch 13 | train_loss 0.6055 acc 0.704 f1 0.703 || val_loss 1.2540 acc 0.389 f1 0.317\n",
            "[W6] GRU Epoch 14 | train_loss 0.5763 acc 0.712 f1 0.710 || val_loss 1.2781 acc 0.403 f1 0.333\n",
            "[W6] GRU Epoch 15 | train_loss 0.5600 acc 0.724 f1 0.723 || val_loss 1.3212 acc 0.406 f1 0.322\n",
            "[W6] GRU Epoch 16 | train_loss 0.5377 acc 0.733 f1 0.731 || val_loss 1.3672 acc 0.394 f1 0.320\n",
            "[W6] GRU Epoch 17 | train_loss 0.5288 acc 0.733 f1 0.733 || val_loss 1.4055 acc 0.394 f1 0.321\n",
            "[W6] GRU Epoch 18 | train_loss 0.5040 acc 0.757 f1 0.757 || val_loss 1.4273 acc 0.386 f1 0.309\n",
            "[W6] GRU Epoch 19 | train_loss 0.4866 acc 0.764 f1 0.764 || val_loss 1.4840 acc 0.400 f1 0.326\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=6 Model=LSTM\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 755, np.int64(1): 731, np.int64(0): 194})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 755, np.int64(1): 755, np.int64(0): 755})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W6] LSTM Epoch 01 | train_loss 1.1004 acc 0.332 f1 0.172 || val_loss 1.0959 acc 0.417 f1 0.196\n",
            "[W6] LSTM Epoch 02 | train_loss 1.0928 acc 0.371 f1 0.324 || val_loss 1.1000 acc 0.297 f1 0.251\n",
            "[W6] LSTM Epoch 03 | train_loss 1.0773 acc 0.431 f1 0.405 || val_loss 1.0910 acc 0.350 f1 0.323\n",
            "[W6] LSTM Epoch 04 | train_loss 1.0371 acc 0.471 f1 0.456 || val_loss 1.0909 acc 0.378 f1 0.352\n",
            "[W6] LSTM Epoch 05 | train_loss 0.9318 acc 0.539 f1 0.530 || val_loss 1.0580 acc 0.386 f1 0.333\n",
            "[W6] LSTM Epoch 06 | train_loss 0.8460 acc 0.574 f1 0.567 || val_loss 1.0391 acc 0.442 f1 0.355\n",
            "[W6] LSTM Epoch 07 | train_loss 0.7762 acc 0.610 f1 0.604 || val_loss 1.1602 acc 0.386 f1 0.329\n",
            "[W6] LSTM Epoch 08 | train_loss 0.7386 acc 0.627 f1 0.626 || val_loss 1.1184 acc 0.411 f1 0.326\n",
            "[W6] LSTM Epoch 09 | train_loss 0.7154 acc 0.630 f1 0.627 || val_loss 1.1450 acc 0.394 f1 0.332\n",
            "[W6] LSTM Epoch 10 | train_loss 0.6811 acc 0.660 f1 0.658 || val_loss 1.1440 acc 0.417 f1 0.331\n",
            "[W6] LSTM Epoch 11 | train_loss 0.6599 acc 0.660 f1 0.657 || val_loss 1.1649 acc 0.394 f1 0.320\n",
            "[W6] LSTM Epoch 12 | train_loss 0.6271 acc 0.672 f1 0.669 || val_loss 1.2416 acc 0.392 f1 0.342\n",
            "[W6] LSTM Epoch 13 | train_loss 0.6183 acc 0.683 f1 0.682 || val_loss 1.2301 acc 0.386 f1 0.315\n",
            "[W6] LSTM Epoch 14 | train_loss 0.5963 acc 0.691 f1 0.690 || val_loss 1.2924 acc 0.367 f1 0.324\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=7 Model=ANN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 724, np.int64(1): 706, np.int64(0): 180})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 724, np.int64(1): 724, np.int64(0): 724})\n",
            "[W7] ANN Epoch 01 | train_loss 1.1176 acc 0.424 f1 0.422 || val_loss 1.1297 acc 0.287 f1 0.280\n",
            "[W7] ANN Epoch 02 | train_loss 0.8698 acc 0.577 f1 0.566 || val_loss 1.1513 acc 0.357 f1 0.337\n",
            "[W7] ANN Epoch 03 | train_loss 0.7555 acc 0.637 f1 0.625 || val_loss 1.1869 acc 0.339 f1 0.315\n",
            "[W7] ANN Epoch 04 | train_loss 0.6309 acc 0.713 f1 0.706 || val_loss 1.1778 acc 0.354 f1 0.314\n",
            "[W7] ANN Epoch 05 | train_loss 0.5398 acc 0.754 f1 0.751 || val_loss 1.2115 acc 0.362 f1 0.322\n",
            "[W7] ANN Epoch 06 | train_loss 0.4919 acc 0.769 f1 0.767 || val_loss 1.2397 acc 0.388 f1 0.329\n",
            "[W7] ANN Epoch 07 | train_loss 0.4556 acc 0.793 f1 0.792 || val_loss 1.3373 acc 0.377 f1 0.315\n",
            "[W7] ANN Epoch 08 | train_loss 0.3876 acc 0.836 f1 0.835 || val_loss 1.3352 acc 0.383 f1 0.319\n",
            "[W7] ANN Epoch 09 | train_loss 0.3367 acc 0.872 f1 0.871 || val_loss 1.4947 acc 0.374 f1 0.306\n",
            "[W7] ANN Epoch 10 | train_loss 0.3057 acc 0.880 f1 0.879 || val_loss 1.5113 acc 0.383 f1 0.330\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=7 Model=CNN1D\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 724, np.int64(1): 706, np.int64(0): 180})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 724, np.int64(1): 724, np.int64(0): 724})\n",
            "[W7] CNN1D Epoch 01 | train_loss 1.0407 acc 0.463 f1 0.467 || val_loss 1.0003 acc 0.475 f1 0.326\n",
            "[W7] CNN1D Epoch 02 | train_loss 0.8679 acc 0.551 f1 0.550 || val_loss 1.0577 acc 0.429 f1 0.265\n",
            "[W7] CNN1D Epoch 03 | train_loss 0.7961 acc 0.587 f1 0.586 || val_loss 1.0678 acc 0.397 f1 0.307\n",
            "[W7] CNN1D Epoch 04 | train_loss 0.7420 acc 0.619 f1 0.619 || val_loss 1.1732 acc 0.391 f1 0.271\n",
            "[W7] CNN1D Epoch 05 | train_loss 0.7223 acc 0.635 f1 0.637 || val_loss 1.1679 acc 0.365 f1 0.274\n",
            "[W7] CNN1D Epoch 06 | train_loss 0.6888 acc 0.652 f1 0.652 || val_loss 1.1879 acc 0.406 f1 0.347\n",
            "[W7] CNN1D Epoch 07 | train_loss 0.6624 acc 0.676 f1 0.675 || val_loss 1.2076 acc 0.386 f1 0.326\n",
            "[W7] CNN1D Epoch 08 | train_loss 0.6418 acc 0.698 f1 0.698 || val_loss 1.2334 acc 0.383 f1 0.332\n",
            "[W7] CNN1D Epoch 09 | train_loss 0.6136 acc 0.701 f1 0.700 || val_loss 1.2218 acc 0.412 f1 0.315\n",
            "[W7] CNN1D Epoch 10 | train_loss 0.5908 acc 0.728 f1 0.729 || val_loss 1.2116 acc 0.443 f1 0.351\n",
            "[W7] CNN1D Epoch 11 | train_loss 0.5647 acc 0.729 f1 0.728 || val_loss 1.2928 acc 0.400 f1 0.302\n",
            "[W7] CNN1D Epoch 12 | train_loss 0.5471 acc 0.754 f1 0.754 || val_loss 1.3305 acc 0.380 f1 0.323\n",
            "[W7] CNN1D Epoch 13 | train_loss 0.5170 acc 0.779 f1 0.779 || val_loss 1.3694 acc 0.400 f1 0.344\n",
            "[W7] CNN1D Epoch 14 | train_loss 0.5089 acc 0.771 f1 0.770 || val_loss 1.3326 acc 0.423 f1 0.334\n",
            "[W7] CNN1D Epoch 15 | train_loss 0.4906 acc 0.777 f1 0.777 || val_loss 1.3709 acc 0.423 f1 0.362\n",
            "[W7] CNN1D Epoch 16 | train_loss 0.4709 acc 0.785 f1 0.784 || val_loss 1.3614 acc 0.435 f1 0.357\n",
            "[W7] CNN1D Epoch 17 | train_loss 0.4667 acc 0.793 f1 0.793 || val_loss 1.4390 acc 0.420 f1 0.308\n",
            "[W7] CNN1D Epoch 18 | train_loss 0.4573 acc 0.789 f1 0.788 || val_loss 1.4035 acc 0.438 f1 0.356\n",
            "[W7] CNN1D Epoch 19 | train_loss 0.4435 acc 0.801 f1 0.801 || val_loss 1.4075 acc 0.414 f1 0.330\n",
            "[W7] CNN1D Epoch 20 | train_loss 0.4238 acc 0.822 f1 0.821 || val_loss 1.4762 acc 0.449 f1 0.354\n",
            "[W7] CNN1D Epoch 21 | train_loss 0.4061 acc 0.825 f1 0.824 || val_loss 1.4950 acc 0.406 f1 0.329\n",
            "[W7] CNN1D Epoch 22 | train_loss 0.3979 acc 0.835 f1 0.835 || val_loss 1.4792 acc 0.412 f1 0.328\n",
            "[W7] CNN1D Epoch 23 | train_loss 0.3805 acc 0.841 f1 0.842 || val_loss 1.5525 acc 0.412 f1 0.346\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=7 Model=RNN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 724, np.int64(1): 706, np.int64(0): 180})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 724, np.int64(1): 724, np.int64(0): 724})\n",
            "[W7] RNN Epoch 01 | train_loss 1.0961 acc 0.360 f1 0.292 || val_loss 1.0972 acc 0.403 f1 0.318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W7] RNN Epoch 02 | train_loss 1.0736 acc 0.435 f1 0.436 || val_loss 1.1079 acc 0.339 f1 0.305\n",
            "[W7] RNN Epoch 03 | train_loss 1.0408 acc 0.475 f1 0.469 || val_loss 1.1344 acc 0.275 f1 0.265\n",
            "[W7] RNN Epoch 04 | train_loss 1.0030 acc 0.519 f1 0.511 || val_loss 1.1236 acc 0.313 f1 0.289\n",
            "[W7] RNN Epoch 05 | train_loss 0.9618 acc 0.534 f1 0.521 || val_loss 1.1380 acc 0.325 f1 0.297\n",
            "[W7] RNN Epoch 06 | train_loss 0.9180 acc 0.562 f1 0.552 || val_loss 1.1381 acc 0.351 f1 0.305\n",
            "[W7] RNN Epoch 07 | train_loss 0.8819 acc 0.583 f1 0.572 || val_loss 1.1549 acc 0.371 f1 0.327\n",
            "[W7] RNN Epoch 08 | train_loss 0.8456 acc 0.594 f1 0.584 || val_loss 1.1533 acc 0.365 f1 0.312\n",
            "[W7] RNN Epoch 09 | train_loss 0.8135 acc 0.615 f1 0.607 || val_loss 1.1737 acc 0.374 f1 0.315\n",
            "[W7] RNN Epoch 10 | train_loss 0.7671 acc 0.634 f1 0.626 || val_loss 1.1821 acc 0.371 f1 0.303\n",
            "[W7] RNN Epoch 11 | train_loss 0.7382 acc 0.650 f1 0.643 || val_loss 1.2049 acc 0.362 f1 0.292\n",
            "[W7] RNN Epoch 12 | train_loss 0.7082 acc 0.664 f1 0.659 || val_loss 1.2391 acc 0.345 f1 0.281\n",
            "[W7] RNN Epoch 13 | train_loss 0.6751 acc 0.682 f1 0.676 || val_loss 1.2852 acc 0.354 f1 0.286\n",
            "[W7] RNN Epoch 14 | train_loss 0.6506 acc 0.692 f1 0.687 || val_loss 1.2776 acc 0.342 f1 0.289\n",
            "[W7] RNN Epoch 15 | train_loss 0.6114 acc 0.701 f1 0.697 || val_loss 1.2887 acc 0.388 f1 0.322\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=7 Model=GRU\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 724, np.int64(1): 706, np.int64(0): 180})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 724, np.int64(1): 724, np.int64(0): 724})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W7] GRU Epoch 01 | train_loss 1.0978 acc 0.353 f1 0.293 || val_loss 1.1049 acc 0.299 f1 0.283\n",
            "[W7] GRU Epoch 02 | train_loss 1.0881 acc 0.419 f1 0.413 || val_loss 1.0924 acc 0.348 f1 0.309\n",
            "[W7] GRU Epoch 03 | train_loss 1.0730 acc 0.442 f1 0.438 || val_loss 1.0920 acc 0.322 f1 0.288\n",
            "[W7] GRU Epoch 04 | train_loss 1.0452 acc 0.464 f1 0.453 || val_loss 1.1063 acc 0.316 f1 0.287\n",
            "[W7] GRU Epoch 05 | train_loss 0.9889 acc 0.500 f1 0.491 || val_loss 1.0961 acc 0.362 f1 0.307\n",
            "[W7] GRU Epoch 06 | train_loss 0.8931 acc 0.554 f1 0.548 || val_loss 1.1885 acc 0.336 f1 0.307\n",
            "[W7] GRU Epoch 07 | train_loss 0.8188 acc 0.589 f1 0.586 || val_loss 1.1327 acc 0.394 f1 0.326\n",
            "[W7] GRU Epoch 08 | train_loss 0.7713 acc 0.610 f1 0.609 || val_loss 1.1620 acc 0.409 f1 0.337\n",
            "[W7] GRU Epoch 09 | train_loss 0.7290 acc 0.631 f1 0.629 || val_loss 1.2120 acc 0.380 f1 0.320\n",
            "[W7] GRU Epoch 10 | train_loss 0.7064 acc 0.634 f1 0.632 || val_loss 1.2044 acc 0.394 f1 0.322\n",
            "[W7] GRU Epoch 11 | train_loss 0.6748 acc 0.656 f1 0.655 || val_loss 1.2358 acc 0.406 f1 0.341\n",
            "[W7] GRU Epoch 12 | train_loss 0.6579 acc 0.658 f1 0.656 || val_loss 1.2312 acc 0.397 f1 0.309\n",
            "[W7] GRU Epoch 13 | train_loss 0.6386 acc 0.680 f1 0.679 || val_loss 1.2673 acc 0.403 f1 0.321\n",
            "[W7] GRU Epoch 14 | train_loss 0.6205 acc 0.693 f1 0.693 || val_loss 1.2856 acc 0.394 f1 0.299\n",
            "[W7] GRU Epoch 15 | train_loss 0.6047 acc 0.694 f1 0.692 || val_loss 1.3320 acc 0.429 f1 0.315\n",
            "[W7] GRU Epoch 16 | train_loss 0.5861 acc 0.691 f1 0.689 || val_loss 1.3563 acc 0.414 f1 0.310\n",
            "[W7] GRU Epoch 17 | train_loss 0.5676 acc 0.710 f1 0.708 || val_loss 1.3859 acc 0.417 f1 0.327\n",
            "[W7] GRU Epoch 18 | train_loss 0.5473 acc 0.720 f1 0.719 || val_loss 1.4454 acc 0.394 f1 0.308\n",
            "[W7] GRU Epoch 19 | train_loss 0.5259 acc 0.741 f1 0.739 || val_loss 1.4668 acc 0.397 f1 0.324\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=7 Model=LSTM\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 724, np.int64(1): 706, np.int64(0): 180})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 724, np.int64(1): 724, np.int64(0): 724})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W7] LSTM Epoch 01 | train_loss 1.0980 acc 0.344 f1 0.254 || val_loss 1.1012 acc 0.272 f1 0.249\n",
            "[W7] LSTM Epoch 02 | train_loss 1.0902 acc 0.404 f1 0.376 || val_loss 1.1048 acc 0.290 f1 0.257\n",
            "[W7] LSTM Epoch 03 | train_loss 1.0724 acc 0.424 f1 0.350 || val_loss 1.0944 acc 0.325 f1 0.249\n",
            "[W7] LSTM Epoch 04 | train_loss 1.0238 acc 0.465 f1 0.410 || val_loss 1.0576 acc 0.368 f1 0.246\n",
            "[W7] LSTM Epoch 05 | train_loss 0.9146 acc 0.516 f1 0.472 || val_loss 1.0873 acc 0.359 f1 0.319\n",
            "[W7] LSTM Epoch 06 | train_loss 0.8053 acc 0.568 f1 0.563 || val_loss 1.1283 acc 0.354 f1 0.292\n",
            "[W7] LSTM Epoch 07 | train_loss 0.7536 acc 0.613 f1 0.607 || val_loss 1.1278 acc 0.380 f1 0.312\n",
            "[W7] LSTM Epoch 08 | train_loss 0.7095 acc 0.616 f1 0.608 || val_loss 1.1295 acc 0.394 f1 0.303\n",
            "[W7] LSTM Epoch 09 | train_loss 0.6971 acc 0.629 f1 0.628 || val_loss 1.1558 acc 0.374 f1 0.298\n",
            "[W7] LSTM Epoch 10 | train_loss 0.6606 acc 0.635 f1 0.634 || val_loss 1.1957 acc 0.354 f1 0.286\n",
            "[W7] LSTM Epoch 11 | train_loss 0.6438 acc 0.655 f1 0.653 || val_loss 1.1816 acc 0.391 f1 0.305\n",
            "[W7] LSTM Epoch 12 | train_loss 0.6241 acc 0.646 f1 0.642 || val_loss 1.2255 acc 0.383 f1 0.306\n",
            "[W7] LSTM Epoch 13 | train_loss 0.6050 acc 0.668 f1 0.666 || val_loss 1.2099 acc 0.374 f1 0.297\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=10 Model=ANN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 634, np.int64(1): 616, np.int64(0): 150})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 634, np.int64(2): 634, np.int64(0): 634})\n",
            "[W10] ANN Epoch 01 | train_loss 1.0548 acc 0.450 f1 0.443 || val_loss 1.0992 acc 0.347 f1 0.314\n",
            "[W10] ANN Epoch 02 | train_loss 0.8004 acc 0.598 f1 0.586 || val_loss 1.0944 acc 0.403 f1 0.368\n",
            "[W10] ANN Epoch 03 | train_loss 0.6399 acc 0.698 f1 0.691 || val_loss 1.1354 acc 0.370 f1 0.319\n",
            "[W10] ANN Epoch 04 | train_loss 0.5486 acc 0.754 f1 0.752 || val_loss 1.1947 acc 0.410 f1 0.332\n",
            "[W10] ANN Epoch 05 | train_loss 0.4666 acc 0.788 f1 0.787 || val_loss 1.2262 acc 0.460 f1 0.378\n",
            "[W10] ANN Epoch 06 | train_loss 0.4059 acc 0.825 f1 0.825 || val_loss 1.2785 acc 0.447 f1 0.361\n",
            "[W10] ANN Epoch 07 | train_loss 0.3592 acc 0.854 f1 0.855 || val_loss 1.3546 acc 0.447 f1 0.368\n",
            "[W10] ANN Epoch 08 | train_loss 0.3077 acc 0.880 f1 0.879 || val_loss 1.4162 acc 0.463 f1 0.362\n",
            "[W10] ANN Epoch 09 | train_loss 0.2548 acc 0.903 f1 0.903 || val_loss 1.5194 acc 0.453 f1 0.376\n",
            "[W10] ANN Epoch 10 | train_loss 0.2315 acc 0.912 f1 0.912 || val_loss 1.6682 acc 0.410 f1 0.316\n",
            "[W10] ANN Epoch 11 | train_loss 0.1755 acc 0.936 f1 0.936 || val_loss 1.7873 acc 0.427 f1 0.330\n",
            "[W10] ANN Epoch 12 | train_loss 0.1704 acc 0.930 f1 0.930 || val_loss 1.9082 acc 0.417 f1 0.319\n",
            "[W10] ANN Epoch 13 | train_loss 0.1424 acc 0.948 f1 0.949 || val_loss 2.0248 acc 0.443 f1 0.354\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=10 Model=CNN1D\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 634, np.int64(1): 616, np.int64(0): 150})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 634, np.int64(2): 634, np.int64(0): 634})\n",
            "[W10] CNN1D Epoch 01 | train_loss 1.0522 acc 0.443 f1 0.445 || val_loss 0.9995 acc 0.447 f1 0.312\n",
            "[W10] CNN1D Epoch 02 | train_loss 0.8760 acc 0.550 f1 0.554 || val_loss 1.0267 acc 0.453 f1 0.299\n",
            "[W10] CNN1D Epoch 03 | train_loss 0.7950 acc 0.597 f1 0.599 || val_loss 1.0743 acc 0.377 f1 0.323\n",
            "[W10] CNN1D Epoch 04 | train_loss 0.7682 acc 0.599 f1 0.601 || val_loss 1.0909 acc 0.347 f1 0.286\n",
            "[W10] CNN1D Epoch 05 | train_loss 0.7308 acc 0.625 f1 0.626 || val_loss 1.0864 acc 0.397 f1 0.289\n",
            "[W10] CNN1D Epoch 06 | train_loss 0.7227 acc 0.618 f1 0.619 || val_loss 1.1599 acc 0.357 f1 0.318\n",
            "[W10] CNN1D Epoch 07 | train_loss 0.7125 acc 0.634 f1 0.636 || val_loss 1.1351 acc 0.400 f1 0.352\n",
            "[W10] CNN1D Epoch 08 | train_loss 0.6910 acc 0.653 f1 0.653 || val_loss 1.1316 acc 0.410 f1 0.363\n",
            "[W10] CNN1D Epoch 09 | train_loss 0.6776 acc 0.659 f1 0.660 || val_loss 1.1484 acc 0.410 f1 0.338\n",
            "[W10] CNN1D Epoch 10 | train_loss 0.6595 acc 0.672 f1 0.673 || val_loss 1.1693 acc 0.397 f1 0.301\n",
            "[W10] CNN1D Epoch 11 | train_loss 0.6489 acc 0.679 f1 0.678 || val_loss 1.1632 acc 0.427 f1 0.325\n",
            "[W10] CNN1D Epoch 12 | train_loss 0.6388 acc 0.687 f1 0.688 || val_loss 1.1414 acc 0.443 f1 0.358\n",
            "[W10] CNN1D Epoch 13 | train_loss 0.6241 acc 0.700 f1 0.700 || val_loss 1.1876 acc 0.420 f1 0.338\n",
            "[W10] CNN1D Epoch 14 | train_loss 0.6060 acc 0.719 f1 0.721 || val_loss 1.1860 acc 0.430 f1 0.365\n",
            "[W10] CNN1D Epoch 15 | train_loss 0.6106 acc 0.698 f1 0.699 || val_loss 1.1953 acc 0.420 f1 0.357\n",
            "[W10] CNN1D Epoch 16 | train_loss 0.5838 acc 0.727 f1 0.726 || val_loss 1.2504 acc 0.410 f1 0.339\n",
            "[W10] CNN1D Epoch 17 | train_loss 0.5718 acc 0.712 f1 0.712 || val_loss 1.2205 acc 0.450 f1 0.392\n",
            "[W10] CNN1D Epoch 18 | train_loss 0.5753 acc 0.732 f1 0.733 || val_loss 1.3233 acc 0.363 f1 0.325\n",
            "[W10] CNN1D Epoch 19 | train_loss 0.5566 acc 0.733 f1 0.733 || val_loss 1.3499 acc 0.377 f1 0.332\n",
            "[W10] CNN1D Epoch 20 | train_loss 0.5544 acc 0.729 f1 0.728 || val_loss 1.3357 acc 0.403 f1 0.313\n",
            "[W10] CNN1D Epoch 21 | train_loss 0.5289 acc 0.763 f1 0.763 || val_loss 1.2561 acc 0.427 f1 0.367\n",
            "[W10] CNN1D Epoch 22 | train_loss 0.5309 acc 0.746 f1 0.745 || val_loss 1.3016 acc 0.430 f1 0.363\n",
            "[W10] CNN1D Epoch 23 | train_loss 0.5146 acc 0.761 f1 0.761 || val_loss 1.3762 acc 0.417 f1 0.341\n",
            "[W10] CNN1D Epoch 24 | train_loss 0.4959 acc 0.766 f1 0.766 || val_loss 1.3585 acc 0.417 f1 0.345\n",
            "[W10] CNN1D Epoch 25 | train_loss 0.4825 acc 0.777 f1 0.776 || val_loss 1.3558 acc 0.443 f1 0.386\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=10 Model=RNN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 634, np.int64(1): 616, np.int64(0): 150})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 634, np.int64(2): 634, np.int64(0): 634})\n",
            "[W10] RNN Epoch 01 | train_loss 1.0914 acc 0.395 f1 0.328 || val_loss 1.1012 acc 0.277 f1 0.233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W10] RNN Epoch 02 | train_loss 1.0539 acc 0.467 f1 0.436 || val_loss 1.0908 acc 0.283 f1 0.264\n",
            "[W10] RNN Epoch 03 | train_loss 1.0093 acc 0.501 f1 0.480 || val_loss 1.1068 acc 0.303 f1 0.278\n",
            "[W10] RNN Epoch 04 | train_loss 0.9518 acc 0.537 f1 0.523 || val_loss 1.1631 acc 0.313 f1 0.298\n",
            "[W10] RNN Epoch 05 | train_loss 0.9054 acc 0.549 f1 0.532 || val_loss 1.1993 acc 0.293 f1 0.275\n",
            "[W10] RNN Epoch 06 | train_loss 0.8550 acc 0.593 f1 0.578 || val_loss 1.2212 acc 0.317 f1 0.289\n",
            "[W10] RNN Epoch 07 | train_loss 0.8239 acc 0.600 f1 0.586 || val_loss 1.2642 acc 0.307 f1 0.283\n",
            "[W10] RNN Epoch 08 | train_loss 0.7958 acc 0.614 f1 0.602 || val_loss 1.2686 acc 0.327 f1 0.285\n",
            "[W10] RNN Epoch 09 | train_loss 0.7617 acc 0.631 f1 0.619 || val_loss 1.3056 acc 0.307 f1 0.278\n",
            "[W10] RNN Epoch 10 | train_loss 0.7319 acc 0.638 f1 0.627 || val_loss 1.3138 acc 0.327 f1 0.285\n",
            "[W10] RNN Epoch 11 | train_loss 0.6975 acc 0.669 f1 0.660 || val_loss 1.3508 acc 0.337 f1 0.283\n",
            "[W10] RNN Epoch 12 | train_loss 0.6748 acc 0.660 f1 0.651 || val_loss 1.3855 acc 0.357 f1 0.304\n",
            "[W10] RNN Epoch 13 | train_loss 0.6520 acc 0.686 f1 0.679 || val_loss 1.4228 acc 0.347 f1 0.300\n",
            "[W10] RNN Epoch 14 | train_loss 0.6234 acc 0.690 f1 0.684 || val_loss 1.4784 acc 0.363 f1 0.313\n",
            "[W10] RNN Epoch 15 | train_loss 0.6018 acc 0.710 f1 0.705 || val_loss 1.5050 acc 0.373 f1 0.325\n",
            "[W10] RNN Epoch 16 | train_loss 0.5814 acc 0.712 f1 0.706 || val_loss 1.5439 acc 0.380 f1 0.316\n",
            "[W10] RNN Epoch 17 | train_loss 0.5571 acc 0.722 f1 0.719 || val_loss 1.5855 acc 0.387 f1 0.325\n",
            "[W10] RNN Epoch 18 | train_loss 0.5373 acc 0.731 f1 0.727 || val_loss 1.6719 acc 0.377 f1 0.313\n",
            "[W10] RNN Epoch 19 | train_loss 0.5272 acc 0.733 f1 0.730 || val_loss 1.6687 acc 0.393 f1 0.320\n",
            "[W10] RNN Epoch 20 | train_loss 0.5058 acc 0.739 f1 0.736 || val_loss 1.6957 acc 0.357 f1 0.298\n",
            "[W10] RNN Epoch 21 | train_loss 0.5016 acc 0.739 f1 0.737 || val_loss 1.7178 acc 0.370 f1 0.306\n",
            "[W10] RNN Epoch 22 | train_loss 0.4905 acc 0.747 f1 0.745 || val_loss 1.7284 acc 0.380 f1 0.327\n",
            "[W10] RNN Epoch 23 | train_loss 0.4697 acc 0.752 f1 0.751 || val_loss 1.7857 acc 0.377 f1 0.313\n",
            "[W10] RNN Epoch 24 | train_loss 0.4515 acc 0.773 f1 0.772 || val_loss 1.8553 acc 0.397 f1 0.328\n",
            "[W10] RNN Epoch 25 | train_loss 0.4457 acc 0.780 f1 0.780 || val_loss 1.9120 acc 0.377 f1 0.319\n",
            "[W10] RNN Epoch 26 | train_loss 0.4255 acc 0.784 f1 0.783 || val_loss 1.9173 acc 0.383 f1 0.311\n",
            "[W10] RNN Epoch 27 | train_loss 0.4312 acc 0.782 f1 0.782 || val_loss 1.9467 acc 0.390 f1 0.334\n",
            "[W10] RNN Epoch 28 | train_loss 0.4267 acc 0.781 f1 0.780 || val_loss 1.9905 acc 0.357 f1 0.308\n",
            "[W10] RNN Epoch 29 | train_loss 0.4076 acc 0.798 f1 0.797 || val_loss 2.0768 acc 0.377 f1 0.305\n",
            "[W10] RNN Epoch 30 | train_loss 0.3945 acc 0.815 f1 0.815 || val_loss 2.1073 acc 0.377 f1 0.313\n",
            "[W10] RNN Epoch 31 | train_loss 0.4003 acc 0.807 f1 0.806 || val_loss 2.0969 acc 0.363 f1 0.314\n",
            "[W10] RNN Epoch 32 | train_loss 0.3882 acc 0.820 f1 0.819 || val_loss 2.2015 acc 0.390 f1 0.322\n",
            "[W10] RNN Epoch 33 | train_loss 0.3733 acc 0.821 f1 0.820 || val_loss 2.2561 acc 0.417 f1 0.335\n",
            "[W10] RNN Epoch 34 | train_loss 0.3614 acc 0.831 f1 0.831 || val_loss 2.3159 acc 0.370 f1 0.307\n",
            "[W10] RNN Epoch 35 | train_loss 0.3581 acc 0.841 f1 0.840 || val_loss 2.3082 acc 0.377 f1 0.305\n",
            "[W10] RNN Epoch 36 | train_loss 0.3440 acc 0.833 f1 0.833 || val_loss 2.3784 acc 0.387 f1 0.313\n",
            "[W10] RNN Epoch 37 | train_loss 0.3397 acc 0.851 f1 0.850 || val_loss 2.5034 acc 0.400 f1 0.336\n",
            "[W10] RNN Epoch 38 | train_loss 0.3288 acc 0.851 f1 0.851 || val_loss 2.4666 acc 0.403 f1 0.332\n",
            "[W10] RNN Epoch 39 | train_loss 0.3057 acc 0.881 f1 0.881 || val_loss 2.5227 acc 0.390 f1 0.322\n",
            "[W10] RNN Epoch 40 | train_loss 0.2976 acc 0.868 f1 0.867 || val_loss 2.5700 acc 0.397 f1 0.327\n",
            "[W10] RNN Epoch 41 | train_loss 0.2986 acc 0.873 f1 0.872 || val_loss 2.5693 acc 0.410 f1 0.337\n",
            "[W10] RNN Epoch 42 | train_loss 0.2660 acc 0.892 f1 0.892 || val_loss 2.6900 acc 0.427 f1 0.350\n",
            "[W10] RNN Epoch 43 | train_loss 0.2554 acc 0.894 f1 0.894 || val_loss 2.7648 acc 0.410 f1 0.337\n",
            "[W10] RNN Epoch 44 | train_loss 0.2503 acc 0.897 f1 0.897 || val_loss 2.8708 acc 0.387 f1 0.326\n",
            "[W10] RNN Epoch 45 | train_loss 0.2386 acc 0.913 f1 0.912 || val_loss 2.9126 acc 0.370 f1 0.307\n",
            "[W10] RNN Epoch 46 | train_loss 0.2319 acc 0.906 f1 0.906 || val_loss 3.0434 acc 0.410 f1 0.337\n",
            "[W10] RNN Epoch 47 | train_loss 0.2211 acc 0.917 f1 0.917 || val_loss 3.0407 acc 0.393 f1 0.325\n",
            "[W10] RNN Epoch 48 | train_loss 0.2041 acc 0.926 f1 0.926 || val_loss 3.1180 acc 0.407 f1 0.335\n",
            "[W10] RNN Epoch 49 | train_loss 0.2019 acc 0.929 f1 0.929 || val_loss 3.1818 acc 0.407 f1 0.335\n",
            "[W10] RNN Epoch 50 | train_loss 0.2026 acc 0.922 f1 0.922 || val_loss 3.3443 acc 0.390 f1 0.334\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=10 Model=GRU\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 634, np.int64(1): 616, np.int64(0): 150})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 634, np.int64(2): 634, np.int64(0): 634})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W10] GRU Epoch 01 | train_loss 1.1002 acc 0.325 f1 0.257 || val_loss 1.0834 acc 0.413 f1 0.273\n",
            "[W10] GRU Epoch 02 | train_loss 1.0886 acc 0.386 f1 0.370 || val_loss 1.0947 acc 0.327 f1 0.301\n",
            "[W10] GRU Epoch 03 | train_loss 1.0715 acc 0.448 f1 0.447 || val_loss 1.1120 acc 0.287 f1 0.282\n",
            "[W10] GRU Epoch 04 | train_loss 1.0402 acc 0.471 f1 0.458 || val_loss 1.0967 acc 0.310 f1 0.287\n",
            "[W10] GRU Epoch 05 | train_loss 0.9647 acc 0.532 f1 0.520 || val_loss 1.1196 acc 0.353 f1 0.325\n",
            "[W10] GRU Epoch 06 | train_loss 0.8651 acc 0.577 f1 0.570 || val_loss 1.1332 acc 0.347 f1 0.312\n",
            "[W10] GRU Epoch 07 | train_loss 0.7979 acc 0.587 f1 0.584 || val_loss 1.0611 acc 0.420 f1 0.337\n",
            "[W10] GRU Epoch 08 | train_loss 0.7512 acc 0.621 f1 0.621 || val_loss 1.1158 acc 0.413 f1 0.319\n",
            "[W10] GRU Epoch 09 | train_loss 0.7311 acc 0.623 f1 0.624 || val_loss 1.1117 acc 0.413 f1 0.358\n",
            "[W10] GRU Epoch 10 | train_loss 0.6947 acc 0.648 f1 0.648 || val_loss 1.1050 acc 0.410 f1 0.319\n",
            "[W10] GRU Epoch 11 | train_loss 0.6671 acc 0.672 f1 0.672 || val_loss 1.2099 acc 0.373 f1 0.326\n",
            "[W10] GRU Epoch 12 | train_loss 0.6441 acc 0.668 f1 0.669 || val_loss 1.1973 acc 0.387 f1 0.320\n",
            "[W10] GRU Epoch 13 | train_loss 0.6427 acc 0.672 f1 0.672 || val_loss 1.1795 acc 0.380 f1 0.299\n",
            "[W10] GRU Epoch 14 | train_loss 0.6151 acc 0.699 f1 0.700 || val_loss 1.2073 acc 0.373 f1 0.293\n",
            "[W10] GRU Epoch 15 | train_loss 0.6028 acc 0.695 f1 0.695 || val_loss 1.2650 acc 0.380 f1 0.279\n",
            "[W10] GRU Epoch 16 | train_loss 0.5885 acc 0.712 f1 0.712 || val_loss 1.2788 acc 0.387 f1 0.313\n",
            "[W10] GRU Epoch 17 | train_loss 0.5753 acc 0.729 f1 0.728 || val_loss 1.2848 acc 0.377 f1 0.312\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=10 Model=LSTM\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 634, np.int64(1): 616, np.int64(0): 150})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 634, np.int64(2): 634, np.int64(0): 634})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W10] LSTM Epoch 01 | train_loss 1.0998 acc 0.337 f1 0.264 || val_loss 1.1033 acc 0.353 f1 0.276\n",
            "[W10] LSTM Epoch 02 | train_loss 1.0928 acc 0.411 f1 0.404 || val_loss 1.1030 acc 0.303 f1 0.300\n",
            "[W10] LSTM Epoch 03 | train_loss 1.0825 acc 0.424 f1 0.399 || val_loss 1.1088 acc 0.320 f1 0.315\n",
            "[W10] LSTM Epoch 04 | train_loss 1.0301 acc 0.467 f1 0.447 || val_loss 1.1635 acc 0.287 f1 0.254\n",
            "[W10] LSTM Epoch 05 | train_loss 0.9265 acc 0.533 f1 0.507 || val_loss 1.1222 acc 0.373 f1 0.333\n",
            "[W10] LSTM Epoch 06 | train_loss 0.8104 acc 0.573 f1 0.570 || val_loss 1.0759 acc 0.390 f1 0.293\n",
            "[W10] LSTM Epoch 07 | train_loss 0.7745 acc 0.595 f1 0.594 || val_loss 1.1053 acc 0.377 f1 0.298\n",
            "[W10] LSTM Epoch 08 | train_loss 0.7253 acc 0.619 f1 0.615 || val_loss 1.1873 acc 0.410 f1 0.349\n",
            "[W10] LSTM Epoch 09 | train_loss 0.7141 acc 0.628 f1 0.627 || val_loss 1.1077 acc 0.423 f1 0.319\n",
            "[W10] LSTM Epoch 10 | train_loss 0.6971 acc 0.637 f1 0.636 || val_loss 1.1828 acc 0.400 f1 0.316\n",
            "[W10] LSTM Epoch 11 | train_loss 0.6763 acc 0.648 f1 0.647 || val_loss 1.1682 acc 0.410 f1 0.329\n",
            "[W10] LSTM Epoch 12 | train_loss 0.6755 acc 0.649 f1 0.650 || val_loss 1.1865 acc 0.410 f1 0.356\n",
            "[W10] LSTM Epoch 13 | train_loss 0.6570 acc 0.657 f1 0.658 || val_loss 1.1920 acc 0.420 f1 0.330\n",
            "[W10] LSTM Epoch 14 | train_loss 0.6414 acc 0.672 f1 0.672 || val_loss 1.1965 acc 0.430 f1 0.317\n",
            "[W10] LSTM Epoch 15 | train_loss 0.6270 acc 0.680 f1 0.679 || val_loss 1.2538 acc 0.397 f1 0.320\n",
            "[W10] LSTM Epoch 16 | train_loss 0.6093 acc 0.697 f1 0.698 || val_loss 1.2719 acc 0.420 f1 0.352\n",
            "[W10] LSTM Epoch 17 | train_loss 0.6061 acc 0.688 f1 0.688 || val_loss 1.2689 acc 0.417 f1 0.343\n",
            "[W10] LSTM Epoch 18 | train_loss 0.5922 acc 0.708 f1 0.708 || val_loss 1.2679 acc 0.403 f1 0.330\n",
            "[W10] LSTM Epoch 19 | train_loss 0.5782 acc 0.715 f1 0.715 || val_loss 1.3209 acc 0.417 f1 0.342\n",
            "[W10] LSTM Epoch 20 | train_loss 0.5564 acc 0.726 f1 0.726 || val_loss 1.3307 acc 0.400 f1 0.295\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=14 Model=ANN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(1): 505, np.int64(2): 492, np.int64(0): 123})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 505, np.int64(1): 505, np.int64(0): 505})\n",
            "[W14] ANN Epoch 01 | train_loss 1.0972 acc 0.453 f1 0.448 || val_loss 1.1705 acc 0.246 f1 0.247\n",
            "[W14] ANN Epoch 02 | train_loss 0.7762 acc 0.645 f1 0.632 || val_loss 1.1642 acc 0.346 f1 0.334\n",
            "[W14] ANN Epoch 03 | train_loss 0.6403 acc 0.713 f1 0.703 || val_loss 1.1647 acc 0.362 f1 0.343\n",
            "[W14] ANN Epoch 04 | train_loss 0.5065 acc 0.782 f1 0.778 || val_loss 1.2187 acc 0.371 f1 0.344\n",
            "[W14] ANN Epoch 05 | train_loss 0.4017 acc 0.846 f1 0.845 || val_loss 1.2282 acc 0.396 f1 0.339\n",
            "[W14] ANN Epoch 06 | train_loss 0.3165 acc 0.887 f1 0.887 || val_loss 1.3262 acc 0.404 f1 0.333\n",
            "[W14] ANN Epoch 07 | train_loss 0.2580 acc 0.910 f1 0.909 || val_loss 1.4634 acc 0.433 f1 0.354\n",
            "[W14] ANN Epoch 08 | train_loss 0.2049 acc 0.939 f1 0.939 || val_loss 1.5002 acc 0.425 f1 0.345\n",
            "[W14] ANN Epoch 09 | train_loss 0.1630 acc 0.946 f1 0.946 || val_loss 1.5952 acc 0.425 f1 0.345\n",
            "[W14] ANN Epoch 10 | train_loss 0.1503 acc 0.953 f1 0.953 || val_loss 1.7066 acc 0.408 f1 0.334\n",
            "[W14] ANN Epoch 11 | train_loss 0.1047 acc 0.970 f1 0.970 || val_loss 1.7780 acc 0.442 f1 0.349\n",
            "[W14] ANN Epoch 12 | train_loss 0.1116 acc 0.958 f1 0.958 || val_loss 1.9472 acc 0.425 f1 0.335\n",
            "[W14] ANN Epoch 13 | train_loss 0.0959 acc 0.971 f1 0.971 || val_loss 2.0464 acc 0.433 f1 0.361\n",
            "[W14] ANN Epoch 14 | train_loss 0.0826 acc 0.973 f1 0.973 || val_loss 2.1008 acc 0.421 f1 0.334\n",
            "[W14] ANN Epoch 15 | train_loss 0.0988 acc 0.970 f1 0.970 || val_loss 2.0638 acc 0.429 f1 0.350\n",
            "[W14] ANN Epoch 16 | train_loss 0.0814 acc 0.977 f1 0.977 || val_loss 2.0464 acc 0.400 f1 0.308\n",
            "[W14] ANN Epoch 17 | train_loss 0.0683 acc 0.978 f1 0.978 || val_loss 2.2215 acc 0.425 f1 0.325\n",
            "[W14] ANN Epoch 18 | train_loss 0.0595 acc 0.982 f1 0.982 || val_loss 2.2152 acc 0.433 f1 0.329\n",
            "[W14] ANN Epoch 19 | train_loss 0.0566 acc 0.982 f1 0.982 || val_loss 2.3206 acc 0.438 f1 0.333\n",
            "[W14] ANN Epoch 20 | train_loss 0.0454 acc 0.987 f1 0.987 || val_loss 2.4193 acc 0.429 f1 0.340\n",
            "[W14] ANN Epoch 21 | train_loss 0.0470 acc 0.984 f1 0.984 || val_loss 2.4352 acc 0.412 f1 0.318\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=14 Model=CNN1D\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(1): 505, np.int64(2): 492, np.int64(0): 123})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 505, np.int64(1): 505, np.int64(0): 505})\n",
            "[W14] CNN1D Epoch 01 | train_loss 1.0519 acc 0.461 f1 0.469 || val_loss 0.9992 acc 0.429 f1 0.239\n",
            "[W14] CNN1D Epoch 02 | train_loss 0.9185 acc 0.533 f1 0.532 || val_loss 1.0410 acc 0.442 f1 0.293\n",
            "[W14] CNN1D Epoch 03 | train_loss 0.8331 acc 0.565 f1 0.565 || val_loss 1.0714 acc 0.408 f1 0.272\n",
            "[W14] CNN1D Epoch 04 | train_loss 0.7894 acc 0.579 f1 0.583 || val_loss 1.1146 acc 0.404 f1 0.319\n",
            "[W14] CNN1D Epoch 05 | train_loss 0.7641 acc 0.603 f1 0.606 || val_loss 1.0956 acc 0.412 f1 0.301\n",
            "[W14] CNN1D Epoch 06 | train_loss 0.7465 acc 0.622 f1 0.624 || val_loss 1.1433 acc 0.375 f1 0.261\n",
            "[W14] CNN1D Epoch 07 | train_loss 0.7271 acc 0.620 f1 0.620 || val_loss 1.1452 acc 0.433 f1 0.353\n",
            "[W14] CNN1D Epoch 08 | train_loss 0.7212 acc 0.620 f1 0.622 || val_loss 1.1828 acc 0.404 f1 0.328\n",
            "[W14] CNN1D Epoch 09 | train_loss 0.7203 acc 0.630 f1 0.632 || val_loss 1.1819 acc 0.404 f1 0.339\n",
            "[W14] CNN1D Epoch 10 | train_loss 0.6953 acc 0.644 f1 0.646 || val_loss 1.2201 acc 0.404 f1 0.324\n",
            "[W14] CNN1D Epoch 11 | train_loss 0.6871 acc 0.649 f1 0.650 || val_loss 1.2123 acc 0.400 f1 0.337\n",
            "[W14] CNN1D Epoch 12 | train_loss 0.6710 acc 0.648 f1 0.646 || val_loss 1.2492 acc 0.417 f1 0.333\n",
            "[W14] CNN1D Epoch 13 | train_loss 0.6846 acc 0.655 f1 0.657 || val_loss 1.2426 acc 0.375 f1 0.305\n",
            "[W14] CNN1D Epoch 14 | train_loss 0.6594 acc 0.675 f1 0.678 || val_loss 1.2376 acc 0.412 f1 0.316\n",
            "[W14] CNN1D Epoch 15 | train_loss 0.6675 acc 0.677 f1 0.677 || val_loss 1.2130 acc 0.412 f1 0.328\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=14 Model=RNN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(1): 505, np.int64(2): 492, np.int64(0): 123})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 505, np.int64(1): 505, np.int64(0): 505})\n",
            "[W14] RNN Epoch 01 | train_loss 1.1034 acc 0.360 f1 0.287 || val_loss 1.1203 acc 0.267 f1 0.205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W14] RNN Epoch 02 | train_loss 1.0782 acc 0.442 f1 0.367 || val_loss 1.1081 acc 0.296 f1 0.235\n",
            "[W14] RNN Epoch 03 | train_loss 1.0549 acc 0.465 f1 0.404 || val_loss 1.1097 acc 0.292 f1 0.249\n",
            "[W14] RNN Epoch 04 | train_loss 1.0132 acc 0.509 f1 0.463 || val_loss 1.1248 acc 0.312 f1 0.273\n",
            "[W14] RNN Epoch 05 | train_loss 0.9623 acc 0.521 f1 0.495 || val_loss 1.1258 acc 0.317 f1 0.274\n",
            "[W14] RNN Epoch 06 | train_loss 0.9225 acc 0.547 f1 0.531 || val_loss 1.1682 acc 0.317 f1 0.291\n",
            "[W14] RNN Epoch 07 | train_loss 0.8755 acc 0.568 f1 0.554 || val_loss 1.1908 acc 0.367 f1 0.333\n",
            "[W14] RNN Epoch 08 | train_loss 0.8272 acc 0.599 f1 0.584 || val_loss 1.2176 acc 0.383 f1 0.349\n",
            "[W14] RNN Epoch 09 | train_loss 0.7924 acc 0.630 f1 0.616 || val_loss 1.2539 acc 0.388 f1 0.358\n",
            "[W14] RNN Epoch 10 | train_loss 0.7521 acc 0.632 f1 0.618 || val_loss 1.2892 acc 0.375 f1 0.327\n",
            "[W14] RNN Epoch 11 | train_loss 0.7281 acc 0.649 f1 0.636 || val_loss 1.3134 acc 0.371 f1 0.331\n",
            "[W14] RNN Epoch 12 | train_loss 0.6830 acc 0.670 f1 0.659 || val_loss 1.3822 acc 0.371 f1 0.327\n",
            "[W14] RNN Epoch 13 | train_loss 0.6412 acc 0.688 f1 0.679 || val_loss 1.4459 acc 0.379 f1 0.337\n",
            "[W14] RNN Epoch 14 | train_loss 0.6210 acc 0.700 f1 0.692 || val_loss 1.4969 acc 0.400 f1 0.339\n",
            "[W14] RNN Epoch 15 | train_loss 0.6029 acc 0.701 f1 0.694 || val_loss 1.5142 acc 0.404 f1 0.347\n",
            "[W14] RNN Epoch 16 | train_loss 0.5676 acc 0.726 f1 0.720 || val_loss 1.6419 acc 0.392 f1 0.336\n",
            "[W14] RNN Epoch 17 | train_loss 0.5496 acc 0.737 f1 0.732 || val_loss 1.6271 acc 0.388 f1 0.337\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=14 Model=GRU\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(1): 505, np.int64(2): 492, np.int64(0): 123})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 505, np.int64(1): 505, np.int64(0): 505})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W14] GRU Epoch 01 | train_loss 1.1018 acc 0.325 f1 0.236 || val_loss 1.0952 acc 0.408 f1 0.236\n",
            "[W14] GRU Epoch 02 | train_loss 1.0909 acc 0.388 f1 0.341 || val_loss 1.0969 acc 0.400 f1 0.333\n",
            "[W14] GRU Epoch 03 | train_loss 1.0830 acc 0.429 f1 0.425 || val_loss 1.0974 acc 0.383 f1 0.336\n",
            "[W14] GRU Epoch 04 | train_loss 1.0694 acc 0.468 f1 0.470 || val_loss 1.0959 acc 0.383 f1 0.360\n",
            "[W14] GRU Epoch 05 | train_loss 1.0470 acc 0.492 f1 0.492 || val_loss 1.1017 acc 0.342 f1 0.326\n",
            "[W14] GRU Epoch 06 | train_loss 1.0022 acc 0.514 f1 0.509 || val_loss 1.1532 acc 0.317 f1 0.312\n",
            "[W14] GRU Epoch 07 | train_loss 0.9185 acc 0.566 f1 0.557 || val_loss 1.1322 acc 0.350 f1 0.315\n",
            "[W14] GRU Epoch 08 | train_loss 0.8012 acc 0.601 f1 0.602 || val_loss 1.1600 acc 0.392 f1 0.343\n",
            "[W14] GRU Epoch 09 | train_loss 0.7615 acc 0.627 f1 0.629 || val_loss 1.1760 acc 0.375 f1 0.340\n",
            "[W14] GRU Epoch 10 | train_loss 0.7219 acc 0.635 f1 0.636 || val_loss 1.0937 acc 0.442 f1 0.348\n",
            "[W14] GRU Epoch 11 | train_loss 0.7217 acc 0.651 f1 0.652 || val_loss 1.1362 acc 0.408 f1 0.344\n",
            "[W14] GRU Epoch 12 | train_loss 0.6928 acc 0.668 f1 0.671 || val_loss 1.1370 acc 0.412 f1 0.339\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=14 Model=LSTM\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(1): 505, np.int64(2): 492, np.int64(0): 123})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 505, np.int64(1): 505, np.int64(0): 505})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W14] LSTM Epoch 01 | train_loss 1.1082 acc 0.331 f1 0.167 || val_loss 1.1157 acc 0.442 f1 0.204\n",
            "[W14] LSTM Epoch 02 | train_loss 1.0992 acc 0.374 f1 0.294 || val_loss 1.1110 acc 0.267 f1 0.218\n",
            "[W14] LSTM Epoch 03 | train_loss 1.0899 acc 0.424 f1 0.376 || val_loss 1.1060 acc 0.300 f1 0.297\n",
            "[W14] LSTM Epoch 04 | train_loss 1.0656 acc 0.450 f1 0.427 || val_loss 1.0932 acc 0.350 f1 0.333\n",
            "[W14] LSTM Epoch 05 | train_loss 0.9971 acc 0.484 f1 0.468 || val_loss 1.1478 acc 0.312 f1 0.307\n",
            "[W14] LSTM Epoch 06 | train_loss 0.8704 acc 0.543 f1 0.536 || val_loss 1.0666 acc 0.408 f1 0.325\n",
            "[W14] LSTM Epoch 07 | train_loss 0.8146 acc 0.583 f1 0.577 || val_loss 1.1165 acc 0.400 f1 0.338\n",
            "[W14] LSTM Epoch 08 | train_loss 0.7683 acc 0.618 f1 0.618 || val_loss 1.3950 acc 0.362 f1 0.326\n",
            "[W14] LSTM Epoch 09 | train_loss 0.7588 acc 0.614 f1 0.615 || val_loss 1.3070 acc 0.342 f1 0.311\n",
            "[W14] LSTM Epoch 10 | train_loss 0.7435 acc 0.609 f1 0.607 || val_loss 1.2141 acc 0.433 f1 0.364\n",
            "[W14] LSTM Epoch 11 | train_loss 0.7365 acc 0.620 f1 0.623 || val_loss 1.1524 acc 0.425 f1 0.342\n",
            "[W14] LSTM Epoch 12 | train_loss 0.7016 acc 0.646 f1 0.646 || val_loss 1.1837 acc 0.421 f1 0.328\n",
            "[W14] LSTM Epoch 13 | train_loss 0.7027 acc 0.643 f1 0.646 || val_loss 1.2821 acc 0.396 f1 0.332\n",
            "[W14] LSTM Epoch 14 | train_loss 0.6853 acc 0.648 f1 0.648 || val_loss 1.2027 acc 0.417 f1 0.325\n",
            "[W14] LSTM Epoch 15 | train_loss 0.6801 acc 0.656 f1 0.657 || val_loss 1.3733 acc 0.371 f1 0.313\n",
            "[W14] LSTM Epoch 16 | train_loss 0.6647 acc 0.671 f1 0.671 || val_loss 1.2459 acc 0.433 f1 0.337\n",
            "[W14] LSTM Epoch 17 | train_loss 0.6541 acc 0.675 f1 0.677 || val_loss 1.5041 acc 0.375 f1 0.327\n",
            "[W14] LSTM Epoch 18 | train_loss 0.6311 acc 0.680 f1 0.680 || val_loss 1.4879 acc 0.400 f1 0.336\n",
            "Early stopping.\n",
            "Saved results to outputs2/results_summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "experiment_runner_smote.py\n",
        "\n",
        "PyTorch experiment runner with SMOTE augmentation:\n",
        "\n",
        "- builds sliding windows for window_sizes = [2,3,4,5,6,7,10,14]\n",
        "- creates ANN and sequence inputs\n",
        "- applies SMOTE on training data to balance classes\n",
        "- trains ANN, CNN1D, RNN, GRU, LSTM\n",
        "- participant-wise split (train/val/test)\n",
        "- saves results to CSV, confusion matrix PNGs, and class distributions\n",
        "\n",
        "Usage:\n",
        "    python experiment_runner_smote.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "CSV_PATH = \"stress_detection.csv\"\n",
        "OUTPUT_DIR = Path(\"outputs3\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "WINDOW_SIZES = [2,3,4,5,6,7,10,14]\n",
        "RAW_FEATURES = [\n",
        "    'Openness','Conscientiousness','Extraversion','Agreeableness','Neuroticism',\n",
        "    'sleep_time','wake_time','sleep_duration','PSQI_score',\n",
        "    'call_duration','num_calls','num_sms',\n",
        "    'screen_on_time','skin_conductance','accelerometer',\n",
        "    'mobility_radius','mobility_distance'\n",
        "]\n",
        "STATIC_PERSONALITY = ['Openness','Conscientiousness','Extraversion','Agreeableness','Neuroticism']\n",
        "TARGET_COL = 'PSS_score'\n",
        "CLASS_COL = 'stress_class'\n",
        "\n",
        "TRAIN_P = 0.7\n",
        "VAL_P = 0.15\n",
        "TEST_P = 0.15\n",
        "\n",
        "EPOCHS = 60\n",
        "BATCH_SIZE = 64\n",
        "LR = 1e-3\n",
        "PATIENCE = 8\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "MODEL_NAMES = [\"ANN\",\"CNN1D\",\"RNN\",\"GRU\",\"LSTM\"]\n",
        "\n",
        "# -------------------------\n",
        "# Helpers\n",
        "# -------------------------\n",
        "def stress_to_class(score):\n",
        "    \"\"\"\n",
        "    Equal-width split of 10-40 into 3 classes.\n",
        "    Class 0: 10-19.99\n",
        "    Class 1: 20-29.99\n",
        "    Class 2: 30-40\n",
        "    \"\"\"\n",
        "    s = float(score)\n",
        "    if s < 20: return 0\n",
        "    elif s < 30: return 1\n",
        "    else: return 2\n",
        "\n",
        "def ensure_dir(path):\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Window builder\n",
        "# -------------------------\n",
        "def build_windows(df, raw_features, window_size=4):\n",
        "    X_seq, X_flat, y, participants = [], [], [], []\n",
        "\n",
        "    for pid, df_p in df.groupby(\"participant_id\"):\n",
        "        df_p = df_p.sort_values(\"day\").reset_index(drop=True)\n",
        "        n = len(df_p)\n",
        "        if n <= window_size:\n",
        "            continue\n",
        "        arr = df_p[raw_features].values.astype(float)\n",
        "        for i in range(n - window_size):\n",
        "            window = arr[i:i+window_size]\n",
        "            mean_vals = window.mean(axis=0)\n",
        "            std_vals = window.std(axis=0)\n",
        "            min_vals = window.min(axis=0)\n",
        "            max_vals = window.max(axis=0)\n",
        "            slope_vals = window[-1] - window[0]\n",
        "            agg = np.concatenate([mean_vals, std_vals, min_vals, max_vals, slope_vals])\n",
        "\n",
        "            flat = np.concatenate([window.flatten(), agg])\n",
        "            target_score = df_p[TARGET_COL].iloc[i+window_size]\n",
        "            cls = stress_to_class(target_score)\n",
        "\n",
        "            X_seq.append(window.copy())\n",
        "            X_flat.append(flat.copy())\n",
        "            y.append(int(cls))\n",
        "            participants.append(pid)\n",
        "\n",
        "    return np.array(X_seq), np.array(X_flat), np.array(y), np.array(participants)\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class StressDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# -------------------------\n",
        "# Models\n",
        "# -------------------------\n",
        "class ANNModel(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=[256,128,64], num_classes=3, p_drop=0.3):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last = in_dim\n",
        "        for h in hidden:\n",
        "            layers.append(nn.Linear(last,h))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.BatchNorm1d(h))\n",
        "            layers.append(nn.Dropout(p_drop))\n",
        "            last = h\n",
        "        layers.append(nn.Linear(last,num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "class CNN1DModel(nn.Module):\n",
        "    def __init__(self, seq_len, feat_dim, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(feat_dim,64,2)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.conv2 = nn.Conv1d(64,128,2)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        l = seq_len - (2-1) - (2-1)\n",
        "        if l<1: l=1\n",
        "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128*1,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64,num_classes)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x = x.permute(0,2,1)\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.gap(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=64, num_layers=1, rnn_type='RNN', num_classes=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        if rnn_type=='RNN':\n",
        "            self.rnn = nn.RNN(input_dim,hidden,num_layers,batch_first=True,nonlinearity='tanh',dropout=dropout)\n",
        "        elif rnn_type=='GRU':\n",
        "            self.rnn = nn.GRU(input_dim,hidden,num_layers,batch_first=True,dropout=dropout)\n",
        "        elif rnn_type=='LSTM':\n",
        "            self.rnn = nn.LSTM(input_dim,hidden,num_layers,batch_first=True,dropout=dropout)\n",
        "        else: raise ValueError(\"Unknown rnn_type\")\n",
        "        self.head = nn.Sequential(nn.Linear(hidden,32), nn.ReLU(), nn.Dropout(0.2), nn.Linear(32,num_classes))\n",
        "        self.rnn_type = rnn_type\n",
        "    def forward(self,x):\n",
        "        out,_ = self.rnn(x)\n",
        "        return self.head(out[:,-1,:])\n",
        "\n",
        "# -------------------------\n",
        "# Training & Evaluation\n",
        "# -------------------------\n",
        "def compute_class_weights(y):\n",
        "    counts = Counter(y.tolist())\n",
        "    total = sum(counts.values())\n",
        "    num_classes = len(counts)\n",
        "    weights = []\n",
        "    for i in range(max(counts.keys())+1):\n",
        "        cnt = counts.get(i,0)\n",
        "        weights.append(total/(num_classes*cnt) if cnt>0 else 0.0)\n",
        "    return torch.tensor(weights,dtype=torch.float32,device=DEVICE)\n",
        "\n",
        "def train_one_epoch(model, loader, opt, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    preds, trues = [], []\n",
        "    for xb,yb in loader:\n",
        "        xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        out = model(xb)\n",
        "        loss = criterion(out,yb)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()*xb.size(0)\n",
        "        preds.append(out.detach().argmax(1).cpu().numpy())\n",
        "        trues.append(yb.cpu().numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    trues = np.concatenate(trues)\n",
        "    return total_loss/len(trues), accuracy_score(trues,preds), f1_score(trues,preds,average='macro',zero_division=0)\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss=0\n",
        "    preds,trues=[],[]\n",
        "    with torch.no_grad():\n",
        "        for xb,yb in loader:\n",
        "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            out = model(xb)\n",
        "            total_loss += criterion(out,yb).item()*xb.size(0)\n",
        "            preds.append(out.argmax(1).cpu().numpy())\n",
        "            trues.append(yb.cpu().numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    trues = np.concatenate(trues)\n",
        "    mean_loss = total_loss / len(trues)\n",
        "    acc = accuracy_score(trues,preds)\n",
        "    f1 = f1_score(trues,preds,average='macro',zero_division=0)\n",
        "    prec, rec, f1s, sup = precision_recall_fscore_support(trues,preds,average=None,zero_division=0)\n",
        "    return mean_loss, acc, f1, prec, rec, f1s, sup, preds, trues\n",
        "\n",
        "# -------------------------\n",
        "# Experiment for a window\n",
        "# -------------------------\n",
        "def run_experiment_for_window(df, window_size, model_name, train_pids, val_pids, test_pids):\n",
        "    X_seq, X_flat, y, pids = build_windows(df, RAW_FEATURES, window_size)\n",
        "    if len(y)==0: return None\n",
        "\n",
        "    # split by participant\n",
        "    train_mask = np.isin(pids, train_pids)\n",
        "    val_mask = np.isin(pids, val_pids)\n",
        "    test_mask = np.isin(pids, test_pids)\n",
        "\n",
        "    X_seq_train, X_seq_val, X_seq_test = X_seq[train_mask], X_seq[val_mask], X_seq[test_mask]\n",
        "    X_flat_train, X_flat_val, X_flat_test = X_flat[train_mask], X_flat[val_mask], X_flat[test_mask]\n",
        "    y_train, y_val, y_test = y[train_mask], y[val_mask], y[test_mask]\n",
        "\n",
        "    if len(y_train)==0 or len(y_val)==0 or len(y_test)==0:\n",
        "        print(\"One split empty for window\", window_size)\n",
        "        return None\n",
        "\n",
        "    # -------------------------\n",
        "    # SMOTE augmentation (training set only)\n",
        "    # -------------------------\n",
        "    print(f\"Class distribution BEFORE SMOTE: {Counter(y_train)}\")\n",
        "    smote = SMOTE(random_state=SEED)\n",
        "    if model_name==\"ANN\":\n",
        "        X_train_aug, y_train_aug = smote.fit_resample(X_flat_train, y_train)\n",
        "        X_train_aug = X_train_aug\n",
        "    else:\n",
        "        N,L,F = X_seq_train.shape\n",
        "        X_seq_flat = X_seq_train.reshape(N,L*F)\n",
        "        X_train_aug, y_train_aug = smote.fit_resample(X_seq_flat, y_train)\n",
        "        X_train_aug = X_train_aug.reshape(-1,L,F)\n",
        "    print(f\"Class distribution AFTER SMOTE: {Counter(y_train_aug)}\")\n",
        "\n",
        "    # save class distributions\n",
        "    dist_df = pd.DataFrame({'Class':[0,1,2],\n",
        "                            'Before': [Counter(y_train)[0],Counter(y_train)[1],Counter(y_train)[2]],\n",
        "                            'After': [Counter(y_train_aug)[0],Counter(y_train_aug)[1],Counter(y_train_aug)[2]]})\n",
        "    dist_df.to_csv(OUTPUT_DIR/f'class_distribution_w{window_size}_{model_name}.csv', index=False)\n",
        "\n",
        "    # -------------------------\n",
        "    # Scaling\n",
        "    # -------------------------\n",
        "    if model_name==\"ANN\":\n",
        "        scaler = StandardScaler().fit(X_train_aug)\n",
        "        Xtr = scaler.transform(X_train_aug)\n",
        "        Xv = scaler.transform(X_flat_val)\n",
        "        Xt = scaler.transform(X_flat_test)\n",
        "        input_dim = Xtr.shape[1]\n",
        "    else:\n",
        "        N,L,F = X_train_aug.shape\n",
        "        scaler = StandardScaler().fit(X_train_aug.reshape(-1,F))\n",
        "        Xtr = scaler.transform(X_train_aug.reshape(-1,F)).reshape(N,L,F)\n",
        "        Xv = scaler.transform(X_seq_val.reshape(-1,F)).reshape(X_seq_val.shape)\n",
        "        Xt = scaler.transform(X_seq_test.reshape(-1,F)).reshape(X_seq_test.shape)\n",
        "        input_dim = F\n",
        "\n",
        "    train_ds = StressDataset(Xtr, y_train_aug)\n",
        "    val_ds = StressDataset(Xv, y_val)\n",
        "    test_ds = StressDataset(Xt, y_test)\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # -------------------------\n",
        "    # Model\n",
        "    # -------------------------\n",
        "    if model_name==\"ANN\": model = ANNModel(input_dim).to(DEVICE)\n",
        "    elif model_name==\"CNN1D\": model = CNN1DModel(Xtr.shape[1], Xtr.shape[2]).to(DEVICE)\n",
        "    elif model_name in (\"RNN\",\"GRU\",\"LSTM\"): model = RNNModel(input_dim, hidden=64, num_layers=1, rnn_type=model_name).to(DEVICE)\n",
        "    else: raise ValueError(\"Unknown model\")\n",
        "\n",
        "    # loss and optimizer\n",
        "    class_weights = compute_class_weights(torch.tensor(y_train_aug))\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
        "\n",
        "    # -------------------------\n",
        "    # Training loop with early stopping\n",
        "    # -------------------------\n",
        "    best_val_f1, best_state, cur_wait = -1.0, None, 0\n",
        "    for epoch in range(1,EPOCHS+1):\n",
        "        train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, opt, criterion)\n",
        "        val_loss, val_acc, val_f1, *_ = evaluate(model, val_loader, criterion)\n",
        "        print(f\"[W{window_size}] {model_name} Epoch {epoch:02d} | train_loss {train_loss:.4f} acc {train_acc:.3f} f1 {train_f1:.3f} || val_loss {val_loss:.4f} acc {val_acc:.3f} f1 {val_f1:.3f}\")\n",
        "        if val_f1 > best_val_f1+1e-4:\n",
        "            best_val_f1 = val_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            cur_wait = 0\n",
        "        else:\n",
        "            cur_wait += 1\n",
        "            if cur_wait >= PATIENCE:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # test\n",
        "    if best_state is not None: model.load_state_dict(best_state)\n",
        "    test_loss, test_acc, test_f1, prec, rec, f1s, sup, preds, trues = evaluate(model, test_loader, criterion)\n",
        "\n",
        "    # save confusion matrix\n",
        "    cm = confusion_matrix(trues,preds)\n",
        "    np.save(OUTPUT_DIR/f\"cm_w{window_size}_{model_name}.npy\", cm)\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(cm, cmap='Blues')\n",
        "    plt.title(f\"CM W{window_size} {model_name}\")\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"pred\")\n",
        "    plt.ylabel(\"true\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUTPUT_DIR/f\"cm_w{window_size}_{model_name}.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return {\n",
        "        \"window\": window_size,\n",
        "        \"model\": model_name,\n",
        "        \"train_samples\": len(y_train_aug),\n",
        "        \"val_samples\": len(y_val),\n",
        "        \"test_samples\": len(y_test),\n",
        "        \"test_loss\": float(test_loss),\n",
        "        \"test_acc\": float(test_acc),\n",
        "        \"test_macro_f1\": float(test_f1),\n",
        "        \"per_class_prec\": prec.tolist(),\n",
        "        \"per_class_rec\": rec.tolist(),\n",
        "        \"per_class_f1\": f1s.tolist(),\n",
        "        \"support\": sup.tolist()\n",
        "    }\n",
        "\n",
        "# -------------------------\n",
        "# Main\n",
        "# -------------------------\n",
        "def main():\n",
        "    print(\"Device:\", DEVICE)\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    df = df.sort_values(['participant_id','day']).reset_index(drop=True)\n",
        "    df[CLASS_COL] = df[TARGET_COL].apply(stress_to_class)\n",
        "\n",
        "    # participant split\n",
        "    pids = df['participant_id'].unique()\n",
        "    random.shuffle(pids)\n",
        "    n = len(pids)\n",
        "    n_train = int(n*TRAIN_P)\n",
        "    n_val = int(n*VAL_P)\n",
        "    train_pids = pids[:n_train]\n",
        "    val_pids = pids[n_train:n_train+n_val]\n",
        "    test_pids = pids[n_train+n_val:]\n",
        "    print(f\"Participants: total {n} train {len(train_pids)} val {len(val_pids)} test {len(test_pids)}\")\n",
        "\n",
        "    results=[]\n",
        "    for w in WINDOW_SIZES:\n",
        "        for mname in MODEL_NAMES:\n",
        "            print(\"=\"*60)\n",
        "            print(f\"Running Window={w} Model={mname}\")\n",
        "            try:\n",
        "                res = run_experiment_for_window(df, w, mname, train_pids, val_pids, test_pids)\n",
        "                if res is not None:\n",
        "                    results.append(res)\n",
        "                    pd.DataFrame(results).to_csv(OUTPUT_DIR/\"results_summary.csv\", index=False)\n",
        "            except Exception as e:\n",
        "                print(\"Error running\", w, mname, e)\n",
        "\n",
        "    if results:\n",
        "        pd.DataFrame(results).to_csv(OUTPUT_DIR/\"results_summary.csv\", index=False)\n",
        "        print(\"Saved results to\", OUTPUT_DIR/\"results_summary.csv\")\n",
        "    else:\n",
        "        print(\"No results to save.\")\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr_Mh7IY2Xgg",
        "outputId": "dc3306e0-cd2e-418a-f011-fd7028fd4a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Participants: total 100 train 70 val 15 test 15\n",
            "============================================================\n",
            "Running Window=2 Model=ANN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 679, np.int64(1): 655, np.int64(0): 626})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 679, np.int64(0): 679, np.int64(2): 679})\n",
            "[W2] ANN Epoch 01 | train_loss 1.2072 acc 0.327 f1 0.326 || val_loss 1.1125 acc 0.352 f1 0.351\n",
            "[W2] ANN Epoch 02 | train_loss 1.1158 acc 0.382 f1 0.381 || val_loss 1.1183 acc 0.357 f1 0.356\n",
            "[W2] ANN Epoch 03 | train_loss 1.0770 acc 0.419 f1 0.419 || val_loss 1.1218 acc 0.300 f1 0.300\n",
            "[W2] ANN Epoch 04 | train_loss 1.0358 acc 0.471 f1 0.471 || val_loss 1.1314 acc 0.340 f1 0.341\n",
            "[W2] ANN Epoch 05 | train_loss 0.9950 acc 0.503 f1 0.503 || val_loss 1.1502 acc 0.324 f1 0.324\n",
            "[W2] ANN Epoch 06 | train_loss 0.9727 acc 0.524 f1 0.524 || val_loss 1.1742 acc 0.307 f1 0.306\n",
            "[W2] ANN Epoch 07 | train_loss 0.9598 acc 0.545 f1 0.545 || val_loss 1.1912 acc 0.307 f1 0.306\n",
            "[W2] ANN Epoch 08 | train_loss 0.9253 acc 0.554 f1 0.554 || val_loss 1.2049 acc 0.319 f1 0.319\n",
            "[W2] ANN Epoch 09 | train_loss 0.8891 acc 0.586 f1 0.585 || val_loss 1.2344 acc 0.310 f1 0.307\n",
            "[W2] ANN Epoch 10 | train_loss 0.8664 acc 0.585 f1 0.585 || val_loss 1.2657 acc 0.321 f1 0.321\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=2 Model=CNN1D\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 679, np.int64(1): 655, np.int64(0): 626})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 679, np.int64(0): 679, np.int64(2): 679})\n",
            "Error running 2 CNN1D Calculated padded input size per channel: (1). Kernel size: (2). Kernel size can't be greater than actual input size\n",
            "============================================================\n",
            "Running Window=2 Model=RNN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 679, np.int64(1): 655, np.int64(0): 626})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 679, np.int64(0): 679, np.int64(2): 679})\n",
            "[W2] RNN Epoch 01 | train_loss 1.1034 acc 0.336 f1 0.277 || val_loss 1.1027 acc 0.345 f1 0.323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W2] RNN Epoch 02 | train_loss 1.0914 acc 0.374 f1 0.363 || val_loss 1.1040 acc 0.336 f1 0.330\n",
            "[W2] RNN Epoch 03 | train_loss 1.0840 acc 0.405 f1 0.403 || val_loss 1.1059 acc 0.310 f1 0.309\n",
            "[W2] RNN Epoch 04 | train_loss 1.0788 acc 0.418 f1 0.418 || val_loss 1.1078 acc 0.293 f1 0.291\n",
            "[W2] RNN Epoch 05 | train_loss 1.0715 acc 0.424 f1 0.423 || val_loss 1.1136 acc 0.307 f1 0.305\n",
            "[W2] RNN Epoch 06 | train_loss 1.0689 acc 0.426 f1 0.425 || val_loss 1.1152 acc 0.312 f1 0.311\n",
            "[W2] RNN Epoch 07 | train_loss 1.0619 acc 0.436 f1 0.436 || val_loss 1.1179 acc 0.314 f1 0.313\n",
            "[W2] RNN Epoch 08 | train_loss 1.0554 acc 0.449 f1 0.449 || val_loss 1.1211 acc 0.314 f1 0.309\n",
            "[W2] RNN Epoch 09 | train_loss 1.0507 acc 0.452 f1 0.451 || val_loss 1.1249 acc 0.321 f1 0.321\n",
            "[W2] RNN Epoch 10 | train_loss 1.0409 acc 0.462 f1 0.462 || val_loss 1.1241 acc 0.310 f1 0.307\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=2 Model=GRU\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 679, np.int64(1): 655, np.int64(0): 626})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 679, np.int64(0): 679, np.int64(2): 679})\n",
            "[W2] GRU Epoch 01 | train_loss 1.1030 acc 0.329 f1 0.261 || val_loss 1.1015 acc 0.333 f1 0.273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W2] GRU Epoch 02 | train_loss 1.0965 acc 0.366 f1 0.349 || val_loss 1.0994 acc 0.333 f1 0.324\n",
            "[W2] GRU Epoch 03 | train_loss 1.0929 acc 0.375 f1 0.373 || val_loss 1.0997 acc 0.314 f1 0.315\n",
            "[W2] GRU Epoch 04 | train_loss 1.0893 acc 0.399 f1 0.399 || val_loss 1.1008 acc 0.319 f1 0.320\n",
            "[W2] GRU Epoch 05 | train_loss 1.0855 acc 0.405 f1 0.404 || val_loss 1.1009 acc 0.326 f1 0.326\n",
            "[W2] GRU Epoch 06 | train_loss 1.0785 acc 0.424 f1 0.424 || val_loss 1.1028 acc 0.329 f1 0.329\n",
            "[W2] GRU Epoch 07 | train_loss 1.0746 acc 0.425 f1 0.425 || val_loss 1.1054 acc 0.319 f1 0.319\n",
            "[W2] GRU Epoch 08 | train_loss 1.0701 acc 0.425 f1 0.424 || val_loss 1.1049 acc 0.321 f1 0.322\n",
            "[W2] GRU Epoch 09 | train_loss 1.0628 acc 0.423 f1 0.423 || val_loss 1.1089 acc 0.336 f1 0.337\n",
            "[W2] GRU Epoch 10 | train_loss 1.0545 acc 0.449 f1 0.448 || val_loss 1.1125 acc 0.338 f1 0.338\n",
            "[W2] GRU Epoch 11 | train_loss 1.0465 acc 0.459 f1 0.458 || val_loss 1.1156 acc 0.350 f1 0.350\n",
            "[W2] GRU Epoch 12 | train_loss 1.0330 acc 0.468 f1 0.468 || val_loss 1.1226 acc 0.357 f1 0.357\n",
            "[W2] GRU Epoch 13 | train_loss 1.0199 acc 0.491 f1 0.490 || val_loss 1.1314 acc 0.348 f1 0.348\n",
            "[W2] GRU Epoch 14 | train_loss 1.0033 acc 0.494 f1 0.494 || val_loss 1.1341 acc 0.340 f1 0.341\n",
            "[W2] GRU Epoch 15 | train_loss 0.9917 acc 0.533 f1 0.532 || val_loss 1.1454 acc 0.350 f1 0.350\n",
            "[W2] GRU Epoch 16 | train_loss 0.9682 acc 0.544 f1 0.543 || val_loss 1.1571 acc 0.350 f1 0.350\n",
            "[W2] GRU Epoch 17 | train_loss 0.9570 acc 0.556 f1 0.556 || val_loss 1.1744 acc 0.345 f1 0.346\n",
            "[W2] GRU Epoch 18 | train_loss 0.9194 acc 0.586 f1 0.585 || val_loss 1.1920 acc 0.348 f1 0.346\n",
            "[W2] GRU Epoch 19 | train_loss 0.9065 acc 0.585 f1 0.585 || val_loss 1.2042 acc 0.364 f1 0.364\n",
            "[W2] GRU Epoch 20 | train_loss 0.8757 acc 0.619 f1 0.619 || val_loss 1.2284 acc 0.333 f1 0.333\n",
            "[W2] GRU Epoch 21 | train_loss 0.8581 acc 0.633 f1 0.633 || val_loss 1.2433 acc 0.348 f1 0.343\n",
            "[W2] GRU Epoch 22 | train_loss 0.8337 acc 0.642 f1 0.641 || val_loss 1.2717 acc 0.345 f1 0.344\n",
            "[W2] GRU Epoch 23 | train_loss 0.8058 acc 0.658 f1 0.658 || val_loss 1.2923 acc 0.345 f1 0.344\n",
            "[W2] GRU Epoch 24 | train_loss 0.7909 acc 0.657 f1 0.657 || val_loss 1.3187 acc 0.348 f1 0.343\n",
            "[W2] GRU Epoch 25 | train_loss 0.7545 acc 0.683 f1 0.683 || val_loss 1.3347 acc 0.352 f1 0.351\n",
            "[W2] GRU Epoch 26 | train_loss 0.7340 acc 0.695 f1 0.694 || val_loss 1.3598 acc 0.367 f1 0.363\n",
            "[W2] GRU Epoch 27 | train_loss 0.7175 acc 0.700 f1 0.700 || val_loss 1.4011 acc 0.333 f1 0.332\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=2 Model=LSTM\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 679, np.int64(1): 655, np.int64(0): 626})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 679, np.int64(0): 679, np.int64(2): 679})\n",
            "[W2] LSTM Epoch 01 | train_loss 1.0999 acc 0.339 f1 0.252 || val_loss 1.0991 acc 0.355 f1 0.275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W2] LSTM Epoch 02 | train_loss 1.0977 acc 0.353 f1 0.303 || val_loss 1.0977 acc 0.345 f1 0.304\n",
            "[W2] LSTM Epoch 03 | train_loss 1.0948 acc 0.373 f1 0.348 || val_loss 1.0961 acc 0.352 f1 0.339\n",
            "[W2] LSTM Epoch 04 | train_loss 1.0924 acc 0.385 f1 0.377 || val_loss 1.0949 acc 0.352 f1 0.344\n",
            "[W2] LSTM Epoch 05 | train_loss 1.0895 acc 0.394 f1 0.388 || val_loss 1.0946 acc 0.348 f1 0.341\n",
            "[W2] LSTM Epoch 06 | train_loss 1.0850 acc 0.390 f1 0.390 || val_loss 1.0950 acc 0.348 f1 0.346\n",
            "[W2] LSTM Epoch 07 | train_loss 1.0799 acc 0.412 f1 0.410 || val_loss 1.0973 acc 0.345 f1 0.344\n",
            "[W2] LSTM Epoch 08 | train_loss 1.0773 acc 0.413 f1 0.406 || val_loss 1.0999 acc 0.348 f1 0.344\n",
            "[W2] LSTM Epoch 09 | train_loss 1.0679 acc 0.426 f1 0.423 || val_loss 1.1020 acc 0.338 f1 0.337\n",
            "[W2] LSTM Epoch 10 | train_loss 1.0601 acc 0.435 f1 0.433 || val_loss 1.1071 acc 0.336 f1 0.335\n",
            "[W2] LSTM Epoch 11 | train_loss 1.0519 acc 0.451 f1 0.450 || val_loss 1.1119 acc 0.352 f1 0.345\n",
            "[W2] LSTM Epoch 12 | train_loss 1.0345 acc 0.482 f1 0.477 || val_loss 1.1189 acc 0.348 f1 0.347\n",
            "[W2] LSTM Epoch 13 | train_loss 1.0202 acc 0.484 f1 0.482 || val_loss 1.1283 acc 0.329 f1 0.329\n",
            "[W2] LSTM Epoch 14 | train_loss 1.0029 acc 0.501 f1 0.494 || val_loss 1.1360 acc 0.329 f1 0.329\n",
            "[W2] LSTM Epoch 15 | train_loss 0.9865 acc 0.525 f1 0.523 || val_loss 1.1487 acc 0.338 f1 0.337\n",
            "[W2] LSTM Epoch 16 | train_loss 0.9569 acc 0.546 f1 0.545 || val_loss 1.1669 acc 0.345 f1 0.344\n",
            "[W2] LSTM Epoch 17 | train_loss 0.9355 acc 0.560 f1 0.556 || val_loss 1.1826 acc 0.340 f1 0.339\n",
            "[W2] LSTM Epoch 18 | train_loss 0.9107 acc 0.593 f1 0.590 || val_loss 1.2104 acc 0.333 f1 0.329\n",
            "[W2] LSTM Epoch 19 | train_loss 0.8807 acc 0.614 f1 0.613 || val_loss 1.2294 acc 0.355 f1 0.355\n",
            "[W2] LSTM Epoch 20 | train_loss 0.8501 acc 0.623 f1 0.621 || val_loss 1.2660 acc 0.326 f1 0.322\n",
            "[W2] LSTM Epoch 21 | train_loss 0.8291 acc 0.644 f1 0.643 || val_loss 1.2848 acc 0.338 f1 0.337\n",
            "[W2] LSTM Epoch 22 | train_loss 0.7980 acc 0.662 f1 0.662 || val_loss 1.3138 acc 0.340 f1 0.340\n",
            "[W2] LSTM Epoch 23 | train_loss 0.7708 acc 0.671 f1 0.670 || val_loss 1.3659 acc 0.340 f1 0.338\n",
            "[W2] LSTM Epoch 24 | train_loss 0.7443 acc 0.679 f1 0.678 || val_loss 1.3855 acc 0.352 f1 0.353\n",
            "[W2] LSTM Epoch 25 | train_loss 0.7142 acc 0.702 f1 0.701 || val_loss 1.4262 acc 0.336 f1 0.334\n",
            "[W2] LSTM Epoch 26 | train_loss 0.6814 acc 0.711 f1 0.711 || val_loss 1.4541 acc 0.338 f1 0.338\n",
            "[W2] LSTM Epoch 27 | train_loss 0.6569 acc 0.732 f1 0.732 || val_loss 1.4950 acc 0.329 f1 0.327\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=ANN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 660, np.int64(1): 632, np.int64(0): 598})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 660, np.int64(1): 660, np.int64(2): 660})\n",
            "[W3] ANN Epoch 01 | train_loss 1.2164 acc 0.338 f1 0.336 || val_loss 1.1131 acc 0.336 f1 0.336\n",
            "[W3] ANN Epoch 02 | train_loss 1.0978 acc 0.420 f1 0.419 || val_loss 1.1152 acc 0.358 f1 0.358\n",
            "[W3] ANN Epoch 03 | train_loss 1.0456 acc 0.455 f1 0.454 || val_loss 1.1197 acc 0.348 f1 0.348\n",
            "[W3] ANN Epoch 04 | train_loss 1.0270 acc 0.473 f1 0.473 || val_loss 1.1420 acc 0.319 f1 0.318\n",
            "[W3] ANN Epoch 05 | train_loss 0.9907 acc 0.509 f1 0.509 || val_loss 1.1569 acc 0.326 f1 0.326\n",
            "[W3] ANN Epoch 06 | train_loss 0.9556 acc 0.557 f1 0.556 || val_loss 1.1781 acc 0.351 f1 0.350\n",
            "[W3] ANN Epoch 07 | train_loss 0.9297 acc 0.560 f1 0.560 || val_loss 1.1967 acc 0.333 f1 0.333\n",
            "[W3] ANN Epoch 08 | train_loss 0.8711 acc 0.599 f1 0.599 || val_loss 1.2407 acc 0.331 f1 0.330\n",
            "[W3] ANN Epoch 09 | train_loss 0.8371 acc 0.612 f1 0.611 || val_loss 1.2631 acc 0.348 f1 0.347\n",
            "[W3] ANN Epoch 10 | train_loss 0.8057 acc 0.628 f1 0.628 || val_loss 1.3053 acc 0.346 f1 0.345\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 660, np.int64(1): 632, np.int64(0): 598})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 660, np.int64(1): 660, np.int64(2): 660})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.1094 acc 0.338 f1 0.334 || val_loss 1.1019 acc 0.321 f1 0.301\n",
            "[W3] CNN1D Epoch 02 | train_loss 1.0706 acc 0.430 f1 0.422 || val_loss 1.1014 acc 0.353 f1 0.347\n",
            "[W3] CNN1D Epoch 03 | train_loss 1.0411 acc 0.477 f1 0.477 || val_loss 1.1087 acc 0.353 f1 0.351\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.9975 acc 0.537 f1 0.537 || val_loss 1.1183 acc 0.353 f1 0.353\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.9409 acc 0.573 f1 0.573 || val_loss 1.1457 acc 0.358 f1 0.357\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.8779 acc 0.607 f1 0.607 || val_loss 1.1966 acc 0.346 f1 0.345\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.7946 acc 0.670 f1 0.670 || val_loss 1.2557 acc 0.375 f1 0.374\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.6968 acc 0.716 f1 0.716 || val_loss 1.3608 acc 0.360 f1 0.359\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.6163 acc 0.758 f1 0.758 || val_loss 1.3903 acc 0.400 f1 0.398\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.5451 acc 0.793 f1 0.793 || val_loss 1.5215 acc 0.346 f1 0.346\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.4559 acc 0.841 f1 0.841 || val_loss 1.6780 acc 0.356 f1 0.352\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3964 acc 0.871 f1 0.871 || val_loss 1.7122 acc 0.368 f1 0.367\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3371 acc 0.882 f1 0.882 || val_loss 1.7948 acc 0.360 f1 0.360\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2929 acc 0.905 f1 0.905 || val_loss 1.9142 acc 0.351 f1 0.350\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2267 acc 0.930 f1 0.930 || val_loss 2.0580 acc 0.343 f1 0.343\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1834 acc 0.951 f1 0.951 || val_loss 2.1936 acc 0.356 f1 0.356\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1594 acc 0.952 f1 0.952 || val_loss 2.3330 acc 0.373 f1 0.373\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 660, np.int64(1): 632, np.int64(0): 598})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 660, np.int64(1): 660, np.int64(2): 660})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1029 acc 0.334 f1 0.295 || val_loss 1.1026 acc 0.314 f1 0.294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0914 acc 0.383 f1 0.373 || val_loss 1.1031 acc 0.311 f1 0.309\n",
            "[W3] RNN Epoch 03 | train_loss 1.0844 acc 0.403 f1 0.401 || val_loss 1.1077 acc 0.319 f1 0.319\n",
            "[W3] RNN Epoch 04 | train_loss 1.0788 acc 0.416 f1 0.411 || val_loss 1.1117 acc 0.304 f1 0.301\n",
            "[W3] RNN Epoch 05 | train_loss 1.0756 acc 0.421 f1 0.419 || val_loss 1.1189 acc 0.321 f1 0.319\n",
            "[W3] RNN Epoch 06 | train_loss 1.0650 acc 0.428 f1 0.422 || val_loss 1.1235 acc 0.323 f1 0.321\n",
            "[W3] RNN Epoch 07 | train_loss 1.0590 acc 0.445 f1 0.444 || val_loss 1.1286 acc 0.314 f1 0.308\n",
            "[W3] RNN Epoch 08 | train_loss 1.0508 acc 0.468 f1 0.467 || val_loss 1.1301 acc 0.311 f1 0.310\n",
            "[W3] RNN Epoch 09 | train_loss 1.0464 acc 0.460 f1 0.458 || val_loss 1.1343 acc 0.311 f1 0.309\n",
            "[W3] RNN Epoch 10 | train_loss 1.0409 acc 0.472 f1 0.472 || val_loss 1.1365 acc 0.314 f1 0.311\n",
            "[W3] RNN Epoch 11 | train_loss 1.0301 acc 0.490 f1 0.489 || val_loss 1.1486 acc 0.319 f1 0.318\n",
            "[W3] RNN Epoch 12 | train_loss 1.0254 acc 0.479 f1 0.478 || val_loss 1.1489 acc 0.331 f1 0.330\n",
            "[W3] RNN Epoch 13 | train_loss 1.0118 acc 0.517 f1 0.517 || val_loss 1.1525 acc 0.316 f1 0.315\n",
            "[W3] RNN Epoch 14 | train_loss 1.0027 acc 0.516 f1 0.516 || val_loss 1.1539 acc 0.341 f1 0.339\n",
            "[W3] RNN Epoch 15 | train_loss 0.9926 acc 0.532 f1 0.531 || val_loss 1.1644 acc 0.326 f1 0.325\n",
            "[W3] RNN Epoch 16 | train_loss 0.9860 acc 0.528 f1 0.528 || val_loss 1.1741 acc 0.321 f1 0.321\n",
            "[W3] RNN Epoch 17 | train_loss 0.9711 acc 0.554 f1 0.554 || val_loss 1.1799 acc 0.321 f1 0.320\n",
            "[W3] RNN Epoch 18 | train_loss 0.9711 acc 0.543 f1 0.543 || val_loss 1.1804 acc 0.309 f1 0.309\n",
            "[W3] RNN Epoch 19 | train_loss 0.9603 acc 0.542 f1 0.542 || val_loss 1.1861 acc 0.319 f1 0.318\n",
            "[W3] RNN Epoch 20 | train_loss 0.9540 acc 0.566 f1 0.566 || val_loss 1.1962 acc 0.316 f1 0.315\n",
            "[W3] RNN Epoch 21 | train_loss 0.9368 acc 0.563 f1 0.563 || val_loss 1.2053 acc 0.314 f1 0.314\n",
            "[W3] RNN Epoch 22 | train_loss 0.9305 acc 0.578 f1 0.578 || val_loss 1.2108 acc 0.316 f1 0.317\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 660, np.int64(1): 632, np.int64(0): 598})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 660, np.int64(1): 660, np.int64(2): 660})\n",
            "[W3] GRU Epoch 01 | train_loss 1.1042 acc 0.334 f1 0.172 || val_loss 1.1026 acc 0.343 f1 0.184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 02 | train_loss 1.0952 acc 0.354 f1 0.277 || val_loss 1.1018 acc 0.311 f1 0.282\n",
            "[W3] GRU Epoch 03 | train_loss 1.0905 acc 0.386 f1 0.383 || val_loss 1.1013 acc 0.343 f1 0.344\n",
            "[W3] GRU Epoch 04 | train_loss 1.0846 acc 0.402 f1 0.398 || val_loss 1.1025 acc 0.336 f1 0.335\n",
            "[W3] GRU Epoch 05 | train_loss 1.0789 acc 0.410 f1 0.408 || val_loss 1.1060 acc 0.351 f1 0.346\n",
            "[W3] GRU Epoch 06 | train_loss 1.0742 acc 0.415 f1 0.414 || val_loss 1.1095 acc 0.360 f1 0.357\n",
            "[W3] GRU Epoch 07 | train_loss 1.0705 acc 0.422 f1 0.420 || val_loss 1.1118 acc 0.341 f1 0.339\n",
            "[W3] GRU Epoch 08 | train_loss 1.0612 acc 0.440 f1 0.440 || val_loss 1.1145 acc 0.351 f1 0.349\n",
            "[W3] GRU Epoch 09 | train_loss 1.0495 acc 0.447 f1 0.446 || val_loss 1.1213 acc 0.346 f1 0.345\n",
            "[W3] GRU Epoch 10 | train_loss 1.0407 acc 0.462 f1 0.461 || val_loss 1.1260 acc 0.353 f1 0.352\n",
            "[W3] GRU Epoch 11 | train_loss 1.0267 acc 0.488 f1 0.488 || val_loss 1.1323 acc 0.351 f1 0.347\n",
            "[W3] GRU Epoch 12 | train_loss 1.0154 acc 0.498 f1 0.497 || val_loss 1.1425 acc 0.331 f1 0.329\n",
            "[W3] GRU Epoch 13 | train_loss 0.9978 acc 0.511 f1 0.511 || val_loss 1.1488 acc 0.331 f1 0.324\n",
            "[W3] GRU Epoch 14 | train_loss 0.9759 acc 0.539 f1 0.539 || val_loss 1.1587 acc 0.336 f1 0.330\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 660, np.int64(1): 632, np.int64(0): 598})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 660, np.int64(1): 660, np.int64(2): 660})\n",
            "[W3] LSTM Epoch 01 | train_loss 1.0994 acc 0.326 f1 0.314 || val_loss 1.0998 acc 0.286 f1 0.285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 02 | train_loss 1.0965 acc 0.364 f1 0.364 || val_loss 1.0993 acc 0.296 f1 0.293\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0942 acc 0.382 f1 0.381 || val_loss 1.0993 acc 0.309 f1 0.306\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0921 acc 0.392 f1 0.389 || val_loss 1.1004 acc 0.319 f1 0.318\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0879 acc 0.402 f1 0.402 || val_loss 1.1012 acc 0.323 f1 0.321\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0821 acc 0.419 f1 0.418 || val_loss 1.1045 acc 0.321 f1 0.319\n",
            "[W3] LSTM Epoch 07 | train_loss 1.0753 acc 0.421 f1 0.420 || val_loss 1.1088 acc 0.328 f1 0.329\n",
            "[W3] LSTM Epoch 08 | train_loss 1.0662 acc 0.435 f1 0.434 || val_loss 1.1143 acc 0.341 f1 0.339\n",
            "[W3] LSTM Epoch 09 | train_loss 1.0579 acc 0.440 f1 0.440 || val_loss 1.1275 acc 0.333 f1 0.333\n",
            "[W3] LSTM Epoch 10 | train_loss 1.0464 acc 0.457 f1 0.457 || val_loss 1.1330 acc 0.333 f1 0.333\n",
            "[W3] LSTM Epoch 11 | train_loss 1.0296 acc 0.482 f1 0.481 || val_loss 1.1429 acc 0.353 f1 0.351\n",
            "[W3] LSTM Epoch 12 | train_loss 1.0122 acc 0.498 f1 0.498 || val_loss 1.1578 acc 0.336 f1 0.336\n",
            "[W3] LSTM Epoch 13 | train_loss 0.9870 acc 0.517 f1 0.517 || val_loss 1.1813 acc 0.333 f1 0.333\n",
            "[W3] LSTM Epoch 14 | train_loss 0.9578 acc 0.553 f1 0.552 || val_loss 1.2007 acc 0.328 f1 0.327\n",
            "[W3] LSTM Epoch 15 | train_loss 0.9228 acc 0.570 f1 0.569 || val_loss 1.2278 acc 0.333 f1 0.329\n",
            "[W3] LSTM Epoch 16 | train_loss 0.8916 acc 0.600 f1 0.599 || val_loss 1.2656 acc 0.326 f1 0.322\n",
            "[W3] LSTM Epoch 17 | train_loss 0.8543 acc 0.640 f1 0.640 || val_loss 1.3063 acc 0.333 f1 0.334\n",
            "[W3] LSTM Epoch 18 | train_loss 0.8141 acc 0.656 f1 0.656 || val_loss 1.3484 acc 0.326 f1 0.326\n",
            "[W3] LSTM Epoch 19 | train_loss 0.7662 acc 0.679 f1 0.678 || val_loss 1.4252 acc 0.316 f1 0.316\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=4 Model=ANN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 631, np.int64(1): 613, np.int64(0): 576})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 631, np.int64(0): 631, np.int64(2): 631})\n",
            "[W4] ANN Epoch 01 | train_loss 1.2261 acc 0.335 f1 0.333 || val_loss 1.1430 acc 0.297 f1 0.282\n",
            "[W4] ANN Epoch 02 | train_loss 1.0827 acc 0.412 f1 0.409 || val_loss 1.1695 acc 0.279 f1 0.269\n",
            "[W4] ANN Epoch 03 | train_loss 1.0493 acc 0.440 f1 0.439 || val_loss 1.1915 acc 0.305 f1 0.298\n",
            "[W4] ANN Epoch 04 | train_loss 1.0045 acc 0.491 f1 0.489 || val_loss 1.1967 acc 0.313 f1 0.304\n",
            "[W4] ANN Epoch 05 | train_loss 0.9651 acc 0.517 f1 0.517 || val_loss 1.2250 acc 0.290 f1 0.283\n",
            "[W4] ANN Epoch 06 | train_loss 0.9156 acc 0.575 f1 0.575 || val_loss 1.2537 acc 0.295 f1 0.292\n",
            "[W4] ANN Epoch 07 | train_loss 0.8636 acc 0.605 f1 0.605 || val_loss 1.2818 acc 0.297 f1 0.296\n",
            "[W4] ANN Epoch 08 | train_loss 0.8358 acc 0.623 f1 0.622 || val_loss 1.3167 acc 0.285 f1 0.280\n",
            "[W4] ANN Epoch 09 | train_loss 0.7720 acc 0.663 f1 0.662 || val_loss 1.3742 acc 0.285 f1 0.279\n",
            "[W4] ANN Epoch 10 | train_loss 0.7277 acc 0.685 f1 0.685 || val_loss 1.4316 acc 0.287 f1 0.282\n",
            "[W4] ANN Epoch 11 | train_loss 0.6574 acc 0.727 f1 0.727 || val_loss 1.4515 acc 0.305 f1 0.299\n",
            "[W4] ANN Epoch 12 | train_loss 0.6462 acc 0.716 f1 0.716 || val_loss 1.5381 acc 0.338 f1 0.335\n",
            "[W4] ANN Epoch 13 | train_loss 0.6038 acc 0.752 f1 0.752 || val_loss 1.6246 acc 0.310 f1 0.307\n",
            "[W4] ANN Epoch 14 | train_loss 0.5519 acc 0.775 f1 0.775 || val_loss 1.6014 acc 0.341 f1 0.338\n",
            "[W4] ANN Epoch 15 | train_loss 0.4807 acc 0.807 f1 0.807 || val_loss 1.6917 acc 0.336 f1 0.332\n",
            "[W4] ANN Epoch 16 | train_loss 0.5026 acc 0.795 f1 0.795 || val_loss 1.6958 acc 0.338 f1 0.337\n",
            "[W4] ANN Epoch 17 | train_loss 0.4776 acc 0.814 f1 0.814 || val_loss 1.7930 acc 0.318 f1 0.313\n",
            "[W4] ANN Epoch 18 | train_loss 0.4649 acc 0.807 f1 0.807 || val_loss 1.8231 acc 0.333 f1 0.332\n",
            "[W4] ANN Epoch 19 | train_loss 0.4381 acc 0.827 f1 0.827 || val_loss 1.8539 acc 0.318 f1 0.315\n",
            "[W4] ANN Epoch 20 | train_loss 0.4273 acc 0.836 f1 0.836 || val_loss 1.8617 acc 0.321 f1 0.315\n",
            "[W4] ANN Epoch 21 | train_loss 0.3766 acc 0.855 f1 0.855 || val_loss 1.9674 acc 0.333 f1 0.326\n",
            "[W4] ANN Epoch 22 | train_loss 0.3334 acc 0.873 f1 0.873 || val_loss 1.9737 acc 0.354 f1 0.347\n",
            "[W4] ANN Epoch 23 | train_loss 0.3395 acc 0.863 f1 0.863 || val_loss 2.0248 acc 0.344 f1 0.335\n",
            "[W4] ANN Epoch 24 | train_loss 0.3305 acc 0.870 f1 0.870 || val_loss 1.9906 acc 0.331 f1 0.327\n",
            "[W4] ANN Epoch 25 | train_loss 0.3001 acc 0.877 f1 0.878 || val_loss 2.0853 acc 0.336 f1 0.333\n",
            "[W4] ANN Epoch 26 | train_loss 0.2691 acc 0.896 f1 0.896 || val_loss 2.1417 acc 0.344 f1 0.337\n",
            "[W4] ANN Epoch 27 | train_loss 0.2650 acc 0.894 f1 0.894 || val_loss 2.2292 acc 0.369 f1 0.367\n",
            "[W4] ANN Epoch 28 | train_loss 0.2782 acc 0.887 f1 0.888 || val_loss 2.3193 acc 0.344 f1 0.338\n",
            "[W4] ANN Epoch 29 | train_loss 0.2581 acc 0.902 f1 0.902 || val_loss 2.2445 acc 0.326 f1 0.322\n",
            "[W4] ANN Epoch 30 | train_loss 0.2700 acc 0.897 f1 0.897 || val_loss 2.2043 acc 0.328 f1 0.319\n",
            "[W4] ANN Epoch 31 | train_loss 0.2506 acc 0.906 f1 0.906 || val_loss 2.2577 acc 0.344 f1 0.335\n",
            "[W4] ANN Epoch 32 | train_loss 0.2241 acc 0.920 f1 0.920 || val_loss 2.2206 acc 0.333 f1 0.329\n",
            "[W4] ANN Epoch 33 | train_loss 0.2543 acc 0.909 f1 0.909 || val_loss 2.3018 acc 0.369 f1 0.367\n",
            "[W4] ANN Epoch 34 | train_loss 0.2521 acc 0.913 f1 0.913 || val_loss 2.3656 acc 0.336 f1 0.332\n",
            "[W4] ANN Epoch 35 | train_loss 0.2150 acc 0.920 f1 0.920 || val_loss 2.4346 acc 0.344 f1 0.341\n",
            "[W4] ANN Epoch 36 | train_loss 0.2165 acc 0.928 f1 0.928 || val_loss 2.3994 acc 0.354 f1 0.350\n",
            "[W4] ANN Epoch 37 | train_loss 0.2025 acc 0.927 f1 0.927 || val_loss 2.5022 acc 0.349 f1 0.339\n",
            "[W4] ANN Epoch 38 | train_loss 0.2038 acc 0.925 f1 0.925 || val_loss 2.4851 acc 0.336 f1 0.332\n",
            "[W4] ANN Epoch 39 | train_loss 0.1918 acc 0.929 f1 0.929 || val_loss 2.5542 acc 0.333 f1 0.327\n",
            "[W4] ANN Epoch 40 | train_loss 0.2016 acc 0.927 f1 0.927 || val_loss 2.6510 acc 0.318 f1 0.314\n",
            "[W4] ANN Epoch 41 | train_loss 0.2225 acc 0.916 f1 0.916 || val_loss 2.6290 acc 0.321 f1 0.316\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=4 Model=CNN1D\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 631, np.int64(1): 613, np.int64(0): 576})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 631, np.int64(0): 631, np.int64(2): 631})\n",
            "[W4] CNN1D Epoch 01 | train_loss 1.0984 acc 0.352 f1 0.340 || val_loss 1.0992 acc 0.344 f1 0.330\n",
            "[W4] CNN1D Epoch 02 | train_loss 1.0730 acc 0.446 f1 0.446 || val_loss 1.1030 acc 0.338 f1 0.337\n",
            "[W4] CNN1D Epoch 03 | train_loss 1.0485 acc 0.473 f1 0.471 || val_loss 1.1143 acc 0.341 f1 0.329\n",
            "[W4] CNN1D Epoch 04 | train_loss 1.0143 acc 0.511 f1 0.511 || val_loss 1.1394 acc 0.349 f1 0.318\n",
            "[W4] CNN1D Epoch 05 | train_loss 0.9735 acc 0.557 f1 0.557 || val_loss 1.1526 acc 0.351 f1 0.350\n",
            "[W4] CNN1D Epoch 06 | train_loss 0.9088 acc 0.605 f1 0.605 || val_loss 1.2278 acc 0.333 f1 0.310\n",
            "[W4] CNN1D Epoch 07 | train_loss 0.8436 acc 0.631 f1 0.631 || val_loss 1.2398 acc 0.341 f1 0.340\n",
            "[W4] CNN1D Epoch 08 | train_loss 0.7705 acc 0.675 f1 0.675 || val_loss 1.3051 acc 0.354 f1 0.353\n",
            "[W4] CNN1D Epoch 09 | train_loss 0.7096 acc 0.719 f1 0.719 || val_loss 1.3714 acc 0.323 f1 0.319\n",
            "[W4] CNN1D Epoch 10 | train_loss 0.6488 acc 0.739 f1 0.739 || val_loss 1.4279 acc 0.344 f1 0.342\n",
            "[W4] CNN1D Epoch 11 | train_loss 0.5859 acc 0.770 f1 0.770 || val_loss 1.4817 acc 0.367 f1 0.361\n",
            "[W4] CNN1D Epoch 12 | train_loss 0.5238 acc 0.794 f1 0.794 || val_loss 1.6188 acc 0.323 f1 0.312\n",
            "[W4] CNN1D Epoch 13 | train_loss 0.4804 acc 0.808 f1 0.808 || val_loss 1.6710 acc 0.341 f1 0.334\n",
            "[W4] CNN1D Epoch 14 | train_loss 0.4419 acc 0.833 f1 0.833 || val_loss 1.7244 acc 0.359 f1 0.351\n",
            "[W4] CNN1D Epoch 15 | train_loss 0.4004 acc 0.847 f1 0.847 || val_loss 1.7490 acc 0.331 f1 0.328\n",
            "[W4] CNN1D Epoch 16 | train_loss 0.3512 acc 0.875 f1 0.875 || val_loss 1.8496 acc 0.359 f1 0.358\n",
            "[W4] CNN1D Epoch 17 | train_loss 0.3117 acc 0.890 f1 0.890 || val_loss 2.0207 acc 0.331 f1 0.329\n",
            "[W4] CNN1D Epoch 18 | train_loss 0.3016 acc 0.891 f1 0.891 || val_loss 2.0189 acc 0.333 f1 0.330\n",
            "[W4] CNN1D Epoch 19 | train_loss 0.2677 acc 0.903 f1 0.903 || val_loss 2.1013 acc 0.351 f1 0.351\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=4 Model=RNN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 631, np.int64(1): 613, np.int64(0): 576})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 631, np.int64(0): 631, np.int64(2): 631})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W4] RNN Epoch 01 | train_loss 1.1068 acc 0.309 f1 0.288 || val_loss 1.1042 acc 0.328 f1 0.310\n",
            "[W4] RNN Epoch 02 | train_loss 1.0918 acc 0.376 f1 0.372 || val_loss 1.1042 acc 0.292 f1 0.283\n",
            "[W4] RNN Epoch 03 | train_loss 1.0868 acc 0.401 f1 0.392 || val_loss 1.1062 acc 0.292 f1 0.285\n",
            "[W4] RNN Epoch 04 | train_loss 1.0801 acc 0.406 f1 0.401 || val_loss 1.1128 acc 0.297 f1 0.286\n",
            "[W4] RNN Epoch 05 | train_loss 1.0716 acc 0.424 f1 0.408 || val_loss 1.1153 acc 0.292 f1 0.285\n",
            "[W4] RNN Epoch 06 | train_loss 1.0664 acc 0.443 f1 0.442 || val_loss 1.1193 acc 0.303 f1 0.301\n",
            "[W4] RNN Epoch 07 | train_loss 1.0541 acc 0.464 f1 0.463 || val_loss 1.1277 acc 0.310 f1 0.307\n",
            "[W4] RNN Epoch 08 | train_loss 1.0479 acc 0.464 f1 0.464 || val_loss 1.1331 acc 0.292 f1 0.290\n",
            "[W4] RNN Epoch 09 | train_loss 1.0359 acc 0.475 f1 0.475 || val_loss 1.1349 acc 0.318 f1 0.312\n",
            "[W4] RNN Epoch 10 | train_loss 1.0260 acc 0.491 f1 0.491 || val_loss 1.1431 acc 0.290 f1 0.285\n",
            "[W4] RNN Epoch 11 | train_loss 1.0087 acc 0.507 f1 0.507 || val_loss 1.1510 acc 0.290 f1 0.289\n",
            "[W4] RNN Epoch 12 | train_loss 0.9998 acc 0.528 f1 0.528 || val_loss 1.1576 acc 0.292 f1 0.289\n",
            "[W4] RNN Epoch 13 | train_loss 0.9839 acc 0.522 f1 0.521 || val_loss 1.1689 acc 0.279 f1 0.279\n",
            "[W4] RNN Epoch 14 | train_loss 0.9711 acc 0.541 f1 0.542 || val_loss 1.1829 acc 0.297 f1 0.294\n",
            "[W4] RNN Epoch 15 | train_loss 0.9584 acc 0.545 f1 0.544 || val_loss 1.1966 acc 0.303 f1 0.302\n",
            "[W4] RNN Epoch 16 | train_loss 0.9503 acc 0.540 f1 0.540 || val_loss 1.2054 acc 0.295 f1 0.292\n",
            "[W4] RNN Epoch 17 | train_loss 0.9258 acc 0.582 f1 0.582 || val_loss 1.2188 acc 0.279 f1 0.279\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=4 Model=GRU\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 631, np.int64(1): 613, np.int64(0): 576})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 631, np.int64(0): 631, np.int64(2): 631})\n",
            "[W4] GRU Epoch 01 | train_loss 1.1028 acc 0.318 f1 0.181 || val_loss 1.1017 acc 0.321 f1 0.181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W4] GRU Epoch 02 | train_loss 1.0969 acc 0.355 f1 0.294 || val_loss 1.1009 acc 0.328 f1 0.294\n",
            "[W4] GRU Epoch 03 | train_loss 1.0933 acc 0.384 f1 0.379 || val_loss 1.1014 acc 0.331 f1 0.329\n",
            "[W4] GRU Epoch 04 | train_loss 1.0891 acc 0.402 f1 0.400 || val_loss 1.1043 acc 0.321 f1 0.321\n",
            "[W4] GRU Epoch 05 | train_loss 1.0835 acc 0.400 f1 0.399 || val_loss 1.1074 acc 0.318 f1 0.318\n",
            "[W4] GRU Epoch 06 | train_loss 1.0763 acc 0.415 f1 0.414 || val_loss 1.1133 acc 0.292 f1 0.293\n",
            "[W4] GRU Epoch 07 | train_loss 1.0687 acc 0.429 f1 0.429 || val_loss 1.1215 acc 0.297 f1 0.298\n",
            "[W4] GRU Epoch 08 | train_loss 1.0630 acc 0.438 f1 0.437 || val_loss 1.1285 acc 0.303 f1 0.303\n",
            "[W4] GRU Epoch 09 | train_loss 1.0509 acc 0.444 f1 0.444 || val_loss 1.1337 acc 0.303 f1 0.300\n",
            "[W4] GRU Epoch 10 | train_loss 1.0411 acc 0.458 f1 0.458 || val_loss 1.1483 acc 0.292 f1 0.292\n",
            "[W4] GRU Epoch 11 | train_loss 1.0309 acc 0.466 f1 0.466 || val_loss 1.1518 acc 0.287 f1 0.283\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=4 Model=LSTM\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 631, np.int64(1): 613, np.int64(0): 576})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 631, np.int64(0): 631, np.int64(2): 631})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W4] LSTM Epoch 01 | train_loss 1.0991 acc 0.332 f1 0.233 || val_loss 1.0999 acc 0.305 f1 0.251\n",
            "[W4] LSTM Epoch 02 | train_loss 1.0962 acc 0.370 f1 0.351 || val_loss 1.1001 acc 0.326 f1 0.312\n",
            "[W4] LSTM Epoch 03 | train_loss 1.0945 acc 0.382 f1 0.381 || val_loss 1.1005 acc 0.341 f1 0.340\n",
            "[W4] LSTM Epoch 04 | train_loss 1.0900 acc 0.404 f1 0.398 || val_loss 1.1027 acc 0.323 f1 0.320\n",
            "[W4] LSTM Epoch 05 | train_loss 1.0846 acc 0.407 f1 0.407 || val_loss 1.1066 acc 0.318 f1 0.318\n",
            "[W4] LSTM Epoch 06 | train_loss 1.0797 acc 0.415 f1 0.413 || val_loss 1.1114 acc 0.328 f1 0.328\n",
            "[W4] LSTM Epoch 07 | train_loss 1.0728 acc 0.403 f1 0.395 || val_loss 1.1210 acc 0.292 f1 0.289\n",
            "[W4] LSTM Epoch 08 | train_loss 1.0640 acc 0.429 f1 0.429 || val_loss 1.1253 acc 0.321 f1 0.320\n",
            "[W4] LSTM Epoch 09 | train_loss 1.0520 acc 0.442 f1 0.442 || val_loss 1.1343 acc 0.328 f1 0.329\n",
            "[W4] LSTM Epoch 10 | train_loss 1.0384 acc 0.459 f1 0.455 || val_loss 1.1539 acc 0.310 f1 0.310\n",
            "[W4] LSTM Epoch 11 | train_loss 1.0166 acc 0.474 f1 0.473 || val_loss 1.1701 acc 0.313 f1 0.311\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=5 Model=ANN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 610, np.int64(1): 588, np.int64(0): 552})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 610, np.int64(2): 610, np.int64(1): 610})\n",
            "[W5] ANN Epoch 01 | train_loss 1.2060 acc 0.331 f1 0.328 || val_loss 1.1190 acc 0.344 f1 0.335\n",
            "[W5] ANN Epoch 02 | train_loss 1.0766 acc 0.413 f1 0.412 || val_loss 1.1346 acc 0.325 f1 0.323\n",
            "[W5] ANN Epoch 03 | train_loss 1.0376 acc 0.472 f1 0.471 || val_loss 1.1443 acc 0.301 f1 0.295\n",
            "[W5] ANN Epoch 04 | train_loss 0.9781 acc 0.507 f1 0.505 || val_loss 1.1582 acc 0.320 f1 0.314\n",
            "[W5] ANN Epoch 05 | train_loss 0.9438 acc 0.542 f1 0.541 || val_loss 1.1942 acc 0.307 f1 0.303\n",
            "[W5] ANN Epoch 06 | train_loss 0.8718 acc 0.613 f1 0.613 || val_loss 1.2344 acc 0.315 f1 0.313\n",
            "[W5] ANN Epoch 07 | train_loss 0.8223 acc 0.636 f1 0.636 || val_loss 1.2845 acc 0.307 f1 0.306\n",
            "[W5] ANN Epoch 08 | train_loss 0.7511 acc 0.680 f1 0.680 || val_loss 1.3681 acc 0.315 f1 0.312\n",
            "[W5] ANN Epoch 09 | train_loss 0.7337 acc 0.677 f1 0.677 || val_loss 1.4128 acc 0.291 f1 0.289\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=5 Model=CNN1D\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 610, np.int64(1): 588, np.int64(0): 552})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 610, np.int64(2): 610, np.int64(1): 610})\n",
            "[W5] CNN1D Epoch 01 | train_loss 1.1051 acc 0.342 f1 0.339 || val_loss 1.1005 acc 0.349 f1 0.277\n",
            "[W5] CNN1D Epoch 02 | train_loss 1.0845 acc 0.404 f1 0.404 || val_loss 1.1078 acc 0.355 f1 0.353\n",
            "[W5] CNN1D Epoch 03 | train_loss 1.0657 acc 0.448 f1 0.447 || val_loss 1.1111 acc 0.365 f1 0.364\n",
            "[W5] CNN1D Epoch 04 | train_loss 1.0379 acc 0.487 f1 0.486 || val_loss 1.1283 acc 0.347 f1 0.342\n",
            "[W5] CNN1D Epoch 05 | train_loss 1.0030 acc 0.515 f1 0.514 || val_loss 1.1492 acc 0.333 f1 0.324\n",
            "[W5] CNN1D Epoch 06 | train_loss 0.9615 acc 0.559 f1 0.559 || val_loss 1.1851 acc 0.304 f1 0.296\n",
            "[W5] CNN1D Epoch 07 | train_loss 0.9165 acc 0.583 f1 0.583 || val_loss 1.2195 acc 0.299 f1 0.296\n",
            "[W5] CNN1D Epoch 08 | train_loss 0.8709 acc 0.606 f1 0.606 || val_loss 1.2419 acc 0.299 f1 0.298\n",
            "[W5] CNN1D Epoch 09 | train_loss 0.8234 acc 0.638 f1 0.638 || val_loss 1.3050 acc 0.304 f1 0.301\n",
            "[W5] CNN1D Epoch 10 | train_loss 0.7754 acc 0.658 f1 0.658 || val_loss 1.3357 acc 0.312 f1 0.305\n",
            "[W5] CNN1D Epoch 11 | train_loss 0.7306 acc 0.687 f1 0.687 || val_loss 1.4016 acc 0.323 f1 0.320\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=5 Model=RNN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 610, np.int64(1): 588, np.int64(0): 552})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 610, np.int64(2): 610, np.int64(1): 610})\n",
            "[W5] RNN Epoch 01 | train_loss 1.1020 acc 0.337 f1 0.264 || val_loss 1.0998 acc 0.339 f1 0.283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W5] RNN Epoch 02 | train_loss 1.0916 acc 0.384 f1 0.371 || val_loss 1.1011 acc 0.320 f1 0.301\n",
            "[W5] RNN Epoch 03 | train_loss 1.0864 acc 0.408 f1 0.398 || val_loss 1.1058 acc 0.296 f1 0.287\n",
            "[W5] RNN Epoch 04 | train_loss 1.0794 acc 0.426 f1 0.420 || val_loss 1.1118 acc 0.291 f1 0.283\n",
            "[W5] RNN Epoch 05 | train_loss 1.0733 acc 0.410 f1 0.401 || val_loss 1.1230 acc 0.293 f1 0.282\n",
            "[W5] RNN Epoch 06 | train_loss 1.0626 acc 0.443 f1 0.439 || val_loss 1.1311 acc 0.285 f1 0.281\n",
            "[W5] RNN Epoch 07 | train_loss 1.0580 acc 0.443 f1 0.441 || val_loss 1.1452 acc 0.269 f1 0.264\n",
            "[W5] RNN Epoch 08 | train_loss 1.0423 acc 0.471 f1 0.470 || val_loss 1.1541 acc 0.253 f1 0.253\n",
            "[W5] RNN Epoch 09 | train_loss 1.0353 acc 0.470 f1 0.470 || val_loss 1.1656 acc 0.267 f1 0.264\n",
            "[W5] RNN Epoch 10 | train_loss 1.0165 acc 0.502 f1 0.501 || val_loss 1.1821 acc 0.272 f1 0.272\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=5 Model=GRU\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 610, np.int64(1): 588, np.int64(0): 552})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 610, np.int64(2): 610, np.int64(1): 610})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W5] GRU Epoch 01 | train_loss 1.1004 acc 0.332 f1 0.310 || val_loss 1.0985 acc 0.355 f1 0.329\n",
            "[W5] GRU Epoch 02 | train_loss 1.0948 acc 0.372 f1 0.352 || val_loss 1.1006 acc 0.347 f1 0.339\n",
            "[W5] GRU Epoch 03 | train_loss 1.0908 acc 0.390 f1 0.378 || val_loss 1.1027 acc 0.363 f1 0.354\n",
            "[W5] GRU Epoch 04 | train_loss 1.0861 acc 0.421 f1 0.418 || val_loss 1.1046 acc 0.336 f1 0.336\n",
            "[W5] GRU Epoch 05 | train_loss 1.0821 acc 0.411 f1 0.399 || val_loss 1.1079 acc 0.344 f1 0.346\n",
            "[W5] GRU Epoch 06 | train_loss 1.0740 acc 0.422 f1 0.420 || val_loss 1.1145 acc 0.341 f1 0.341\n",
            "[W5] GRU Epoch 07 | train_loss 1.0669 acc 0.428 f1 0.421 || val_loss 1.1196 acc 0.341 f1 0.344\n",
            "[W5] GRU Epoch 08 | train_loss 1.0590 acc 0.443 f1 0.442 || val_loss 1.1278 acc 0.325 f1 0.325\n",
            "[W5] GRU Epoch 09 | train_loss 1.0511 acc 0.434 f1 0.427 || val_loss 1.1372 acc 0.333 f1 0.336\n",
            "[W5] GRU Epoch 10 | train_loss 1.0405 acc 0.456 f1 0.452 || val_loss 1.1478 acc 0.331 f1 0.331\n",
            "[W5] GRU Epoch 11 | train_loss 1.0283 acc 0.477 f1 0.475 || val_loss 1.1561 acc 0.323 f1 0.323\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=5 Model=LSTM\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 610, np.int64(1): 588, np.int64(0): 552})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 610, np.int64(2): 610, np.int64(1): 610})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W5] LSTM Epoch 01 | train_loss 1.1032 acc 0.333 f1 0.167 || val_loss 1.1043 acc 0.309 f1 0.158\n",
            "[W5] LSTM Epoch 02 | train_loss 1.0980 acc 0.352 f1 0.252 || val_loss 1.1008 acc 0.309 f1 0.236\n",
            "[W5] LSTM Epoch 03 | train_loss 1.0924 acc 0.389 f1 0.375 || val_loss 1.1001 acc 0.323 f1 0.315\n",
            "[W5] LSTM Epoch 04 | train_loss 1.0891 acc 0.384 f1 0.380 || val_loss 1.1048 acc 0.331 f1 0.330\n",
            "[W5] LSTM Epoch 05 | train_loss 1.0811 acc 0.404 f1 0.386 || val_loss 1.1141 acc 0.293 f1 0.290\n",
            "[W5] LSTM Epoch 06 | train_loss 1.0750 acc 0.411 f1 0.410 || val_loss 1.1212 acc 0.304 f1 0.305\n",
            "[W5] LSTM Epoch 07 | train_loss 1.0645 acc 0.432 f1 0.430 || val_loss 1.1297 acc 0.293 f1 0.293\n",
            "[W5] LSTM Epoch 08 | train_loss 1.0554 acc 0.430 f1 0.428 || val_loss 1.1496 acc 0.315 f1 0.314\n",
            "[W5] LSTM Epoch 09 | train_loss 1.0431 acc 0.465 f1 0.465 || val_loss 1.1525 acc 0.288 f1 0.286\n",
            "[W5] LSTM Epoch 10 | train_loss 1.0245 acc 0.479 f1 0.478 || val_loss 1.1758 acc 0.307 f1 0.307\n",
            "[W5] LSTM Epoch 11 | train_loss 1.0100 acc 0.488 f1 0.487 || val_loss 1.1896 acc 0.317 f1 0.315\n",
            "[W5] LSTM Epoch 12 | train_loss 0.9893 acc 0.509 f1 0.509 || val_loss 1.2041 acc 0.317 f1 0.317\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=6 Model=ANN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 586, np.int64(1): 563, np.int64(0): 531})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 586, np.int64(1): 586, np.int64(0): 586})\n",
            "[W6] ANN Epoch 01 | train_loss 1.2325 acc 0.342 f1 0.341 || val_loss 1.1422 acc 0.317 f1 0.289\n",
            "[W6] ANN Epoch 02 | train_loss 1.0910 acc 0.427 f1 0.426 || val_loss 1.1656 acc 0.306 f1 0.300\n",
            "[W6] ANN Epoch 03 | train_loss 1.0015 acc 0.503 f1 0.502 || val_loss 1.1820 acc 0.314 f1 0.309\n",
            "[W6] ANN Epoch 04 | train_loss 0.9624 acc 0.545 f1 0.543 || val_loss 1.2202 acc 0.317 f1 0.315\n",
            "[W6] ANN Epoch 05 | train_loss 0.8970 acc 0.585 f1 0.585 || val_loss 1.2528 acc 0.289 f1 0.285\n",
            "[W6] ANN Epoch 06 | train_loss 0.8435 acc 0.618 f1 0.617 || val_loss 1.3004 acc 0.300 f1 0.296\n",
            "[W6] ANN Epoch 07 | train_loss 0.7699 acc 0.660 f1 0.659 || val_loss 1.3510 acc 0.283 f1 0.282\n",
            "[W6] ANN Epoch 08 | train_loss 0.7054 acc 0.696 f1 0.695 || val_loss 1.4141 acc 0.308 f1 0.307\n",
            "[W6] ANN Epoch 09 | train_loss 0.6419 acc 0.734 f1 0.734 || val_loss 1.4627 acc 0.300 f1 0.294\n",
            "[W6] ANN Epoch 10 | train_loss 0.5838 acc 0.765 f1 0.764 || val_loss 1.5416 acc 0.281 f1 0.278\n",
            "[W6] ANN Epoch 11 | train_loss 0.5208 acc 0.787 f1 0.787 || val_loss 1.6683 acc 0.300 f1 0.298\n",
            "[W6] ANN Epoch 12 | train_loss 0.4640 acc 0.816 f1 0.816 || val_loss 1.6460 acc 0.306 f1 0.304\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=6 Model=CNN1D\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 586, np.int64(1): 563, np.int64(0): 531})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 586, np.int64(1): 586, np.int64(0): 586})\n",
            "[W6] CNN1D Epoch 01 | train_loss 1.1039 acc 0.332 f1 0.328 || val_loss 1.1039 acc 0.286 f1 0.239\n",
            "[W6] CNN1D Epoch 02 | train_loss 1.0822 acc 0.413 f1 0.412 || val_loss 1.1132 acc 0.317 f1 0.315\n",
            "[W6] CNN1D Epoch 03 | train_loss 1.0571 acc 0.453 f1 0.452 || val_loss 1.1319 acc 0.319 f1 0.307\n",
            "[W6] CNN1D Epoch 04 | train_loss 1.0377 acc 0.471 f1 0.468 || val_loss 1.1455 acc 0.331 f1 0.327\n",
            "[W6] CNN1D Epoch 05 | train_loss 1.0068 acc 0.518 f1 0.518 || val_loss 1.1663 acc 0.333 f1 0.333\n",
            "[W6] CNN1D Epoch 06 | train_loss 0.9743 acc 0.536 f1 0.535 || val_loss 1.1949 acc 0.325 f1 0.308\n",
            "[W6] CNN1D Epoch 07 | train_loss 0.9440 acc 0.549 f1 0.550 || val_loss 1.2267 acc 0.292 f1 0.289\n",
            "[W6] CNN1D Epoch 08 | train_loss 0.8937 acc 0.604 f1 0.603 || val_loss 1.2619 acc 0.311 f1 0.311\n",
            "[W6] CNN1D Epoch 09 | train_loss 0.8781 acc 0.609 f1 0.609 || val_loss 1.2587 acc 0.319 f1 0.318\n",
            "[W6] CNN1D Epoch 10 | train_loss 0.8369 acc 0.620 f1 0.620 || val_loss 1.3307 acc 0.314 f1 0.312\n",
            "[W6] CNN1D Epoch 11 | train_loss 0.8156 acc 0.632 f1 0.632 || val_loss 1.3147 acc 0.325 f1 0.325\n",
            "[W6] CNN1D Epoch 12 | train_loss 0.7886 acc 0.660 f1 0.660 || val_loss 1.3991 acc 0.325 f1 0.320\n",
            "[W6] CNN1D Epoch 13 | train_loss 0.7552 acc 0.659 f1 0.659 || val_loss 1.3882 acc 0.308 f1 0.299\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=6 Model=RNN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 586, np.int64(1): 563, np.int64(0): 531})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 586, np.int64(1): 586, np.int64(0): 586})\n",
            "[W6] RNN Epoch 01 | train_loss 1.1032 acc 0.322 f1 0.196 || val_loss 1.0994 acc 0.353 f1 0.265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W6] RNN Epoch 02 | train_loss 1.0922 acc 0.371 f1 0.363 || val_loss 1.0985 acc 0.339 f1 0.335\n",
            "[W6] RNN Epoch 03 | train_loss 1.0862 acc 0.399 f1 0.394 || val_loss 1.0997 acc 0.353 f1 0.351\n",
            "[W6] RNN Epoch 04 | train_loss 1.0759 acc 0.433 f1 0.432 || val_loss 1.1065 acc 0.308 f1 0.305\n",
            "[W6] RNN Epoch 05 | train_loss 1.0662 acc 0.440 f1 0.438 || val_loss 1.1154 acc 0.314 f1 0.311\n",
            "[W6] RNN Epoch 06 | train_loss 1.0554 acc 0.450 f1 0.449 || val_loss 1.1278 acc 0.339 f1 0.338\n",
            "[W6] RNN Epoch 07 | train_loss 1.0403 acc 0.467 f1 0.464 || val_loss 1.1420 acc 0.322 f1 0.321\n",
            "[W6] RNN Epoch 08 | train_loss 1.0307 acc 0.486 f1 0.484 || val_loss 1.1456 acc 0.339 f1 0.339\n",
            "[W6] RNN Epoch 09 | train_loss 1.0150 acc 0.502 f1 0.501 || val_loss 1.1563 acc 0.333 f1 0.333\n",
            "[W6] RNN Epoch 10 | train_loss 1.0071 acc 0.499 f1 0.498 || val_loss 1.1711 acc 0.344 f1 0.343\n",
            "[W6] RNN Epoch 11 | train_loss 0.9940 acc 0.519 f1 0.519 || val_loss 1.1822 acc 0.336 f1 0.336\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=6 Model=GRU\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 586, np.int64(1): 563, np.int64(0): 531})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 586, np.int64(1): 586, np.int64(0): 586})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W6] GRU Epoch 01 | train_loss 1.1041 acc 0.331 f1 0.182 || val_loss 1.1045 acc 0.319 f1 0.203\n",
            "[W6] GRU Epoch 02 | train_loss 1.0949 acc 0.366 f1 0.331 || val_loss 1.1050 acc 0.333 f1 0.313\n",
            "[W6] GRU Epoch 03 | train_loss 1.0893 acc 0.391 f1 0.388 || val_loss 1.1071 acc 0.331 f1 0.330\n",
            "[W6] GRU Epoch 04 | train_loss 1.0823 acc 0.421 f1 0.418 || val_loss 1.1107 acc 0.311 f1 0.311\n",
            "[W6] GRU Epoch 05 | train_loss 1.0780 acc 0.412 f1 0.412 || val_loss 1.1204 acc 0.311 f1 0.309\n",
            "[W6] GRU Epoch 06 | train_loss 1.0682 acc 0.434 f1 0.432 || val_loss 1.1279 acc 0.317 f1 0.313\n",
            "[W6] GRU Epoch 07 | train_loss 1.0599 acc 0.425 f1 0.422 || val_loss 1.1384 acc 0.331 f1 0.329\n",
            "[W6] GRU Epoch 08 | train_loss 1.0507 acc 0.450 f1 0.447 || val_loss 1.1476 acc 0.339 f1 0.339\n",
            "[W6] GRU Epoch 09 | train_loss 1.0407 acc 0.467 f1 0.467 || val_loss 1.1530 acc 0.328 f1 0.327\n",
            "[W6] GRU Epoch 10 | train_loss 1.0277 acc 0.486 f1 0.485 || val_loss 1.1619 acc 0.333 f1 0.333\n",
            "[W6] GRU Epoch 11 | train_loss 1.0143 acc 0.502 f1 0.502 || val_loss 1.1726 acc 0.322 f1 0.322\n",
            "[W6] GRU Epoch 12 | train_loss 1.0006 acc 0.501 f1 0.500 || val_loss 1.1891 acc 0.314 f1 0.313\n",
            "[W6] GRU Epoch 13 | train_loss 0.9781 acc 0.524 f1 0.524 || val_loss 1.2088 acc 0.311 f1 0.310\n",
            "[W6] GRU Epoch 14 | train_loss 0.9648 acc 0.530 f1 0.529 || val_loss 1.2186 acc 0.308 f1 0.305\n",
            "[W6] GRU Epoch 15 | train_loss 0.9337 acc 0.571 f1 0.570 || val_loss 1.2400 acc 0.319 f1 0.319\n",
            "[W6] GRU Epoch 16 | train_loss 0.9146 acc 0.586 f1 0.586 || val_loss 1.2586 acc 0.342 f1 0.341\n",
            "[W6] GRU Epoch 17 | train_loss 0.8850 acc 0.608 f1 0.608 || val_loss 1.3075 acc 0.311 f1 0.305\n",
            "[W6] GRU Epoch 18 | train_loss 0.8660 acc 0.604 f1 0.604 || val_loss 1.3217 acc 0.331 f1 0.321\n",
            "[W6] GRU Epoch 19 | train_loss 0.8233 acc 0.626 f1 0.626 || val_loss 1.3529 acc 0.319 f1 0.314\n",
            "[W6] GRU Epoch 20 | train_loss 0.7880 acc 0.658 f1 0.658 || val_loss 1.3774 acc 0.333 f1 0.334\n",
            "[W6] GRU Epoch 21 | train_loss 0.7601 acc 0.671 f1 0.671 || val_loss 1.4136 acc 0.328 f1 0.327\n",
            "[W6] GRU Epoch 22 | train_loss 0.7147 acc 0.694 f1 0.694 || val_loss 1.4555 acc 0.331 f1 0.331\n",
            "[W6] GRU Epoch 23 | train_loss 0.6853 acc 0.714 f1 0.715 || val_loss 1.5250 acc 0.325 f1 0.325\n",
            "[W6] GRU Epoch 24 | train_loss 0.6479 acc 0.741 f1 0.741 || val_loss 1.5505 acc 0.317 f1 0.315\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=6 Model=LSTM\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 586, np.int64(1): 563, np.int64(0): 531})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 586, np.int64(1): 586, np.int64(0): 586})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W6] LSTM Epoch 01 | train_loss 1.0992 acc 0.334 f1 0.305 || val_loss 1.0988 acc 0.333 f1 0.333\n",
            "[W6] LSTM Epoch 02 | train_loss 1.0962 acc 0.375 f1 0.375 || val_loss 1.0999 acc 0.319 f1 0.315\n",
            "[W6] LSTM Epoch 03 | train_loss 1.0937 acc 0.396 f1 0.391 || val_loss 1.1008 acc 0.314 f1 0.314\n",
            "[W6] LSTM Epoch 04 | train_loss 1.0891 acc 0.417 f1 0.415 || val_loss 1.1058 acc 0.325 f1 0.323\n",
            "[W6] LSTM Epoch 05 | train_loss 1.0808 acc 0.416 f1 0.407 || val_loss 1.1193 acc 0.331 f1 0.310\n",
            "[W6] LSTM Epoch 06 | train_loss 1.0721 acc 0.423 f1 0.407 || val_loss 1.1264 acc 0.314 f1 0.310\n",
            "[W6] LSTM Epoch 07 | train_loss 1.0608 acc 0.441 f1 0.428 || val_loss 1.1405 acc 0.325 f1 0.315\n",
            "[W6] LSTM Epoch 08 | train_loss 1.0451 acc 0.457 f1 0.451 || val_loss 1.1525 acc 0.325 f1 0.315\n",
            "[W6] LSTM Epoch 09 | train_loss 1.0400 acc 0.454 f1 0.445 || val_loss 1.1607 acc 0.325 f1 0.319\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=7 Model=ANN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 562, np.int64(1): 546, np.int64(0): 502})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 562, np.int64(0): 562, np.int64(2): 562})\n",
            "[W7] ANN Epoch 01 | train_loss 1.2221 acc 0.335 f1 0.331 || val_loss 1.1265 acc 0.339 f1 0.279\n",
            "[W7] ANN Epoch 02 | train_loss 1.0741 acc 0.431 f1 0.429 || val_loss 1.1493 acc 0.304 f1 0.291\n",
            "[W7] ANN Epoch 03 | train_loss 1.0005 acc 0.489 f1 0.489 || val_loss 1.1694 acc 0.316 f1 0.312\n",
            "[W7] ANN Epoch 04 | train_loss 0.9479 acc 0.547 f1 0.547 || val_loss 1.1968 acc 0.304 f1 0.299\n",
            "[W7] ANN Epoch 05 | train_loss 0.8841 acc 0.598 f1 0.597 || val_loss 1.2167 acc 0.307 f1 0.300\n",
            "[W7] ANN Epoch 06 | train_loss 0.8186 acc 0.639 f1 0.639 || val_loss 1.2710 acc 0.307 f1 0.306\n",
            "[W7] ANN Epoch 07 | train_loss 0.7368 acc 0.683 f1 0.682 || val_loss 1.3447 acc 0.301 f1 0.299\n",
            "[W7] ANN Epoch 08 | train_loss 0.6908 acc 0.716 f1 0.715 || val_loss 1.4092 acc 0.316 f1 0.312\n",
            "[W7] ANN Epoch 09 | train_loss 0.5892 acc 0.762 f1 0.761 || val_loss 1.4274 acc 0.330 f1 0.330\n",
            "[W7] ANN Epoch 10 | train_loss 0.5576 acc 0.786 f1 0.786 || val_loss 1.4852 acc 0.328 f1 0.327\n",
            "[W7] ANN Epoch 11 | train_loss 0.4919 acc 0.814 f1 0.814 || val_loss 1.5419 acc 0.342 f1 0.341\n",
            "[W7] ANN Epoch 12 | train_loss 0.4765 acc 0.809 f1 0.809 || val_loss 1.6286 acc 0.342 f1 0.341\n",
            "[W7] ANN Epoch 13 | train_loss 0.4109 acc 0.849 f1 0.849 || val_loss 1.6583 acc 0.322 f1 0.322\n",
            "[W7] ANN Epoch 14 | train_loss 0.3316 acc 0.881 f1 0.881 || val_loss 1.7203 acc 0.371 f1 0.370\n",
            "[W7] ANN Epoch 15 | train_loss 0.3205 acc 0.884 f1 0.884 || val_loss 1.8174 acc 0.359 f1 0.358\n",
            "[W7] ANN Epoch 16 | train_loss 0.3111 acc 0.882 f1 0.882 || val_loss 1.9087 acc 0.351 f1 0.351\n",
            "[W7] ANN Epoch 17 | train_loss 0.3145 acc 0.881 f1 0.881 || val_loss 1.9294 acc 0.351 f1 0.350\n",
            "[W7] ANN Epoch 18 | train_loss 0.2767 acc 0.898 f1 0.898 || val_loss 1.9645 acc 0.351 f1 0.350\n",
            "[W7] ANN Epoch 19 | train_loss 0.2559 acc 0.907 f1 0.907 || val_loss 2.0181 acc 0.357 f1 0.356\n",
            "[W7] ANN Epoch 20 | train_loss 0.2305 acc 0.916 f1 0.916 || val_loss 2.0727 acc 0.345 f1 0.345\n",
            "[W7] ANN Epoch 21 | train_loss 0.2241 acc 0.924 f1 0.924 || val_loss 2.0752 acc 0.365 f1 0.365\n",
            "[W7] ANN Epoch 22 | train_loss 0.1940 acc 0.925 f1 0.925 || val_loss 2.1672 acc 0.345 f1 0.344\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=7 Model=CNN1D\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 562, np.int64(1): 546, np.int64(0): 502})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 562, np.int64(0): 562, np.int64(2): 562})\n",
            "[W7] CNN1D Epoch 01 | train_loss 1.1006 acc 0.337 f1 0.335 || val_loss 1.1036 acc 0.336 f1 0.320\n",
            "[W7] CNN1D Epoch 02 | train_loss 1.0784 acc 0.420 f1 0.412 || val_loss 1.1165 acc 0.342 f1 0.277\n",
            "[W7] CNN1D Epoch 03 | train_loss 1.0647 acc 0.439 f1 0.439 || val_loss 1.1257 acc 0.325 f1 0.325\n",
            "[W7] CNN1D Epoch 04 | train_loss 1.0446 acc 0.459 f1 0.457 || val_loss 1.1370 acc 0.339 f1 0.332\n",
            "[W7] CNN1D Epoch 05 | train_loss 1.0232 acc 0.492 f1 0.489 || val_loss 1.1566 acc 0.328 f1 0.301\n",
            "[W7] CNN1D Epoch 06 | train_loss 1.0034 acc 0.511 f1 0.510 || val_loss 1.1679 acc 0.316 f1 0.317\n",
            "[W7] CNN1D Epoch 07 | train_loss 0.9638 acc 0.540 f1 0.536 || val_loss 1.1916 acc 0.325 f1 0.311\n",
            "[W7] CNN1D Epoch 08 | train_loss 0.9333 acc 0.567 f1 0.566 || val_loss 1.2481 acc 0.313 f1 0.302\n",
            "[W7] CNN1D Epoch 09 | train_loss 0.9135 acc 0.574 f1 0.573 || val_loss 1.2459 acc 0.322 f1 0.288\n",
            "[W7] CNN1D Epoch 10 | train_loss 0.8880 acc 0.590 f1 0.590 || val_loss 1.2628 acc 0.365 f1 0.364\n",
            "[W7] CNN1D Epoch 11 | train_loss 0.8463 acc 0.611 f1 0.610 || val_loss 1.3206 acc 0.325 f1 0.309\n",
            "[W7] CNN1D Epoch 12 | train_loss 0.8377 acc 0.616 f1 0.615 || val_loss 1.2933 acc 0.345 f1 0.338\n",
            "[W7] CNN1D Epoch 13 | train_loss 0.8079 acc 0.646 f1 0.645 || val_loss 1.3549 acc 0.351 f1 0.333\n",
            "[W7] CNN1D Epoch 14 | train_loss 0.7696 acc 0.658 f1 0.658 || val_loss 1.3863 acc 0.330 f1 0.323\n",
            "[W7] CNN1D Epoch 15 | train_loss 0.7618 acc 0.666 f1 0.665 || val_loss 1.3512 acc 0.354 f1 0.354\n",
            "[W7] CNN1D Epoch 16 | train_loss 0.7305 acc 0.689 f1 0.688 || val_loss 1.3863 acc 0.359 f1 0.359\n",
            "[W7] CNN1D Epoch 17 | train_loss 0.7055 acc 0.698 f1 0.697 || val_loss 1.4500 acc 0.336 f1 0.332\n",
            "[W7] CNN1D Epoch 18 | train_loss 0.6987 acc 0.688 f1 0.688 || val_loss 1.4590 acc 0.342 f1 0.334\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=7 Model=RNN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 562, np.int64(1): 546, np.int64(0): 502})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 562, np.int64(0): 562, np.int64(2): 562})\n",
            "[W7] RNN Epoch 01 | train_loss 1.1025 acc 0.335 f1 0.263 || val_loss 1.1015 acc 0.310 f1 0.273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W7] RNN Epoch 02 | train_loss 1.0900 acc 0.382 f1 0.372 || val_loss 1.1019 acc 0.336 f1 0.331\n",
            "[W7] RNN Epoch 03 | train_loss 1.0837 acc 0.403 f1 0.402 || val_loss 1.1052 acc 0.299 f1 0.299\n",
            "[W7] RNN Epoch 04 | train_loss 1.0762 acc 0.419 f1 0.419 || val_loss 1.1108 acc 0.304 f1 0.302\n",
            "[W7] RNN Epoch 05 | train_loss 1.0637 acc 0.428 f1 0.427 || val_loss 1.1217 acc 0.284 f1 0.279\n",
            "[W7] RNN Epoch 06 | train_loss 1.0581 acc 0.437 f1 0.432 || val_loss 1.1312 acc 0.293 f1 0.290\n",
            "[W7] RNN Epoch 07 | train_loss 1.0453 acc 0.461 f1 0.459 || val_loss 1.1377 acc 0.290 f1 0.291\n",
            "[W7] RNN Epoch 08 | train_loss 1.0319 acc 0.482 f1 0.482 || val_loss 1.1522 acc 0.290 f1 0.290\n",
            "[W7] RNN Epoch 09 | train_loss 1.0192 acc 0.483 f1 0.482 || val_loss 1.1656 acc 0.281 f1 0.280\n",
            "[W7] RNN Epoch 10 | train_loss 1.0017 acc 0.498 f1 0.495 || val_loss 1.1781 acc 0.296 f1 0.295\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=7 Model=GRU\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 562, np.int64(1): 546, np.int64(0): 502})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 562, np.int64(0): 562, np.int64(2): 562})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W7] GRU Epoch 01 | train_loss 1.1009 acc 0.332 f1 0.269 || val_loss 1.0994 acc 0.313 f1 0.251\n",
            "[W7] GRU Epoch 02 | train_loss 1.0957 acc 0.374 f1 0.346 || val_loss 1.0993 acc 0.313 f1 0.292\n",
            "[W7] GRU Epoch 03 | train_loss 1.0926 acc 0.393 f1 0.388 || val_loss 1.1012 acc 0.296 f1 0.296\n",
            "[W7] GRU Epoch 04 | train_loss 1.0877 acc 0.409 f1 0.404 || val_loss 1.1039 acc 0.333 f1 0.330\n",
            "[W7] GRU Epoch 05 | train_loss 1.0815 acc 0.420 f1 0.419 || val_loss 1.1097 acc 0.330 f1 0.330\n",
            "[W7] GRU Epoch 06 | train_loss 1.0728 acc 0.431 f1 0.429 || val_loss 1.1179 acc 0.328 f1 0.328\n",
            "[W7] GRU Epoch 07 | train_loss 1.0670 acc 0.431 f1 0.426 || val_loss 1.1235 acc 0.325 f1 0.324\n",
            "[W7] GRU Epoch 08 | train_loss 1.0517 acc 0.449 f1 0.446 || val_loss 1.1321 acc 0.322 f1 0.321\n",
            "[W7] GRU Epoch 09 | train_loss 1.0409 acc 0.469 f1 0.468 || val_loss 1.1406 acc 0.310 f1 0.310\n",
            "[W7] GRU Epoch 10 | train_loss 1.0298 acc 0.472 f1 0.468 || val_loss 1.1457 acc 0.313 f1 0.313\n",
            "[W7] GRU Epoch 11 | train_loss 1.0181 acc 0.486 f1 0.481 || val_loss 1.1652 acc 0.307 f1 0.305\n",
            "[W7] GRU Epoch 12 | train_loss 1.0003 acc 0.505 f1 0.503 || val_loss 1.1676 acc 0.339 f1 0.339\n",
            "[W7] GRU Epoch 13 | train_loss 0.9789 acc 0.521 f1 0.519 || val_loss 1.1823 acc 0.339 f1 0.338\n",
            "[W7] GRU Epoch 14 | train_loss 0.9610 acc 0.531 f1 0.529 || val_loss 1.1941 acc 0.328 f1 0.327\n",
            "[W7] GRU Epoch 15 | train_loss 0.9367 acc 0.572 f1 0.572 || val_loss 1.2145 acc 0.330 f1 0.329\n",
            "[W7] GRU Epoch 16 | train_loss 0.9147 acc 0.584 f1 0.582 || val_loss 1.2322 acc 0.351 f1 0.350\n",
            "[W7] GRU Epoch 17 | train_loss 0.8778 acc 0.601 f1 0.599 || val_loss 1.2699 acc 0.345 f1 0.344\n",
            "[W7] GRU Epoch 18 | train_loss 0.8629 acc 0.617 f1 0.617 || val_loss 1.2891 acc 0.345 f1 0.342\n",
            "[W7] GRU Epoch 19 | train_loss 0.8426 acc 0.627 f1 0.626 || val_loss 1.2947 acc 0.339 f1 0.338\n",
            "[W7] GRU Epoch 20 | train_loss 0.7999 acc 0.647 f1 0.646 || val_loss 1.3214 acc 0.371 f1 0.366\n",
            "[W7] GRU Epoch 21 | train_loss 0.7594 acc 0.675 f1 0.674 || val_loss 1.3733 acc 0.354 f1 0.353\n",
            "[W7] GRU Epoch 22 | train_loss 0.7417 acc 0.682 f1 0.681 || val_loss 1.3848 acc 0.386 f1 0.385\n",
            "[W7] GRU Epoch 23 | train_loss 0.7010 acc 0.699 f1 0.698 || val_loss 1.4227 acc 0.371 f1 0.369\n",
            "[W7] GRU Epoch 24 | train_loss 0.6777 acc 0.717 f1 0.716 || val_loss 1.5018 acc 0.351 f1 0.348\n",
            "[W7] GRU Epoch 25 | train_loss 0.6388 acc 0.741 f1 0.740 || val_loss 1.5261 acc 0.380 f1 0.380\n",
            "[W7] GRU Epoch 26 | train_loss 0.5994 acc 0.760 f1 0.760 || val_loss 1.5444 acc 0.368 f1 0.365\n",
            "[W7] GRU Epoch 27 | train_loss 0.5750 acc 0.765 f1 0.764 || val_loss 1.5960 acc 0.383 f1 0.381\n",
            "[W7] GRU Epoch 28 | train_loss 0.5457 acc 0.775 f1 0.774 || val_loss 1.7156 acc 0.377 f1 0.373\n",
            "[W7] GRU Epoch 29 | train_loss 0.5125 acc 0.806 f1 0.806 || val_loss 1.7629 acc 0.371 f1 0.370\n",
            "[W7] GRU Epoch 30 | train_loss 0.4880 acc 0.804 f1 0.804 || val_loss 1.8038 acc 0.371 f1 0.369\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=7 Model=LSTM\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 562, np.int64(1): 546, np.int64(0): 502})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 562, np.int64(0): 562, np.int64(2): 562})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W7] LSTM Epoch 01 | train_loss 1.0989 acc 0.326 f1 0.233 || val_loss 1.1000 acc 0.330 f1 0.258\n",
            "[W7] LSTM Epoch 02 | train_loss 1.0963 acc 0.368 f1 0.337 || val_loss 1.1006 acc 0.316 f1 0.280\n",
            "[W7] LSTM Epoch 03 | train_loss 1.0936 acc 0.394 f1 0.368 || val_loss 1.1022 acc 0.299 f1 0.263\n",
            "[W7] LSTM Epoch 04 | train_loss 1.0909 acc 0.388 f1 0.350 || val_loss 1.1061 acc 0.301 f1 0.260\n",
            "[W7] LSTM Epoch 05 | train_loss 1.0856 acc 0.394 f1 0.372 || val_loss 1.1138 acc 0.301 f1 0.278\n",
            "[W7] LSTM Epoch 06 | train_loss 1.0811 acc 0.396 f1 0.383 || val_loss 1.1251 acc 0.316 f1 0.306\n",
            "[W7] LSTM Epoch 07 | train_loss 1.0694 acc 0.418 f1 0.408 || val_loss 1.1330 acc 0.328 f1 0.325\n",
            "[W7] LSTM Epoch 08 | train_loss 1.0589 acc 0.438 f1 0.432 || val_loss 1.1457 acc 0.322 f1 0.322\n",
            "[W7] LSTM Epoch 09 | train_loss 1.0465 acc 0.444 f1 0.440 || val_loss 1.1546 acc 0.325 f1 0.324\n",
            "[W7] LSTM Epoch 10 | train_loss 1.0335 acc 0.464 f1 0.458 || val_loss 1.1703 acc 0.310 f1 0.307\n",
            "[W7] LSTM Epoch 11 | train_loss 1.0116 acc 0.493 f1 0.490 || val_loss 1.1914 acc 0.319 f1 0.320\n",
            "[W7] LSTM Epoch 12 | train_loss 0.9920 acc 0.507 f1 0.504 || val_loss 1.2238 acc 0.304 f1 0.305\n",
            "[W7] LSTM Epoch 13 | train_loss 0.9571 acc 0.538 f1 0.535 || val_loss 1.2506 acc 0.322 f1 0.311\n",
            "[W7] LSTM Epoch 14 | train_loss 0.9444 acc 0.550 f1 0.546 || val_loss 1.2744 acc 0.316 f1 0.310\n",
            "[W7] LSTM Epoch 15 | train_loss 0.9097 acc 0.575 f1 0.571 || val_loss 1.2768 acc 0.304 f1 0.301\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=10 Model=ANN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 491, np.int64(1): 473, np.int64(0): 436})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 491, np.int64(2): 491, np.int64(1): 491})\n",
            "Error running 10 ANN Expected more than 1 value per channel when training, got input size torch.Size([1, 256])\n",
            "============================================================\n",
            "Running Window=10 Model=CNN1D\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 491, np.int64(1): 473, np.int64(0): 436})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 491, np.int64(2): 491, np.int64(1): 491})\n",
            "[W10] CNN1D Epoch 01 | train_loss 1.1031 acc 0.352 f1 0.344 || val_loss 1.1034 acc 0.367 f1 0.326\n",
            "[W10] CNN1D Epoch 02 | train_loss 1.0939 acc 0.355 f1 0.334 || val_loss 1.1091 acc 0.347 f1 0.330\n",
            "[W10] CNN1D Epoch 03 | train_loss 1.0807 acc 0.407 f1 0.394 || val_loss 1.1153 acc 0.340 f1 0.326\n",
            "[W10] CNN1D Epoch 04 | train_loss 1.0696 acc 0.430 f1 0.411 || val_loss 1.1280 acc 0.310 f1 0.303\n",
            "[W10] CNN1D Epoch 05 | train_loss 1.0580 acc 0.439 f1 0.432 || val_loss 1.1359 acc 0.327 f1 0.324\n",
            "[W10] CNN1D Epoch 06 | train_loss 1.0488 acc 0.447 f1 0.434 || val_loss 1.1380 acc 0.363 f1 0.359\n",
            "[W10] CNN1D Epoch 07 | train_loss 1.0369 acc 0.447 f1 0.425 || val_loss 1.1635 acc 0.320 f1 0.288\n",
            "[W10] CNN1D Epoch 08 | train_loss 1.0249 acc 0.477 f1 0.473 || val_loss 1.1543 acc 0.327 f1 0.322\n",
            "[W10] CNN1D Epoch 09 | train_loss 1.0115 acc 0.487 f1 0.484 || val_loss 1.1666 acc 0.313 f1 0.313\n",
            "[W10] CNN1D Epoch 10 | train_loss 1.0039 acc 0.487 f1 0.480 || val_loss 1.1869 acc 0.313 f1 0.309\n",
            "[W10] CNN1D Epoch 11 | train_loss 0.9773 acc 0.524 f1 0.517 || val_loss 1.1975 acc 0.310 f1 0.285\n",
            "[W10] CNN1D Epoch 12 | train_loss 0.9618 acc 0.538 f1 0.536 || val_loss 1.2015 acc 0.300 f1 0.297\n",
            "[W10] CNN1D Epoch 13 | train_loss 0.9503 acc 0.546 f1 0.543 || val_loss 1.2214 acc 0.307 f1 0.299\n",
            "[W10] CNN1D Epoch 14 | train_loss 0.9421 acc 0.534 f1 0.531 || val_loss 1.2178 acc 0.307 f1 0.306\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=10 Model=RNN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 491, np.int64(1): 473, np.int64(0): 436})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 491, np.int64(2): 491, np.int64(1): 491})\n",
            "[W10] RNN Epoch 01 | train_loss 1.1043 acc 0.334 f1 0.280 || val_loss 1.0974 acc 0.340 f1 0.321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W10] RNN Epoch 02 | train_loss 1.0908 acc 0.378 f1 0.352 || val_loss 1.0992 acc 0.337 f1 0.323\n",
            "[W10] RNN Epoch 03 | train_loss 1.0859 acc 0.401 f1 0.398 || val_loss 1.1004 acc 0.330 f1 0.330\n",
            "[W10] RNN Epoch 04 | train_loss 1.0777 acc 0.445 f1 0.443 || val_loss 1.1024 acc 0.340 f1 0.341\n",
            "[W10] RNN Epoch 05 | train_loss 1.0691 acc 0.443 f1 0.440 || val_loss 1.1081 acc 0.350 f1 0.350\n",
            "[W10] RNN Epoch 06 | train_loss 1.0622 acc 0.463 f1 0.463 || val_loss 1.1143 acc 0.330 f1 0.328\n",
            "[W10] RNN Epoch 07 | train_loss 1.0539 acc 0.466 f1 0.463 || val_loss 1.1188 acc 0.287 f1 0.282\n",
            "[W10] RNN Epoch 08 | train_loss 1.0483 acc 0.460 f1 0.457 || val_loss 1.1259 acc 0.293 f1 0.290\n",
            "[W10] RNN Epoch 09 | train_loss 1.0413 acc 0.477 f1 0.474 || val_loss 1.1300 acc 0.290 f1 0.289\n",
            "[W10] RNN Epoch 10 | train_loss 1.0339 acc 0.478 f1 0.476 || val_loss 1.1301 acc 0.320 f1 0.319\n",
            "[W10] RNN Epoch 11 | train_loss 1.0277 acc 0.467 f1 0.462 || val_loss 1.1306 acc 0.373 f1 0.372\n",
            "[W10] RNN Epoch 12 | train_loss 1.0204 acc 0.496 f1 0.493 || val_loss 1.1420 acc 0.317 f1 0.314\n",
            "[W10] RNN Epoch 13 | train_loss 1.0192 acc 0.480 f1 0.477 || val_loss 1.1582 acc 0.307 f1 0.303\n",
            "[W10] RNN Epoch 14 | train_loss 1.0111 acc 0.481 f1 0.479 || val_loss 1.1650 acc 0.310 f1 0.308\n",
            "[W10] RNN Epoch 15 | train_loss 1.0062 acc 0.491 f1 0.488 || val_loss 1.1665 acc 0.323 f1 0.315\n",
            "[W10] RNN Epoch 16 | train_loss 1.0040 acc 0.511 f1 0.508 || val_loss 1.1727 acc 0.320 f1 0.314\n",
            "[W10] RNN Epoch 17 | train_loss 0.9938 acc 0.510 f1 0.505 || val_loss 1.1760 acc 0.333 f1 0.327\n",
            "[W10] RNN Epoch 18 | train_loss 0.9852 acc 0.517 f1 0.511 || val_loss 1.1757 acc 0.330 f1 0.324\n",
            "[W10] RNN Epoch 19 | train_loss 0.9834 acc 0.520 f1 0.519 || val_loss 1.1913 acc 0.317 f1 0.316\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=10 Model=GRU\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 491, np.int64(1): 473, np.int64(0): 436})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 491, np.int64(2): 491, np.int64(1): 491})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W10] GRU Epoch 01 | train_loss 1.0997 acc 0.320 f1 0.298 || val_loss 1.0983 acc 0.357 f1 0.345\n",
            "[W10] GRU Epoch 02 | train_loss 1.0935 acc 0.386 f1 0.381 || val_loss 1.0982 acc 0.353 f1 0.326\n",
            "[W10] GRU Epoch 03 | train_loss 1.0901 acc 0.378 f1 0.358 || val_loss 1.0990 acc 0.353 f1 0.337\n",
            "[W10] GRU Epoch 04 | train_loss 1.0857 acc 0.404 f1 0.400 || val_loss 1.1000 acc 0.330 f1 0.320\n",
            "[W10] GRU Epoch 05 | train_loss 1.0797 acc 0.428 f1 0.425 || val_loss 1.1029 acc 0.357 f1 0.339\n",
            "[W10] GRU Epoch 06 | train_loss 1.0744 acc 0.429 f1 0.429 || val_loss 1.1071 acc 0.327 f1 0.323\n",
            "[W10] GRU Epoch 07 | train_loss 1.0701 acc 0.430 f1 0.429 || val_loss 1.1133 acc 0.317 f1 0.311\n",
            "[W10] GRU Epoch 08 | train_loss 1.0637 acc 0.428 f1 0.422 || val_loss 1.1193 acc 0.343 f1 0.342\n",
            "[W10] GRU Epoch 09 | train_loss 1.0573 acc 0.435 f1 0.434 || val_loss 1.1254 acc 0.317 f1 0.316\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=10 Model=LSTM\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 491, np.int64(1): 473, np.int64(0): 436})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 491, np.int64(2): 491, np.int64(1): 491})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W10] LSTM Epoch 01 | train_loss 1.1037 acc 0.327 f1 0.200 || val_loss 1.1033 acc 0.333 f1 0.199\n",
            "[W10] LSTM Epoch 02 | train_loss 1.0998 acc 0.335 f1 0.265 || val_loss 1.1024 acc 0.270 f1 0.217\n",
            "[W10] LSTM Epoch 03 | train_loss 1.0981 acc 0.371 f1 0.291 || val_loss 1.1022 acc 0.303 f1 0.201\n",
            "[W10] LSTM Epoch 04 | train_loss 1.0961 acc 0.346 f1 0.253 || val_loss 1.1017 acc 0.310 f1 0.234\n",
            "[W10] LSTM Epoch 05 | train_loss 1.0913 acc 0.390 f1 0.366 || val_loss 1.1027 acc 0.307 f1 0.261\n",
            "[W10] LSTM Epoch 06 | train_loss 1.0901 acc 0.365 f1 0.316 || val_loss 1.1063 acc 0.290 f1 0.260\n",
            "[W10] LSTM Epoch 07 | train_loss 1.0828 acc 0.401 f1 0.383 || val_loss 1.1074 acc 0.307 f1 0.293\n",
            "[W10] LSTM Epoch 08 | train_loss 1.0782 acc 0.420 f1 0.411 || val_loss 1.1136 acc 0.313 f1 0.302\n",
            "[W10] LSTM Epoch 09 | train_loss 1.0714 acc 0.420 f1 0.413 || val_loss 1.1218 acc 0.307 f1 0.305\n",
            "[W10] LSTM Epoch 10 | train_loss 1.0634 acc 0.429 f1 0.423 || val_loss 1.1250 acc 0.307 f1 0.306\n",
            "[W10] LSTM Epoch 11 | train_loss 1.0514 acc 0.444 f1 0.440 || val_loss 1.1378 acc 0.340 f1 0.336\n",
            "[W10] LSTM Epoch 12 | train_loss 1.0476 acc 0.439 f1 0.419 || val_loss 1.1331 acc 0.353 f1 0.350\n",
            "[W10] LSTM Epoch 13 | train_loss 1.0418 acc 0.460 f1 0.448 || val_loss 1.1460 acc 0.363 f1 0.361\n",
            "[W10] LSTM Epoch 14 | train_loss 1.0356 acc 0.461 f1 0.451 || val_loss 1.1557 acc 0.333 f1 0.333\n",
            "[W10] LSTM Epoch 15 | train_loss 1.0230 acc 0.487 f1 0.484 || val_loss 1.1568 acc 0.327 f1 0.326\n",
            "[W10] LSTM Epoch 16 | train_loss 1.0117 acc 0.486 f1 0.480 || val_loss 1.1948 acc 0.310 f1 0.309\n",
            "[W10] LSTM Epoch 17 | train_loss 0.9991 acc 0.499 f1 0.496 || val_loss 1.2084 acc 0.330 f1 0.329\n",
            "[W10] LSTM Epoch 18 | train_loss 0.9796 acc 0.525 f1 0.520 || val_loss 1.2423 acc 0.303 f1 0.303\n",
            "[W10] LSTM Epoch 19 | train_loss 0.9702 acc 0.540 f1 0.540 || val_loss 1.2321 acc 0.307 f1 0.306\n",
            "[W10] LSTM Epoch 20 | train_loss 0.9535 acc 0.545 f1 0.544 || val_loss 1.2544 acc 0.310 f1 0.308\n",
            "[W10] LSTM Epoch 21 | train_loss 0.9595 acc 0.558 f1 0.555 || val_loss 1.2227 acc 0.287 f1 0.287\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=14 Model=ANN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(1): 388, np.int64(2): 377, np.int64(0): 355})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 388, np.int64(1): 388, np.int64(0): 388})\n",
            "[W14] ANN Epoch 01 | train_loss 1.2520 acc 0.336 f1 0.336 || val_loss 1.1062 acc 0.308 f1 0.305\n",
            "[W14] ANN Epoch 02 | train_loss 1.0391 acc 0.466 f1 0.466 || val_loss 1.1136 acc 0.342 f1 0.342\n",
            "[W14] ANN Epoch 03 | train_loss 0.9311 acc 0.554 f1 0.554 || val_loss 1.1361 acc 0.358 f1 0.359\n",
            "[W14] ANN Epoch 04 | train_loss 0.8411 acc 0.615 f1 0.614 || val_loss 1.1832 acc 0.367 f1 0.367\n",
            "[W14] ANN Epoch 05 | train_loss 0.7498 acc 0.680 f1 0.680 || val_loss 1.2629 acc 0.350 f1 0.350\n",
            "[W14] ANN Epoch 06 | train_loss 0.6404 acc 0.749 f1 0.748 || val_loss 1.3424 acc 0.317 f1 0.317\n",
            "[W14] ANN Epoch 07 | train_loss 0.5670 acc 0.759 f1 0.759 || val_loss 1.4177 acc 0.317 f1 0.314\n",
            "[W14] ANN Epoch 08 | train_loss 0.4788 acc 0.820 f1 0.820 || val_loss 1.4806 acc 0.312 f1 0.312\n",
            "[W14] ANN Epoch 09 | train_loss 0.4006 acc 0.861 f1 0.861 || val_loss 1.5785 acc 0.338 f1 0.334\n",
            "[W14] ANN Epoch 10 | train_loss 0.3348 acc 0.881 f1 0.881 || val_loss 1.6857 acc 0.329 f1 0.329\n",
            "[W14] ANN Epoch 11 | train_loss 0.2766 acc 0.909 f1 0.909 || val_loss 1.7930 acc 0.350 f1 0.346\n",
            "[W14] ANN Epoch 12 | train_loss 0.2251 acc 0.928 f1 0.928 || val_loss 1.8397 acc 0.342 f1 0.341\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=14 Model=CNN1D\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(1): 388, np.int64(2): 377, np.int64(0): 355})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 388, np.int64(1): 388, np.int64(0): 388})\n",
            "[W14] CNN1D Epoch 01 | train_loss 1.1081 acc 0.322 f1 0.319 || val_loss 1.1021 acc 0.308 f1 0.230\n",
            "[W14] CNN1D Epoch 02 | train_loss 1.0901 acc 0.373 f1 0.346 || val_loss 1.0994 acc 0.329 f1 0.220\n",
            "[W14] CNN1D Epoch 03 | train_loss 1.0815 acc 0.393 f1 0.365 || val_loss 1.1026 acc 0.300 f1 0.297\n",
            "[W14] CNN1D Epoch 04 | train_loss 1.0683 acc 0.436 f1 0.425 || val_loss 1.1036 acc 0.308 f1 0.276\n",
            "[W14] CNN1D Epoch 05 | train_loss 1.0578 acc 0.433 f1 0.420 || val_loss 1.1047 acc 0.300 f1 0.295\n",
            "[W14] CNN1D Epoch 06 | train_loss 1.0409 acc 0.462 f1 0.460 || val_loss 1.1140 acc 0.350 f1 0.317\n",
            "[W14] CNN1D Epoch 07 | train_loss 1.0361 acc 0.475 f1 0.475 || val_loss 1.1223 acc 0.292 f1 0.284\n",
            "[W14] CNN1D Epoch 08 | train_loss 1.0225 acc 0.477 f1 0.472 || val_loss 1.1224 acc 0.304 f1 0.284\n",
            "[W14] CNN1D Epoch 09 | train_loss 1.0084 acc 0.491 f1 0.489 || val_loss 1.1306 acc 0.304 f1 0.304\n",
            "[W14] CNN1D Epoch 10 | train_loss 0.9904 acc 0.506 f1 0.504 || val_loss 1.1297 acc 0.304 f1 0.293\n",
            "[W14] CNN1D Epoch 11 | train_loss 0.9808 acc 0.497 f1 0.497 || val_loss 1.1663 acc 0.300 f1 0.278\n",
            "[W14] CNN1D Epoch 12 | train_loss 0.9752 acc 0.524 f1 0.524 || val_loss 1.1431 acc 0.317 f1 0.305\n",
            "[W14] CNN1D Epoch 13 | train_loss 0.9637 acc 0.517 f1 0.517 || val_loss 1.1635 acc 0.287 f1 0.272\n",
            "[W14] CNN1D Epoch 14 | train_loss 0.9622 acc 0.517 f1 0.518 || val_loss 1.1766 acc 0.300 f1 0.297\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=14 Model=RNN\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(1): 388, np.int64(2): 377, np.int64(0): 355})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 388, np.int64(1): 388, np.int64(0): 388})\n",
            "[W14] RNN Epoch 01 | train_loss 1.1032 acc 0.332 f1 0.283 || val_loss 1.1029 acc 0.317 f1 0.284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W14] RNN Epoch 02 | train_loss 1.0872 acc 0.407 f1 0.393 || val_loss 1.1050 acc 0.338 f1 0.327\n",
            "[W14] RNN Epoch 03 | train_loss 1.0794 acc 0.429 f1 0.420 || val_loss 1.1107 acc 0.333 f1 0.329\n",
            "[W14] RNN Epoch 04 | train_loss 1.0675 acc 0.451 f1 0.450 || val_loss 1.1182 acc 0.300 f1 0.300\n",
            "[W14] RNN Epoch 05 | train_loss 1.0573 acc 0.465 f1 0.465 || val_loss 1.1287 acc 0.317 f1 0.317\n",
            "[W14] RNN Epoch 06 | train_loss 1.0455 acc 0.466 f1 0.462 || val_loss 1.1436 acc 0.312 f1 0.309\n",
            "[W14] RNN Epoch 07 | train_loss 1.0319 acc 0.475 f1 0.473 || val_loss 1.1541 acc 0.300 f1 0.300\n",
            "[W14] RNN Epoch 08 | train_loss 1.0211 acc 0.487 f1 0.481 || val_loss 1.1632 acc 0.304 f1 0.303\n",
            "[W14] RNN Epoch 09 | train_loss 1.0056 acc 0.511 f1 0.508 || val_loss 1.1732 acc 0.304 f1 0.301\n",
            "[W14] RNN Epoch 10 | train_loss 0.9996 acc 0.524 f1 0.521 || val_loss 1.1723 acc 0.321 f1 0.321\n",
            "[W14] RNN Epoch 11 | train_loss 0.9809 acc 0.519 f1 0.514 || val_loss 1.1901 acc 0.317 f1 0.316\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=14 Model=GRU\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(1): 388, np.int64(2): 377, np.int64(0): 355})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 388, np.int64(1): 388, np.int64(0): 388})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W14] GRU Epoch 01 | train_loss 1.0971 acc 0.347 f1 0.288 || val_loss 1.1005 acc 0.354 f1 0.307\n",
            "[W14] GRU Epoch 02 | train_loss 1.0920 acc 0.389 f1 0.377 || val_loss 1.1022 acc 0.321 f1 0.308\n",
            "[W14] GRU Epoch 03 | train_loss 1.0875 acc 0.418 f1 0.415 || val_loss 1.1031 acc 0.317 f1 0.312\n",
            "[W14] GRU Epoch 04 | train_loss 1.0818 acc 0.416 f1 0.407 || val_loss 1.1059 acc 0.342 f1 0.327\n",
            "[W14] GRU Epoch 05 | train_loss 1.0732 acc 0.434 f1 0.423 || val_loss 1.1120 acc 0.317 f1 0.305\n",
            "[W14] GRU Epoch 06 | train_loss 1.0631 acc 0.464 f1 0.459 || val_loss 1.1230 acc 0.317 f1 0.314\n",
            "[W14] GRU Epoch 07 | train_loss 1.0541 acc 0.463 f1 0.462 || val_loss 1.1387 acc 0.317 f1 0.316\n",
            "[W14] GRU Epoch 08 | train_loss 1.0498 acc 0.469 f1 0.468 || val_loss 1.1404 acc 0.312 f1 0.313\n",
            "[W14] GRU Epoch 09 | train_loss 1.0348 acc 0.477 f1 0.475 || val_loss 1.1501 acc 0.308 f1 0.307\n",
            "[W14] GRU Epoch 10 | train_loss 1.0231 acc 0.486 f1 0.486 || val_loss 1.1561 acc 0.300 f1 0.300\n",
            "[W14] GRU Epoch 11 | train_loss 1.0165 acc 0.494 f1 0.492 || val_loss 1.1644 acc 0.304 f1 0.299\n",
            "[W14] GRU Epoch 12 | train_loss 1.0036 acc 0.505 f1 0.505 || val_loss 1.1758 acc 0.321 f1 0.320\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=14 Model=LSTM\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(1): 388, np.int64(2): 377, np.int64(0): 355})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(2): 388, np.int64(1): 388, np.int64(0): 388})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W14] LSTM Epoch 01 | train_loss 1.0989 acc 0.345 f1 0.260 || val_loss 1.1023 acc 0.267 f1 0.209\n",
            "[W14] LSTM Epoch 02 | train_loss 1.0946 acc 0.394 f1 0.344 || val_loss 1.1014 acc 0.312 f1 0.286\n",
            "[W14] LSTM Epoch 03 | train_loss 1.0901 acc 0.413 f1 0.384 || val_loss 1.1025 acc 0.287 f1 0.286\n",
            "[W14] LSTM Epoch 04 | train_loss 1.0860 acc 0.413 f1 0.406 || val_loss 1.1052 acc 0.296 f1 0.294\n",
            "[W14] LSTM Epoch 05 | train_loss 1.0798 acc 0.423 f1 0.407 || val_loss 1.1101 acc 0.304 f1 0.303\n",
            "[W14] LSTM Epoch 06 | train_loss 1.0698 acc 0.419 f1 0.411 || val_loss 1.1180 acc 0.300 f1 0.297\n",
            "[W14] LSTM Epoch 07 | train_loss 1.0599 acc 0.433 f1 0.424 || val_loss 1.1304 acc 0.300 f1 0.292\n",
            "[W14] LSTM Epoch 08 | train_loss 1.0470 acc 0.460 f1 0.457 || val_loss 1.1337 acc 0.287 f1 0.288\n",
            "[W14] LSTM Epoch 09 | train_loss 1.0351 acc 0.480 f1 0.477 || val_loss 1.1532 acc 0.275 f1 0.273\n",
            "[W14] LSTM Epoch 10 | train_loss 1.0249 acc 0.492 f1 0.492 || val_loss 1.1609 acc 0.304 f1 0.293\n",
            "[W14] LSTM Epoch 11 | train_loss 1.0172 acc 0.494 f1 0.485 || val_loss 1.1673 acc 0.329 f1 0.329\n",
            "[W14] LSTM Epoch 12 | train_loss 1.0082 acc 0.504 f1 0.501 || val_loss 1.1670 acc 0.308 f1 0.308\n",
            "[W14] LSTM Epoch 13 | train_loss 0.9957 acc 0.519 f1 0.506 || val_loss 1.1720 acc 0.321 f1 0.320\n",
            "[W14] LSTM Epoch 14 | train_loss 0.9723 acc 0.535 f1 0.534 || val_loss 1.2015 acc 0.317 f1 0.314\n",
            "[W14] LSTM Epoch 15 | train_loss 0.9584 acc 0.542 f1 0.541 || val_loss 1.2196 acc 0.308 f1 0.304\n",
            "[W14] LSTM Epoch 16 | train_loss 0.9424 acc 0.551 f1 0.551 || val_loss 1.2268 acc 0.312 f1 0.302\n",
            "[W14] LSTM Epoch 17 | train_loss 0.9077 acc 0.573 f1 0.572 || val_loss 1.2625 acc 0.300 f1 0.297\n",
            "[W14] LSTM Epoch 18 | train_loss 0.8856 acc 0.595 f1 0.594 || val_loss 1.2651 acc 0.312 f1 0.303\n",
            "[W14] LSTM Epoch 19 | train_loss 0.8714 acc 0.593 f1 0.592 || val_loss 1.2598 acc 0.317 f1 0.316\n",
            "Early stopping.\n",
            "Saved results to outputs3/results_summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "experiment_runner_smote.py\n",
        "\n",
        "PyTorch experiment runner with SMOTE augmentation:\n",
        "\n",
        "- builds sliding windows for window_sizes = [3,4,5,6,7,10,14]\n",
        "- creates ANN and sequence inputs\n",
        "- applies SMOTE on training data to balance classes\n",
        "- trains ANN, CNN1D, RNN, GRU, LSTM\n",
        "- participant-wise split (train/val/test)\n",
        "- saves results to CSV, confusion matrix PNGs, and class distributions\n",
        "\n",
        "Usage:\n",
        "    python experiment_runner_smote.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "CSV_PATH = \"stress_detection.csv\"\n",
        "OUTPUT_DIR = Path(\"LOSO_outputs1\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "WINDOW_SIZES = [3]\n",
        "RAW_FEATURES = [\n",
        "    'Openness','Conscientiousness','Extraversion','Agreeableness','Neuroticism',\n",
        "    'sleep_time','wake_time','sleep_duration','PSQI_score',\n",
        "    'call_duration','num_calls','num_sms',\n",
        "    'screen_on_time','skin_conductance','accelerometer',\n",
        "    'mobility_radius','mobility_distance'\n",
        "]\n",
        "STATIC_PERSONALITY = ['Openness','Conscientiousness','Extraversion','Agreeableness','Neuroticism']\n",
        "TARGET_COL = 'PSS_score'\n",
        "CLASS_COL = 'stress_class'\n",
        "\n",
        "TRAIN_P = 0.7\n",
        "VAL_P = 0.15\n",
        "TEST_P = 0.15\n",
        "\n",
        "EPOCHS = 60\n",
        "BATCH_SIZE = 64\n",
        "LR = 1e-3\n",
        "PATIENCE = 8\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "MODEL_NAMES = [\"ANN\",\"CNN1D\",\"RNN\",\"GRU\",\"LSTM\"]\n",
        "\n",
        "# -------------------------\n",
        "# Helpers\n",
        "# -------------------------\n",
        "def stress_to_class(score):\n",
        "    s = float(score)\n",
        "    if s <= 13: return 0\n",
        "    if s <= 26: return 1\n",
        "    return 2\n",
        "\n",
        "def ensure_dir(path):\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Window builder\n",
        "# -------------------------\n",
        "def build_windows(df, raw_features, window_size=4):\n",
        "    X_seq, X_flat, y, participants = [], [], [], []\n",
        "\n",
        "    for pid, df_p in df.groupby(\"participant_id\"):\n",
        "        df_p = df_p.sort_values(\"day\").reset_index(drop=True)\n",
        "        n = len(df_p)\n",
        "        if n <= window_size:\n",
        "            continue\n",
        "        arr = df_p[raw_features].values.astype(float)\n",
        "        for i in range(n - window_size):\n",
        "            window = arr[i:i+window_size]\n",
        "            mean_vals = window.mean(axis=0)\n",
        "            std_vals = window.std(axis=0)\n",
        "            min_vals = window.min(axis=0)\n",
        "            max_vals = window.max(axis=0)\n",
        "            slope_vals = window[-1] - window[0]\n",
        "            agg = np.concatenate([mean_vals, std_vals, min_vals, max_vals, slope_vals])\n",
        "\n",
        "            flat = np.concatenate([window.flatten(), agg])\n",
        "            target_score = df_p[TARGET_COL].iloc[i+window_size]\n",
        "            cls = stress_to_class(target_score)\n",
        "\n",
        "            X_seq.append(window.copy())\n",
        "            X_flat.append(flat.copy())\n",
        "            y.append(int(cls))\n",
        "            participants.append(pid)\n",
        "\n",
        "    return np.array(X_seq), np.array(X_flat), np.array(y), np.array(participants)\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class StressDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# -------------------------\n",
        "# Models\n",
        "# -------------------------\n",
        "class ANNModel(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=[256,128,64], num_classes=3, p_drop=0.3):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last = in_dim\n",
        "        for h in hidden:\n",
        "            layers.append(nn.Linear(last,h))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.BatchNorm1d(h))\n",
        "            layers.append(nn.Dropout(p_drop))\n",
        "            last = h\n",
        "        layers.append(nn.Linear(last,num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "class CNN1DModel(nn.Module):\n",
        "    def __init__(self, seq_len, feat_dim, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(feat_dim,64,2)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.conv2 = nn.Conv1d(64,128,2)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        l = seq_len - (2-1) - (2-1)\n",
        "        if l<1: l=1\n",
        "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128*1,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64,num_classes)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x = x.permute(0,2,1)\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.gap(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=64, num_layers=1, rnn_type='RNN', num_classes=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        if rnn_type=='RNN':\n",
        "            self.rnn = nn.RNN(input_dim,hidden,num_layers,batch_first=True,nonlinearity='tanh',dropout=dropout)\n",
        "        elif rnn_type=='GRU':\n",
        "            self.rnn = nn.GRU(input_dim,hidden,num_layers,batch_first=True,dropout=dropout)\n",
        "        elif rnn_type=='LSTM':\n",
        "            self.rnn = nn.LSTM(input_dim,hidden,num_layers,batch_first=True,dropout=dropout)\n",
        "        else: raise ValueError(\"Unknown rnn_type\")\n",
        "        self.head = nn.Sequential(nn.Linear(hidden,32), nn.ReLU(), nn.Dropout(0.2), nn.Linear(32,num_classes))\n",
        "        self.rnn_type = rnn_type\n",
        "    def forward(self,x):\n",
        "        out,_ = self.rnn(x)\n",
        "        return self.head(out[:,-1,:])\n",
        "\n",
        "# -------------------------\n",
        "# Training & Evaluation\n",
        "# -------------------------\n",
        "def compute_class_weights(y):\n",
        "    counts = Counter(y.tolist())\n",
        "    total = sum(counts.values())\n",
        "    num_classes = len(counts)\n",
        "    weights = []\n",
        "    for i in range(max(counts.keys())+1):\n",
        "        cnt = counts.get(i,0)\n",
        "        weights.append(total/(num_classes*cnt) if cnt>0 else 0.0)\n",
        "    return torch.tensor(weights,dtype=torch.float32,device=DEVICE)\n",
        "\n",
        "def train_one_epoch(model, loader, opt, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    preds, trues = [], []\n",
        "    for xb,yb in loader:\n",
        "        xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        out = model(xb)\n",
        "        loss = criterion(out,yb)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()*xb.size(0)\n",
        "        preds.append(out.detach().argmax(1).cpu().numpy())\n",
        "        trues.append(yb.cpu().numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    trues = np.concatenate(trues)\n",
        "    return total_loss/len(trues), accuracy_score(trues,preds), f1_score(trues,preds,average='macro',zero_division=0)\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss=0\n",
        "    preds,trues=[],[]\n",
        "    with torch.no_grad():\n",
        "        for xb,yb in loader:\n",
        "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            out = model(xb)\n",
        "            total_loss += criterion(out,yb).item()*xb.size(0)\n",
        "            preds.append(out.argmax(1).cpu().numpy())\n",
        "            trues.append(yb.cpu().numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    trues = np.concatenate(trues)\n",
        "    mean_loss = total_loss / len(trues)\n",
        "    acc = accuracy_score(trues,preds)\n",
        "    f1 = f1_score(trues,preds,average='macro',zero_division=0)\n",
        "    prec, rec, f1s, sup = precision_recall_fscore_support(trues,preds,average=None,zero_division=0)\n",
        "    return mean_loss, acc, f1, prec, rec, f1s, sup, preds, trues\n",
        "\n",
        "# -------------------------\n",
        "# Experiment for a window\n",
        "# -------------------------\n",
        "def run_experiment_for_window(df, window_size, model_name, train_pids, val_pids, test_pids):\n",
        "    X_seq, X_flat, y, pids = build_windows(df, RAW_FEATURES, window_size)\n",
        "    if len(y)==0: return None\n",
        "\n",
        "    # split by participant\n",
        "    train_mask = np.isin(pids, train_pids)\n",
        "    val_mask = np.isin(pids, val_pids)\n",
        "    test_mask = np.isin(pids, test_pids)\n",
        "\n",
        "    X_seq_train, X_seq_val, X_seq_test = X_seq[train_mask], X_seq[val_mask], X_seq[test_mask]\n",
        "    X_flat_train, X_flat_val, X_flat_test = X_flat[train_mask], X_flat[val_mask], X_flat[test_mask]\n",
        "    y_train, y_val, y_test = y[train_mask], y[val_mask], y[test_mask]\n",
        "\n",
        "    if len(y_train)==0 or len(y_val)==0 or len(y_test)==0:\n",
        "        print(\"One split empty for window\", window_size)\n",
        "        return None\n",
        "\n",
        "    # -------------------------\n",
        "    # SMOTE augmentation (training set only)\n",
        "    # -------------------------\n",
        "    print(f\"Class distribution BEFORE SMOTE: {Counter(y_train)}\")\n",
        "    smote = SMOTE(random_state=SEED)\n",
        "    if model_name==\"ANN\":\n",
        "        X_train_aug, y_train_aug = smote.fit_resample(X_flat_train, y_train)\n",
        "        X_train_aug = X_train_aug\n",
        "    else:\n",
        "        N,L,F = X_seq_train.shape\n",
        "        X_seq_flat = X_seq_train.reshape(N,L*F)\n",
        "        X_train_aug, y_train_aug = smote.fit_resample(X_seq_flat, y_train)\n",
        "        X_train_aug = X_train_aug.reshape(-1,L,F)\n",
        "    print(f\"Class distribution AFTER SMOTE: {Counter(y_train_aug)}\")\n",
        "\n",
        "    # save class distributions\n",
        "    dist_df = pd.DataFrame({'Class':[0,1,2],\n",
        "                            'Before': [Counter(y_train)[0],Counter(y_train)[1],Counter(y_train)[2]],\n",
        "                            'After': [Counter(y_train_aug)[0],Counter(y_train_aug)[1],Counter(y_train_aug)[2]]})\n",
        "    dist_df.to_csv(OUTPUT_DIR/f'class_distribution_w{window_size}_{model_name}.csv', index=False)\n",
        "\n",
        "    # -------------------------\n",
        "    # Scaling\n",
        "    # -------------------------\n",
        "    if model_name==\"ANN\":\n",
        "        scaler = StandardScaler().fit(X_train_aug)\n",
        "        Xtr = scaler.transform(X_train_aug)\n",
        "        Xv = scaler.transform(X_flat_val)\n",
        "        Xt = scaler.transform(X_flat_test)\n",
        "        input_dim = Xtr.shape[1]\n",
        "    else:\n",
        "        N,L,F = X_train_aug.shape\n",
        "        scaler = StandardScaler().fit(X_train_aug.reshape(-1,F))\n",
        "        Xtr = scaler.transform(X_train_aug.reshape(-1,F)).reshape(N,L,F)\n",
        "        Xv = scaler.transform(X_seq_val.reshape(-1,F)).reshape(X_seq_val.shape)\n",
        "        Xt = scaler.transform(X_seq_test.reshape(-1,F)).reshape(X_seq_test.shape)\n",
        "        input_dim = F\n",
        "\n",
        "    train_ds = StressDataset(Xtr, y_train_aug)\n",
        "    val_ds = StressDataset(Xv, y_val)\n",
        "    test_ds = StressDataset(Xt, y_test)\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # -------------------------\n",
        "    # Model\n",
        "    # -------------------------\n",
        "    if model_name==\"ANN\": model = ANNModel(input_dim).to(DEVICE)\n",
        "    elif model_name==\"CNN1D\": model = CNN1DModel(Xtr.shape[1], Xtr.shape[2]).to(DEVICE)\n",
        "    elif model_name in (\"RNN\",\"GRU\",\"LSTM\"): model = RNNModel(input_dim, hidden=64, num_layers=1, rnn_type=model_name).to(DEVICE)\n",
        "    else: raise ValueError(\"Unknown model\")\n",
        "\n",
        "    # loss and optimizer\n",
        "    class_weights = compute_class_weights(torch.tensor(y_train_aug))\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
        "\n",
        "    # -------------------------\n",
        "    # Training loop with early stopping\n",
        "    # -------------------------\n",
        "    best_val_f1, best_state, cur_wait = -1.0, None, 0\n",
        "    for epoch in range(1,EPOCHS+1):\n",
        "        train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, opt, criterion)\n",
        "        val_loss, val_acc, val_f1, *_ = evaluate(model, val_loader, criterion)\n",
        "        print(f\"[W{window_size}] {model_name} Epoch {epoch:02d} | train_loss {train_loss:.4f} acc {train_acc:.3f} f1 {train_f1:.3f} || val_loss {val_loss:.4f} acc {val_acc:.3f} f1 {val_f1:.3f}\")\n",
        "        if val_f1 > best_val_f1+1e-4:\n",
        "            best_val_f1 = val_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            cur_wait = 0\n",
        "        else:\n",
        "            cur_wait += 1\n",
        "            if cur_wait >= PATIENCE:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # test\n",
        "    if best_state is not None: model.load_state_dict(best_state)\n",
        "    test_loss, test_acc, test_f1, prec, rec, f1s, sup, preds, trues = evaluate(model, test_loader, criterion)\n",
        "\n",
        "    # save confusion matrix\n",
        "    cm = confusion_matrix(trues,preds)\n",
        "    np.save(OUTPUT_DIR/f\"cm_w{window_size}_{model_name}.npy\", cm)\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(cm, cmap='Blues')\n",
        "    plt.title(f\"CM W{window_size} {model_name}\")\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"pred\")\n",
        "    plt.ylabel(\"true\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUTPUT_DIR/f\"cm_w{window_size}_{model_name}.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return {\n",
        "        \"window\": window_size,\n",
        "        \"model\": model_name,\n",
        "        \"train_samples\": len(y_train_aug),\n",
        "        \"val_samples\": len(y_val),\n",
        "        \"test_samples\": len(y_test),\n",
        "        \"test_loss\": float(test_loss),\n",
        "        \"test_acc\": float(test_acc),\n",
        "        \"test_macro_f1\": float(test_f1),\n",
        "        \"per_class_prec\": prec.tolist(),\n",
        "        \"per_class_rec\": rec.tolist(),\n",
        "        \"per_class_f1\": f1s.tolist(),\n",
        "        \"support\": sup.tolist()\n",
        "    }\n",
        "\n",
        "# -------------------------\n",
        "# Main\n",
        "# -------------------------\n",
        "def main():\n",
        "    print(\"Device:\", DEVICE)\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    df = df.sort_values(['participant_id','day']).reset_index(drop=True)\n",
        "    df[CLASS_COL] = df[TARGET_COL].apply(stress_to_class)\n",
        "\n",
        "    # participant split\n",
        "    pids = df['participant_id'].unique()\n",
        "    random.shuffle(pids)\n",
        "    n = len(pids)\n",
        "    n_train = int(n*TRAIN_P)\n",
        "    n_val = int(n*VAL_P)\n",
        "    train_pids = pids[:n_train]\n",
        "    val_pids = pids[n_train:n_train+n_val]\n",
        "    test_pids = pids[n_train+n_val:]\n",
        "    print(f\"Participants: total {n} train {len(train_pids)} val {len(val_pids)} test {len(test_pids)}\")\n",
        "\n",
        "    results=[]\n",
        "    for w in WINDOW_SIZES:\n",
        "        for mname in MODEL_NAMES:\n",
        "            print(\"=\"*60)\n",
        "            print(f\"Running Window={w} Model={mname}\")\n",
        "            try:\n",
        "                res = run_experiment_for_window(df, w, mname, train_pids, val_pids, test_pids)\n",
        "                if res is not None:\n",
        "                    results.append(res)\n",
        "                    pd.DataFrame(results).to_csv(OUTPUT_DIR/\"results_summary.csv\", index=False)\n",
        "            except Exception as e:\n",
        "                print(\"Error running\", w, mname, e)\n",
        "\n",
        "    if results:\n",
        "        pd.DataFrame(results).to_csv(OUTPUT_DIR/\"results_summary.csv\", index=False)\n",
        "        print(\"Saved results to\", OUTPUT_DIR/\"results_summary.csv\")\n",
        "    else:\n",
        "        print(\"No results to save.\")\n",
        "\n",
        "# -------------------------\n",
        "# Main LOSO\n",
        "# -------------------------\n",
        "def main_loso():\n",
        "    print(\"Device:\", DEVICE)\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    df = df.sort_values(['participant_id','day']).reset_index(drop=True)\n",
        "    df[CLASS_COL] = df[TARGET_COL].apply(stress_to_class)\n",
        "\n",
        "    pids = df['participant_id'].unique()\n",
        "    results = []\n",
        "\n",
        "    for test_pid in tqdm(pids, desc=\"LOSO participants\"):\n",
        "        train_val_pids = [pid for pid in pids if pid != test_pid]\n",
        "\n",
        "        # Optional: split train_val_pids into train/val\n",
        "        n_train = int(len(train_val_pids) * (TRAIN_P/(TRAIN_P+VAL_P)))\n",
        "        train_pids = train_val_pids[:n_train]\n",
        "        val_pids = train_val_pids[n_train:]\n",
        "\n",
        "        print(f\"LOSO: Test subject={test_pid} | Train subjects={len(train_pids)} | Val subjects={len(val_pids)}\")\n",
        "\n",
        "        for w in WINDOW_SIZES:\n",
        "            for mname in MODEL_NAMES:\n",
        "                print(\"=\"*60)\n",
        "                print(f\"Running Window={w} Model={mname} Test subject={test_pid}\")\n",
        "                try:\n",
        "                    res = run_experiment_for_window(df, w, mname, train_pids, val_pids, [test_pid])\n",
        "                    if res is not None:\n",
        "                        res[\"held_out_subject\"] = test_pid\n",
        "                        results.append(res)\n",
        "                        # save intermediate results\n",
        "                        pd.DataFrame(results).to_csv(OUTPUT_DIR/\"results_loso.csv\", index=False)\n",
        "                except Exception as e:\n",
        "                    print(\"Error running\", w, mname, \"for subject\", test_pid, e)\n",
        "\n",
        "    if results:\n",
        "        pd.DataFrame(results).to_csv(OUTPUT_DIR/\"results_loso.csv\", index=False)\n",
        "        print(\"Saved LOSO results to\", OUTPUT_DIR/\"results_loso.csv\")\n",
        "    else:\n",
        "        print(\"No results to save.\")\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main_loso()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7xIvWEk7s1Y",
        "outputId": "5ccb5958-1f54-4c2d-ada7-b0b16985e846"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=1 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=1\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 932, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 984, np.int64(2): 984, np.int64(1): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1128 acc 0.410 f1 0.407 || val_loss 1.1229 acc 0.331 f1 0.314\n",
            "[W3] ANN Epoch 02 | train_loss 0.9835 acc 0.497 f1 0.490 || val_loss 1.1312 acc 0.364 f1 0.338\n",
            "[W3] ANN Epoch 03 | train_loss 0.8912 acc 0.541 f1 0.533 || val_loss 1.1224 acc 0.344 f1 0.314\n",
            "[W3] ANN Epoch 04 | train_loss 0.8094 acc 0.597 f1 0.589 || val_loss 1.1153 acc 0.368 f1 0.317\n",
            "[W3] ANN Epoch 05 | train_loss 0.7454 acc 0.643 f1 0.637 || val_loss 1.1263 acc 0.379 f1 0.317\n",
            "[W3] ANN Epoch 06 | train_loss 0.7153 acc 0.644 f1 0.640 || val_loss 1.1393 acc 0.370 f1 0.323\n",
            "[W3] ANN Epoch 07 | train_loss 0.6591 acc 0.684 f1 0.681 || val_loss 1.1263 acc 0.379 f1 0.320\n",
            "[W3] ANN Epoch 08 | train_loss 0.6248 acc 0.705 f1 0.703 || val_loss 1.1723 acc 0.387 f1 0.322\n",
            "[W3] ANN Epoch 09 | train_loss 0.6095 acc 0.709 f1 0.707 || val_loss 1.1866 acc 0.416 f1 0.340\n",
            "[W3] ANN Epoch 10 | train_loss 0.5832 acc 0.722 f1 0.720 || val_loss 1.1836 acc 0.414 f1 0.333\n",
            "[W3] ANN Epoch 11 | train_loss 0.5550 acc 0.736 f1 0.735 || val_loss 1.2006 acc 0.442 f1 0.373\n",
            "[W3] ANN Epoch 12 | train_loss 0.5230 acc 0.747 f1 0.747 || val_loss 1.2554 acc 0.407 f1 0.345\n",
            "[W3] ANN Epoch 13 | train_loss 0.4997 acc 0.771 f1 0.770 || val_loss 1.2680 acc 0.459 f1 0.372\n",
            "[W3] ANN Epoch 14 | train_loss 0.4815 acc 0.785 f1 0.785 || val_loss 1.2819 acc 0.444 f1 0.356\n",
            "[W3] ANN Epoch 15 | train_loss 0.4882 acc 0.779 f1 0.779 || val_loss 1.3187 acc 0.424 f1 0.354\n",
            "[W3] ANN Epoch 16 | train_loss 0.4582 acc 0.793 f1 0.792 || val_loss 1.2805 acc 0.434 f1 0.364\n",
            "[W3] ANN Epoch 17 | train_loss 0.4548 acc 0.797 f1 0.797 || val_loss 1.3141 acc 0.461 f1 0.374\n",
            "[W3] ANN Epoch 18 | train_loss 0.4603 acc 0.802 f1 0.802 || val_loss 1.3511 acc 0.430 f1 0.351\n",
            "[W3] ANN Epoch 19 | train_loss 0.4161 acc 0.822 f1 0.821 || val_loss 1.3654 acc 0.438 f1 0.365\n",
            "[W3] ANN Epoch 20 | train_loss 0.4327 acc 0.807 f1 0.806 || val_loss 1.4125 acc 0.426 f1 0.363\n",
            "[W3] ANN Epoch 21 | train_loss 0.3923 acc 0.828 f1 0.828 || val_loss 1.4182 acc 0.428 f1 0.349\n",
            "[W3] ANN Epoch 22 | train_loss 0.3771 acc 0.845 f1 0.845 || val_loss 1.4294 acc 0.455 f1 0.369\n",
            "[W3] ANN Epoch 23 | train_loss 0.3763 acc 0.845 f1 0.845 || val_loss 1.4894 acc 0.426 f1 0.352\n",
            "[W3] ANN Epoch 24 | train_loss 0.4152 acc 0.824 f1 0.824 || val_loss 1.4664 acc 0.440 f1 0.363\n",
            "[W3] ANN Epoch 25 | train_loss 0.3847 acc 0.834 f1 0.833 || val_loss 1.4503 acc 0.436 f1 0.362\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=1\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 932, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 984, np.int64(2): 984, np.int64(1): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0649 acc 0.416 f1 0.416 || val_loss 1.0206 acc 0.430 f1 0.345\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9293 acc 0.556 f1 0.549 || val_loss 1.0220 acc 0.442 f1 0.355\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8035 acc 0.605 f1 0.601 || val_loss 1.0776 acc 0.409 f1 0.349\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7092 acc 0.662 f1 0.658 || val_loss 1.1560 acc 0.385 f1 0.322\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6243 acc 0.704 f1 0.702 || val_loss 1.2171 acc 0.377 f1 0.301\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5769 acc 0.724 f1 0.722 || val_loss 1.2331 acc 0.416 f1 0.347\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5128 acc 0.771 f1 0.770 || val_loss 1.3042 acc 0.407 f1 0.352\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4684 acc 0.793 f1 0.792 || val_loss 1.3711 acc 0.401 f1 0.351\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4332 acc 0.814 f1 0.814 || val_loss 1.4086 acc 0.405 f1 0.329\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3979 acc 0.837 f1 0.836 || val_loss 1.4972 acc 0.397 f1 0.341\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=1\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 932, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 984, np.int64(2): 984, np.int64(1): 984})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1007 acc 0.352 f1 0.312 || val_loss 1.0973 acc 0.379 f1 0.330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0825 acc 0.419 f1 0.412 || val_loss 1.1021 acc 0.311 f1 0.296\n",
            "[W3] RNN Epoch 03 | train_loss 1.0632 acc 0.444 f1 0.438 || val_loss 1.0945 acc 0.346 f1 0.321\n",
            "[W3] RNN Epoch 04 | train_loss 1.0465 acc 0.454 f1 0.448 || val_loss 1.0884 acc 0.337 f1 0.305\n",
            "[W3] RNN Epoch 05 | train_loss 1.0242 acc 0.480 f1 0.474 || val_loss 1.0860 acc 0.354 f1 0.317\n",
            "[W3] RNN Epoch 06 | train_loss 1.0050 acc 0.496 f1 0.489 || val_loss 1.0728 acc 0.389 f1 0.345\n",
            "[W3] RNN Epoch 07 | train_loss 0.9860 acc 0.518 f1 0.511 || val_loss 1.1041 acc 0.337 f1 0.310\n",
            "[W3] RNN Epoch 08 | train_loss 0.9631 acc 0.532 f1 0.523 || val_loss 1.0902 acc 0.370 f1 0.323\n",
            "[W3] RNN Epoch 09 | train_loss 0.9432 acc 0.548 f1 0.537 || val_loss 1.1128 acc 0.350 f1 0.321\n",
            "[W3] RNN Epoch 10 | train_loss 0.9215 acc 0.563 f1 0.553 || val_loss 1.1245 acc 0.350 f1 0.317\n",
            "[W3] RNN Epoch 11 | train_loss 0.9064 acc 0.565 f1 0.554 || val_loss 1.1155 acc 0.364 f1 0.331\n",
            "[W3] RNN Epoch 12 | train_loss 0.8745 acc 0.587 f1 0.578 || val_loss 1.1396 acc 0.360 f1 0.324\n",
            "[W3] RNN Epoch 13 | train_loss 0.8552 acc 0.600 f1 0.590 || val_loss 1.1205 acc 0.379 f1 0.332\n",
            "[W3] RNN Epoch 14 | train_loss 0.8329 acc 0.618 f1 0.609 || val_loss 1.1348 acc 0.364 f1 0.313\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=1\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 932, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 984, np.int64(2): 984, np.int64(1): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0993 acc 0.324 f1 0.242 || val_loss 1.0930 acc 0.356 f1 0.301\n",
            "[W3] GRU Epoch 02 | train_loss 1.0923 acc 0.379 f1 0.376 || val_loss 1.0961 acc 0.337 f1 0.315\n",
            "[W3] GRU Epoch 03 | train_loss 1.0834 acc 0.419 f1 0.417 || val_loss 1.0903 acc 0.342 f1 0.310\n",
            "[W3] GRU Epoch 04 | train_loss 1.0682 acc 0.430 f1 0.426 || val_loss 1.0878 acc 0.329 f1 0.300\n",
            "[W3] GRU Epoch 05 | train_loss 1.0486 acc 0.467 f1 0.465 || val_loss 1.0802 acc 0.356 f1 0.318\n",
            "[W3] GRU Epoch 06 | train_loss 1.0146 acc 0.503 f1 0.497 || val_loss 1.0814 acc 0.352 f1 0.314\n",
            "[W3] GRU Epoch 07 | train_loss 0.9634 acc 0.534 f1 0.528 || val_loss 1.0445 acc 0.407 f1 0.326\n",
            "[W3] GRU Epoch 08 | train_loss 0.9087 acc 0.565 f1 0.560 || val_loss 1.1377 acc 0.377 f1 0.339\n",
            "[W3] GRU Epoch 09 | train_loss 0.8405 acc 0.601 f1 0.594 || val_loss 1.1067 acc 0.377 f1 0.304\n",
            "[W3] GRU Epoch 10 | train_loss 0.7899 acc 0.619 f1 0.614 || val_loss 1.1484 acc 0.374 f1 0.312\n",
            "[W3] GRU Epoch 11 | train_loss 0.7446 acc 0.641 f1 0.637 || val_loss 1.1518 acc 0.381 f1 0.301\n",
            "[W3] GRU Epoch 12 | train_loss 0.7050 acc 0.656 f1 0.652 || val_loss 1.1878 acc 0.401 f1 0.324\n",
            "[W3] GRU Epoch 13 | train_loss 0.6684 acc 0.686 f1 0.684 || val_loss 1.2105 acc 0.416 f1 0.334\n",
            "[W3] GRU Epoch 14 | train_loss 0.6253 acc 0.704 f1 0.701 || val_loss 1.2319 acc 0.422 f1 0.336\n",
            "[W3] GRU Epoch 15 | train_loss 0.5941 acc 0.717 f1 0.715 || val_loss 1.2965 acc 0.414 f1 0.331\n",
            "[W3] GRU Epoch 16 | train_loss 0.5652 acc 0.728 f1 0.726 || val_loss 1.3256 acc 0.393 f1 0.314\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=1\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 932, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(0): 984, np.int64(2): 984, np.int64(1): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1024 acc 0.340 f1 0.210 || val_loss 1.1131 acc 0.251 f1 0.209\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0942 acc 0.384 f1 0.364 || val_loss 1.0986 acc 0.284 f1 0.270\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0837 acc 0.400 f1 0.363 || val_loss 1.0917 acc 0.298 f1 0.263\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0642 acc 0.431 f1 0.399 || val_loss 1.0882 acc 0.317 f1 0.281\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0208 acc 0.477 f1 0.453 || val_loss 1.0813 acc 0.325 f1 0.291\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9383 acc 0.534 f1 0.516 || val_loss 1.1248 acc 0.350 f1 0.309\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8542 acc 0.567 f1 0.559 || val_loss 1.1374 acc 0.366 f1 0.312\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7984 acc 0.595 f1 0.587 || val_loss 1.1215 acc 0.395 f1 0.310\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7531 acc 0.621 f1 0.613 || val_loss 1.1519 acc 0.346 f1 0.280\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7150 acc 0.633 f1 0.627 || val_loss 1.1651 acc 0.389 f1 0.323\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6897 acc 0.650 f1 0.645 || val_loss 1.2151 acc 0.383 f1 0.315\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6480 acc 0.664 f1 0.658 || val_loss 1.1944 acc 0.432 f1 0.338\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6221 acc 0.678 f1 0.674 || val_loss 1.2317 acc 0.424 f1 0.339\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5973 acc 0.694 f1 0.690 || val_loss 1.2974 acc 0.420 f1 0.342\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5729 acc 0.699 f1 0.696 || val_loss 1.3898 acc 0.393 f1 0.327\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5409 acc 0.724 f1 0.721 || val_loss 1.4256 acc 0.395 f1 0.326\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5228 acc 0.729 f1 0.727 || val_loss 1.4747 acc 0.399 f1 0.323\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5075 acc 0.741 f1 0.740 || val_loss 1.5219 acc 0.401 f1 0.330\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4862 acc 0.743 f1 0.742 || val_loss 1.5599 acc 0.414 f1 0.343\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4647 acc 0.771 f1 0.770 || val_loss 1.6591 acc 0.416 f1 0.338\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4563 acc 0.769 f1 0.768 || val_loss 1.7189 acc 0.401 f1 0.330\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4313 acc 0.786 f1 0.786 || val_loss 1.7627 acc 0.395 f1 0.322\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4215 acc 0.787 f1 0.786 || val_loss 1.8314 acc 0.403 f1 0.333\n",
            "[W3] LSTM Epoch 24 | train_loss 0.4111 acc 0.793 f1 0.792 || val_loss 1.8666 acc 0.412 f1 0.337\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3993 acc 0.801 f1 0.801 || val_loss 1.9339 acc 0.403 f1 0.328\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3844 acc 0.820 f1 0.820 || val_loss 1.9823 acc 0.418 f1 0.340\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3710 acc 0.816 f1 0.816 || val_loss 2.0740 acc 0.393 f1 0.325\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:   1%|          | 1/100 [00:29<48:08, 29.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=2 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=2\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 934, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1347 acc 0.397 f1 0.386 || val_loss 1.1838 acc 0.280 f1 0.273\n",
            "[W3] ANN Epoch 02 | train_loss 0.9520 acc 0.526 f1 0.515 || val_loss 1.1714 acc 0.298 f1 0.287\n",
            "[W3] ANN Epoch 03 | train_loss 0.8613 acc 0.582 f1 0.569 || val_loss 1.1588 acc 0.335 f1 0.316\n",
            "[W3] ANN Epoch 04 | train_loss 0.7599 acc 0.629 f1 0.621 || val_loss 1.1696 acc 0.354 f1 0.321\n",
            "[W3] ANN Epoch 05 | train_loss 0.7313 acc 0.648 f1 0.643 || val_loss 1.1467 acc 0.395 f1 0.354\n",
            "[W3] ANN Epoch 06 | train_loss 0.6666 acc 0.686 f1 0.682 || val_loss 1.1682 acc 0.401 f1 0.359\n",
            "[W3] ANN Epoch 07 | train_loss 0.6088 acc 0.706 f1 0.704 || val_loss 1.1949 acc 0.374 f1 0.337\n",
            "[W3] ANN Epoch 08 | train_loss 0.5887 acc 0.724 f1 0.722 || val_loss 1.1798 acc 0.407 f1 0.353\n",
            "[W3] ANN Epoch 09 | train_loss 0.5607 acc 0.737 f1 0.736 || val_loss 1.2114 acc 0.416 f1 0.370\n",
            "[W3] ANN Epoch 10 | train_loss 0.5318 acc 0.752 f1 0.750 || val_loss 1.2212 acc 0.412 f1 0.357\n",
            "[W3] ANN Epoch 11 | train_loss 0.5077 acc 0.768 f1 0.768 || val_loss 1.2600 acc 0.405 f1 0.351\n",
            "[W3] ANN Epoch 12 | train_loss 0.4790 acc 0.780 f1 0.779 || val_loss 1.2986 acc 0.414 f1 0.363\n",
            "[W3] ANN Epoch 13 | train_loss 0.4948 acc 0.772 f1 0.771 || val_loss 1.3013 acc 0.436 f1 0.372\n",
            "[W3] ANN Epoch 14 | train_loss 0.4369 acc 0.809 f1 0.808 || val_loss 1.3363 acc 0.440 f1 0.379\n",
            "[W3] ANN Epoch 15 | train_loss 0.4336 acc 0.812 f1 0.812 || val_loss 1.4052 acc 0.430 f1 0.368\n",
            "[W3] ANN Epoch 16 | train_loss 0.4185 acc 0.824 f1 0.824 || val_loss 1.4364 acc 0.440 f1 0.361\n",
            "[W3] ANN Epoch 17 | train_loss 0.4125 acc 0.824 f1 0.824 || val_loss 1.4230 acc 0.438 f1 0.365\n",
            "[W3] ANN Epoch 18 | train_loss 0.3900 acc 0.834 f1 0.834 || val_loss 1.4435 acc 0.440 f1 0.371\n",
            "[W3] ANN Epoch 19 | train_loss 0.3605 acc 0.843 f1 0.843 || val_loss 1.4677 acc 0.447 f1 0.372\n",
            "[W3] ANN Epoch 20 | train_loss 0.3722 acc 0.848 f1 0.848 || val_loss 1.4701 acc 0.440 f1 0.363\n",
            "[W3] ANN Epoch 21 | train_loss 0.3382 acc 0.858 f1 0.858 || val_loss 1.5170 acc 0.432 f1 0.355\n",
            "[W3] ANN Epoch 22 | train_loss 0.3438 acc 0.857 f1 0.857 || val_loss 1.4972 acc 0.457 f1 0.388\n",
            "[W3] ANN Epoch 23 | train_loss 0.3230 acc 0.873 f1 0.873 || val_loss 1.5717 acc 0.438 f1 0.362\n",
            "[W3] ANN Epoch 24 | train_loss 0.3445 acc 0.862 f1 0.862 || val_loss 1.5950 acc 0.426 f1 0.361\n",
            "[W3] ANN Epoch 25 | train_loss 0.3021 acc 0.875 f1 0.875 || val_loss 1.5425 acc 0.403 f1 0.345\n",
            "[W3] ANN Epoch 26 | train_loss 0.2851 acc 0.886 f1 0.886 || val_loss 1.6339 acc 0.434 f1 0.369\n",
            "[W3] ANN Epoch 27 | train_loss 0.2738 acc 0.887 f1 0.887 || val_loss 1.6115 acc 0.414 f1 0.348\n",
            "[W3] ANN Epoch 28 | train_loss 0.2718 acc 0.891 f1 0.891 || val_loss 1.7070 acc 0.420 f1 0.348\n",
            "[W3] ANN Epoch 29 | train_loss 0.2810 acc 0.886 f1 0.886 || val_loss 1.6416 acc 0.432 f1 0.362\n",
            "[W3] ANN Epoch 30 | train_loss 0.2722 acc 0.886 f1 0.886 || val_loss 1.6684 acc 0.457 f1 0.381\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=2\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 934, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0695 acc 0.404 f1 0.406 || val_loss 1.0324 acc 0.424 f1 0.307\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9412 acc 0.538 f1 0.533 || val_loss 1.0442 acc 0.422 f1 0.333\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8087 acc 0.605 f1 0.599 || val_loss 1.1092 acc 0.397 f1 0.335\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7176 acc 0.665 f1 0.662 || val_loss 1.1678 acc 0.397 f1 0.325\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6209 acc 0.708 f1 0.706 || val_loss 1.2499 acc 0.374 f1 0.313\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5597 acc 0.744 f1 0.742 || val_loss 1.3220 acc 0.416 f1 0.331\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.4944 acc 0.783 f1 0.782 || val_loss 1.4118 acc 0.387 f1 0.330\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4380 acc 0.808 f1 0.807 || val_loss 1.4900 acc 0.426 f1 0.349\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.3944 acc 0.828 f1 0.827 || val_loss 1.5428 acc 0.385 f1 0.321\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3518 acc 0.851 f1 0.851 || val_loss 1.6414 acc 0.432 f1 0.352\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3090 acc 0.876 f1 0.875 || val_loss 1.7576 acc 0.405 f1 0.336\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2783 acc 0.885 f1 0.885 || val_loss 1.8127 acc 0.428 f1 0.349\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2489 acc 0.906 f1 0.906 || val_loss 1.8684 acc 0.414 f1 0.338\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.1964 acc 0.932 f1 0.932 || val_loss 1.9786 acc 0.407 f1 0.332\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1792 acc 0.937 f1 0.937 || val_loss 2.1722 acc 0.407 f1 0.336\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1655 acc 0.945 f1 0.945 || val_loss 2.2658 acc 0.397 f1 0.313\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1346 acc 0.955 f1 0.955 || val_loss 2.4661 acc 0.397 f1 0.325\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1335 acc 0.959 f1 0.959 || val_loss 2.3412 acc 0.434 f1 0.362\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1051 acc 0.968 f1 0.968 || val_loss 2.5198 acc 0.401 f1 0.325\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.0866 acc 0.972 f1 0.972 || val_loss 2.6058 acc 0.409 f1 0.346\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.0773 acc 0.977 f1 0.977 || val_loss 2.7435 acc 0.414 f1 0.346\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.0971 acc 0.966 f1 0.966 || val_loss 2.6948 acc 0.412 f1 0.339\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.0773 acc 0.978 f1 0.978 || val_loss 2.8124 acc 0.412 f1 0.347\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.0669 acc 0.976 f1 0.976 || val_loss 2.9469 acc 0.418 f1 0.331\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.0730 acc 0.979 f1 0.979 || val_loss 2.9348 acc 0.436 f1 0.359\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.0604 acc 0.981 f1 0.981 || val_loss 3.0607 acc 0.428 f1 0.358\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=2\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 934, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0988 acc 0.337 f1 0.334 || val_loss 1.1068 acc 0.286 f1 0.278\n",
            "[W3] RNN Epoch 02 | train_loss 1.0826 acc 0.417 f1 0.403 || val_loss 1.1136 acc 0.270 f1 0.266\n",
            "[W3] RNN Epoch 03 | train_loss 1.0687 acc 0.437 f1 0.421 || val_loss 1.0929 acc 0.331 f1 0.313\n",
            "[W3] RNN Epoch 04 | train_loss 1.0526 acc 0.449 f1 0.442 || val_loss 1.0857 acc 0.337 f1 0.309\n",
            "[W3] RNN Epoch 05 | train_loss 1.0377 acc 0.475 f1 0.470 || val_loss 1.0892 acc 0.337 f1 0.313\n",
            "[W3] RNN Epoch 06 | train_loss 1.0117 acc 0.496 f1 0.488 || val_loss 1.1262 acc 0.309 f1 0.290\n",
            "[W3] RNN Epoch 07 | train_loss 0.9976 acc 0.505 f1 0.499 || val_loss 1.0998 acc 0.337 f1 0.311\n",
            "[W3] RNN Epoch 08 | train_loss 0.9810 acc 0.514 f1 0.506 || val_loss 1.0891 acc 0.352 f1 0.315\n",
            "[W3] RNN Epoch 09 | train_loss 0.9613 acc 0.527 f1 0.521 || val_loss 1.1052 acc 0.340 f1 0.306\n",
            "[W3] RNN Epoch 10 | train_loss 0.9349 acc 0.544 f1 0.536 || val_loss 1.1151 acc 0.350 f1 0.321\n",
            "[W3] RNN Epoch 11 | train_loss 0.9094 acc 0.569 f1 0.561 || val_loss 1.1286 acc 0.333 f1 0.308\n",
            "[W3] RNN Epoch 12 | train_loss 0.8854 acc 0.579 f1 0.572 || val_loss 1.1066 acc 0.348 f1 0.317\n",
            "[W3] RNN Epoch 13 | train_loss 0.8646 acc 0.592 f1 0.584 || val_loss 1.1193 acc 0.360 f1 0.329\n",
            "[W3] RNN Epoch 14 | train_loss 0.8345 acc 0.615 f1 0.607 || val_loss 1.1209 acc 0.362 f1 0.329\n",
            "[W3] RNN Epoch 15 | train_loss 0.8114 acc 0.625 f1 0.616 || val_loss 1.1066 acc 0.368 f1 0.334\n",
            "[W3] RNN Epoch 16 | train_loss 0.7889 acc 0.628 f1 0.620 || val_loss 1.1187 acc 0.366 f1 0.327\n",
            "[W3] RNN Epoch 17 | train_loss 0.7686 acc 0.642 f1 0.634 || val_loss 1.1341 acc 0.370 f1 0.326\n",
            "[W3] RNN Epoch 18 | train_loss 0.7362 acc 0.660 f1 0.653 || val_loss 1.1527 acc 0.374 f1 0.343\n",
            "[W3] RNN Epoch 19 | train_loss 0.7013 acc 0.674 f1 0.667 || val_loss 1.1538 acc 0.372 f1 0.330\n",
            "[W3] RNN Epoch 20 | train_loss 0.6899 acc 0.680 f1 0.673 || val_loss 1.1727 acc 0.364 f1 0.330\n",
            "[W3] RNN Epoch 21 | train_loss 0.6590 acc 0.690 f1 0.683 || val_loss 1.1794 acc 0.362 f1 0.325\n",
            "[W3] RNN Epoch 22 | train_loss 0.6433 acc 0.698 f1 0.692 || val_loss 1.2061 acc 0.377 f1 0.338\n",
            "[W3] RNN Epoch 23 | train_loss 0.6234 acc 0.713 f1 0.708 || val_loss 1.2301 acc 0.360 f1 0.320\n",
            "[W3] RNN Epoch 24 | train_loss 0.6013 acc 0.712 f1 0.707 || val_loss 1.1983 acc 0.393 f1 0.334\n",
            "[W3] RNN Epoch 25 | train_loss 0.5820 acc 0.731 f1 0.727 || val_loss 1.2379 acc 0.383 f1 0.329\n",
            "[W3] RNN Epoch 26 | train_loss 0.5583 acc 0.735 f1 0.731 || val_loss 1.2339 acc 0.385 f1 0.332\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=2\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 934, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0963 acc 0.372 f1 0.318 || val_loss 1.1006 acc 0.309 f1 0.295\n",
            "[W3] GRU Epoch 02 | train_loss 1.0847 acc 0.424 f1 0.422 || val_loss 1.0925 acc 0.354 f1 0.330\n",
            "[W3] GRU Epoch 03 | train_loss 1.0724 acc 0.443 f1 0.439 || val_loss 1.0810 acc 0.362 f1 0.326\n",
            "[W3] GRU Epoch 04 | train_loss 1.0533 acc 0.477 f1 0.471 || val_loss 1.0828 acc 0.352 f1 0.319\n",
            "[W3] GRU Epoch 05 | train_loss 1.0195 acc 0.502 f1 0.495 || val_loss 1.0886 acc 0.362 f1 0.325\n",
            "[W3] GRU Epoch 06 | train_loss 0.9734 acc 0.537 f1 0.528 || val_loss 1.0846 acc 0.374 f1 0.325\n",
            "[W3] GRU Epoch 07 | train_loss 0.9013 acc 0.576 f1 0.570 || val_loss 1.0868 acc 0.385 f1 0.326\n",
            "[W3] GRU Epoch 08 | train_loss 0.8379 acc 0.592 f1 0.585 || val_loss 1.1140 acc 0.397 f1 0.328\n",
            "[W3] GRU Epoch 09 | train_loss 0.7890 acc 0.607 f1 0.602 || val_loss 1.1466 acc 0.393 f1 0.332\n",
            "[W3] GRU Epoch 10 | train_loss 0.7437 acc 0.646 f1 0.642 || val_loss 1.1927 acc 0.377 f1 0.319\n",
            "[W3] GRU Epoch 11 | train_loss 0.7077 acc 0.661 f1 0.657 || val_loss 1.1684 acc 0.414 f1 0.341\n",
            "[W3] GRU Epoch 12 | train_loss 0.6728 acc 0.668 f1 0.666 || val_loss 1.2063 acc 0.399 f1 0.329\n",
            "[W3] GRU Epoch 13 | train_loss 0.6418 acc 0.690 f1 0.686 || val_loss 1.2437 acc 0.395 f1 0.332\n",
            "[W3] GRU Epoch 14 | train_loss 0.6154 acc 0.698 f1 0.696 || val_loss 1.2643 acc 0.387 f1 0.317\n",
            "[W3] GRU Epoch 15 | train_loss 0.5911 acc 0.712 f1 0.710 || val_loss 1.3180 acc 0.387 f1 0.322\n",
            "[W3] GRU Epoch 16 | train_loss 0.5664 acc 0.715 f1 0.713 || val_loss 1.3172 acc 0.393 f1 0.314\n",
            "[W3] GRU Epoch 17 | train_loss 0.5407 acc 0.730 f1 0.729 || val_loss 1.3747 acc 0.393 f1 0.320\n",
            "[W3] GRU Epoch 18 | train_loss 0.5249 acc 0.745 f1 0.743 || val_loss 1.3926 acc 0.383 f1 0.302\n",
            "[W3] GRU Epoch 19 | train_loss 0.5037 acc 0.749 f1 0.747 || val_loss 1.4710 acc 0.387 f1 0.315\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=2\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 934, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0986 acc 0.336 f1 0.211 || val_loss 1.0985 acc 0.372 f1 0.252\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0926 acc 0.397 f1 0.380 || val_loss 1.0961 acc 0.311 f1 0.295\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0834 acc 0.420 f1 0.418 || val_loss 1.0909 acc 0.315 f1 0.300\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0643 acc 0.453 f1 0.436 || val_loss 1.0756 acc 0.346 f1 0.324\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0282 acc 0.482 f1 0.470 || val_loss 1.1011 acc 0.342 f1 0.322\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9558 acc 0.527 f1 0.517 || val_loss 1.0490 acc 0.393 f1 0.319\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8720 acc 0.557 f1 0.550 || val_loss 1.0733 acc 0.416 f1 0.340\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8048 acc 0.599 f1 0.593 || val_loss 1.1580 acc 0.372 f1 0.326\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7588 acc 0.604 f1 0.598 || val_loss 1.1502 acc 0.403 f1 0.332\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7201 acc 0.637 f1 0.632 || val_loss 1.2131 acc 0.381 f1 0.339\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6902 acc 0.647 f1 0.643 || val_loss 1.2167 acc 0.399 f1 0.341\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6576 acc 0.666 f1 0.662 || val_loss 1.2320 acc 0.397 f1 0.330\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6358 acc 0.681 f1 0.678 || val_loss 1.2667 acc 0.418 f1 0.341\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6053 acc 0.693 f1 0.690 || val_loss 1.2872 acc 0.405 f1 0.332\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5817 acc 0.700 f1 0.698 || val_loss 1.3626 acc 0.395 f1 0.328\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5519 acc 0.717 f1 0.716 || val_loss 1.4445 acc 0.401 f1 0.335\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5341 acc 0.732 f1 0.730 || val_loss 1.4730 acc 0.426 f1 0.357\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5062 acc 0.747 f1 0.745 || val_loss 1.4962 acc 0.409 f1 0.339\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4894 acc 0.742 f1 0.741 || val_loss 1.5479 acc 0.416 f1 0.335\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4705 acc 0.753 f1 0.753 || val_loss 1.6280 acc 0.420 f1 0.347\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4501 acc 0.772 f1 0.771 || val_loss 1.6762 acc 0.432 f1 0.355\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4370 acc 0.780 f1 0.779 || val_loss 1.7544 acc 0.430 f1 0.354\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4180 acc 0.794 f1 0.793 || val_loss 1.8049 acc 0.440 f1 0.362\n",
            "[W3] LSTM Epoch 24 | train_loss 0.4066 acc 0.800 f1 0.799 || val_loss 1.8743 acc 0.418 f1 0.348\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3932 acc 0.810 f1 0.810 || val_loss 1.9259 acc 0.420 f1 0.349\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3801 acc 0.816 f1 0.816 || val_loss 1.9478 acc 0.436 f1 0.364\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3715 acc 0.823 f1 0.823 || val_loss 2.0389 acc 0.412 f1 0.338\n",
            "[W3] LSTM Epoch 28 | train_loss 0.3518 acc 0.834 f1 0.834 || val_loss 2.0758 acc 0.424 f1 0.350\n",
            "[W3] LSTM Epoch 29 | train_loss 0.3362 acc 0.850 f1 0.850 || val_loss 2.1147 acc 0.440 f1 0.363\n",
            "[W3] LSTM Epoch 30 | train_loss 0.3168 acc 0.863 f1 0.862 || val_loss 2.1749 acc 0.416 f1 0.344\n",
            "[W3] LSTM Epoch 31 | train_loss 0.3038 acc 0.869 f1 0.869 || val_loss 2.2332 acc 0.432 f1 0.351\n",
            "[W3] LSTM Epoch 32 | train_loss 0.2947 acc 0.866 f1 0.866 || val_loss 2.3118 acc 0.428 f1 0.353\n",
            "[W3] LSTM Epoch 33 | train_loss 0.2704 acc 0.886 f1 0.886 || val_loss 2.3341 acc 0.416 f1 0.338\n",
            "[W3] LSTM Epoch 34 | train_loss 0.2507 acc 0.894 f1 0.894 || val_loss 2.4288 acc 0.416 f1 0.340\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:   2%|         | 2/100 [01:12<1:00:53, 37.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=3 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=3\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1185 acc 0.404 f1 0.401 || val_loss 1.0973 acc 0.360 f1 0.316\n",
            "[W3] ANN Epoch 02 | train_loss 0.9621 acc 0.522 f1 0.518 || val_loss 1.0958 acc 0.377 f1 0.335\n",
            "[W3] ANN Epoch 03 | train_loss 0.8665 acc 0.577 f1 0.572 || val_loss 1.0955 acc 0.407 f1 0.349\n",
            "[W3] ANN Epoch 04 | train_loss 0.7733 acc 0.618 f1 0.615 || val_loss 1.0973 acc 0.422 f1 0.349\n",
            "[W3] ANN Epoch 05 | train_loss 0.7144 acc 0.659 f1 0.656 || val_loss 1.1173 acc 0.420 f1 0.366\n",
            "[W3] ANN Epoch 06 | train_loss 0.6521 acc 0.685 f1 0.683 || val_loss 1.1649 acc 0.426 f1 0.346\n",
            "[W3] ANN Epoch 07 | train_loss 0.6200 acc 0.698 f1 0.695 || val_loss 1.1974 acc 0.409 f1 0.321\n",
            "[W3] ANN Epoch 08 | train_loss 0.5928 acc 0.722 f1 0.721 || val_loss 1.1789 acc 0.412 f1 0.331\n",
            "[W3] ANN Epoch 09 | train_loss 0.5667 acc 0.745 f1 0.744 || val_loss 1.2220 acc 0.422 f1 0.333\n",
            "[W3] ANN Epoch 10 | train_loss 0.5372 acc 0.749 f1 0.748 || val_loss 1.2390 acc 0.430 f1 0.340\n",
            "[W3] ANN Epoch 11 | train_loss 0.5249 acc 0.760 f1 0.760 || val_loss 1.2948 acc 0.428 f1 0.359\n",
            "[W3] ANN Epoch 12 | train_loss 0.4935 acc 0.779 f1 0.779 || val_loss 1.2853 acc 0.428 f1 0.343\n",
            "[W3] ANN Epoch 13 | train_loss 0.4739 acc 0.792 f1 0.792 || val_loss 1.3561 acc 0.383 f1 0.301\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=3\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0893 acc 0.382 f1 0.377 || val_loss 1.0527 acc 0.426 f1 0.364\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9751 acc 0.530 f1 0.519 || val_loss 1.0284 acc 0.412 f1 0.336\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8428 acc 0.602 f1 0.597 || val_loss 1.0917 acc 0.389 f1 0.317\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7483 acc 0.646 f1 0.644 || val_loss 1.1481 acc 0.352 f1 0.305\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6782 acc 0.685 f1 0.682 || val_loss 1.1841 acc 0.385 f1 0.328\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6188 acc 0.723 f1 0.719 || val_loss 1.2517 acc 0.387 f1 0.351\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5656 acc 0.751 f1 0.748 || val_loss 1.2495 acc 0.401 f1 0.361\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5406 acc 0.759 f1 0.758 || val_loss 1.3087 acc 0.383 f1 0.327\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4940 acc 0.795 f1 0.792 || val_loss 1.3418 acc 0.379 f1 0.328\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=3\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1041 acc 0.344 f1 0.293 || val_loss 1.1082 acc 0.302 f1 0.296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0880 acc 0.396 f1 0.387 || val_loss 1.0986 acc 0.333 f1 0.312\n",
            "[W3] RNN Epoch 03 | train_loss 1.0779 acc 0.418 f1 0.416 || val_loss 1.0967 acc 0.346 f1 0.326\n",
            "[W3] RNN Epoch 04 | train_loss 1.0694 acc 0.435 f1 0.431 || val_loss 1.0989 acc 0.342 f1 0.325\n",
            "[W3] RNN Epoch 05 | train_loss 1.0545 acc 0.447 f1 0.443 || val_loss 1.1018 acc 0.309 f1 0.292\n",
            "[W3] RNN Epoch 06 | train_loss 1.0423 acc 0.453 f1 0.448 || val_loss 1.1056 acc 0.302 f1 0.290\n",
            "[W3] RNN Epoch 07 | train_loss 1.0237 acc 0.489 f1 0.483 || val_loss 1.0968 acc 0.319 f1 0.299\n",
            "[W3] RNN Epoch 08 | train_loss 1.0044 acc 0.507 f1 0.505 || val_loss 1.1067 acc 0.321 f1 0.308\n",
            "[W3] RNN Epoch 09 | train_loss 0.9885 acc 0.518 f1 0.516 || val_loss 1.1042 acc 0.327 f1 0.309\n",
            "[W3] RNN Epoch 10 | train_loss 0.9674 acc 0.535 f1 0.530 || val_loss 1.1084 acc 0.335 f1 0.319\n",
            "[W3] RNN Epoch 11 | train_loss 0.9431 acc 0.557 f1 0.551 || val_loss 1.1038 acc 0.354 f1 0.327\n",
            "[W3] RNN Epoch 12 | train_loss 0.9296 acc 0.556 f1 0.548 || val_loss 1.1176 acc 0.352 f1 0.330\n",
            "[W3] RNN Epoch 13 | train_loss 0.9040 acc 0.569 f1 0.561 || val_loss 1.1169 acc 0.356 f1 0.335\n",
            "[W3] RNN Epoch 14 | train_loss 0.8847 acc 0.586 f1 0.578 || val_loss 1.1015 acc 0.364 f1 0.328\n",
            "[W3] RNN Epoch 15 | train_loss 0.8578 acc 0.607 f1 0.600 || val_loss 1.1045 acc 0.366 f1 0.330\n",
            "[W3] RNN Epoch 16 | train_loss 0.8400 acc 0.612 f1 0.605 || val_loss 1.1080 acc 0.370 f1 0.333\n",
            "[W3] RNN Epoch 17 | train_loss 0.8187 acc 0.632 f1 0.625 || val_loss 1.1093 acc 0.372 f1 0.339\n",
            "[W3] RNN Epoch 18 | train_loss 0.7932 acc 0.627 f1 0.621 || val_loss 1.1270 acc 0.377 f1 0.342\n",
            "[W3] RNN Epoch 19 | train_loss 0.7735 acc 0.651 f1 0.643 || val_loss 1.1268 acc 0.387 f1 0.344\n",
            "[W3] RNN Epoch 20 | train_loss 0.7695 acc 0.656 f1 0.652 || val_loss 1.1292 acc 0.381 f1 0.335\n",
            "[W3] RNN Epoch 21 | train_loss 0.7389 acc 0.666 f1 0.660 || val_loss 1.1310 acc 0.389 f1 0.340\n",
            "[W3] RNN Epoch 22 | train_loss 0.7200 acc 0.669 f1 0.663 || val_loss 1.1571 acc 0.377 f1 0.325\n",
            "[W3] RNN Epoch 23 | train_loss 0.6915 acc 0.693 f1 0.687 || val_loss 1.1585 acc 0.391 f1 0.342\n",
            "[W3] RNN Epoch 24 | train_loss 0.6970 acc 0.687 f1 0.683 || val_loss 1.1759 acc 0.383 f1 0.333\n",
            "[W3] RNN Epoch 25 | train_loss 0.6603 acc 0.705 f1 0.699 || val_loss 1.1799 acc 0.377 f1 0.330\n",
            "[W3] RNN Epoch 26 | train_loss 0.6597 acc 0.708 f1 0.703 || val_loss 1.1898 acc 0.389 f1 0.334\n",
            "[W3] RNN Epoch 27 | train_loss 0.6284 acc 0.720 f1 0.715 || val_loss 1.2022 acc 0.397 f1 0.330\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=3\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0990 acc 0.339 f1 0.283 || val_loss 1.0957 acc 0.344 f1 0.294\n",
            "[W3] GRU Epoch 02 | train_loss 1.0927 acc 0.382 f1 0.373 || val_loss 1.1028 acc 0.292 f1 0.283\n",
            "[W3] GRU Epoch 03 | train_loss 1.0857 acc 0.409 f1 0.404 || val_loss 1.1005 acc 0.327 f1 0.311\n",
            "[W3] GRU Epoch 04 | train_loss 1.0764 acc 0.429 f1 0.426 || val_loss 1.0969 acc 0.321 f1 0.303\n",
            "[W3] GRU Epoch 05 | train_loss 1.0624 acc 0.440 f1 0.434 || val_loss 1.1010 acc 0.340 f1 0.321\n",
            "[W3] GRU Epoch 06 | train_loss 1.0346 acc 0.493 f1 0.490 || val_loss 1.0964 acc 0.350 f1 0.328\n",
            "[W3] GRU Epoch 07 | train_loss 0.9850 acc 0.537 f1 0.531 || val_loss 1.0943 acc 0.370 f1 0.332\n",
            "[W3] GRU Epoch 08 | train_loss 0.9113 acc 0.562 f1 0.554 || val_loss 1.1453 acc 0.319 f1 0.285\n",
            "[W3] GRU Epoch 09 | train_loss 0.8569 acc 0.595 f1 0.587 || val_loss 1.1037 acc 0.370 f1 0.308\n",
            "[W3] GRU Epoch 10 | train_loss 0.8162 acc 0.602 f1 0.596 || val_loss 1.1232 acc 0.374 f1 0.304\n",
            "[W3] GRU Epoch 11 | train_loss 0.7662 acc 0.638 f1 0.634 || val_loss 1.1918 acc 0.360 f1 0.310\n",
            "[W3] GRU Epoch 12 | train_loss 0.7627 acc 0.629 f1 0.624 || val_loss 1.1737 acc 0.391 f1 0.320\n",
            "[W3] GRU Epoch 13 | train_loss 0.7141 acc 0.660 f1 0.656 || val_loss 1.1935 acc 0.389 f1 0.316\n",
            "[W3] GRU Epoch 14 | train_loss 0.6801 acc 0.674 f1 0.670 || val_loss 1.2120 acc 0.389 f1 0.315\n",
            "[W3] GRU Epoch 15 | train_loss 0.6602 acc 0.688 f1 0.686 || val_loss 1.2201 acc 0.407 f1 0.320\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=3\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0994 acc 0.329 f1 0.223 || val_loss 1.0864 acc 0.416 f1 0.288\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0939 acc 0.363 f1 0.340 || val_loss 1.0851 acc 0.385 f1 0.315\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0878 acc 0.388 f1 0.380 || val_loss 1.0807 acc 0.360 f1 0.312\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0754 acc 0.417 f1 0.414 || val_loss 1.0826 acc 0.342 f1 0.317\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0597 acc 0.440 f1 0.433 || val_loss 1.0871 acc 0.323 f1 0.307\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0212 acc 0.489 f1 0.482 || val_loss 1.0743 acc 0.362 f1 0.323\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9451 acc 0.549 f1 0.541 || val_loss 1.0985 acc 0.354 f1 0.306\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8542 acc 0.593 f1 0.585 || val_loss 1.1354 acc 0.385 f1 0.310\n",
            "[W3] LSTM Epoch 09 | train_loss 0.8010 acc 0.604 f1 0.598 || val_loss 1.1819 acc 0.352 f1 0.297\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7481 acc 0.636 f1 0.631 || val_loss 1.2047 acc 0.340 f1 0.281\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7102 acc 0.655 f1 0.651 || val_loss 1.2231 acc 0.362 f1 0.287\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6970 acc 0.666 f1 0.662 || val_loss 1.2626 acc 0.372 f1 0.304\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6573 acc 0.684 f1 0.679 || val_loss 1.2925 acc 0.372 f1 0.306\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6301 acc 0.696 f1 0.693 || val_loss 1.3367 acc 0.362 f1 0.290\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:   3%|         | 3/100 [01:35<50:11, 31.05s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=4 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=4\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 934, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1507 acc 0.391 f1 0.390 || val_loss 1.1212 acc 0.321 f1 0.296\n",
            "[W3] ANN Epoch 02 | train_loss 0.9577 acc 0.531 f1 0.526 || val_loss 1.1141 acc 0.360 f1 0.333\n",
            "[W3] ANN Epoch 03 | train_loss 0.8645 acc 0.584 f1 0.577 || val_loss 1.1149 acc 0.368 f1 0.319\n",
            "[W3] ANN Epoch 04 | train_loss 0.7937 acc 0.605 f1 0.600 || val_loss 1.1031 acc 0.379 f1 0.335\n",
            "[W3] ANN Epoch 05 | train_loss 0.7157 acc 0.658 f1 0.654 || val_loss 1.1313 acc 0.397 f1 0.337\n",
            "[W3] ANN Epoch 06 | train_loss 0.6740 acc 0.679 f1 0.676 || val_loss 1.1226 acc 0.397 f1 0.325\n",
            "[W3] ANN Epoch 07 | train_loss 0.6394 acc 0.693 f1 0.691 || val_loss 1.1599 acc 0.401 f1 0.337\n",
            "[W3] ANN Epoch 08 | train_loss 0.5895 acc 0.727 f1 0.726 || val_loss 1.1457 acc 0.438 f1 0.362\n",
            "[W3] ANN Epoch 09 | train_loss 0.5619 acc 0.750 f1 0.749 || val_loss 1.1904 acc 0.409 f1 0.335\n",
            "[W3] ANN Epoch 10 | train_loss 0.5586 acc 0.736 f1 0.734 || val_loss 1.1719 acc 0.473 f1 0.381\n",
            "[W3] ANN Epoch 11 | train_loss 0.5043 acc 0.774 f1 0.774 || val_loss 1.2053 acc 0.449 f1 0.369\n",
            "[W3] ANN Epoch 12 | train_loss 0.5118 acc 0.766 f1 0.765 || val_loss 1.2448 acc 0.438 f1 0.368\n",
            "[W3] ANN Epoch 13 | train_loss 0.4746 acc 0.792 f1 0.792 || val_loss 1.2678 acc 0.414 f1 0.344\n",
            "[W3] ANN Epoch 14 | train_loss 0.4407 acc 0.812 f1 0.812 || val_loss 1.3010 acc 0.430 f1 0.355\n",
            "[W3] ANN Epoch 15 | train_loss 0.4332 acc 0.807 f1 0.807 || val_loss 1.3250 acc 0.416 f1 0.334\n",
            "[W3] ANN Epoch 16 | train_loss 0.4196 acc 0.814 f1 0.813 || val_loss 1.3549 acc 0.387 f1 0.323\n",
            "[W3] ANN Epoch 17 | train_loss 0.4169 acc 0.827 f1 0.827 || val_loss 1.3278 acc 0.414 f1 0.338\n",
            "[W3] ANN Epoch 18 | train_loss 0.4081 acc 0.824 f1 0.824 || val_loss 1.3508 acc 0.453 f1 0.368\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=4\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 934, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0735 acc 0.414 f1 0.415 || val_loss 1.0245 acc 0.430 f1 0.308\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9505 acc 0.550 f1 0.545 || val_loss 1.0657 acc 0.385 f1 0.319\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8253 acc 0.606 f1 0.601 || val_loss 1.1137 acc 0.401 f1 0.335\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7116 acc 0.666 f1 0.663 || val_loss 1.2058 acc 0.374 f1 0.316\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6364 acc 0.705 f1 0.703 || val_loss 1.2123 acc 0.374 f1 0.300\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5668 acc 0.741 f1 0.741 || val_loss 1.2934 acc 0.401 f1 0.315\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5073 acc 0.772 f1 0.771 || val_loss 1.3450 acc 0.401 f1 0.327\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4434 acc 0.807 f1 0.807 || val_loss 1.4537 acc 0.397 f1 0.333\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4284 acc 0.815 f1 0.814 || val_loss 1.4654 acc 0.372 f1 0.311\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3517 acc 0.858 f1 0.858 || val_loss 1.5764 acc 0.403 f1 0.320\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3092 acc 0.876 f1 0.876 || val_loss 1.6736 acc 0.407 f1 0.326\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=4\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 934, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0972 acc 0.338 f1 0.335 || val_loss 1.0945 acc 0.370 f1 0.337\n",
            "[W3] RNN Epoch 02 | train_loss 1.0813 acc 0.423 f1 0.420 || val_loss 1.0819 acc 0.383 f1 0.347\n",
            "[W3] RNN Epoch 03 | train_loss 1.0633 acc 0.453 f1 0.450 || val_loss 1.0779 acc 0.346 f1 0.319\n",
            "[W3] RNN Epoch 04 | train_loss 1.0404 acc 0.475 f1 0.466 || val_loss 1.0882 acc 0.331 f1 0.315\n",
            "[W3] RNN Epoch 05 | train_loss 1.0174 acc 0.481 f1 0.471 || val_loss 1.0594 acc 0.381 f1 0.341\n",
            "[W3] RNN Epoch 06 | train_loss 0.9894 acc 0.513 f1 0.502 || val_loss 1.0506 acc 0.377 f1 0.333\n",
            "[W3] RNN Epoch 07 | train_loss 0.9696 acc 0.530 f1 0.520 || val_loss 1.1038 acc 0.368 f1 0.349\n",
            "[W3] RNN Epoch 08 | train_loss 0.9387 acc 0.545 f1 0.534 || val_loss 1.0519 acc 0.430 f1 0.376\n",
            "[W3] RNN Epoch 09 | train_loss 0.9150 acc 0.557 f1 0.546 || val_loss 1.0805 acc 0.409 f1 0.371\n",
            "[W3] RNN Epoch 10 | train_loss 0.8927 acc 0.571 f1 0.560 || val_loss 1.0845 acc 0.405 f1 0.363\n",
            "[W3] RNN Epoch 11 | train_loss 0.8749 acc 0.585 f1 0.576 || val_loss 1.1141 acc 0.397 f1 0.366\n",
            "[W3] RNN Epoch 12 | train_loss 0.8506 acc 0.604 f1 0.594 || val_loss 1.0953 acc 0.397 f1 0.339\n",
            "[W3] RNN Epoch 13 | train_loss 0.8357 acc 0.603 f1 0.592 || val_loss 1.1080 acc 0.397 f1 0.347\n",
            "[W3] RNN Epoch 14 | train_loss 0.7984 acc 0.627 f1 0.617 || val_loss 1.1076 acc 0.405 f1 0.353\n",
            "[W3] RNN Epoch 15 | train_loss 0.7829 acc 0.630 f1 0.621 || val_loss 1.1326 acc 0.393 f1 0.334\n",
            "[W3] RNN Epoch 16 | train_loss 0.7626 acc 0.638 f1 0.630 || val_loss 1.1695 acc 0.387 f1 0.349\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=4\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 934, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0986 acc 0.349 f1 0.298 || val_loss 1.1003 acc 0.309 f1 0.296\n",
            "[W3] GRU Epoch 02 | train_loss 1.0872 acc 0.413 f1 0.405 || val_loss 1.0885 acc 0.327 f1 0.298\n",
            "[W3] GRU Epoch 03 | train_loss 1.0774 acc 0.429 f1 0.428 || val_loss 1.0806 acc 0.335 f1 0.305\n",
            "[W3] GRU Epoch 04 | train_loss 1.0642 acc 0.445 f1 0.439 || val_loss 1.0734 acc 0.356 f1 0.314\n",
            "[W3] GRU Epoch 05 | train_loss 1.0428 acc 0.468 f1 0.463 || val_loss 1.0865 acc 0.356 f1 0.324\n",
            "[W3] GRU Epoch 06 | train_loss 1.0054 acc 0.515 f1 0.509 || val_loss 1.0978 acc 0.354 f1 0.316\n",
            "[W3] GRU Epoch 07 | train_loss 0.9385 acc 0.550 f1 0.542 || val_loss 1.1503 acc 0.319 f1 0.288\n",
            "[W3] GRU Epoch 08 | train_loss 0.8654 acc 0.591 f1 0.583 || val_loss 1.1789 acc 0.325 f1 0.283\n",
            "[W3] GRU Epoch 09 | train_loss 0.8110 acc 0.603 f1 0.597 || val_loss 1.2316 acc 0.333 f1 0.300\n",
            "[W3] GRU Epoch 10 | train_loss 0.7653 acc 0.624 f1 0.618 || val_loss 1.1709 acc 0.366 f1 0.307\n",
            "[W3] GRU Epoch 11 | train_loss 0.7175 acc 0.652 f1 0.646 || val_loss 1.2227 acc 0.370 f1 0.315\n",
            "[W3] GRU Epoch 12 | train_loss 0.6857 acc 0.660 f1 0.656 || val_loss 1.2386 acc 0.358 f1 0.288\n",
            "[W3] GRU Epoch 13 | train_loss 0.6539 acc 0.681 f1 0.677 || val_loss 1.2853 acc 0.387 f1 0.321\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=4\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 934, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1002 acc 0.331 f1 0.183 || val_loss 1.0873 acc 0.442 f1 0.228\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0928 acc 0.363 f1 0.316 || val_loss 1.0848 acc 0.383 f1 0.311\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0813 acc 0.414 f1 0.405 || val_loss 1.0802 acc 0.356 f1 0.334\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0653 acc 0.432 f1 0.420 || val_loss 1.0580 acc 0.383 f1 0.342\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0318 acc 0.463 f1 0.451 || val_loss 1.0472 acc 0.416 f1 0.367\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9648 acc 0.521 f1 0.513 || val_loss 1.0698 acc 0.391 f1 0.350\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8767 acc 0.556 f1 0.546 || val_loss 1.1029 acc 0.383 f1 0.327\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8158 acc 0.591 f1 0.583 || val_loss 1.1951 acc 0.333 f1 0.301\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7778 acc 0.619 f1 0.612 || val_loss 1.1253 acc 0.407 f1 0.322\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7271 acc 0.637 f1 0.629 || val_loss 1.1773 acc 0.420 f1 0.350\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6989 acc 0.644 f1 0.638 || val_loss 1.2065 acc 0.409 f1 0.337\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6637 acc 0.670 f1 0.665 || val_loss 1.2333 acc 0.407 f1 0.349\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6376 acc 0.687 f1 0.683 || val_loss 1.2826 acc 0.414 f1 0.333\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:   4%|         | 4/100 [01:58<44:32, 27.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=5 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=5\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 978, np.int64(1): 933, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 978, np.int64(2): 978, np.int64(0): 978})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1240 acc 0.405 f1 0.400 || val_loss 1.1741 acc 0.267 f1 0.263\n",
            "[W3] ANN Epoch 02 | train_loss 0.9827 acc 0.503 f1 0.492 || val_loss 1.1741 acc 0.282 f1 0.269\n",
            "[W3] ANN Epoch 03 | train_loss 0.8772 acc 0.566 f1 0.556 || val_loss 1.1628 acc 0.327 f1 0.298\n",
            "[W3] ANN Epoch 04 | train_loss 0.7927 acc 0.616 f1 0.607 || val_loss 1.1583 acc 0.381 f1 0.335\n",
            "[W3] ANN Epoch 05 | train_loss 0.7288 acc 0.646 f1 0.639 || val_loss 1.1715 acc 0.385 f1 0.331\n",
            "[W3] ANN Epoch 06 | train_loss 0.6791 acc 0.678 f1 0.674 || val_loss 1.1830 acc 0.364 f1 0.304\n",
            "[W3] ANN Epoch 07 | train_loss 0.6357 acc 0.690 f1 0.687 || val_loss 1.2241 acc 0.379 f1 0.313\n",
            "[W3] ANN Epoch 08 | train_loss 0.5911 acc 0.728 f1 0.725 || val_loss 1.2036 acc 0.401 f1 0.337\n",
            "[W3] ANN Epoch 09 | train_loss 0.5640 acc 0.738 f1 0.737 || val_loss 1.2441 acc 0.370 f1 0.298\n",
            "[W3] ANN Epoch 10 | train_loss 0.5308 acc 0.764 f1 0.762 || val_loss 1.2580 acc 0.424 f1 0.349\n",
            "[W3] ANN Epoch 11 | train_loss 0.5099 acc 0.765 f1 0.764 || val_loss 1.2914 acc 0.418 f1 0.345\n",
            "[W3] ANN Epoch 12 | train_loss 0.4791 acc 0.784 f1 0.783 || val_loss 1.3101 acc 0.405 f1 0.336\n",
            "[W3] ANN Epoch 13 | train_loss 0.4694 acc 0.796 f1 0.795 || val_loss 1.3287 acc 0.395 f1 0.319\n",
            "[W3] ANN Epoch 14 | train_loss 0.4507 acc 0.803 f1 0.802 || val_loss 1.3788 acc 0.383 f1 0.315\n",
            "[W3] ANN Epoch 15 | train_loss 0.4349 acc 0.812 f1 0.811 || val_loss 1.4110 acc 0.407 f1 0.333\n",
            "[W3] ANN Epoch 16 | train_loss 0.4108 acc 0.825 f1 0.824 || val_loss 1.3963 acc 0.407 f1 0.329\n",
            "[W3] ANN Epoch 17 | train_loss 0.4203 acc 0.818 f1 0.818 || val_loss 1.4149 acc 0.418 f1 0.346\n",
            "[W3] ANN Epoch 18 | train_loss 0.3704 acc 0.849 f1 0.849 || val_loss 1.4887 acc 0.414 f1 0.338\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=5\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 978, np.int64(1): 933, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 978, np.int64(2): 978, np.int64(0): 978})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0778 acc 0.391 f1 0.393 || val_loss 1.0409 acc 0.449 f1 0.376\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9522 acc 0.555 f1 0.550 || val_loss 1.0277 acc 0.403 f1 0.315\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8248 acc 0.606 f1 0.601 || val_loss 1.0806 acc 0.399 f1 0.342\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7275 acc 0.647 f1 0.643 || val_loss 1.1169 acc 0.416 f1 0.367\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6461 acc 0.695 f1 0.691 || val_loss 1.1896 acc 0.381 f1 0.327\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5759 acc 0.736 f1 0.734 || val_loss 1.2270 acc 0.401 f1 0.341\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5139 acc 0.769 f1 0.768 || val_loss 1.3495 acc 0.381 f1 0.327\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4661 acc 0.791 f1 0.789 || val_loss 1.3380 acc 0.432 f1 0.352\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4015 acc 0.830 f1 0.829 || val_loss 1.4249 acc 0.420 f1 0.347\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=5\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 978, np.int64(1): 933, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 978, np.int64(2): 978, np.int64(0): 978})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0963 acc 0.361 f1 0.329 || val_loss 1.0903 acc 0.342 f1 0.317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0781 acc 0.411 f1 0.401 || val_loss 1.0883 acc 0.315 f1 0.294\n",
            "[W3] RNN Epoch 03 | train_loss 1.0602 acc 0.439 f1 0.430 || val_loss 1.0855 acc 0.321 f1 0.310\n",
            "[W3] RNN Epoch 04 | train_loss 1.0410 acc 0.460 f1 0.445 || val_loss 1.0757 acc 0.350 f1 0.331\n",
            "[W3] RNN Epoch 05 | train_loss 1.0240 acc 0.476 f1 0.466 || val_loss 1.0733 acc 0.342 f1 0.315\n",
            "[W3] RNN Epoch 06 | train_loss 0.9973 acc 0.512 f1 0.505 || val_loss 1.0727 acc 0.342 f1 0.311\n",
            "[W3] RNN Epoch 07 | train_loss 0.9752 acc 0.514 f1 0.503 || val_loss 1.0686 acc 0.342 f1 0.305\n",
            "[W3] RNN Epoch 08 | train_loss 0.9491 acc 0.544 f1 0.536 || val_loss 1.0783 acc 0.350 f1 0.312\n",
            "[W3] RNN Epoch 09 | train_loss 0.9351 acc 0.538 f1 0.526 || val_loss 1.0774 acc 0.366 f1 0.328\n",
            "[W3] RNN Epoch 10 | train_loss 0.9113 acc 0.554 f1 0.540 || val_loss 1.0924 acc 0.354 f1 0.321\n",
            "[W3] RNN Epoch 11 | train_loss 0.8954 acc 0.561 f1 0.551 || val_loss 1.0910 acc 0.354 f1 0.318\n",
            "[W3] RNN Epoch 12 | train_loss 0.8647 acc 0.588 f1 0.577 || val_loss 1.0864 acc 0.368 f1 0.327\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=5\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 978, np.int64(1): 933, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 978, np.int64(2): 978, np.int64(0): 978})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0992 acc 0.352 f1 0.285 || val_loss 1.1031 acc 0.300 f1 0.297\n",
            "[W3] GRU Epoch 02 | train_loss 1.0879 acc 0.393 f1 0.385 || val_loss 1.0934 acc 0.317 f1 0.303\n",
            "[W3] GRU Epoch 03 | train_loss 1.0768 acc 0.426 f1 0.425 || val_loss 1.0889 acc 0.335 f1 0.318\n",
            "[W3] GRU Epoch 04 | train_loss 1.0606 acc 0.451 f1 0.442 || val_loss 1.0691 acc 0.352 f1 0.317\n",
            "[W3] GRU Epoch 05 | train_loss 1.0365 acc 0.480 f1 0.477 || val_loss 1.0860 acc 0.317 f1 0.295\n",
            "[W3] GRU Epoch 06 | train_loss 0.9968 acc 0.507 f1 0.495 || val_loss 1.0698 acc 0.360 f1 0.325\n",
            "[W3] GRU Epoch 07 | train_loss 0.9377 acc 0.547 f1 0.540 || val_loss 1.0603 acc 0.377 f1 0.328\n",
            "[W3] GRU Epoch 08 | train_loss 0.8690 acc 0.587 f1 0.579 || val_loss 1.0642 acc 0.403 f1 0.349\n",
            "[W3] GRU Epoch 09 | train_loss 0.8112 acc 0.619 f1 0.613 || val_loss 1.1225 acc 0.370 f1 0.325\n",
            "[W3] GRU Epoch 10 | train_loss 0.7565 acc 0.635 f1 0.627 || val_loss 1.1228 acc 0.391 f1 0.340\n",
            "[W3] GRU Epoch 11 | train_loss 0.7198 acc 0.651 f1 0.646 || val_loss 1.1501 acc 0.377 f1 0.314\n",
            "[W3] GRU Epoch 12 | train_loss 0.6823 acc 0.676 f1 0.671 || val_loss 1.1672 acc 0.399 f1 0.339\n",
            "[W3] GRU Epoch 13 | train_loss 0.6491 acc 0.698 f1 0.694 || val_loss 1.1929 acc 0.401 f1 0.341\n",
            "[W3] GRU Epoch 14 | train_loss 0.6193 acc 0.701 f1 0.697 || val_loss 1.2224 acc 0.391 f1 0.337\n",
            "[W3] GRU Epoch 15 | train_loss 0.5845 acc 0.725 f1 0.722 || val_loss 1.2482 acc 0.399 f1 0.340\n",
            "[W3] GRU Epoch 16 | train_loss 0.5616 acc 0.737 f1 0.734 || val_loss 1.3274 acc 0.383 f1 0.338\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=5\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 978, np.int64(1): 933, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 978, np.int64(2): 978, np.int64(0): 978})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0996 acc 0.333 f1 0.227 || val_loss 1.0936 acc 0.428 f1 0.273\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0939 acc 0.367 f1 0.356 || val_loss 1.0972 acc 0.350 f1 0.324\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0852 acc 0.407 f1 0.397 || val_loss 1.0897 acc 0.370 f1 0.340\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0663 acc 0.444 f1 0.437 || val_loss 1.0883 acc 0.344 f1 0.317\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0182 acc 0.496 f1 0.486 || val_loss 1.0694 acc 0.348 f1 0.298\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9163 acc 0.543 f1 0.534 || val_loss 1.1585 acc 0.305 f1 0.273\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8438 acc 0.576 f1 0.568 || val_loss 1.1145 acc 0.383 f1 0.301\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7920 acc 0.609 f1 0.603 || val_loss 1.1443 acc 0.383 f1 0.315\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7545 acc 0.625 f1 0.619 || val_loss 1.2034 acc 0.354 f1 0.304\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7235 acc 0.640 f1 0.634 || val_loss 1.1991 acc 0.389 f1 0.312\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6894 acc 0.662 f1 0.657 || val_loss 1.2330 acc 0.397 f1 0.327\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:   5%|         | 5/100 [02:19<40:20, 25.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=6 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=6\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 928, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1361 acc 0.395 f1 0.393 || val_loss 1.1616 acc 0.302 f1 0.287\n",
            "[W3] ANN Epoch 02 | train_loss 0.9780 acc 0.502 f1 0.496 || val_loss 1.1788 acc 0.294 f1 0.278\n",
            "[W3] ANN Epoch 03 | train_loss 0.8670 acc 0.582 f1 0.573 || val_loss 1.1380 acc 0.381 f1 0.349\n",
            "[W3] ANN Epoch 04 | train_loss 0.7922 acc 0.620 f1 0.614 || val_loss 1.1472 acc 0.360 f1 0.316\n",
            "[W3] ANN Epoch 05 | train_loss 0.7293 acc 0.652 f1 0.646 || val_loss 1.1578 acc 0.395 f1 0.352\n",
            "[W3] ANN Epoch 06 | train_loss 0.6774 acc 0.681 f1 0.677 || val_loss 1.1514 acc 0.405 f1 0.356\n",
            "[W3] ANN Epoch 07 | train_loss 0.6409 acc 0.699 f1 0.696 || val_loss 1.1914 acc 0.389 f1 0.325\n",
            "[W3] ANN Epoch 08 | train_loss 0.6139 acc 0.723 f1 0.722 || val_loss 1.1627 acc 0.409 f1 0.330\n",
            "[W3] ANN Epoch 09 | train_loss 0.5806 acc 0.730 f1 0.728 || val_loss 1.2266 acc 0.401 f1 0.337\n",
            "[W3] ANN Epoch 10 | train_loss 0.5573 acc 0.746 f1 0.744 || val_loss 1.1883 acc 0.426 f1 0.346\n",
            "[W3] ANN Epoch 11 | train_loss 0.5262 acc 0.762 f1 0.761 || val_loss 1.2338 acc 0.428 f1 0.357\n",
            "[W3] ANN Epoch 12 | train_loss 0.5065 acc 0.766 f1 0.765 || val_loss 1.2373 acc 0.416 f1 0.352\n",
            "[W3] ANN Epoch 13 | train_loss 0.5197 acc 0.762 f1 0.760 || val_loss 1.2698 acc 0.420 f1 0.356\n",
            "[W3] ANN Epoch 14 | train_loss 0.4801 acc 0.782 f1 0.781 || val_loss 1.2850 acc 0.416 f1 0.350\n",
            "[W3] ANN Epoch 15 | train_loss 0.4550 acc 0.801 f1 0.800 || val_loss 1.3137 acc 0.440 f1 0.367\n",
            "[W3] ANN Epoch 16 | train_loss 0.4147 acc 0.821 f1 0.821 || val_loss 1.3466 acc 0.428 f1 0.369\n",
            "[W3] ANN Epoch 17 | train_loss 0.4307 acc 0.809 f1 0.809 || val_loss 1.3504 acc 0.453 f1 0.372\n",
            "[W3] ANN Epoch 18 | train_loss 0.4195 acc 0.814 f1 0.813 || val_loss 1.3520 acc 0.436 f1 0.359\n",
            "[W3] ANN Epoch 19 | train_loss 0.3743 acc 0.837 f1 0.837 || val_loss 1.4417 acc 0.397 f1 0.333\n",
            "[W3] ANN Epoch 20 | train_loss 0.3667 acc 0.837 f1 0.836 || val_loss 1.4377 acc 0.424 f1 0.342\n",
            "[W3] ANN Epoch 21 | train_loss 0.3620 acc 0.844 f1 0.844 || val_loss 1.4843 acc 0.414 f1 0.347\n",
            "[W3] ANN Epoch 22 | train_loss 0.3572 acc 0.846 f1 0.846 || val_loss 1.5173 acc 0.424 f1 0.355\n",
            "[W3] ANN Epoch 23 | train_loss 0.3476 acc 0.853 f1 0.853 || val_loss 1.4788 acc 0.428 f1 0.360\n",
            "[W3] ANN Epoch 24 | train_loss 0.3291 acc 0.867 f1 0.866 || val_loss 1.5352 acc 0.418 f1 0.339\n",
            "[W3] ANN Epoch 25 | train_loss 0.3381 acc 0.858 f1 0.858 || val_loss 1.5377 acc 0.422 f1 0.349\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=6\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 928, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0758 acc 0.389 f1 0.389 || val_loss 1.0369 acc 0.399 f1 0.321\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9414 acc 0.545 f1 0.539 || val_loss 1.0774 acc 0.383 f1 0.306\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8190 acc 0.618 f1 0.612 || val_loss 1.1145 acc 0.377 f1 0.322\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7339 acc 0.654 f1 0.649 || val_loss 1.1805 acc 0.364 f1 0.300\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6551 acc 0.694 f1 0.691 || val_loss 1.2156 acc 0.364 f1 0.316\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5865 acc 0.724 f1 0.720 || val_loss 1.2846 acc 0.377 f1 0.322\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5308 acc 0.744 f1 0.742 || val_loss 1.3595 acc 0.399 f1 0.330\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4852 acc 0.773 f1 0.773 || val_loss 1.3850 acc 0.407 f1 0.352\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4384 acc 0.795 f1 0.794 || val_loss 1.4733 acc 0.387 f1 0.330\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3969 acc 0.822 f1 0.821 || val_loss 1.5477 acc 0.414 f1 0.342\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3697 acc 0.839 f1 0.839 || val_loss 1.6100 acc 0.397 f1 0.326\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3306 acc 0.863 f1 0.863 || val_loss 1.6320 acc 0.416 f1 0.353\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2962 acc 0.880 f1 0.880 || val_loss 1.7136 acc 0.426 f1 0.352\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2581 acc 0.896 f1 0.896 || val_loss 1.7944 acc 0.405 f1 0.341\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2388 acc 0.911 f1 0.911 || val_loss 1.9463 acc 0.389 f1 0.309\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2035 acc 0.927 f1 0.926 || val_loss 2.0217 acc 0.412 f1 0.326\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1933 acc 0.933 f1 0.933 || val_loss 2.1276 acc 0.381 f1 0.322\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1596 acc 0.945 f1 0.945 || val_loss 2.1550 acc 0.405 f1 0.332\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1386 acc 0.955 f1 0.955 || val_loss 2.3244 acc 0.416 f1 0.342\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1410 acc 0.954 f1 0.954 || val_loss 2.4026 acc 0.407 f1 0.328\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=6\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 928, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0992 acc 0.335 f1 0.316 || val_loss 1.0929 acc 0.368 f1 0.338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0819 acc 0.412 f1 0.409 || val_loss 1.0923 acc 0.354 f1 0.327\n",
            "[W3] RNN Epoch 03 | train_loss 1.0640 acc 0.453 f1 0.453 || val_loss 1.0833 acc 0.385 f1 0.351\n",
            "[W3] RNN Epoch 04 | train_loss 1.0449 acc 0.459 f1 0.457 || val_loss 1.0879 acc 0.360 f1 0.334\n",
            "[W3] RNN Epoch 05 | train_loss 1.0282 acc 0.468 f1 0.467 || val_loss 1.0844 acc 0.360 f1 0.334\n",
            "[W3] RNN Epoch 06 | train_loss 1.0058 acc 0.507 f1 0.501 || val_loss 1.0519 acc 0.397 f1 0.355\n",
            "[W3] RNN Epoch 07 | train_loss 0.9776 acc 0.521 f1 0.518 || val_loss 1.0770 acc 0.352 f1 0.326\n",
            "[W3] RNN Epoch 08 | train_loss 0.9495 acc 0.540 f1 0.533 || val_loss 1.0446 acc 0.387 f1 0.343\n",
            "[W3] RNN Epoch 09 | train_loss 0.9230 acc 0.553 f1 0.544 || val_loss 1.0475 acc 0.391 f1 0.346\n",
            "[W3] RNN Epoch 10 | train_loss 0.8900 acc 0.579 f1 0.569 || val_loss 1.0504 acc 0.372 f1 0.337\n",
            "[W3] RNN Epoch 11 | train_loss 0.8666 acc 0.591 f1 0.581 || val_loss 1.0403 acc 0.387 f1 0.348\n",
            "[W3] RNN Epoch 12 | train_loss 0.8485 acc 0.599 f1 0.590 || val_loss 1.0648 acc 0.389 f1 0.354\n",
            "[W3] RNN Epoch 13 | train_loss 0.8245 acc 0.604 f1 0.595 || val_loss 1.0721 acc 0.387 f1 0.355\n",
            "[W3] RNN Epoch 14 | train_loss 0.7915 acc 0.617 f1 0.607 || val_loss 1.0703 acc 0.374 f1 0.334\n",
            "[W3] RNN Epoch 15 | train_loss 0.7839 acc 0.618 f1 0.609 || val_loss 1.0659 acc 0.420 f1 0.370\n",
            "[W3] RNN Epoch 16 | train_loss 0.7609 acc 0.643 f1 0.636 || val_loss 1.0787 acc 0.393 f1 0.358\n",
            "[W3] RNN Epoch 17 | train_loss 0.7361 acc 0.653 f1 0.645 || val_loss 1.0748 acc 0.416 f1 0.365\n",
            "[W3] RNN Epoch 18 | train_loss 0.7224 acc 0.646 f1 0.638 || val_loss 1.0999 acc 0.409 f1 0.371\n",
            "[W3] RNN Epoch 19 | train_loss 0.7038 acc 0.665 f1 0.658 || val_loss 1.0967 acc 0.449 f1 0.391\n",
            "[W3] RNN Epoch 20 | train_loss 0.6770 acc 0.680 f1 0.673 || val_loss 1.0943 acc 0.438 f1 0.384\n",
            "[W3] RNN Epoch 21 | train_loss 0.6527 acc 0.676 f1 0.670 || val_loss 1.1111 acc 0.438 f1 0.395\n",
            "[W3] RNN Epoch 22 | train_loss 0.6419 acc 0.681 f1 0.676 || val_loss 1.1269 acc 0.442 f1 0.391\n",
            "[W3] RNN Epoch 23 | train_loss 0.6161 acc 0.707 f1 0.702 || val_loss 1.1378 acc 0.457 f1 0.396\n",
            "[W3] RNN Epoch 24 | train_loss 0.6036 acc 0.713 f1 0.707 || val_loss 1.1554 acc 0.459 f1 0.411\n",
            "[W3] RNN Epoch 25 | train_loss 0.5892 acc 0.714 f1 0.710 || val_loss 1.1613 acc 0.469 f1 0.415\n",
            "[W3] RNN Epoch 26 | train_loss 0.5780 acc 0.725 f1 0.721 || val_loss 1.1854 acc 0.467 f1 0.411\n",
            "[W3] RNN Epoch 27 | train_loss 0.5564 acc 0.732 f1 0.728 || val_loss 1.1932 acc 0.473 f1 0.415\n",
            "[W3] RNN Epoch 28 | train_loss 0.5436 acc 0.738 f1 0.734 || val_loss 1.2165 acc 0.467 f1 0.416\n",
            "[W3] RNN Epoch 29 | train_loss 0.5323 acc 0.738 f1 0.735 || val_loss 1.2282 acc 0.488 f1 0.429\n",
            "[W3] RNN Epoch 30 | train_loss 0.5166 acc 0.749 f1 0.746 || val_loss 1.2367 acc 0.463 f1 0.409\n",
            "[W3] RNN Epoch 31 | train_loss 0.5117 acc 0.752 f1 0.749 || val_loss 1.2757 acc 0.469 f1 0.419\n",
            "[W3] RNN Epoch 32 | train_loss 0.4999 acc 0.757 f1 0.754 || val_loss 1.2680 acc 0.449 f1 0.398\n",
            "[W3] RNN Epoch 33 | train_loss 0.4828 acc 0.758 f1 0.755 || val_loss 1.2954 acc 0.490 f1 0.438\n",
            "[W3] RNN Epoch 34 | train_loss 0.4779 acc 0.765 f1 0.763 || val_loss 1.3076 acc 0.459 f1 0.399\n",
            "[W3] RNN Epoch 35 | train_loss 0.4585 acc 0.778 f1 0.777 || val_loss 1.3345 acc 0.459 f1 0.408\n",
            "[W3] RNN Epoch 36 | train_loss 0.4540 acc 0.780 f1 0.778 || val_loss 1.3540 acc 0.473 f1 0.413\n",
            "[W3] RNN Epoch 37 | train_loss 0.4492 acc 0.784 f1 0.782 || val_loss 1.3763 acc 0.449 f1 0.392\n",
            "[W3] RNN Epoch 38 | train_loss 0.4439 acc 0.784 f1 0.782 || val_loss 1.3919 acc 0.457 f1 0.393\n",
            "[W3] RNN Epoch 39 | train_loss 0.4349 acc 0.787 f1 0.786 || val_loss 1.4114 acc 0.467 f1 0.396\n",
            "[W3] RNN Epoch 40 | train_loss 0.4217 acc 0.805 f1 0.804 || val_loss 1.4313 acc 0.471 f1 0.411\n",
            "[W3] RNN Epoch 41 | train_loss 0.4099 acc 0.810 f1 0.809 || val_loss 1.4523 acc 0.461 f1 0.398\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=6\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 928, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1006 acc 0.339 f1 0.240 || val_loss 1.0962 acc 0.383 f1 0.248\n",
            "[W3] GRU Epoch 02 | train_loss 1.0892 acc 0.393 f1 0.368 || val_loss 1.0926 acc 0.348 f1 0.318\n",
            "[W3] GRU Epoch 03 | train_loss 1.0753 acc 0.428 f1 0.429 || val_loss 1.1002 acc 0.317 f1 0.302\n",
            "[W3] GRU Epoch 04 | train_loss 1.0555 acc 0.457 f1 0.449 || val_loss 1.0926 acc 0.337 f1 0.315\n",
            "[W3] GRU Epoch 05 | train_loss 1.0214 acc 0.493 f1 0.486 || val_loss 1.0925 acc 0.356 f1 0.323\n",
            "[W3] GRU Epoch 06 | train_loss 0.9524 acc 0.540 f1 0.531 || val_loss 1.1197 acc 0.346 f1 0.305\n",
            "[W3] GRU Epoch 07 | train_loss 0.8805 acc 0.568 f1 0.558 || val_loss 1.1064 acc 0.381 f1 0.318\n",
            "[W3] GRU Epoch 08 | train_loss 0.8213 acc 0.593 f1 0.586 || val_loss 1.1441 acc 0.368 f1 0.313\n",
            "[W3] GRU Epoch 09 | train_loss 0.7853 acc 0.609 f1 0.603 || val_loss 1.1189 acc 0.393 f1 0.323\n",
            "[W3] GRU Epoch 10 | train_loss 0.7450 acc 0.648 f1 0.644 || val_loss 1.1937 acc 0.356 f1 0.303\n",
            "[W3] GRU Epoch 11 | train_loss 0.7195 acc 0.648 f1 0.644 || val_loss 1.1793 acc 0.381 f1 0.306\n",
            "[W3] GRU Epoch 12 | train_loss 0.6860 acc 0.659 f1 0.656 || val_loss 1.2337 acc 0.356 f1 0.297\n",
            "[W3] GRU Epoch 13 | train_loss 0.6634 acc 0.673 f1 0.669 || val_loss 1.2289 acc 0.364 f1 0.293\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=6\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 928, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1009 acc 0.336 f1 0.209 || val_loss 1.1115 acc 0.140 f1 0.108\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0964 acc 0.380 f1 0.323 || val_loss 1.1083 acc 0.243 f1 0.239\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0911 acc 0.420 f1 0.410 || val_loss 1.1035 acc 0.315 f1 0.296\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0798 acc 0.429 f1 0.428 || val_loss 1.1115 acc 0.288 f1 0.281\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0589 acc 0.456 f1 0.454 || val_loss 1.1097 acc 0.319 f1 0.299\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0104 acc 0.507 f1 0.499 || val_loss 1.1254 acc 0.323 f1 0.303\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9257 acc 0.561 f1 0.551 || val_loss 1.1307 acc 0.352 f1 0.319\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8478 acc 0.600 f1 0.595 || val_loss 1.1508 acc 0.364 f1 0.321\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7944 acc 0.621 f1 0.617 || val_loss 1.1544 acc 0.383 f1 0.333\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7408 acc 0.659 f1 0.656 || val_loss 1.1755 acc 0.387 f1 0.328\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7032 acc 0.663 f1 0.660 || val_loss 1.1948 acc 0.379 f1 0.317\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6761 acc 0.690 f1 0.686 || val_loss 1.1918 acc 0.416 f1 0.345\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6320 acc 0.711 f1 0.708 || val_loss 1.2142 acc 0.420 f1 0.337\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5995 acc 0.726 f1 0.725 || val_loss 1.2653 acc 0.434 f1 0.366\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5835 acc 0.727 f1 0.727 || val_loss 1.3235 acc 0.403 f1 0.337\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5451 acc 0.753 f1 0.752 || val_loss 1.3459 acc 0.412 f1 0.334\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5190 acc 0.760 f1 0.759 || val_loss 1.3844 acc 0.420 f1 0.344\n",
            "[W3] LSTM Epoch 18 | train_loss 0.4899 acc 0.768 f1 0.768 || val_loss 1.4376 acc 0.424 f1 0.348\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4652 acc 0.787 f1 0.786 || val_loss 1.4705 acc 0.416 f1 0.327\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4331 acc 0.802 f1 0.802 || val_loss 1.5386 acc 0.416 f1 0.332\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4195 acc 0.809 f1 0.809 || val_loss 1.5880 acc 0.424 f1 0.347\n",
            "[W3] LSTM Epoch 22 | train_loss 0.3953 acc 0.822 f1 0.822 || val_loss 1.6406 acc 0.409 f1 0.323\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:   6%|         | 6/100 [02:55<45:17, 28.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=7 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=7\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 925, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1304 acc 0.408 f1 0.407 || val_loss 1.1309 acc 0.309 f1 0.300\n",
            "[W3] ANN Epoch 02 | train_loss 0.9684 acc 0.519 f1 0.510 || val_loss 1.1275 acc 0.348 f1 0.320\n",
            "[W3] ANN Epoch 03 | train_loss 0.8714 acc 0.568 f1 0.560 || val_loss 1.1393 acc 0.360 f1 0.336\n",
            "[W3] ANN Epoch 04 | train_loss 0.7819 acc 0.627 f1 0.620 || val_loss 1.1255 acc 0.397 f1 0.359\n",
            "[W3] ANN Epoch 05 | train_loss 0.7205 acc 0.648 f1 0.642 || val_loss 1.1493 acc 0.381 f1 0.327\n",
            "[W3] ANN Epoch 06 | train_loss 0.6769 acc 0.684 f1 0.680 || val_loss 1.1310 acc 0.405 f1 0.349\n",
            "[W3] ANN Epoch 07 | train_loss 0.6455 acc 0.689 f1 0.687 || val_loss 1.1647 acc 0.403 f1 0.343\n",
            "[W3] ANN Epoch 08 | train_loss 0.6161 acc 0.711 f1 0.708 || val_loss 1.1553 acc 0.426 f1 0.364\n",
            "[W3] ANN Epoch 09 | train_loss 0.5821 acc 0.732 f1 0.729 || val_loss 1.2101 acc 0.407 f1 0.346\n",
            "[W3] ANN Epoch 10 | train_loss 0.5558 acc 0.745 f1 0.745 || val_loss 1.2156 acc 0.414 f1 0.354\n",
            "[W3] ANN Epoch 11 | train_loss 0.5174 acc 0.757 f1 0.756 || val_loss 1.2065 acc 0.412 f1 0.345\n",
            "[W3] ANN Epoch 12 | train_loss 0.4882 acc 0.770 f1 0.768 || val_loss 1.2799 acc 0.383 f1 0.337\n",
            "[W3] ANN Epoch 13 | train_loss 0.5003 acc 0.780 f1 0.780 || val_loss 1.3026 acc 0.405 f1 0.327\n",
            "[W3] ANN Epoch 14 | train_loss 0.4843 acc 0.773 f1 0.773 || val_loss 1.3322 acc 0.418 f1 0.346\n",
            "[W3] ANN Epoch 15 | train_loss 0.4636 acc 0.796 f1 0.795 || val_loss 1.2919 acc 0.418 f1 0.363\n",
            "[W3] ANN Epoch 16 | train_loss 0.4407 acc 0.811 f1 0.810 || val_loss 1.3397 acc 0.416 f1 0.358\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=7\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 925, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0800 acc 0.400 f1 0.401 || val_loss 1.0491 acc 0.393 f1 0.313\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9518 acc 0.556 f1 0.550 || val_loss 1.0503 acc 0.436 f1 0.359\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8114 acc 0.619 f1 0.615 || val_loss 1.1466 acc 0.358 f1 0.314\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7141 acc 0.673 f1 0.670 || val_loss 1.1898 acc 0.372 f1 0.313\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6270 acc 0.715 f1 0.715 || val_loss 1.2633 acc 0.397 f1 0.332\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5758 acc 0.740 f1 0.739 || val_loss 1.2753 acc 0.385 f1 0.330\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.4999 acc 0.776 f1 0.776 || val_loss 1.3449 acc 0.385 f1 0.336\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4433 acc 0.811 f1 0.810 || val_loss 1.4010 acc 0.409 f1 0.326\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.3944 acc 0.837 f1 0.837 || val_loss 1.5056 acc 0.412 f1 0.359\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3570 acc 0.848 f1 0.848 || val_loss 1.5906 acc 0.418 f1 0.340\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=7\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 925, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0999 acc 0.335 f1 0.275 || val_loss 1.0952 acc 0.358 f1 0.325\n",
            "[W3] RNN Epoch 02 | train_loss 1.0831 acc 0.415 f1 0.403 || val_loss 1.0963 acc 0.325 f1 0.308\n",
            "[W3] RNN Epoch 03 | train_loss 1.0648 acc 0.439 f1 0.430 || val_loss 1.0890 acc 0.342 f1 0.318\n",
            "[W3] RNN Epoch 04 | train_loss 1.0474 acc 0.458 f1 0.446 || val_loss 1.0909 acc 0.309 f1 0.290\n",
            "[W3] RNN Epoch 05 | train_loss 1.0259 acc 0.476 f1 0.465 || val_loss 1.0658 acc 0.360 f1 0.323\n",
            "[W3] RNN Epoch 06 | train_loss 1.0022 acc 0.502 f1 0.493 || val_loss 1.0929 acc 0.329 f1 0.310\n",
            "[W3] RNN Epoch 07 | train_loss 0.9812 acc 0.514 f1 0.503 || val_loss 1.0815 acc 0.348 f1 0.320\n",
            "[W3] RNN Epoch 08 | train_loss 0.9591 acc 0.535 f1 0.522 || val_loss 1.0790 acc 0.364 f1 0.335\n",
            "[W3] RNN Epoch 09 | train_loss 0.9358 acc 0.543 f1 0.528 || val_loss 1.1093 acc 0.337 f1 0.315\n",
            "[W3] RNN Epoch 10 | train_loss 0.9123 acc 0.564 f1 0.552 || val_loss 1.0856 acc 0.393 f1 0.355\n",
            "[W3] RNN Epoch 11 | train_loss 0.8915 acc 0.566 f1 0.552 || val_loss 1.0853 acc 0.387 f1 0.348\n",
            "[W3] RNN Epoch 12 | train_loss 0.8671 acc 0.591 f1 0.581 || val_loss 1.0871 acc 0.377 f1 0.337\n",
            "[W3] RNN Epoch 13 | train_loss 0.8475 acc 0.600 f1 0.589 || val_loss 1.1197 acc 0.374 f1 0.341\n",
            "[W3] RNN Epoch 14 | train_loss 0.8247 acc 0.610 f1 0.597 || val_loss 1.1223 acc 0.387 f1 0.341\n",
            "[W3] RNN Epoch 15 | train_loss 0.8002 acc 0.624 f1 0.613 || val_loss 1.1065 acc 0.389 f1 0.310\n",
            "[W3] RNN Epoch 16 | train_loss 0.7796 acc 0.639 f1 0.631 || val_loss 1.1219 acc 0.389 f1 0.322\n",
            "[W3] RNN Epoch 17 | train_loss 0.7577 acc 0.647 f1 0.637 || val_loss 1.1310 acc 0.407 f1 0.339\n",
            "[W3] RNN Epoch 18 | train_loss 0.7340 acc 0.653 f1 0.643 || val_loss 1.1531 acc 0.391 f1 0.319\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=7\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 925, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0986 acc 0.334 f1 0.302 || val_loss 1.0989 acc 0.352 f1 0.312\n",
            "[W3] GRU Epoch 02 | train_loss 1.0890 acc 0.399 f1 0.398 || val_loss 1.0988 acc 0.305 f1 0.291\n",
            "[W3] GRU Epoch 03 | train_loss 1.0801 acc 0.443 f1 0.431 || val_loss 1.0872 acc 0.352 f1 0.312\n",
            "[W3] GRU Epoch 04 | train_loss 1.0658 acc 0.449 f1 0.446 || val_loss 1.1005 acc 0.321 f1 0.310\n",
            "[W3] GRU Epoch 05 | train_loss 1.0376 acc 0.487 f1 0.476 || val_loss 1.0910 acc 0.327 f1 0.298\n",
            "[W3] GRU Epoch 06 | train_loss 0.9896 acc 0.527 f1 0.517 || val_loss 1.0969 acc 0.329 f1 0.291\n",
            "[W3] GRU Epoch 07 | train_loss 0.9135 acc 0.554 f1 0.545 || val_loss 1.1646 acc 0.298 f1 0.275\n",
            "[W3] GRU Epoch 08 | train_loss 0.8321 acc 0.603 f1 0.596 || val_loss 1.1479 acc 0.364 f1 0.312\n",
            "[W3] GRU Epoch 09 | train_loss 0.7831 acc 0.621 f1 0.616 || val_loss 1.1907 acc 0.374 f1 0.325\n",
            "[W3] GRU Epoch 10 | train_loss 0.7454 acc 0.635 f1 0.629 || val_loss 1.1559 acc 0.409 f1 0.343\n",
            "[W3] GRU Epoch 11 | train_loss 0.7092 acc 0.657 f1 0.652 || val_loss 1.2174 acc 0.414 f1 0.351\n",
            "[W3] GRU Epoch 12 | train_loss 0.6732 acc 0.661 f1 0.658 || val_loss 1.2464 acc 0.414 f1 0.347\n",
            "[W3] GRU Epoch 13 | train_loss 0.6489 acc 0.673 f1 0.669 || val_loss 1.2861 acc 0.405 f1 0.341\n",
            "[W3] GRU Epoch 14 | train_loss 0.6230 acc 0.685 f1 0.682 || val_loss 1.3057 acc 0.426 f1 0.363\n",
            "[W3] GRU Epoch 15 | train_loss 0.6019 acc 0.706 f1 0.703 || val_loss 1.3203 acc 0.422 f1 0.352\n",
            "[W3] GRU Epoch 16 | train_loss 0.5848 acc 0.720 f1 0.718 || val_loss 1.2990 acc 0.420 f1 0.342\n",
            "[W3] GRU Epoch 17 | train_loss 0.5542 acc 0.720 f1 0.717 || val_loss 1.3605 acc 0.420 f1 0.350\n",
            "[W3] GRU Epoch 18 | train_loss 0.5280 acc 0.732 f1 0.730 || val_loss 1.4214 acc 0.434 f1 0.362\n",
            "[W3] GRU Epoch 19 | train_loss 0.5160 acc 0.736 f1 0.734 || val_loss 1.4409 acc 0.420 f1 0.346\n",
            "[W3] GRU Epoch 20 | train_loss 0.4942 acc 0.749 f1 0.747 || val_loss 1.4914 acc 0.416 f1 0.339\n",
            "[W3] GRU Epoch 21 | train_loss 0.4746 acc 0.770 f1 0.769 || val_loss 1.5409 acc 0.444 f1 0.360\n",
            "[W3] GRU Epoch 22 | train_loss 0.4572 acc 0.774 f1 0.773 || val_loss 1.6162 acc 0.428 f1 0.356\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=7\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 925, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0984 acc 0.342 f1 0.315 || val_loss 1.0948 acc 0.348 f1 0.318\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0936 acc 0.383 f1 0.374 || val_loss 1.0980 acc 0.307 f1 0.301\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0852 acc 0.422 f1 0.411 || val_loss 1.1004 acc 0.290 f1 0.286\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0669 acc 0.446 f1 0.437 || val_loss 1.1019 acc 0.331 f1 0.320\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0225 acc 0.493 f1 0.477 || val_loss 1.1007 acc 0.350 f1 0.323\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9276 acc 0.544 f1 0.535 || val_loss 1.0826 acc 0.389 f1 0.338\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8476 acc 0.581 f1 0.576 || val_loss 1.1343 acc 0.387 f1 0.338\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7927 acc 0.605 f1 0.599 || val_loss 1.0977 acc 0.424 f1 0.343\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7569 acc 0.627 f1 0.622 || val_loss 1.1404 acc 0.405 f1 0.334\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7194 acc 0.647 f1 0.643 || val_loss 1.1825 acc 0.397 f1 0.340\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6879 acc 0.659 f1 0.654 || val_loss 1.2173 acc 0.412 f1 0.344\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6585 acc 0.673 f1 0.671 || val_loss 1.2143 acc 0.414 f1 0.343\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6267 acc 0.687 f1 0.685 || val_loss 1.2992 acc 0.401 f1 0.347\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5944 acc 0.710 f1 0.707 || val_loss 1.3110 acc 0.395 f1 0.319\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5721 acc 0.719 f1 0.717 || val_loss 1.3397 acc 0.405 f1 0.338\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5506 acc 0.733 f1 0.731 || val_loss 1.3230 acc 0.418 f1 0.344\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5311 acc 0.738 f1 0.736 || val_loss 1.3463 acc 0.405 f1 0.317\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5034 acc 0.755 f1 0.754 || val_loss 1.4345 acc 0.418 f1 0.333\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4860 acc 0.761 f1 0.760 || val_loss 1.4823 acc 0.401 f1 0.326\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4703 acc 0.773 f1 0.773 || val_loss 1.5390 acc 0.403 f1 0.339\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4552 acc 0.784 f1 0.784 || val_loss 1.5408 acc 0.405 f1 0.332\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:   7%|         | 7/100 [03:22<43:35, 28.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=8 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=8\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1585 acc 0.382 f1 0.383 || val_loss 1.1384 acc 0.302 f1 0.281\n",
            "[W3] ANN Epoch 02 | train_loss 0.9821 acc 0.506 f1 0.501 || val_loss 1.1018 acc 0.348 f1 0.306\n",
            "[W3] ANN Epoch 03 | train_loss 0.8868 acc 0.566 f1 0.561 || val_loss 1.1147 acc 0.352 f1 0.298\n",
            "[W3] ANN Epoch 04 | train_loss 0.8242 acc 0.596 f1 0.589 || val_loss 1.1054 acc 0.350 f1 0.294\n",
            "[W3] ANN Epoch 05 | train_loss 0.7782 acc 0.617 f1 0.612 || val_loss 1.1053 acc 0.366 f1 0.301\n",
            "[W3] ANN Epoch 06 | train_loss 0.7250 acc 0.649 f1 0.644 || val_loss 1.1475 acc 0.401 f1 0.332\n",
            "[W3] ANN Epoch 07 | train_loss 0.6741 acc 0.679 f1 0.676 || val_loss 1.1418 acc 0.409 f1 0.334\n",
            "[W3] ANN Epoch 08 | train_loss 0.6539 acc 0.687 f1 0.683 || val_loss 1.1651 acc 0.387 f1 0.311\n",
            "[W3] ANN Epoch 09 | train_loss 0.6343 acc 0.699 f1 0.696 || val_loss 1.1537 acc 0.401 f1 0.321\n",
            "[W3] ANN Epoch 10 | train_loss 0.5885 acc 0.716 f1 0.715 || val_loss 1.1750 acc 0.414 f1 0.331\n",
            "[W3] ANN Epoch 11 | train_loss 0.5819 acc 0.722 f1 0.721 || val_loss 1.2133 acc 0.391 f1 0.321\n",
            "[W3] ANN Epoch 12 | train_loss 0.5724 acc 0.730 f1 0.729 || val_loss 1.2302 acc 0.399 f1 0.326\n",
            "[W3] ANN Epoch 13 | train_loss 0.5385 acc 0.753 f1 0.752 || val_loss 1.2253 acc 0.420 f1 0.331\n",
            "[W3] ANN Epoch 14 | train_loss 0.5365 acc 0.752 f1 0.752 || val_loss 1.2264 acc 0.407 f1 0.327\n",
            "[W3] ANN Epoch 15 | train_loss 0.5195 acc 0.761 f1 0.760 || val_loss 1.2574 acc 0.426 f1 0.340\n",
            "[W3] ANN Epoch 16 | train_loss 0.5090 acc 0.768 f1 0.767 || val_loss 1.2659 acc 0.442 f1 0.352\n",
            "[W3] ANN Epoch 17 | train_loss 0.5150 acc 0.764 f1 0.764 || val_loss 1.2837 acc 0.426 f1 0.335\n",
            "[W3] ANN Epoch 18 | train_loss 0.4837 acc 0.788 f1 0.788 || val_loss 1.2924 acc 0.420 f1 0.327\n",
            "[W3] ANN Epoch 19 | train_loss 0.4970 acc 0.784 f1 0.783 || val_loss 1.3018 acc 0.426 f1 0.347\n",
            "[W3] ANN Epoch 20 | train_loss 0.4644 acc 0.795 f1 0.795 || val_loss 1.3693 acc 0.409 f1 0.340\n",
            "[W3] ANN Epoch 21 | train_loss 0.4799 acc 0.791 f1 0.791 || val_loss 1.3051 acc 0.426 f1 0.334\n",
            "[W3] ANN Epoch 22 | train_loss 0.4532 acc 0.807 f1 0.807 || val_loss 1.3556 acc 0.414 f1 0.324\n",
            "[W3] ANN Epoch 23 | train_loss 0.4444 acc 0.809 f1 0.808 || val_loss 1.3741 acc 0.418 f1 0.338\n",
            "[W3] ANN Epoch 24 | train_loss 0.4165 acc 0.823 f1 0.823 || val_loss 1.3690 acc 0.420 f1 0.330\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=8\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0739 acc 0.400 f1 0.402 || val_loss 1.0265 acc 0.379 f1 0.275\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9641 acc 0.521 f1 0.513 || val_loss 1.0468 acc 0.372 f1 0.315\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8347 acc 0.593 f1 0.589 || val_loss 1.1131 acc 0.389 f1 0.314\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7398 acc 0.655 f1 0.649 || val_loss 1.2138 acc 0.321 f1 0.278\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6532 acc 0.698 f1 0.697 || val_loss 1.2680 acc 0.352 f1 0.298\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6117 acc 0.719 f1 0.715 || val_loss 1.2645 acc 0.362 f1 0.307\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5300 acc 0.769 f1 0.768 || val_loss 1.3663 acc 0.366 f1 0.305\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4700 acc 0.791 f1 0.791 || val_loss 1.4301 acc 0.412 f1 0.345\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4343 acc 0.807 f1 0.807 || val_loss 1.4799 acc 0.377 f1 0.310\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4149 acc 0.825 f1 0.824 || val_loss 1.5820 acc 0.379 f1 0.317\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3580 acc 0.846 f1 0.845 || val_loss 1.6174 acc 0.418 f1 0.341\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3332 acc 0.868 f1 0.868 || val_loss 1.6838 acc 0.418 f1 0.341\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2881 acc 0.885 f1 0.886 || val_loss 1.7883 acc 0.395 f1 0.324\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2972 acc 0.885 f1 0.884 || val_loss 1.8888 acc 0.381 f1 0.317\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2749 acc 0.898 f1 0.898 || val_loss 1.9230 acc 0.393 f1 0.332\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2082 acc 0.921 f1 0.921 || val_loss 2.0627 acc 0.377 f1 0.304\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=8\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0971 acc 0.368 f1 0.348 || val_loss 1.1081 acc 0.288 f1 0.282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0763 acc 0.436 f1 0.415 || val_loss 1.0939 acc 0.333 f1 0.323\n",
            "[W3] RNN Epoch 03 | train_loss 1.0599 acc 0.446 f1 0.435 || val_loss 1.0820 acc 0.342 f1 0.327\n",
            "[W3] RNN Epoch 04 | train_loss 1.0494 acc 0.452 f1 0.437 || val_loss 1.0894 acc 0.327 f1 0.315\n",
            "[W3] RNN Epoch 05 | train_loss 1.0353 acc 0.470 f1 0.462 || val_loss 1.0963 acc 0.317 f1 0.304\n",
            "[W3] RNN Epoch 06 | train_loss 1.0198 acc 0.475 f1 0.460 || val_loss 1.0655 acc 0.372 f1 0.337\n",
            "[W3] RNN Epoch 07 | train_loss 0.9932 acc 0.498 f1 0.487 || val_loss 1.0689 acc 0.366 f1 0.337\n",
            "[W3] RNN Epoch 08 | train_loss 0.9673 acc 0.526 f1 0.517 || val_loss 1.0896 acc 0.358 f1 0.339\n",
            "[W3] RNN Epoch 09 | train_loss 0.9455 acc 0.545 f1 0.532 || val_loss 1.0685 acc 0.389 f1 0.360\n",
            "[W3] RNN Epoch 10 | train_loss 0.9145 acc 0.559 f1 0.549 || val_loss 1.0696 acc 0.403 f1 0.368\n",
            "[W3] RNN Epoch 11 | train_loss 0.8966 acc 0.574 f1 0.565 || val_loss 1.0840 acc 0.379 f1 0.351\n",
            "[W3] RNN Epoch 12 | train_loss 0.8685 acc 0.579 f1 0.567 || val_loss 1.0916 acc 0.401 f1 0.375\n",
            "[W3] RNN Epoch 13 | train_loss 0.8600 acc 0.578 f1 0.567 || val_loss 1.0814 acc 0.412 f1 0.369\n",
            "[W3] RNN Epoch 14 | train_loss 0.8431 acc 0.595 f1 0.585 || val_loss 1.0993 acc 0.412 f1 0.368\n",
            "[W3] RNN Epoch 15 | train_loss 0.8236 acc 0.604 f1 0.594 || val_loss 1.1297 acc 0.391 f1 0.366\n",
            "[W3] RNN Epoch 16 | train_loss 0.7982 acc 0.610 f1 0.601 || val_loss 1.1158 acc 0.407 f1 0.367\n",
            "[W3] RNN Epoch 17 | train_loss 0.7690 acc 0.641 f1 0.632 || val_loss 1.1199 acc 0.428 f1 0.380\n",
            "[W3] RNN Epoch 18 | train_loss 0.7600 acc 0.628 f1 0.618 || val_loss 1.1414 acc 0.422 f1 0.380\n",
            "[W3] RNN Epoch 19 | train_loss 0.7428 acc 0.641 f1 0.633 || val_loss 1.1423 acc 0.432 f1 0.386\n",
            "[W3] RNN Epoch 20 | train_loss 0.7155 acc 0.661 f1 0.653 || val_loss 1.1616 acc 0.420 f1 0.364\n",
            "[W3] RNN Epoch 21 | train_loss 0.7031 acc 0.662 f1 0.654 || val_loss 1.1693 acc 0.422 f1 0.355\n",
            "[W3] RNN Epoch 22 | train_loss 0.6738 acc 0.677 f1 0.670 || val_loss 1.2045 acc 0.403 f1 0.357\n",
            "[W3] RNN Epoch 23 | train_loss 0.6719 acc 0.674 f1 0.667 || val_loss 1.2127 acc 0.412 f1 0.354\n",
            "[W3] RNN Epoch 24 | train_loss 0.6413 acc 0.698 f1 0.691 || val_loss 1.2150 acc 0.424 f1 0.350\n",
            "[W3] RNN Epoch 25 | train_loss 0.6278 acc 0.700 f1 0.695 || val_loss 1.2430 acc 0.412 f1 0.358\n",
            "[W3] RNN Epoch 26 | train_loss 0.6216 acc 0.688 f1 0.681 || val_loss 1.2554 acc 0.422 f1 0.356\n",
            "[W3] RNN Epoch 27 | train_loss 0.6050 acc 0.701 f1 0.695 || val_loss 1.2777 acc 0.409 f1 0.341\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=8\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0997 acc 0.330 f1 0.240 || val_loss 1.0906 acc 0.407 f1 0.322\n",
            "[W3] GRU Epoch 02 | train_loss 1.0920 acc 0.371 f1 0.370 || val_loss 1.0859 acc 0.374 f1 0.326\n",
            "[W3] GRU Epoch 03 | train_loss 1.0835 acc 0.404 f1 0.401 || val_loss 1.0819 acc 0.372 f1 0.335\n",
            "[W3] GRU Epoch 04 | train_loss 1.0680 acc 0.440 f1 0.437 || val_loss 1.0779 acc 0.366 f1 0.330\n",
            "[W3] GRU Epoch 05 | train_loss 1.0447 acc 0.465 f1 0.460 || val_loss 1.0754 acc 0.366 f1 0.334\n",
            "[W3] GRU Epoch 06 | train_loss 1.0031 acc 0.494 f1 0.481 || val_loss 1.0853 acc 0.366 f1 0.326\n",
            "[W3] GRU Epoch 07 | train_loss 0.9379 acc 0.549 f1 0.542 || val_loss 1.1331 acc 0.321 f1 0.292\n",
            "[W3] GRU Epoch 08 | train_loss 0.8733 acc 0.576 f1 0.569 || val_loss 1.0811 acc 0.399 f1 0.328\n",
            "[W3] GRU Epoch 09 | train_loss 0.8202 acc 0.604 f1 0.600 || val_loss 1.1307 acc 0.381 f1 0.332\n",
            "[W3] GRU Epoch 10 | train_loss 0.7946 acc 0.620 f1 0.614 || val_loss 1.1223 acc 0.416 f1 0.348\n",
            "[W3] GRU Epoch 11 | train_loss 0.7597 acc 0.628 f1 0.623 || val_loss 1.1159 acc 0.430 f1 0.346\n",
            "[W3] GRU Epoch 12 | train_loss 0.7240 acc 0.650 f1 0.646 || val_loss 1.1563 acc 0.395 f1 0.336\n",
            "[W3] GRU Epoch 13 | train_loss 0.7007 acc 0.668 f1 0.665 || val_loss 1.1885 acc 0.395 f1 0.333\n",
            "[W3] GRU Epoch 14 | train_loss 0.6747 acc 0.673 f1 0.670 || val_loss 1.2175 acc 0.385 f1 0.325\n",
            "[W3] GRU Epoch 15 | train_loss 0.6468 acc 0.683 f1 0.681 || val_loss 1.2452 acc 0.383 f1 0.333\n",
            "[W3] GRU Epoch 16 | train_loss 0.6175 acc 0.707 f1 0.704 || val_loss 1.2668 acc 0.387 f1 0.338\n",
            "[W3] GRU Epoch 17 | train_loss 0.6073 acc 0.703 f1 0.700 || val_loss 1.2703 acc 0.393 f1 0.338\n",
            "[W3] GRU Epoch 18 | train_loss 0.5731 acc 0.719 f1 0.717 || val_loss 1.2908 acc 0.401 f1 0.344\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=8\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1004 acc 0.338 f1 0.233 || val_loss 1.0840 acc 0.449 f1 0.249\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0957 acc 0.355 f1 0.316 || val_loss 1.0936 acc 0.385 f1 0.334\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0881 acc 0.406 f1 0.404 || val_loss 1.0990 acc 0.315 f1 0.300\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0728 acc 0.435 f1 0.422 || val_loss 1.0936 acc 0.333 f1 0.306\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0433 acc 0.470 f1 0.450 || val_loss 1.0841 acc 0.381 f1 0.343\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9789 acc 0.511 f1 0.493 || val_loss 1.0913 acc 0.356 f1 0.308\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8958 acc 0.551 f1 0.544 || val_loss 1.0977 acc 0.395 f1 0.336\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8297 acc 0.592 f1 0.586 || val_loss 1.1134 acc 0.399 f1 0.335\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7826 acc 0.601 f1 0.595 || val_loss 1.1387 acc 0.389 f1 0.324\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7529 acc 0.626 f1 0.621 || val_loss 1.1969 acc 0.358 f1 0.316\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7200 acc 0.632 f1 0.628 || val_loss 1.1682 acc 0.372 f1 0.296\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6786 acc 0.656 f1 0.653 || val_loss 1.1780 acc 0.393 f1 0.314\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6601 acc 0.668 f1 0.665 || val_loss 1.1930 acc 0.393 f1 0.304\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:   8%|         | 8/100 [03:50<43:17, 28.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=9 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=9\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1512 acc 0.396 f1 0.391 || val_loss 1.1348 acc 0.311 f1 0.293\n",
            "[W3] ANN Epoch 02 | train_loss 0.9698 acc 0.503 f1 0.494 || val_loss 1.1205 acc 0.329 f1 0.296\n",
            "[W3] ANN Epoch 03 | train_loss 0.8740 acc 0.569 f1 0.559 || val_loss 1.1108 acc 0.325 f1 0.287\n",
            "[W3] ANN Epoch 04 | train_loss 0.8098 acc 0.604 f1 0.597 || val_loss 1.0923 acc 0.354 f1 0.297\n",
            "[W3] ANN Epoch 05 | train_loss 0.7301 acc 0.637 f1 0.630 || val_loss 1.1265 acc 0.360 f1 0.293\n",
            "[W3] ANN Epoch 06 | train_loss 0.6920 acc 0.673 f1 0.669 || val_loss 1.1051 acc 0.395 f1 0.353\n",
            "[W3] ANN Epoch 07 | train_loss 0.6535 acc 0.695 f1 0.692 || val_loss 1.1253 acc 0.430 f1 0.348\n",
            "[W3] ANN Epoch 08 | train_loss 0.6227 acc 0.693 f1 0.689 || val_loss 1.1196 acc 0.409 f1 0.346\n",
            "[W3] ANN Epoch 09 | train_loss 0.5917 acc 0.716 f1 0.715 || val_loss 1.1340 acc 0.405 f1 0.337\n",
            "[W3] ANN Epoch 10 | train_loss 0.5482 acc 0.753 f1 0.752 || val_loss 1.1868 acc 0.434 f1 0.352\n",
            "[W3] ANN Epoch 11 | train_loss 0.5484 acc 0.745 f1 0.744 || val_loss 1.1923 acc 0.412 f1 0.340\n",
            "[W3] ANN Epoch 12 | train_loss 0.5103 acc 0.768 f1 0.768 || val_loss 1.2017 acc 0.455 f1 0.369\n",
            "[W3] ANN Epoch 13 | train_loss 0.5010 acc 0.775 f1 0.774 || val_loss 1.2450 acc 0.418 f1 0.336\n",
            "[W3] ANN Epoch 14 | train_loss 0.4835 acc 0.786 f1 0.786 || val_loss 1.2789 acc 0.426 f1 0.352\n",
            "[W3] ANN Epoch 15 | train_loss 0.4738 acc 0.792 f1 0.791 || val_loss 1.2633 acc 0.436 f1 0.349\n",
            "[W3] ANN Epoch 16 | train_loss 0.4583 acc 0.796 f1 0.796 || val_loss 1.3035 acc 0.444 f1 0.361\n",
            "[W3] ANN Epoch 17 | train_loss 0.4191 acc 0.823 f1 0.823 || val_loss 1.3931 acc 0.416 f1 0.331\n",
            "[W3] ANN Epoch 18 | train_loss 0.4181 acc 0.824 f1 0.823 || val_loss 1.3796 acc 0.453 f1 0.362\n",
            "[W3] ANN Epoch 19 | train_loss 0.4128 acc 0.830 f1 0.830 || val_loss 1.4361 acc 0.414 f1 0.319\n",
            "[W3] ANN Epoch 20 | train_loss 0.3746 acc 0.838 f1 0.837 || val_loss 1.4084 acc 0.428 f1 0.330\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=9\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0848 acc 0.382 f1 0.383 || val_loss 1.0295 acc 0.412 f1 0.303\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9625 acc 0.540 f1 0.538 || val_loss 1.0427 acc 0.414 f1 0.328\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8344 acc 0.603 f1 0.599 || val_loss 1.1247 acc 0.368 f1 0.323\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7293 acc 0.648 f1 0.645 || val_loss 1.1658 acc 0.364 f1 0.279\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6671 acc 0.684 f1 0.681 || val_loss 1.1684 acc 0.379 f1 0.315\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6149 acc 0.709 f1 0.708 || val_loss 1.2063 acc 0.377 f1 0.312\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5609 acc 0.745 f1 0.744 || val_loss 1.2616 acc 0.379 f1 0.329\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5252 acc 0.760 f1 0.758 || val_loss 1.2759 acc 0.403 f1 0.354\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4736 acc 0.792 f1 0.791 || val_loss 1.3758 acc 0.397 f1 0.329\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4512 acc 0.798 f1 0.797 || val_loss 1.3989 acc 0.387 f1 0.329\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.4187 acc 0.819 f1 0.819 || val_loss 1.4237 acc 0.399 f1 0.339\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.4117 acc 0.826 f1 0.824 || val_loss 1.4107 acc 0.414 f1 0.354\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3664 acc 0.851 f1 0.850 || val_loss 1.4634 acc 0.414 f1 0.342\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3458 acc 0.856 f1 0.855 || val_loss 1.4452 acc 0.403 f1 0.333\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.3282 acc 0.861 f1 0.860 || val_loss 1.5414 acc 0.414 f1 0.333\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2901 acc 0.881 f1 0.880 || val_loss 1.6707 acc 0.426 f1 0.346\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=9\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0974 acc 0.351 f1 0.338 || val_loss 1.0887 acc 0.395 f1 0.348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0815 acc 0.431 f1 0.429 || val_loss 1.0856 acc 0.360 f1 0.328\n",
            "[W3] RNN Epoch 03 | train_loss 1.0679 acc 0.437 f1 0.435 || val_loss 1.0826 acc 0.335 f1 0.312\n",
            "[W3] RNN Epoch 04 | train_loss 1.0493 acc 0.450 f1 0.450 || val_loss 1.0838 acc 0.325 f1 0.301\n",
            "[W3] RNN Epoch 05 | train_loss 1.0269 acc 0.477 f1 0.470 || val_loss 1.0702 acc 0.337 f1 0.310\n",
            "[W3] RNN Epoch 06 | train_loss 1.0073 acc 0.506 f1 0.502 || val_loss 1.0897 acc 0.337 f1 0.318\n",
            "[W3] RNN Epoch 07 | train_loss 0.9887 acc 0.518 f1 0.512 || val_loss 1.0774 acc 0.366 f1 0.337\n",
            "[W3] RNN Epoch 08 | train_loss 0.9660 acc 0.537 f1 0.529 || val_loss 1.0735 acc 0.368 f1 0.335\n",
            "[W3] RNN Epoch 09 | train_loss 0.9513 acc 0.542 f1 0.534 || val_loss 1.0818 acc 0.383 f1 0.349\n",
            "[W3] RNN Epoch 10 | train_loss 0.9290 acc 0.550 f1 0.542 || val_loss 1.0846 acc 0.372 f1 0.339\n",
            "[W3] RNN Epoch 11 | train_loss 0.9052 acc 0.568 f1 0.559 || val_loss 1.0919 acc 0.350 f1 0.321\n",
            "[W3] RNN Epoch 12 | train_loss 0.8865 acc 0.584 f1 0.572 || val_loss 1.0948 acc 0.364 f1 0.329\n",
            "[W3] RNN Epoch 13 | train_loss 0.8640 acc 0.596 f1 0.588 || val_loss 1.1028 acc 0.358 f1 0.323\n",
            "[W3] RNN Epoch 14 | train_loss 0.8441 acc 0.604 f1 0.593 || val_loss 1.0993 acc 0.356 f1 0.314\n",
            "[W3] RNN Epoch 15 | train_loss 0.8308 acc 0.606 f1 0.599 || val_loss 1.1218 acc 0.352 f1 0.318\n",
            "[W3] RNN Epoch 16 | train_loss 0.8129 acc 0.617 f1 0.607 || val_loss 1.0919 acc 0.383 f1 0.336\n",
            "[W3] RNN Epoch 17 | train_loss 0.7813 acc 0.633 f1 0.625 || val_loss 1.1474 acc 0.342 f1 0.310\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=9\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0958 acc 0.371 f1 0.370 || val_loss 1.0966 acc 0.302 f1 0.281\n",
            "[W3] GRU Epoch 02 | train_loss 1.0890 acc 0.419 f1 0.416 || val_loss 1.0878 acc 0.360 f1 0.303\n",
            "[W3] GRU Epoch 03 | train_loss 1.0818 acc 0.421 f1 0.422 || val_loss 1.0810 acc 0.372 f1 0.322\n",
            "[W3] GRU Epoch 04 | train_loss 1.0687 acc 0.440 f1 0.439 || val_loss 1.0830 acc 0.346 f1 0.319\n",
            "[W3] GRU Epoch 05 | train_loss 1.0496 acc 0.470 f1 0.467 || val_loss 1.0840 acc 0.337 f1 0.314\n",
            "[W3] GRU Epoch 06 | train_loss 1.0215 acc 0.500 f1 0.497 || val_loss 1.0701 acc 0.377 f1 0.340\n",
            "[W3] GRU Epoch 07 | train_loss 0.9849 acc 0.534 f1 0.526 || val_loss 1.0547 acc 0.385 f1 0.335\n",
            "[W3] GRU Epoch 08 | train_loss 0.9231 acc 0.578 f1 0.571 || val_loss 1.0665 acc 0.393 f1 0.333\n",
            "[W3] GRU Epoch 09 | train_loss 0.8685 acc 0.584 f1 0.577 || val_loss 1.0643 acc 0.407 f1 0.336\n",
            "[W3] GRU Epoch 10 | train_loss 0.8152 acc 0.606 f1 0.599 || val_loss 1.0765 acc 0.409 f1 0.348\n",
            "[W3] GRU Epoch 11 | train_loss 0.7809 acc 0.616 f1 0.611 || val_loss 1.1187 acc 0.393 f1 0.331\n",
            "[W3] GRU Epoch 12 | train_loss 0.7432 acc 0.632 f1 0.625 || val_loss 1.1217 acc 0.405 f1 0.344\n",
            "[W3] GRU Epoch 13 | train_loss 0.7009 acc 0.661 f1 0.656 || val_loss 1.1832 acc 0.377 f1 0.328\n",
            "[W3] GRU Epoch 14 | train_loss 0.6772 acc 0.671 f1 0.666 || val_loss 1.2006 acc 0.387 f1 0.333\n",
            "[W3] GRU Epoch 15 | train_loss 0.6503 acc 0.686 f1 0.681 || val_loss 1.1866 acc 0.407 f1 0.338\n",
            "[W3] GRU Epoch 16 | train_loss 0.6217 acc 0.698 f1 0.695 || val_loss 1.2347 acc 0.405 f1 0.344\n",
            "[W3] GRU Epoch 17 | train_loss 0.5889 acc 0.717 f1 0.714 || val_loss 1.2494 acc 0.416 f1 0.355\n",
            "[W3] GRU Epoch 18 | train_loss 0.5754 acc 0.714 f1 0.710 || val_loss 1.2847 acc 0.414 f1 0.350\n",
            "[W3] GRU Epoch 19 | train_loss 0.5413 acc 0.731 f1 0.728 || val_loss 1.3361 acc 0.414 f1 0.350\n",
            "[W3] GRU Epoch 20 | train_loss 0.5516 acc 0.733 f1 0.732 || val_loss 1.3582 acc 0.412 f1 0.349\n",
            "[W3] GRU Epoch 21 | train_loss 0.5049 acc 0.764 f1 0.763 || val_loss 1.3891 acc 0.420 f1 0.347\n",
            "[W3] GRU Epoch 22 | train_loss 0.4866 acc 0.765 f1 0.763 || val_loss 1.4108 acc 0.428 f1 0.353\n",
            "[W3] GRU Epoch 23 | train_loss 0.4737 acc 0.779 f1 0.779 || val_loss 1.4250 acc 0.444 f1 0.361\n",
            "[W3] GRU Epoch 24 | train_loss 0.4572 acc 0.784 f1 0.784 || val_loss 1.4677 acc 0.436 f1 0.360\n",
            "[W3] GRU Epoch 25 | train_loss 0.4685 acc 0.776 f1 0.775 || val_loss 1.5283 acc 0.438 f1 0.365\n",
            "[W3] GRU Epoch 26 | train_loss 0.4275 acc 0.797 f1 0.796 || val_loss 1.5340 acc 0.428 f1 0.353\n",
            "[W3] GRU Epoch 27 | train_loss 0.4092 acc 0.814 f1 0.813 || val_loss 1.5728 acc 0.449 f1 0.369\n",
            "[W3] GRU Epoch 28 | train_loss 0.3941 acc 0.817 f1 0.816 || val_loss 1.6021 acc 0.442 f1 0.360\n",
            "[W3] GRU Epoch 29 | train_loss 0.3879 acc 0.817 f1 0.816 || val_loss 1.6595 acc 0.428 f1 0.349\n",
            "[W3] GRU Epoch 30 | train_loss 0.3744 acc 0.821 f1 0.821 || val_loss 1.6771 acc 0.426 f1 0.355\n",
            "[W3] GRU Epoch 31 | train_loss 0.3566 acc 0.838 f1 0.838 || val_loss 1.7099 acc 0.442 f1 0.364\n",
            "[W3] GRU Epoch 32 | train_loss 0.3418 acc 0.842 f1 0.842 || val_loss 1.7582 acc 0.424 f1 0.342\n",
            "[W3] GRU Epoch 33 | train_loss 0.3289 acc 0.855 f1 0.855 || val_loss 1.8049 acc 0.414 f1 0.341\n",
            "[W3] GRU Epoch 34 | train_loss 0.3129 acc 0.864 f1 0.864 || val_loss 1.8413 acc 0.434 f1 0.356\n",
            "[W3] GRU Epoch 35 | train_loss 0.2915 acc 0.877 f1 0.877 || val_loss 1.9067 acc 0.426 f1 0.351\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=9\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1002 acc 0.343 f1 0.257 || val_loss 1.1044 acc 0.181 f1 0.178\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0961 acc 0.371 f1 0.363 || val_loss 1.0991 acc 0.325 f1 0.302\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0915 acc 0.387 f1 0.366 || val_loss 1.0974 acc 0.327 f1 0.299\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0858 acc 0.409 f1 0.391 || val_loss 1.0970 acc 0.337 f1 0.314\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0750 acc 0.423 f1 0.416 || val_loss 1.0973 acc 0.302 f1 0.287\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0526 acc 0.471 f1 0.458 || val_loss 1.0816 acc 0.323 f1 0.296\n",
            "[W3] LSTM Epoch 07 | train_loss 1.0049 acc 0.507 f1 0.503 || val_loss 1.1291 acc 0.300 f1 0.288\n",
            "[W3] LSTM Epoch 08 | train_loss 0.9232 acc 0.567 f1 0.555 || val_loss 1.1070 acc 0.337 f1 0.300\n",
            "[W3] LSTM Epoch 09 | train_loss 0.8637 acc 0.587 f1 0.578 || val_loss 1.1062 acc 0.352 f1 0.298\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7992 acc 0.601 f1 0.593 || val_loss 1.1123 acc 0.383 f1 0.311\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7637 acc 0.633 f1 0.629 || val_loss 1.1457 acc 0.372 f1 0.303\n",
            "[W3] LSTM Epoch 12 | train_loss 0.7164 acc 0.655 f1 0.651 || val_loss 1.1904 acc 0.370 f1 0.307\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:   9%|         | 9/100 [04:20<43:32, 28.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=10 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=10\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1440 acc 0.386 f1 0.384 || val_loss 1.1194 acc 0.340 f1 0.313\n",
            "[W3] ANN Epoch 02 | train_loss 0.9759 acc 0.508 f1 0.501 || val_loss 1.1193 acc 0.337 f1 0.312\n",
            "[W3] ANN Epoch 03 | train_loss 0.8851 acc 0.564 f1 0.557 || val_loss 1.1159 acc 0.356 f1 0.326\n",
            "[W3] ANN Epoch 04 | train_loss 0.8262 acc 0.595 f1 0.589 || val_loss 1.0771 acc 0.395 f1 0.341\n",
            "[W3] ANN Epoch 05 | train_loss 0.7735 acc 0.627 f1 0.622 || val_loss 1.0998 acc 0.395 f1 0.312\n",
            "[W3] ANN Epoch 06 | train_loss 0.7284 acc 0.648 f1 0.645 || val_loss 1.1073 acc 0.403 f1 0.342\n",
            "[W3] ANN Epoch 07 | train_loss 0.6971 acc 0.663 f1 0.660 || val_loss 1.1341 acc 0.407 f1 0.347\n",
            "[W3] ANN Epoch 08 | train_loss 0.6537 acc 0.679 f1 0.677 || val_loss 1.1406 acc 0.428 f1 0.354\n",
            "[W3] ANN Epoch 09 | train_loss 0.6365 acc 0.691 f1 0.689 || val_loss 1.1643 acc 0.405 f1 0.339\n",
            "[W3] ANN Epoch 10 | train_loss 0.6109 acc 0.711 f1 0.709 || val_loss 1.1765 acc 0.422 f1 0.357\n",
            "[W3] ANN Epoch 11 | train_loss 0.6044 acc 0.703 f1 0.701 || val_loss 1.1883 acc 0.422 f1 0.356\n",
            "[W3] ANN Epoch 12 | train_loss 0.5811 acc 0.719 f1 0.717 || val_loss 1.1750 acc 0.432 f1 0.360\n",
            "[W3] ANN Epoch 13 | train_loss 0.5611 acc 0.732 f1 0.731 || val_loss 1.1984 acc 0.422 f1 0.352\n",
            "[W3] ANN Epoch 14 | train_loss 0.5587 acc 0.732 f1 0.731 || val_loss 1.1895 acc 0.444 f1 0.369\n",
            "[W3] ANN Epoch 15 | train_loss 0.5440 acc 0.753 f1 0.752 || val_loss 1.2181 acc 0.444 f1 0.372\n",
            "[W3] ANN Epoch 16 | train_loss 0.5329 acc 0.746 f1 0.744 || val_loss 1.2023 acc 0.438 f1 0.361\n",
            "[W3] ANN Epoch 17 | train_loss 0.5300 acc 0.754 f1 0.753 || val_loss 1.2259 acc 0.422 f1 0.343\n",
            "[W3] ANN Epoch 18 | train_loss 0.4960 acc 0.778 f1 0.778 || val_loss 1.2493 acc 0.414 f1 0.343\n",
            "[W3] ANN Epoch 19 | train_loss 0.4960 acc 0.775 f1 0.774 || val_loss 1.2716 acc 0.430 f1 0.354\n",
            "[W3] ANN Epoch 20 | train_loss 0.4732 acc 0.787 f1 0.785 || val_loss 1.2818 acc 0.424 f1 0.346\n",
            "[W3] ANN Epoch 21 | train_loss 0.5344 acc 0.764 f1 0.764 || val_loss 1.2464 acc 0.465 f1 0.375\n",
            "[W3] ANN Epoch 22 | train_loss 0.4935 acc 0.785 f1 0.784 || val_loss 1.2697 acc 0.451 f1 0.363\n",
            "[W3] ANN Epoch 23 | train_loss 0.4812 acc 0.785 f1 0.785 || val_loss 1.2919 acc 0.440 f1 0.365\n",
            "[W3] ANN Epoch 24 | train_loss 0.4981 acc 0.778 f1 0.778 || val_loss 1.2711 acc 0.442 f1 0.367\n",
            "[W3] ANN Epoch 25 | train_loss 0.4489 acc 0.806 f1 0.807 || val_loss 1.2760 acc 0.453 f1 0.352\n",
            "[W3] ANN Epoch 26 | train_loss 0.4796 acc 0.788 f1 0.787 || val_loss 1.3048 acc 0.428 f1 0.358\n",
            "[W3] ANN Epoch 27 | train_loss 0.4370 acc 0.808 f1 0.807 || val_loss 1.2682 acc 0.436 f1 0.360\n",
            "[W3] ANN Epoch 28 | train_loss 0.4219 acc 0.812 f1 0.811 || val_loss 1.3151 acc 0.447 f1 0.376\n",
            "[W3] ANN Epoch 29 | train_loss 0.4125 acc 0.824 f1 0.823 || val_loss 1.3023 acc 0.438 f1 0.356\n",
            "[W3] ANN Epoch 30 | train_loss 0.4270 acc 0.822 f1 0.822 || val_loss 1.3232 acc 0.442 f1 0.363\n",
            "[W3] ANN Epoch 31 | train_loss 0.4060 acc 0.828 f1 0.828 || val_loss 1.3573 acc 0.436 f1 0.361\n",
            "[W3] ANN Epoch 32 | train_loss 0.3870 acc 0.840 f1 0.840 || val_loss 1.4202 acc 0.426 f1 0.328\n",
            "[W3] ANN Epoch 33 | train_loss 0.4091 acc 0.829 f1 0.828 || val_loss 1.4174 acc 0.438 f1 0.369\n",
            "[W3] ANN Epoch 34 | train_loss 0.3860 acc 0.832 f1 0.832 || val_loss 1.4400 acc 0.428 f1 0.354\n",
            "[W3] ANN Epoch 35 | train_loss 0.3953 acc 0.831 f1 0.831 || val_loss 1.4360 acc 0.442 f1 0.365\n",
            "[W3] ANN Epoch 36 | train_loss 0.3751 acc 0.844 f1 0.844 || val_loss 1.4699 acc 0.430 f1 0.362\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=10\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0716 acc 0.409 f1 0.412 || val_loss 1.0370 acc 0.416 f1 0.352\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9299 acc 0.543 f1 0.533 || val_loss 1.0140 acc 0.426 f1 0.323\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8090 acc 0.609 f1 0.604 || val_loss 1.0892 acc 0.399 f1 0.342\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7217 acc 0.654 f1 0.651 || val_loss 1.1127 acc 0.418 f1 0.334\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6567 acc 0.688 f1 0.686 || val_loss 1.1596 acc 0.385 f1 0.324\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5949 acc 0.721 f1 0.719 || val_loss 1.1974 acc 0.391 f1 0.319\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5330 acc 0.761 f1 0.761 || val_loss 1.2812 acc 0.385 f1 0.325\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5173 acc 0.773 f1 0.772 || val_loss 1.2788 acc 0.393 f1 0.326\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4706 acc 0.799 f1 0.798 || val_loss 1.2898 acc 0.422 f1 0.349\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=10\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0991 acc 0.339 f1 0.320 || val_loss 1.0968 acc 0.331 f1 0.301\n",
            "[W3] RNN Epoch 02 | train_loss 1.0807 acc 0.434 f1 0.433 || val_loss 1.1045 acc 0.282 f1 0.266\n",
            "[W3] RNN Epoch 03 | train_loss 1.0641 acc 0.440 f1 0.431 || val_loss 1.0982 acc 0.321 f1 0.303\n",
            "[W3] RNN Epoch 04 | train_loss 1.0476 acc 0.459 f1 0.448 || val_loss 1.0842 acc 0.331 f1 0.302\n",
            "[W3] RNN Epoch 05 | train_loss 1.0241 acc 0.480 f1 0.473 || val_loss 1.1125 acc 0.327 f1 0.316\n",
            "[W3] RNN Epoch 06 | train_loss 0.9981 acc 0.521 f1 0.511 || val_loss 1.0802 acc 0.350 f1 0.322\n",
            "[W3] RNN Epoch 07 | train_loss 0.9757 acc 0.523 f1 0.516 || val_loss 1.0976 acc 0.329 f1 0.305\n",
            "[W3] RNN Epoch 08 | train_loss 0.9538 acc 0.540 f1 0.531 || val_loss 1.1162 acc 0.344 f1 0.323\n",
            "[W3] RNN Epoch 09 | train_loss 0.9305 acc 0.551 f1 0.541 || val_loss 1.1071 acc 0.337 f1 0.310\n",
            "[W3] RNN Epoch 10 | train_loss 0.9124 acc 0.557 f1 0.548 || val_loss 1.1309 acc 0.344 f1 0.322\n",
            "[W3] RNN Epoch 11 | train_loss 0.8862 acc 0.573 f1 0.563 || val_loss 1.1124 acc 0.358 f1 0.323\n",
            "[W3] RNN Epoch 12 | train_loss 0.8705 acc 0.577 f1 0.569 || val_loss 1.1124 acc 0.344 f1 0.311\n",
            "[W3] RNN Epoch 13 | train_loss 0.8530 acc 0.583 f1 0.573 || val_loss 1.1115 acc 0.358 f1 0.321\n",
            "[W3] RNN Epoch 14 | train_loss 0.8252 acc 0.602 f1 0.593 || val_loss 1.1317 acc 0.337 f1 0.306\n",
            "[W3] RNN Epoch 15 | train_loss 0.8118 acc 0.612 f1 0.602 || val_loss 1.1298 acc 0.374 f1 0.336\n",
            "[W3] RNN Epoch 16 | train_loss 0.7804 acc 0.635 f1 0.626 || val_loss 1.1304 acc 0.377 f1 0.331\n",
            "[W3] RNN Epoch 17 | train_loss 0.7762 acc 0.630 f1 0.621 || val_loss 1.1353 acc 0.372 f1 0.328\n",
            "[W3] RNN Epoch 18 | train_loss 0.7531 acc 0.652 f1 0.645 || val_loss 1.1621 acc 0.362 f1 0.324\n",
            "[W3] RNN Epoch 19 | train_loss 0.7335 acc 0.662 f1 0.654 || val_loss 1.1724 acc 0.366 f1 0.329\n",
            "[W3] RNN Epoch 20 | train_loss 0.7122 acc 0.667 f1 0.659 || val_loss 1.1875 acc 0.364 f1 0.330\n",
            "[W3] RNN Epoch 21 | train_loss 0.6933 acc 0.678 f1 0.670 || val_loss 1.1865 acc 0.387 f1 0.333\n",
            "[W3] RNN Epoch 22 | train_loss 0.6825 acc 0.672 f1 0.664 || val_loss 1.1925 acc 0.391 f1 0.350\n",
            "[W3] RNN Epoch 23 | train_loss 0.6540 acc 0.695 f1 0.689 || val_loss 1.2271 acc 0.374 f1 0.338\n",
            "[W3] RNN Epoch 24 | train_loss 0.6415 acc 0.701 f1 0.695 || val_loss 1.2262 acc 0.391 f1 0.346\n",
            "[W3] RNN Epoch 25 | train_loss 0.6186 acc 0.730 f1 0.725 || val_loss 1.2446 acc 0.399 f1 0.364\n",
            "[W3] RNN Epoch 26 | train_loss 0.5981 acc 0.718 f1 0.712 || val_loss 1.2483 acc 0.395 f1 0.346\n",
            "[W3] RNN Epoch 27 | train_loss 0.5986 acc 0.721 f1 0.716 || val_loss 1.2668 acc 0.395 f1 0.339\n",
            "[W3] RNN Epoch 28 | train_loss 0.5737 acc 0.738 f1 0.734 || val_loss 1.2958 acc 0.395 f1 0.344\n",
            "[W3] RNN Epoch 29 | train_loss 0.5571 acc 0.741 f1 0.737 || val_loss 1.2715 acc 0.405 f1 0.354\n",
            "[W3] RNN Epoch 30 | train_loss 0.5443 acc 0.748 f1 0.745 || val_loss 1.2935 acc 0.405 f1 0.356\n",
            "[W3] RNN Epoch 31 | train_loss 0.5201 acc 0.757 f1 0.753 || val_loss 1.3287 acc 0.393 f1 0.348\n",
            "[W3] RNN Epoch 32 | train_loss 0.5029 acc 0.774 f1 0.771 || val_loss 1.3723 acc 0.381 f1 0.328\n",
            "[W3] RNN Epoch 33 | train_loss 0.4982 acc 0.773 f1 0.770 || val_loss 1.3474 acc 0.409 f1 0.361\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=10\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1009 acc 0.337 f1 0.257 || val_loss 1.0972 acc 0.342 f1 0.241\n",
            "[W3] GRU Epoch 02 | train_loss 1.0911 acc 0.399 f1 0.367 || val_loss 1.0894 acc 0.350 f1 0.322\n",
            "[W3] GRU Epoch 03 | train_loss 1.0817 acc 0.411 f1 0.408 || val_loss 1.0840 acc 0.346 f1 0.319\n",
            "[W3] GRU Epoch 04 | train_loss 1.0682 acc 0.443 f1 0.439 || val_loss 1.0870 acc 0.352 f1 0.331\n",
            "[W3] GRU Epoch 05 | train_loss 1.0517 acc 0.462 f1 0.453 || val_loss 1.0905 acc 0.340 f1 0.320\n",
            "[W3] GRU Epoch 06 | train_loss 1.0211 acc 0.485 f1 0.472 || val_loss 1.0593 acc 0.399 f1 0.339\n",
            "[W3] GRU Epoch 07 | train_loss 0.9721 acc 0.534 f1 0.526 || val_loss 1.0810 acc 0.360 f1 0.321\n",
            "[W3] GRU Epoch 08 | train_loss 0.9065 acc 0.567 f1 0.556 || val_loss 1.0845 acc 0.377 f1 0.318\n",
            "[W3] GRU Epoch 09 | train_loss 0.8372 acc 0.605 f1 0.600 || val_loss 1.1028 acc 0.399 f1 0.334\n",
            "[W3] GRU Epoch 10 | train_loss 0.7879 acc 0.634 f1 0.629 || val_loss 1.1304 acc 0.381 f1 0.318\n",
            "[W3] GRU Epoch 11 | train_loss 0.7440 acc 0.644 f1 0.640 || val_loss 1.1811 acc 0.395 f1 0.330\n",
            "[W3] GRU Epoch 12 | train_loss 0.7018 acc 0.673 f1 0.668 || val_loss 1.1978 acc 0.393 f1 0.324\n",
            "[W3] GRU Epoch 13 | train_loss 0.6709 acc 0.672 f1 0.669 || val_loss 1.2220 acc 0.383 f1 0.308\n",
            "[W3] GRU Epoch 14 | train_loss 0.6382 acc 0.692 f1 0.689 || val_loss 1.2730 acc 0.405 f1 0.330\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=10\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0991 acc 0.346 f1 0.215 || val_loss 1.1053 acc 0.245 f1 0.245\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0913 acc 0.394 f1 0.381 || val_loss 1.0877 acc 0.337 f1 0.314\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0819 acc 0.404 f1 0.403 || val_loss 1.0848 acc 0.317 f1 0.295\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0663 acc 0.447 f1 0.434 || val_loss 1.0760 acc 0.344 f1 0.327\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0432 acc 0.478 f1 0.468 || val_loss 1.0609 acc 0.360 f1 0.331\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0041 acc 0.511 f1 0.504 || val_loss 1.0753 acc 0.356 f1 0.326\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9319 acc 0.561 f1 0.551 || val_loss 1.0897 acc 0.358 f1 0.296\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8535 acc 0.572 f1 0.566 || val_loss 1.1215 acc 0.356 f1 0.282\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7988 acc 0.606 f1 0.600 || val_loss 1.1713 acc 0.356 f1 0.296\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7505 acc 0.635 f1 0.628 || val_loss 1.1857 acc 0.372 f1 0.311\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7081 acc 0.658 f1 0.651 || val_loss 1.2140 acc 0.381 f1 0.317\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6663 acc 0.686 f1 0.681 || val_loss 1.2563 acc 0.403 f1 0.329\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6368 acc 0.690 f1 0.686 || val_loss 1.2945 acc 0.414 f1 0.334\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5985 acc 0.719 f1 0.716 || val_loss 1.3654 acc 0.383 f1 0.309\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5774 acc 0.717 f1 0.714 || val_loss 1.3738 acc 0.412 f1 0.328\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5436 acc 0.736 f1 0.733 || val_loss 1.4227 acc 0.399 f1 0.323\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5227 acc 0.742 f1 0.740 || val_loss 1.4782 acc 0.395 f1 0.320\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5014 acc 0.759 f1 0.757 || val_loss 1.5123 acc 0.393 f1 0.323\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4941 acc 0.750 f1 0.748 || val_loss 1.5839 acc 0.397 f1 0.312\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4691 acc 0.768 f1 0.768 || val_loss 1.6350 acc 0.391 f1 0.312\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4467 acc 0.781 f1 0.780 || val_loss 1.6723 acc 0.397 f1 0.317\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  10%|         | 10/100 [04:51<44:15, 29.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=11 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=11\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1346 acc 0.419 f1 0.415 || val_loss 1.1402 acc 0.319 f1 0.305\n",
            "[W3] ANN Epoch 02 | train_loss 0.9729 acc 0.509 f1 0.501 || val_loss 1.1289 acc 0.344 f1 0.318\n",
            "[W3] ANN Epoch 03 | train_loss 0.8719 acc 0.568 f1 0.561 || val_loss 1.1117 acc 0.401 f1 0.369\n",
            "[W3] ANN Epoch 04 | train_loss 0.8038 acc 0.619 f1 0.615 || val_loss 1.1139 acc 0.401 f1 0.341\n",
            "[W3] ANN Epoch 05 | train_loss 0.7354 acc 0.648 f1 0.646 || val_loss 1.1060 acc 0.426 f1 0.356\n",
            "[W3] ANN Epoch 06 | train_loss 0.7013 acc 0.659 f1 0.656 || val_loss 1.1557 acc 0.405 f1 0.354\n",
            "[W3] ANN Epoch 07 | train_loss 0.6790 acc 0.674 f1 0.670 || val_loss 1.1292 acc 0.436 f1 0.371\n",
            "[W3] ANN Epoch 08 | train_loss 0.6253 acc 0.704 f1 0.702 || val_loss 1.1775 acc 0.395 f1 0.331\n",
            "[W3] ANN Epoch 09 | train_loss 0.6070 acc 0.713 f1 0.712 || val_loss 1.1851 acc 0.414 f1 0.342\n",
            "[W3] ANN Epoch 10 | train_loss 0.5853 acc 0.720 f1 0.719 || val_loss 1.1851 acc 0.434 f1 0.362\n",
            "[W3] ANN Epoch 11 | train_loss 0.5636 acc 0.743 f1 0.741 || val_loss 1.2435 acc 0.418 f1 0.343\n",
            "[W3] ANN Epoch 12 | train_loss 0.5255 acc 0.765 f1 0.764 || val_loss 1.2217 acc 0.422 f1 0.344\n",
            "[W3] ANN Epoch 13 | train_loss 0.4952 acc 0.778 f1 0.777 || val_loss 1.2647 acc 0.434 f1 0.359\n",
            "[W3] ANN Epoch 14 | train_loss 0.4929 acc 0.779 f1 0.778 || val_loss 1.2785 acc 0.442 f1 0.374\n",
            "[W3] ANN Epoch 15 | train_loss 0.4662 acc 0.789 f1 0.788 || val_loss 1.2740 acc 0.424 f1 0.350\n",
            "[W3] ANN Epoch 16 | train_loss 0.4750 acc 0.789 f1 0.788 || val_loss 1.3092 acc 0.438 f1 0.367\n",
            "[W3] ANN Epoch 17 | train_loss 0.4693 acc 0.797 f1 0.797 || val_loss 1.3293 acc 0.442 f1 0.369\n",
            "[W3] ANN Epoch 18 | train_loss 0.4262 acc 0.814 f1 0.813 || val_loss 1.3110 acc 0.449 f1 0.380\n",
            "[W3] ANN Epoch 19 | train_loss 0.3987 acc 0.822 f1 0.822 || val_loss 1.3255 acc 0.469 f1 0.396\n",
            "[W3] ANN Epoch 20 | train_loss 0.4143 acc 0.828 f1 0.827 || val_loss 1.3575 acc 0.471 f1 0.422\n",
            "[W3] ANN Epoch 21 | train_loss 0.4020 acc 0.828 f1 0.828 || val_loss 1.3951 acc 0.469 f1 0.389\n",
            "[W3] ANN Epoch 22 | train_loss 0.3949 acc 0.832 f1 0.831 || val_loss 1.4397 acc 0.438 f1 0.377\n",
            "[W3] ANN Epoch 23 | train_loss 0.3546 acc 0.854 f1 0.853 || val_loss 1.5012 acc 0.428 f1 0.358\n",
            "[W3] ANN Epoch 24 | train_loss 0.3545 acc 0.862 f1 0.862 || val_loss 1.5328 acc 0.432 f1 0.361\n",
            "[W3] ANN Epoch 25 | train_loss 0.3644 acc 0.845 f1 0.845 || val_loss 1.5068 acc 0.444 f1 0.365\n",
            "[W3] ANN Epoch 26 | train_loss 0.3921 acc 0.838 f1 0.838 || val_loss 1.4497 acc 0.467 f1 0.389\n",
            "[W3] ANN Epoch 27 | train_loss 0.3412 acc 0.857 f1 0.857 || val_loss 1.4667 acc 0.455 f1 0.376\n",
            "[W3] ANN Epoch 28 | train_loss 0.3563 acc 0.853 f1 0.853 || val_loss 1.4892 acc 0.457 f1 0.365\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=11\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0720 acc 0.411 f1 0.414 || val_loss 1.0459 acc 0.420 f1 0.337\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9359 acc 0.541 f1 0.538 || val_loss 1.0606 acc 0.383 f1 0.287\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8095 acc 0.601 f1 0.598 || val_loss 1.1046 acc 0.420 f1 0.359\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7261 acc 0.645 f1 0.644 || val_loss 1.1849 acc 0.420 f1 0.348\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6618 acc 0.684 f1 0.681 || val_loss 1.2039 acc 0.416 f1 0.337\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5961 acc 0.713 f1 0.711 || val_loss 1.2460 acc 0.403 f1 0.339\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5537 acc 0.741 f1 0.740 || val_loss 1.2992 acc 0.405 f1 0.345\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4973 acc 0.783 f1 0.782 || val_loss 1.3448 acc 0.383 f1 0.304\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4575 acc 0.784 f1 0.783 || val_loss 1.4062 acc 0.389 f1 0.316\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4330 acc 0.811 f1 0.811 || val_loss 1.4285 acc 0.393 f1 0.323\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3770 acc 0.838 f1 0.838 || val_loss 1.5280 acc 0.397 f1 0.332\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=11\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0960 acc 0.365 f1 0.334 || val_loss 1.0999 acc 0.288 f1 0.277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0782 acc 0.426 f1 0.410 || val_loss 1.0903 acc 0.342 f1 0.316\n",
            "[W3] RNN Epoch 03 | train_loss 1.0618 acc 0.458 f1 0.449 || val_loss 1.0737 acc 0.360 f1 0.322\n",
            "[W3] RNN Epoch 04 | train_loss 1.0403 acc 0.473 f1 0.463 || val_loss 1.0730 acc 0.342 f1 0.315\n",
            "[W3] RNN Epoch 05 | train_loss 1.0208 acc 0.496 f1 0.490 || val_loss 1.0712 acc 0.325 f1 0.297\n",
            "[W3] RNN Epoch 06 | train_loss 0.9941 acc 0.515 f1 0.503 || val_loss 1.0808 acc 0.344 f1 0.318\n",
            "[W3] RNN Epoch 07 | train_loss 0.9646 acc 0.537 f1 0.527 || val_loss 1.0595 acc 0.370 f1 0.331\n",
            "[W3] RNN Epoch 08 | train_loss 0.9494 acc 0.538 f1 0.529 || val_loss 1.0705 acc 0.377 f1 0.342\n",
            "[W3] RNN Epoch 09 | train_loss 0.9075 acc 0.568 f1 0.559 || val_loss 1.0995 acc 0.358 f1 0.328\n",
            "[W3] RNN Epoch 10 | train_loss 0.8920 acc 0.575 f1 0.564 || val_loss 1.0812 acc 0.377 f1 0.343\n",
            "[W3] RNN Epoch 11 | train_loss 0.8714 acc 0.592 f1 0.584 || val_loss 1.0872 acc 0.370 f1 0.333\n",
            "[W3] RNN Epoch 12 | train_loss 0.8424 acc 0.600 f1 0.589 || val_loss 1.0885 acc 0.389 f1 0.340\n",
            "[W3] RNN Epoch 13 | train_loss 0.8201 acc 0.623 f1 0.615 || val_loss 1.1032 acc 0.389 f1 0.349\n",
            "[W3] RNN Epoch 14 | train_loss 0.7973 acc 0.627 f1 0.618 || val_loss 1.0909 acc 0.409 f1 0.350\n",
            "[W3] RNN Epoch 15 | train_loss 0.7820 acc 0.628 f1 0.620 || val_loss 1.1138 acc 0.383 f1 0.340\n",
            "[W3] RNN Epoch 16 | train_loss 0.7541 acc 0.643 f1 0.635 || val_loss 1.1194 acc 0.385 f1 0.331\n",
            "[W3] RNN Epoch 17 | train_loss 0.7239 acc 0.660 f1 0.653 || val_loss 1.1415 acc 0.391 f1 0.339\n",
            "[W3] RNN Epoch 18 | train_loss 0.7069 acc 0.664 f1 0.657 || val_loss 1.1303 acc 0.395 f1 0.336\n",
            "[W3] RNN Epoch 19 | train_loss 0.6915 acc 0.665 f1 0.658 || val_loss 1.1496 acc 0.383 f1 0.326\n",
            "[W3] RNN Epoch 20 | train_loss 0.6771 acc 0.680 f1 0.672 || val_loss 1.1704 acc 0.387 f1 0.331\n",
            "[W3] RNN Epoch 21 | train_loss 0.6356 acc 0.703 f1 0.697 || val_loss 1.1865 acc 0.387 f1 0.339\n",
            "[W3] RNN Epoch 22 | train_loss 0.6386 acc 0.696 f1 0.690 || val_loss 1.2027 acc 0.387 f1 0.338\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=11\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0983 acc 0.354 f1 0.301 || val_loss 1.0953 acc 0.368 f1 0.323\n",
            "[W3] GRU Epoch 02 | train_loss 1.0918 acc 0.396 f1 0.381 || val_loss 1.0922 acc 0.348 f1 0.322\n",
            "[W3] GRU Epoch 03 | train_loss 1.0849 acc 0.416 f1 0.408 || val_loss 1.0908 acc 0.358 f1 0.341\n",
            "[W3] GRU Epoch 04 | train_loss 1.0721 acc 0.437 f1 0.430 || val_loss 1.0830 acc 0.346 f1 0.321\n",
            "[W3] GRU Epoch 05 | train_loss 1.0529 acc 0.457 f1 0.451 || val_loss 1.0939 acc 0.340 f1 0.323\n",
            "[W3] GRU Epoch 06 | train_loss 1.0126 acc 0.490 f1 0.478 || val_loss 1.1226 acc 0.315 f1 0.299\n",
            "[W3] GRU Epoch 07 | train_loss 0.9416 acc 0.549 f1 0.537 || val_loss 1.0688 acc 0.370 f1 0.316\n",
            "[W3] GRU Epoch 08 | train_loss 0.8648 acc 0.572 f1 0.564 || val_loss 1.1221 acc 0.364 f1 0.313\n",
            "[W3] GRU Epoch 09 | train_loss 0.8008 acc 0.599 f1 0.592 || val_loss 1.1202 acc 0.368 f1 0.320\n",
            "[W3] GRU Epoch 10 | train_loss 0.7594 acc 0.626 f1 0.619 || val_loss 1.1486 acc 0.360 f1 0.309\n",
            "[W3] GRU Epoch 11 | train_loss 0.7236 acc 0.643 f1 0.640 || val_loss 1.1748 acc 0.374 f1 0.316\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=11\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0990 acc 0.342 f1 0.232 || val_loss 1.0971 acc 0.358 f1 0.306\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0919 acc 0.394 f1 0.390 || val_loss 1.1016 acc 0.307 f1 0.302\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0826 acc 0.419 f1 0.411 || val_loss 1.0914 acc 0.346 f1 0.329\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0681 acc 0.424 f1 0.413 || val_loss 1.0836 acc 0.317 f1 0.304\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0362 acc 0.474 f1 0.461 || val_loss 1.0736 acc 0.344 f1 0.323\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9622 acc 0.527 f1 0.518 || val_loss 1.1082 acc 0.337 f1 0.306\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8813 acc 0.560 f1 0.544 || val_loss 1.1020 acc 0.381 f1 0.313\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8247 acc 0.580 f1 0.572 || val_loss 1.1260 acc 0.356 f1 0.290\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7693 acc 0.616 f1 0.607 || val_loss 1.1562 acc 0.383 f1 0.303\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7311 acc 0.635 f1 0.631 || val_loss 1.2026 acc 0.356 f1 0.296\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6972 acc 0.651 f1 0.644 || val_loss 1.2268 acc 0.383 f1 0.319\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  11%|         | 11/100 [05:16<41:41, 28.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=12 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=12\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 986, np.int64(1): 930, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 986, np.int64(2): 986, np.int64(0): 986})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1439 acc 0.384 f1 0.385 || val_loss 1.1177 acc 0.317 f1 0.288\n",
            "[W3] ANN Epoch 02 | train_loss 0.9767 acc 0.506 f1 0.499 || val_loss 1.1231 acc 0.358 f1 0.328\n",
            "[W3] ANN Epoch 03 | train_loss 0.8719 acc 0.565 f1 0.555 || val_loss 1.1087 acc 0.391 f1 0.341\n",
            "[W3] ANN Epoch 04 | train_loss 0.7954 acc 0.613 f1 0.607 || val_loss 1.1081 acc 0.381 f1 0.328\n",
            "[W3] ANN Epoch 05 | train_loss 0.7467 acc 0.646 f1 0.640 || val_loss 1.1063 acc 0.385 f1 0.317\n",
            "[W3] ANN Epoch 06 | train_loss 0.6682 acc 0.681 f1 0.678 || val_loss 1.1425 acc 0.405 f1 0.339\n",
            "[W3] ANN Epoch 07 | train_loss 0.6353 acc 0.697 f1 0.695 || val_loss 1.1472 acc 0.412 f1 0.341\n",
            "[W3] ANN Epoch 08 | train_loss 0.5888 acc 0.722 f1 0.720 || val_loss 1.1522 acc 0.395 f1 0.316\n",
            "[W3] ANN Epoch 09 | train_loss 0.5765 acc 0.720 f1 0.719 || val_loss 1.1925 acc 0.409 f1 0.297\n",
            "[W3] ANN Epoch 10 | train_loss 0.5432 acc 0.748 f1 0.747 || val_loss 1.2368 acc 0.407 f1 0.347\n",
            "[W3] ANN Epoch 11 | train_loss 0.5453 acc 0.753 f1 0.752 || val_loss 1.2382 acc 0.447 f1 0.343\n",
            "[W3] ANN Epoch 12 | train_loss 0.5414 acc 0.748 f1 0.747 || val_loss 1.2188 acc 0.430 f1 0.338\n",
            "[W3] ANN Epoch 13 | train_loss 0.4865 acc 0.781 f1 0.780 || val_loss 1.2405 acc 0.434 f1 0.327\n",
            "[W3] ANN Epoch 14 | train_loss 0.4649 acc 0.800 f1 0.799 || val_loss 1.2856 acc 0.438 f1 0.362\n",
            "[W3] ANN Epoch 15 | train_loss 0.4479 acc 0.799 f1 0.799 || val_loss 1.3078 acc 0.449 f1 0.359\n",
            "[W3] ANN Epoch 16 | train_loss 0.4394 acc 0.804 f1 0.803 || val_loss 1.3425 acc 0.440 f1 0.357\n",
            "[W3] ANN Epoch 17 | train_loss 0.4030 acc 0.829 f1 0.829 || val_loss 1.3775 acc 0.461 f1 0.348\n",
            "[W3] ANN Epoch 18 | train_loss 0.3993 acc 0.826 f1 0.826 || val_loss 1.3666 acc 0.444 f1 0.342\n",
            "[W3] ANN Epoch 19 | train_loss 0.3883 acc 0.836 f1 0.835 || val_loss 1.3844 acc 0.457 f1 0.376\n",
            "[W3] ANN Epoch 20 | train_loss 0.3700 acc 0.845 f1 0.845 || val_loss 1.4384 acc 0.473 f1 0.389\n",
            "[W3] ANN Epoch 21 | train_loss 0.3689 acc 0.846 f1 0.846 || val_loss 1.4398 acc 0.486 f1 0.397\n",
            "[W3] ANN Epoch 22 | train_loss 0.3546 acc 0.867 f1 0.867 || val_loss 1.4202 acc 0.463 f1 0.379\n",
            "[W3] ANN Epoch 23 | train_loss 0.3448 acc 0.856 f1 0.856 || val_loss 1.5281 acc 0.449 f1 0.351\n",
            "[W3] ANN Epoch 24 | train_loss 0.3113 acc 0.871 f1 0.871 || val_loss 1.5597 acc 0.426 f1 0.346\n",
            "[W3] ANN Epoch 25 | train_loss 0.3246 acc 0.862 f1 0.862 || val_loss 1.5287 acc 0.469 f1 0.392\n",
            "[W3] ANN Epoch 26 | train_loss 0.3078 acc 0.871 f1 0.870 || val_loss 1.6113 acc 0.440 f1 0.354\n",
            "[W3] ANN Epoch 27 | train_loss 0.3048 acc 0.877 f1 0.876 || val_loss 1.5957 acc 0.453 f1 0.368\n",
            "[W3] ANN Epoch 28 | train_loss 0.3186 acc 0.865 f1 0.864 || val_loss 1.5760 acc 0.440 f1 0.367\n",
            "[W3] ANN Epoch 29 | train_loss 0.2668 acc 0.894 f1 0.894 || val_loss 1.5960 acc 0.457 f1 0.375\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=12\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 986, np.int64(1): 930, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 986, np.int64(2): 986, np.int64(0): 986})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0781 acc 0.398 f1 0.399 || val_loss 1.0413 acc 0.401 f1 0.305\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9538 acc 0.539 f1 0.536 || val_loss 1.0574 acc 0.416 f1 0.335\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8171 acc 0.601 f1 0.597 || val_loss 1.1052 acc 0.403 f1 0.327\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7258 acc 0.655 f1 0.653 || val_loss 1.1282 acc 0.385 f1 0.303\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6453 acc 0.696 f1 0.695 || val_loss 1.2397 acc 0.377 f1 0.306\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5616 acc 0.749 f1 0.748 || val_loss 1.3178 acc 0.393 f1 0.327\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5226 acc 0.765 f1 0.764 || val_loss 1.3061 acc 0.399 f1 0.323\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4665 acc 0.792 f1 0.791 || val_loss 1.3662 acc 0.412 f1 0.310\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4232 acc 0.815 f1 0.815 || val_loss 1.5106 acc 0.385 f1 0.331\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4080 acc 0.836 f1 0.836 || val_loss 1.4870 acc 0.407 f1 0.342\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3435 acc 0.860 f1 0.859 || val_loss 1.5759 acc 0.391 f1 0.321\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3027 acc 0.877 f1 0.876 || val_loss 1.6503 acc 0.383 f1 0.310\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2718 acc 0.897 f1 0.897 || val_loss 1.7809 acc 0.414 f1 0.333\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2493 acc 0.905 f1 0.905 || val_loss 1.8592 acc 0.381 f1 0.307\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2220 acc 0.919 f1 0.919 || val_loss 1.8839 acc 0.426 f1 0.333\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2010 acc 0.931 f1 0.931 || val_loss 1.9666 acc 0.420 f1 0.347\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1761 acc 0.936 f1 0.936 || val_loss 2.0812 acc 0.405 f1 0.332\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1656 acc 0.940 f1 0.940 || val_loss 2.1629 acc 0.403 f1 0.344\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1345 acc 0.960 f1 0.960 || val_loss 2.2476 acc 0.407 f1 0.336\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1181 acc 0.964 f1 0.964 || val_loss 2.3564 acc 0.403 f1 0.338\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.1034 acc 0.965 f1 0.965 || val_loss 2.4649 acc 0.389 f1 0.326\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1103 acc 0.963 f1 0.963 || val_loss 2.4864 acc 0.387 f1 0.313\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.1117 acc 0.966 f1 0.966 || val_loss 2.3746 acc 0.426 f1 0.349\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.0798 acc 0.978 f1 0.978 || val_loss 2.6072 acc 0.405 f1 0.337\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.0832 acc 0.973 f1 0.973 || val_loss 2.5681 acc 0.422 f1 0.344\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.0701 acc 0.979 f1 0.979 || val_loss 2.6547 acc 0.440 f1 0.363\n",
            "[W3] CNN1D Epoch 27 | train_loss 0.0734 acc 0.977 f1 0.977 || val_loss 2.8107 acc 0.397 f1 0.326\n",
            "[W3] CNN1D Epoch 28 | train_loss 0.1029 acc 0.961 f1 0.961 || val_loss 2.7432 acc 0.418 f1 0.345\n",
            "[W3] CNN1D Epoch 29 | train_loss 0.0680 acc 0.977 f1 0.977 || val_loss 2.8220 acc 0.418 f1 0.339\n",
            "[W3] CNN1D Epoch 30 | train_loss 0.0711 acc 0.977 f1 0.977 || val_loss 2.7204 acc 0.432 f1 0.354\n",
            "[W3] CNN1D Epoch 31 | train_loss 0.0680 acc 0.979 f1 0.979 || val_loss 2.7920 acc 0.405 f1 0.338\n",
            "[W3] CNN1D Epoch 32 | train_loss 0.0600 acc 0.980 f1 0.980 || val_loss 2.9505 acc 0.414 f1 0.341\n",
            "[W3] CNN1D Epoch 33 | train_loss 0.0404 acc 0.989 f1 0.988 || val_loss 3.0276 acc 0.422 f1 0.343\n",
            "[W3] CNN1D Epoch 34 | train_loss 0.0568 acc 0.978 f1 0.978 || val_loss 3.0490 acc 0.430 f1 0.350\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=12\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 986, np.int64(1): 930, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 986, np.int64(2): 986, np.int64(0): 986})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0956 acc 0.376 f1 0.332 || val_loss 1.1013 acc 0.313 f1 0.288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0760 acc 0.412 f1 0.391 || val_loss 1.0955 acc 0.311 f1 0.294\n",
            "[W3] RNN Epoch 03 | train_loss 1.0616 acc 0.418 f1 0.405 || val_loss 1.0807 acc 0.344 f1 0.316\n",
            "[W3] RNN Epoch 04 | train_loss 1.0468 acc 0.451 f1 0.447 || val_loss 1.1016 acc 0.327 f1 0.310\n",
            "[W3] RNN Epoch 05 | train_loss 1.0283 acc 0.474 f1 0.468 || val_loss 1.0868 acc 0.333 f1 0.309\n",
            "[W3] RNN Epoch 06 | train_loss 1.0051 acc 0.500 f1 0.493 || val_loss 1.0875 acc 0.346 f1 0.318\n",
            "[W3] RNN Epoch 07 | train_loss 0.9836 acc 0.516 f1 0.506 || val_loss 1.0979 acc 0.323 f1 0.297\n",
            "[W3] RNN Epoch 08 | train_loss 0.9611 acc 0.534 f1 0.525 || val_loss 1.0796 acc 0.356 f1 0.309\n",
            "[W3] RNN Epoch 09 | train_loss 0.9367 acc 0.551 f1 0.545 || val_loss 1.1016 acc 0.327 f1 0.297\n",
            "[W3] RNN Epoch 10 | train_loss 0.9147 acc 0.557 f1 0.548 || val_loss 1.0992 acc 0.360 f1 0.321\n",
            "[W3] RNN Epoch 11 | train_loss 0.8853 acc 0.584 f1 0.575 || val_loss 1.1050 acc 0.362 f1 0.316\n",
            "[W3] RNN Epoch 12 | train_loss 0.8690 acc 0.586 f1 0.577 || val_loss 1.1126 acc 0.350 f1 0.307\n",
            "[W3] RNN Epoch 13 | train_loss 0.8404 acc 0.594 f1 0.585 || val_loss 1.1141 acc 0.350 f1 0.304\n",
            "[W3] RNN Epoch 14 | train_loss 0.8151 acc 0.614 f1 0.605 || val_loss 1.1075 acc 0.381 f1 0.328\n",
            "[W3] RNN Epoch 15 | train_loss 0.7921 acc 0.620 f1 0.610 || val_loss 1.1230 acc 0.377 f1 0.325\n",
            "[W3] RNN Epoch 16 | train_loss 0.7667 acc 0.637 f1 0.629 || val_loss 1.1223 acc 0.391 f1 0.339\n",
            "[W3] RNN Epoch 17 | train_loss 0.7449 acc 0.647 f1 0.639 || val_loss 1.1501 acc 0.389 f1 0.343\n",
            "[W3] RNN Epoch 18 | train_loss 0.7284 acc 0.650 f1 0.642 || val_loss 1.1577 acc 0.389 f1 0.343\n",
            "[W3] RNN Epoch 19 | train_loss 0.7126 acc 0.644 f1 0.635 || val_loss 1.1462 acc 0.405 f1 0.343\n",
            "[W3] RNN Epoch 20 | train_loss 0.6840 acc 0.676 f1 0.669 || val_loss 1.1667 acc 0.407 f1 0.346\n",
            "[W3] RNN Epoch 21 | train_loss 0.6606 acc 0.681 f1 0.675 || val_loss 1.1942 acc 0.397 f1 0.350\n",
            "[W3] RNN Epoch 22 | train_loss 0.6482 acc 0.690 f1 0.682 || val_loss 1.1932 acc 0.397 f1 0.333\n",
            "[W3] RNN Epoch 23 | train_loss 0.6416 acc 0.692 f1 0.686 || val_loss 1.2084 acc 0.420 f1 0.365\n",
            "[W3] RNN Epoch 24 | train_loss 0.6249 acc 0.702 f1 0.696 || val_loss 1.2265 acc 0.422 f1 0.365\n",
            "[W3] RNN Epoch 25 | train_loss 0.6022 acc 0.708 f1 0.702 || val_loss 1.2385 acc 0.422 f1 0.357\n",
            "[W3] RNN Epoch 26 | train_loss 0.5794 acc 0.724 f1 0.719 || val_loss 1.2445 acc 0.447 f1 0.372\n",
            "[W3] RNN Epoch 27 | train_loss 0.5675 acc 0.730 f1 0.725 || val_loss 1.2761 acc 0.420 f1 0.358\n",
            "[W3] RNN Epoch 28 | train_loss 0.5500 acc 0.740 f1 0.736 || val_loss 1.2905 acc 0.434 f1 0.366\n",
            "[W3] RNN Epoch 29 | train_loss 0.5479 acc 0.746 f1 0.743 || val_loss 1.2986 acc 0.424 f1 0.369\n",
            "[W3] RNN Epoch 30 | train_loss 0.5391 acc 0.743 f1 0.739 || val_loss 1.3000 acc 0.432 f1 0.364\n",
            "[W3] RNN Epoch 31 | train_loss 0.5149 acc 0.749 f1 0.746 || val_loss 1.3316 acc 0.430 f1 0.371\n",
            "[W3] RNN Epoch 32 | train_loss 0.5106 acc 0.749 f1 0.746 || val_loss 1.3380 acc 0.436 f1 0.373\n",
            "[W3] RNN Epoch 33 | train_loss 0.5030 acc 0.762 f1 0.759 || val_loss 1.3580 acc 0.428 f1 0.362\n",
            "[W3] RNN Epoch 34 | train_loss 0.4859 acc 0.776 f1 0.774 || val_loss 1.3642 acc 0.432 f1 0.372\n",
            "[W3] RNN Epoch 35 | train_loss 0.4738 acc 0.773 f1 0.771 || val_loss 1.4001 acc 0.414 f1 0.347\n",
            "[W3] RNN Epoch 36 | train_loss 0.4713 acc 0.779 f1 0.776 || val_loss 1.4122 acc 0.428 f1 0.362\n",
            "[W3] RNN Epoch 37 | train_loss 0.4632 acc 0.785 f1 0.783 || val_loss 1.4394 acc 0.434 f1 0.370\n",
            "[W3] RNN Epoch 38 | train_loss 0.4473 acc 0.786 f1 0.784 || val_loss 1.4593 acc 0.442 f1 0.372\n",
            "[W3] RNN Epoch 39 | train_loss 0.4506 acc 0.781 f1 0.780 || val_loss 1.4587 acc 0.434 f1 0.370\n",
            "[W3] RNN Epoch 40 | train_loss 0.4354 acc 0.794 f1 0.792 || val_loss 1.4816 acc 0.442 f1 0.360\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=12\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 986, np.int64(1): 930, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 986, np.int64(2): 986, np.int64(0): 986})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0964 acc 0.353 f1 0.335 || val_loss 1.0898 acc 0.385 f1 0.349\n",
            "[W3] GRU Epoch 02 | train_loss 1.0883 acc 0.411 f1 0.411 || val_loss 1.0953 acc 0.315 f1 0.299\n",
            "[W3] GRU Epoch 03 | train_loss 1.0780 acc 0.433 f1 0.428 || val_loss 1.0921 acc 0.335 f1 0.318\n",
            "[W3] GRU Epoch 04 | train_loss 1.0594 acc 0.460 f1 0.460 || val_loss 1.1022 acc 0.333 f1 0.319\n",
            "[W3] GRU Epoch 05 | train_loss 1.0281 acc 0.502 f1 0.496 || val_loss 1.0732 acc 0.374 f1 0.326\n",
            "[W3] GRU Epoch 06 | train_loss 0.9664 acc 0.553 f1 0.546 || val_loss 1.0843 acc 0.362 f1 0.314\n",
            "[W3] GRU Epoch 07 | train_loss 0.8882 acc 0.567 f1 0.559 || val_loss 1.1269 acc 0.342 f1 0.301\n",
            "[W3] GRU Epoch 08 | train_loss 0.8219 acc 0.598 f1 0.591 || val_loss 1.1399 acc 0.362 f1 0.317\n",
            "[W3] GRU Epoch 09 | train_loss 0.7736 acc 0.638 f1 0.631 || val_loss 1.1359 acc 0.377 f1 0.310\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=12\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 986, np.int64(1): 930, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 986, np.int64(2): 986, np.int64(0): 986})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0983 acc 0.343 f1 0.328 || val_loss 1.1012 acc 0.272 f1 0.248\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0929 acc 0.395 f1 0.384 || val_loss 1.0989 acc 0.286 f1 0.281\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0836 acc 0.406 f1 0.381 || val_loss 1.0866 acc 0.335 f1 0.315\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0603 acc 0.456 f1 0.447 || val_loss 1.0795 acc 0.321 f1 0.291\n",
            "[W3] LSTM Epoch 05 | train_loss 0.9996 acc 0.504 f1 0.492 || val_loss 1.0589 acc 0.360 f1 0.306\n",
            "[W3] LSTM Epoch 06 | train_loss 0.8990 acc 0.553 f1 0.541 || val_loss 1.1174 acc 0.360 f1 0.309\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8314 acc 0.586 f1 0.579 || val_loss 1.1292 acc 0.370 f1 0.307\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7747 acc 0.602 f1 0.595 || val_loss 1.1483 acc 0.393 f1 0.318\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7358 acc 0.622 f1 0.614 || val_loss 1.1614 acc 0.401 f1 0.323\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7065 acc 0.624 f1 0.619 || val_loss 1.1840 acc 0.399 f1 0.324\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6671 acc 0.651 f1 0.647 || val_loss 1.2218 acc 0.407 f1 0.329\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6420 acc 0.661 f1 0.656 || val_loss 1.2690 acc 0.395 f1 0.329\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6142 acc 0.676 f1 0.672 || val_loss 1.2902 acc 0.401 f1 0.321\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5989 acc 0.683 f1 0.678 || val_loss 1.3233 acc 0.432 f1 0.338\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5753 acc 0.690 f1 0.687 || val_loss 1.3533 acc 0.407 f1 0.325\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5468 acc 0.713 f1 0.711 || val_loss 1.4118 acc 0.401 f1 0.336\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5335 acc 0.714 f1 0.713 || val_loss 1.4605 acc 0.416 f1 0.335\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5165 acc 0.721 f1 0.718 || val_loss 1.4735 acc 0.418 f1 0.335\n",
            "[W3] LSTM Epoch 19 | train_loss 0.5034 acc 0.728 f1 0.727 || val_loss 1.5205 acc 0.418 f1 0.337\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4880 acc 0.739 f1 0.738 || val_loss 1.5609 acc 0.385 f1 0.308\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4761 acc 0.738 f1 0.738 || val_loss 1.5911 acc 0.389 f1 0.311\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4631 acc 0.745 f1 0.745 || val_loss 1.6534 acc 0.393 f1 0.314\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  12%|        | 12/100 [05:55<46:04, 31.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=13 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=13\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 929, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1496 acc 0.398 f1 0.390 || val_loss 1.1841 acc 0.263 f1 0.255\n",
            "[W3] ANN Epoch 02 | train_loss 0.9954 acc 0.505 f1 0.496 || val_loss 1.1519 acc 0.317 f1 0.300\n",
            "[W3] ANN Epoch 03 | train_loss 0.9212 acc 0.554 f1 0.547 || val_loss 1.0916 acc 0.368 f1 0.334\n",
            "[W3] ANN Epoch 04 | train_loss 0.8392 acc 0.592 f1 0.585 || val_loss 1.0834 acc 0.389 f1 0.344\n",
            "[W3] ANN Epoch 05 | train_loss 0.7796 acc 0.620 f1 0.615 || val_loss 1.0727 acc 0.383 f1 0.319\n",
            "[W3] ANN Epoch 06 | train_loss 0.7307 acc 0.649 f1 0.645 || val_loss 1.0946 acc 0.387 f1 0.323\n",
            "[W3] ANN Epoch 07 | train_loss 0.7008 acc 0.666 f1 0.664 || val_loss 1.0995 acc 0.399 f1 0.335\n",
            "[W3] ANN Epoch 08 | train_loss 0.6706 acc 0.683 f1 0.681 || val_loss 1.1210 acc 0.414 f1 0.327\n",
            "[W3] ANN Epoch 09 | train_loss 0.6216 acc 0.713 f1 0.712 || val_loss 1.1370 acc 0.407 f1 0.331\n",
            "[W3] ANN Epoch 10 | train_loss 0.6181 acc 0.710 f1 0.708 || val_loss 1.1583 acc 0.381 f1 0.313\n",
            "[W3] ANN Epoch 11 | train_loss 0.5935 acc 0.718 f1 0.717 || val_loss 1.1670 acc 0.383 f1 0.304\n",
            "[W3] ANN Epoch 12 | train_loss 0.5792 acc 0.739 f1 0.738 || val_loss 1.1824 acc 0.362 f1 0.298\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=13\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 929, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0665 acc 0.418 f1 0.421 || val_loss 1.0322 acc 0.453 f1 0.377\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9287 acc 0.545 f1 0.538 || val_loss 1.0856 acc 0.397 f1 0.323\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8180 acc 0.606 f1 0.602 || val_loss 1.1274 acc 0.409 f1 0.332\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7137 acc 0.652 f1 0.648 || val_loss 1.2427 acc 0.401 f1 0.348\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6559 acc 0.685 f1 0.682 || val_loss 1.1933 acc 0.405 f1 0.321\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6061 acc 0.710 f1 0.709 || val_loss 1.2668 acc 0.422 f1 0.333\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5325 acc 0.762 f1 0.761 || val_loss 1.3764 acc 0.399 f1 0.330\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4928 acc 0.774 f1 0.773 || val_loss 1.4173 acc 0.387 f1 0.322\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4679 acc 0.790 f1 0.789 || val_loss 1.4712 acc 0.407 f1 0.326\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=13\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 929, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0993 acc 0.343 f1 0.317 || val_loss 1.0929 acc 0.397 f1 0.327\n",
            "[W3] RNN Epoch 02 | train_loss 1.0858 acc 0.406 f1 0.406 || val_loss 1.0898 acc 0.383 f1 0.346\n",
            "[W3] RNN Epoch 03 | train_loss 1.0710 acc 0.433 f1 0.429 || val_loss 1.0887 acc 0.354 f1 0.331\n",
            "[W3] RNN Epoch 04 | train_loss 1.0519 acc 0.453 f1 0.444 || val_loss 1.0718 acc 0.381 f1 0.350\n",
            "[W3] RNN Epoch 05 | train_loss 1.0269 acc 0.477 f1 0.471 || val_loss 1.0897 acc 0.329 f1 0.311\n",
            "[W3] RNN Epoch 06 | train_loss 1.0075 acc 0.486 f1 0.479 || val_loss 1.0850 acc 0.356 f1 0.332\n",
            "[W3] RNN Epoch 07 | train_loss 0.9883 acc 0.512 f1 0.504 || val_loss 1.0990 acc 0.337 f1 0.317\n",
            "[W3] RNN Epoch 08 | train_loss 0.9640 acc 0.535 f1 0.526 || val_loss 1.0823 acc 0.360 f1 0.325\n",
            "[W3] RNN Epoch 09 | train_loss 0.9314 acc 0.560 f1 0.550 || val_loss 1.0898 acc 0.348 f1 0.310\n",
            "[W3] RNN Epoch 10 | train_loss 0.9150 acc 0.570 f1 0.559 || val_loss 1.1035 acc 0.354 f1 0.319\n",
            "[W3] RNN Epoch 11 | train_loss 0.8890 acc 0.570 f1 0.560 || val_loss 1.0828 acc 0.381 f1 0.328\n",
            "[W3] RNN Epoch 12 | train_loss 0.8714 acc 0.588 f1 0.578 || val_loss 1.0946 acc 0.383 f1 0.328\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=13\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 929, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1027 acc 0.344 f1 0.280 || val_loss 1.1105 acc 0.267 f1 0.230\n",
            "[W3] GRU Epoch 02 | train_loss 1.0932 acc 0.396 f1 0.374 || val_loss 1.1012 acc 0.313 f1 0.304\n",
            "[W3] GRU Epoch 03 | train_loss 1.0862 acc 0.407 f1 0.398 || val_loss 1.0961 acc 0.315 f1 0.301\n",
            "[W3] GRU Epoch 04 | train_loss 1.0727 acc 0.421 f1 0.420 || val_loss 1.0876 acc 0.325 f1 0.302\n",
            "[W3] GRU Epoch 05 | train_loss 1.0524 acc 0.445 f1 0.441 || val_loss 1.0864 acc 0.325 f1 0.291\n",
            "[W3] GRU Epoch 06 | train_loss 1.0193 acc 0.488 f1 0.479 || val_loss 1.0888 acc 0.342 f1 0.299\n",
            "[W3] GRU Epoch 07 | train_loss 0.9537 acc 0.545 f1 0.539 || val_loss 1.1033 acc 0.356 f1 0.315\n",
            "[W3] GRU Epoch 08 | train_loss 0.8810 acc 0.573 f1 0.566 || val_loss 1.1204 acc 0.340 f1 0.294\n",
            "[W3] GRU Epoch 09 | train_loss 0.8339 acc 0.596 f1 0.585 || val_loss 1.1318 acc 0.342 f1 0.297\n",
            "[W3] GRU Epoch 10 | train_loss 0.7809 acc 0.614 f1 0.608 || val_loss 1.1744 acc 0.350 f1 0.316\n",
            "[W3] GRU Epoch 11 | train_loss 0.7438 acc 0.635 f1 0.628 || val_loss 1.1749 acc 0.360 f1 0.305\n",
            "[W3] GRU Epoch 12 | train_loss 0.7125 acc 0.654 f1 0.649 || val_loss 1.1791 acc 0.354 f1 0.304\n",
            "[W3] GRU Epoch 13 | train_loss 0.6784 acc 0.668 f1 0.664 || val_loss 1.2261 acc 0.344 f1 0.296\n",
            "[W3] GRU Epoch 14 | train_loss 0.6579 acc 0.674 f1 0.670 || val_loss 1.2453 acc 0.356 f1 0.309\n",
            "[W3] GRU Epoch 15 | train_loss 0.6251 acc 0.691 f1 0.688 || val_loss 1.2827 acc 0.366 f1 0.310\n",
            "[W3] GRU Epoch 16 | train_loss 0.5959 acc 0.705 f1 0.702 || val_loss 1.2866 acc 0.354 f1 0.284\n",
            "[W3] GRU Epoch 17 | train_loss 0.5843 acc 0.709 f1 0.707 || val_loss 1.3462 acc 0.360 f1 0.301\n",
            "[W3] GRU Epoch 18 | train_loss 0.5587 acc 0.732 f1 0.729 || val_loss 1.3789 acc 0.354 f1 0.289\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=13\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 929, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0992 acc 0.342 f1 0.230 || val_loss 1.1013 acc 0.296 f1 0.294\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0936 acc 0.388 f1 0.380 || val_loss 1.0931 acc 0.346 f1 0.307\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0873 acc 0.401 f1 0.400 || val_loss 1.0909 acc 0.352 f1 0.334\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0743 acc 0.432 f1 0.418 || val_loss 1.0870 acc 0.319 f1 0.295\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0477 acc 0.456 f1 0.443 || val_loss 1.0745 acc 0.340 f1 0.298\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9993 acc 0.510 f1 0.503 || val_loss 1.0854 acc 0.344 f1 0.300\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9138 acc 0.554 f1 0.543 || val_loss 1.1259 acc 0.358 f1 0.305\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8389 acc 0.588 f1 0.580 || val_loss 1.1414 acc 0.366 f1 0.299\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7902 acc 0.611 f1 0.606 || val_loss 1.2629 acc 0.335 f1 0.297\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7595 acc 0.629 f1 0.622 || val_loss 1.1797 acc 0.374 f1 0.304\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7064 acc 0.648 f1 0.642 || val_loss 1.2155 acc 0.389 f1 0.316\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  13%|        | 13/100 [06:14<39:56, 27.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=14 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=14\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 934, np.int64(0): 266})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1332 acc 0.408 f1 0.401 || val_loss 1.1550 acc 0.286 f1 0.282\n",
            "[W3] ANN Epoch 02 | train_loss 0.9726 acc 0.516 f1 0.506 || val_loss 1.1379 acc 0.346 f1 0.314\n",
            "[W3] ANN Epoch 03 | train_loss 0.8586 acc 0.572 f1 0.563 || val_loss 1.1573 acc 0.350 f1 0.318\n",
            "[W3] ANN Epoch 04 | train_loss 0.7757 acc 0.616 f1 0.611 || val_loss 1.1223 acc 0.370 f1 0.328\n",
            "[W3] ANN Epoch 05 | train_loss 0.6943 acc 0.668 f1 0.664 || val_loss 1.1717 acc 0.389 f1 0.340\n",
            "[W3] ANN Epoch 06 | train_loss 0.6626 acc 0.683 f1 0.680 || val_loss 1.1517 acc 0.416 f1 0.344\n",
            "[W3] ANN Epoch 07 | train_loss 0.6188 acc 0.713 f1 0.712 || val_loss 1.1856 acc 0.401 f1 0.337\n",
            "[W3] ANN Epoch 08 | train_loss 0.5817 acc 0.727 f1 0.726 || val_loss 1.1912 acc 0.436 f1 0.357\n",
            "[W3] ANN Epoch 09 | train_loss 0.5700 acc 0.732 f1 0.730 || val_loss 1.2252 acc 0.447 f1 0.379\n",
            "[W3] ANN Epoch 10 | train_loss 0.5256 acc 0.749 f1 0.748 || val_loss 1.2431 acc 0.432 f1 0.360\n",
            "[W3] ANN Epoch 11 | train_loss 0.5250 acc 0.758 f1 0.757 || val_loss 1.2409 acc 0.457 f1 0.373\n",
            "[W3] ANN Epoch 12 | train_loss 0.4817 acc 0.787 f1 0.786 || val_loss 1.2659 acc 0.459 f1 0.366\n",
            "[W3] ANN Epoch 13 | train_loss 0.4751 acc 0.780 f1 0.779 || val_loss 1.2614 acc 0.440 f1 0.357\n",
            "[W3] ANN Epoch 14 | train_loss 0.4572 acc 0.791 f1 0.791 || val_loss 1.2915 acc 0.461 f1 0.373\n",
            "[W3] ANN Epoch 15 | train_loss 0.4591 acc 0.803 f1 0.803 || val_loss 1.2742 acc 0.442 f1 0.359\n",
            "[W3] ANN Epoch 16 | train_loss 0.4433 acc 0.810 f1 0.809 || val_loss 1.3294 acc 0.453 f1 0.362\n",
            "[W3] ANN Epoch 17 | train_loss 0.4231 acc 0.814 f1 0.813 || val_loss 1.3135 acc 0.436 f1 0.354\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=14\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 934, np.int64(0): 266})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0810 acc 0.402 f1 0.403 || val_loss 1.0509 acc 0.397 f1 0.315\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9585 acc 0.548 f1 0.542 || val_loss 1.0341 acc 0.430 f1 0.327\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8097 acc 0.620 f1 0.617 || val_loss 1.1480 acc 0.397 f1 0.354\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7152 acc 0.671 f1 0.668 || val_loss 1.2043 acc 0.372 f1 0.324\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6236 acc 0.702 f1 0.700 || val_loss 1.2272 acc 0.405 f1 0.328\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5567 acc 0.745 f1 0.743 || val_loss 1.2608 acc 0.405 f1 0.324\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.4965 acc 0.787 f1 0.787 || val_loss 1.3619 acc 0.424 f1 0.345\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4415 acc 0.811 f1 0.810 || val_loss 1.4348 acc 0.391 f1 0.315\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4066 acc 0.833 f1 0.833 || val_loss 1.4953 acc 0.409 f1 0.322\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3655 acc 0.846 f1 0.846 || val_loss 1.5599 acc 0.414 f1 0.350\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3260 acc 0.871 f1 0.871 || val_loss 1.6406 acc 0.414 f1 0.323\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=14\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 934, np.int64(0): 266})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0995 acc 0.375 f1 0.308 || val_loss 1.1043 acc 0.317 f1 0.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0814 acc 0.436 f1 0.414 || val_loss 1.0995 acc 0.319 f1 0.282\n",
            "[W3] RNN Epoch 03 | train_loss 1.0674 acc 0.440 f1 0.417 || val_loss 1.0884 acc 0.323 f1 0.286\n",
            "[W3] RNN Epoch 04 | train_loss 1.0489 acc 0.467 f1 0.453 || val_loss 1.0859 acc 0.319 f1 0.292\n",
            "[W3] RNN Epoch 05 | train_loss 1.0246 acc 0.480 f1 0.471 || val_loss 1.0671 acc 0.383 f1 0.332\n",
            "[W3] RNN Epoch 06 | train_loss 1.0034 acc 0.498 f1 0.487 || val_loss 1.0986 acc 0.340 f1 0.312\n",
            "[W3] RNN Epoch 07 | train_loss 0.9757 acc 0.522 f1 0.510 || val_loss 1.0900 acc 0.364 f1 0.334\n",
            "[W3] RNN Epoch 08 | train_loss 0.9565 acc 0.537 f1 0.525 || val_loss 1.0883 acc 0.362 f1 0.333\n",
            "[W3] RNN Epoch 09 | train_loss 0.9370 acc 0.550 f1 0.540 || val_loss 1.1001 acc 0.352 f1 0.318\n",
            "[W3] RNN Epoch 10 | train_loss 0.9079 acc 0.572 f1 0.562 || val_loss 1.0956 acc 0.370 f1 0.334\n",
            "[W3] RNN Epoch 11 | train_loss 0.8810 acc 0.588 f1 0.578 || val_loss 1.0864 acc 0.362 f1 0.318\n",
            "[W3] RNN Epoch 12 | train_loss 0.8479 acc 0.609 f1 0.601 || val_loss 1.0890 acc 0.385 f1 0.330\n",
            "[W3] RNN Epoch 13 | train_loss 0.8296 acc 0.612 f1 0.602 || val_loss 1.0712 acc 0.409 f1 0.357\n",
            "[W3] RNN Epoch 14 | train_loss 0.8085 acc 0.618 f1 0.610 || val_loss 1.1066 acc 0.383 f1 0.336\n",
            "[W3] RNN Epoch 15 | train_loss 0.7774 acc 0.626 f1 0.618 || val_loss 1.0930 acc 0.420 f1 0.356\n",
            "[W3] RNN Epoch 16 | train_loss 0.7596 acc 0.643 f1 0.634 || val_loss 1.0966 acc 0.412 f1 0.351\n",
            "[W3] RNN Epoch 17 | train_loss 0.7317 acc 0.666 f1 0.660 || val_loss 1.1175 acc 0.405 f1 0.350\n",
            "[W3] RNN Epoch 18 | train_loss 0.7056 acc 0.666 f1 0.660 || val_loss 1.1502 acc 0.399 f1 0.350\n",
            "[W3] RNN Epoch 19 | train_loss 0.6897 acc 0.685 f1 0.677 || val_loss 1.1219 acc 0.414 f1 0.360\n",
            "[W3] RNN Epoch 20 | train_loss 0.6621 acc 0.697 f1 0.691 || val_loss 1.1630 acc 0.385 f1 0.340\n",
            "[W3] RNN Epoch 21 | train_loss 0.6436 acc 0.690 f1 0.683 || val_loss 1.1633 acc 0.416 f1 0.367\n",
            "[W3] RNN Epoch 22 | train_loss 0.6287 acc 0.698 f1 0.692 || val_loss 1.1630 acc 0.434 f1 0.371\n",
            "[W3] RNN Epoch 23 | train_loss 0.6075 acc 0.722 f1 0.719 || val_loss 1.1985 acc 0.418 f1 0.357\n",
            "[W3] RNN Epoch 24 | train_loss 0.5949 acc 0.709 f1 0.703 || val_loss 1.2060 acc 0.403 f1 0.335\n",
            "[W3] RNN Epoch 25 | train_loss 0.5794 acc 0.722 f1 0.717 || val_loss 1.2301 acc 0.447 f1 0.367\n",
            "[W3] RNN Epoch 26 | train_loss 0.5565 acc 0.732 f1 0.728 || val_loss 1.2300 acc 0.403 f1 0.335\n",
            "[W3] RNN Epoch 27 | train_loss 0.5535 acc 0.729 f1 0.725 || val_loss 1.2724 acc 0.403 f1 0.342\n",
            "[W3] RNN Epoch 28 | train_loss 0.5376 acc 0.746 f1 0.742 || val_loss 1.2881 acc 0.416 f1 0.340\n",
            "[W3] RNN Epoch 29 | train_loss 0.5237 acc 0.761 f1 0.759 || val_loss 1.3151 acc 0.416 f1 0.343\n",
            "[W3] RNN Epoch 30 | train_loss 0.5065 acc 0.753 f1 0.750 || val_loss 1.3360 acc 0.424 f1 0.346\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=14\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 934, np.int64(0): 266})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1010 acc 0.335 f1 0.192 || val_loss 1.0948 acc 0.385 f1 0.235\n",
            "[W3] GRU Epoch 02 | train_loss 1.0925 acc 0.380 f1 0.357 || val_loss 1.0927 acc 0.389 f1 0.321\n",
            "[W3] GRU Epoch 03 | train_loss 1.0828 acc 0.410 f1 0.409 || val_loss 1.0939 acc 0.333 f1 0.305\n",
            "[W3] GRU Epoch 04 | train_loss 1.0697 acc 0.427 f1 0.426 || val_loss 1.0883 acc 0.352 f1 0.319\n",
            "[W3] GRU Epoch 05 | train_loss 1.0483 acc 0.464 f1 0.462 || val_loss 1.0823 acc 0.327 f1 0.297\n",
            "[W3] GRU Epoch 06 | train_loss 1.0164 acc 0.490 f1 0.486 || val_loss 1.0805 acc 0.335 f1 0.300\n",
            "[W3] GRU Epoch 07 | train_loss 0.9600 acc 0.542 f1 0.538 || val_loss 1.0907 acc 0.348 f1 0.311\n",
            "[W3] GRU Epoch 08 | train_loss 0.8765 acc 0.580 f1 0.572 || val_loss 1.0893 acc 0.346 f1 0.301\n",
            "[W3] GRU Epoch 09 | train_loss 0.8248 acc 0.603 f1 0.597 || val_loss 1.1300 acc 0.354 f1 0.307\n",
            "[W3] GRU Epoch 10 | train_loss 0.7765 acc 0.624 f1 0.620 || val_loss 1.1402 acc 0.360 f1 0.302\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=14\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 934, np.int64(0): 266})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1009 acc 0.333 f1 0.181 || val_loss 1.0904 acc 0.430 f1 0.236\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0947 acc 0.357 f1 0.308 || val_loss 1.0933 acc 0.358 f1 0.326\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0867 acc 0.406 f1 0.404 || val_loss 1.0871 acc 0.319 f1 0.297\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0693 acc 0.439 f1 0.433 || val_loss 1.1044 acc 0.286 f1 0.283\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0313 acc 0.480 f1 0.469 || val_loss 1.0670 acc 0.360 f1 0.332\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9435 acc 0.543 f1 0.532 || val_loss 1.0755 acc 0.381 f1 0.325\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8539 acc 0.582 f1 0.573 || val_loss 1.1189 acc 0.374 f1 0.321\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8015 acc 0.606 f1 0.600 || val_loss 1.1102 acc 0.377 f1 0.302\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7508 acc 0.628 f1 0.622 || val_loss 1.1818 acc 0.370 f1 0.308\n",
            "[W3] LSTM Epoch 10 | train_loss 0.6986 acc 0.666 f1 0.661 || val_loss 1.2124 acc 0.362 f1 0.302\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6743 acc 0.670 f1 0.665 || val_loss 1.2440 acc 0.370 f1 0.304\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6357 acc 0.689 f1 0.685 || val_loss 1.2592 acc 0.397 f1 0.330\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6027 acc 0.705 f1 0.700 || val_loss 1.3010 acc 0.385 f1 0.300\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  14%|        | 14/100 [06:37<37:37, 26.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=15 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=15\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 932, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1319 acc 0.419 f1 0.414 || val_loss 1.1892 acc 0.261 f1 0.258\n",
            "[W3] ANN Epoch 02 | train_loss 0.9423 acc 0.526 f1 0.514 || val_loss 1.1475 acc 0.333 f1 0.303\n",
            "[W3] ANN Epoch 03 | train_loss 0.8627 acc 0.568 f1 0.558 || val_loss 1.1437 acc 0.344 f1 0.301\n",
            "[W3] ANN Epoch 04 | train_loss 0.7903 acc 0.611 f1 0.602 || val_loss 1.1405 acc 0.352 f1 0.309\n",
            "[W3] ANN Epoch 05 | train_loss 0.7276 acc 0.651 f1 0.645 || val_loss 1.1385 acc 0.360 f1 0.304\n",
            "[W3] ANN Epoch 06 | train_loss 0.6988 acc 0.654 f1 0.648 || val_loss 1.1389 acc 0.391 f1 0.328\n",
            "[W3] ANN Epoch 07 | train_loss 0.6692 acc 0.666 f1 0.663 || val_loss 1.1505 acc 0.374 f1 0.302\n",
            "[W3] ANN Epoch 08 | train_loss 0.6108 acc 0.702 f1 0.699 || val_loss 1.1581 acc 0.403 f1 0.323\n",
            "[W3] ANN Epoch 09 | train_loss 0.5939 acc 0.722 f1 0.721 || val_loss 1.1987 acc 0.401 f1 0.332\n",
            "[W3] ANN Epoch 10 | train_loss 0.5774 acc 0.728 f1 0.725 || val_loss 1.2213 acc 0.405 f1 0.334\n",
            "[W3] ANN Epoch 11 | train_loss 0.5502 acc 0.748 f1 0.746 || val_loss 1.2258 acc 0.409 f1 0.333\n",
            "[W3] ANN Epoch 12 | train_loss 0.5156 acc 0.759 f1 0.758 || val_loss 1.2932 acc 0.428 f1 0.353\n",
            "[W3] ANN Epoch 13 | train_loss 0.5196 acc 0.762 f1 0.762 || val_loss 1.2731 acc 0.432 f1 0.345\n",
            "[W3] ANN Epoch 14 | train_loss 0.4775 acc 0.784 f1 0.783 || val_loss 1.3085 acc 0.426 f1 0.345\n",
            "[W3] ANN Epoch 15 | train_loss 0.4772 acc 0.786 f1 0.785 || val_loss 1.3338 acc 0.412 f1 0.325\n",
            "[W3] ANN Epoch 16 | train_loss 0.4563 acc 0.796 f1 0.795 || val_loss 1.3450 acc 0.436 f1 0.341\n",
            "[W3] ANN Epoch 17 | train_loss 0.4494 acc 0.798 f1 0.798 || val_loss 1.3455 acc 0.447 f1 0.356\n",
            "[W3] ANN Epoch 18 | train_loss 0.4439 acc 0.817 f1 0.816 || val_loss 1.3885 acc 0.416 f1 0.328\n",
            "[W3] ANN Epoch 19 | train_loss 0.4152 acc 0.819 f1 0.819 || val_loss 1.3836 acc 0.428 f1 0.327\n",
            "[W3] ANN Epoch 20 | train_loss 0.4098 acc 0.826 f1 0.825 || val_loss 1.4182 acc 0.407 f1 0.317\n",
            "[W3] ANN Epoch 21 | train_loss 0.3873 acc 0.835 f1 0.835 || val_loss 1.4169 acc 0.442 f1 0.358\n",
            "[W3] ANN Epoch 22 | train_loss 0.3661 acc 0.850 f1 0.849 || val_loss 1.4431 acc 0.442 f1 0.366\n",
            "[W3] ANN Epoch 23 | train_loss 0.3995 acc 0.831 f1 0.831 || val_loss 1.4392 acc 0.434 f1 0.348\n",
            "[W3] ANN Epoch 24 | train_loss 0.3537 acc 0.852 f1 0.851 || val_loss 1.4898 acc 0.414 f1 0.341\n",
            "[W3] ANN Epoch 25 | train_loss 0.3730 acc 0.853 f1 0.852 || val_loss 1.5117 acc 0.414 f1 0.352\n",
            "[W3] ANN Epoch 26 | train_loss 0.3832 acc 0.838 f1 0.838 || val_loss 1.5132 acc 0.399 f1 0.320\n",
            "[W3] ANN Epoch 27 | train_loss 0.3433 acc 0.857 f1 0.857 || val_loss 1.5419 acc 0.418 f1 0.332\n",
            "[W3] ANN Epoch 28 | train_loss 0.3126 acc 0.869 f1 0.869 || val_loss 1.5712 acc 0.409 f1 0.348\n",
            "[W3] ANN Epoch 29 | train_loss 0.3351 acc 0.856 f1 0.856 || val_loss 1.5836 acc 0.412 f1 0.345\n",
            "[W3] ANN Epoch 30 | train_loss 0.3403 acc 0.870 f1 0.870 || val_loss 1.5915 acc 0.449 f1 0.378\n",
            "[W3] ANN Epoch 31 | train_loss 0.3158 acc 0.863 f1 0.863 || val_loss 1.6172 acc 0.426 f1 0.355\n",
            "[W3] ANN Epoch 32 | train_loss 0.2903 acc 0.881 f1 0.881 || val_loss 1.6478 acc 0.422 f1 0.353\n",
            "[W3] ANN Epoch 33 | train_loss 0.3127 acc 0.880 f1 0.880 || val_loss 1.6861 acc 0.444 f1 0.349\n",
            "[W3] ANN Epoch 34 | train_loss 0.3034 acc 0.879 f1 0.878 || val_loss 1.6373 acc 0.461 f1 0.362\n",
            "[W3] ANN Epoch 35 | train_loss 0.3086 acc 0.879 f1 0.879 || val_loss 1.6105 acc 0.447 f1 0.358\n",
            "[W3] ANN Epoch 36 | train_loss 0.2823 acc 0.889 f1 0.889 || val_loss 1.6587 acc 0.430 f1 0.359\n",
            "[W3] ANN Epoch 37 | train_loss 0.3003 acc 0.880 f1 0.880 || val_loss 1.6475 acc 0.436 f1 0.350\n",
            "[W3] ANN Epoch 38 | train_loss 0.2607 acc 0.900 f1 0.900 || val_loss 1.6819 acc 0.440 f1 0.351\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=15\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 932, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0722 acc 0.411 f1 0.411 || val_loss 1.0116 acc 0.447 f1 0.353\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9402 acc 0.549 f1 0.543 || val_loss 1.0393 acc 0.442 f1 0.359\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8065 acc 0.615 f1 0.611 || val_loss 1.1262 acc 0.420 f1 0.346\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7099 acc 0.657 f1 0.653 || val_loss 1.1694 acc 0.401 f1 0.326\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6437 acc 0.695 f1 0.692 || val_loss 1.1832 acc 0.418 f1 0.348\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5765 acc 0.729 f1 0.727 || val_loss 1.2384 acc 0.442 f1 0.376\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5125 acc 0.766 f1 0.765 || val_loss 1.2970 acc 0.438 f1 0.361\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4634 acc 0.797 f1 0.796 || val_loss 1.3892 acc 0.424 f1 0.354\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4467 acc 0.788 f1 0.787 || val_loss 1.4682 acc 0.389 f1 0.330\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3982 acc 0.817 f1 0.816 || val_loss 1.4973 acc 0.418 f1 0.352\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3438 acc 0.861 f1 0.861 || val_loss 1.6105 acc 0.438 f1 0.360\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3388 acc 0.860 f1 0.859 || val_loss 1.5950 acc 0.428 f1 0.361\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2990 acc 0.878 f1 0.878 || val_loss 1.7110 acc 0.414 f1 0.342\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2921 acc 0.882 f1 0.882 || val_loss 1.7393 acc 0.436 f1 0.364\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=15\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 932, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0946 acc 0.363 f1 0.358 || val_loss 1.0987 acc 0.350 f1 0.329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0806 acc 0.409 f1 0.408 || val_loss 1.1044 acc 0.337 f1 0.320\n",
            "[W3] RNN Epoch 03 | train_loss 1.0636 acc 0.441 f1 0.434 || val_loss 1.1085 acc 0.335 f1 0.323\n",
            "[W3] RNN Epoch 04 | train_loss 1.0493 acc 0.459 f1 0.453 || val_loss 1.0945 acc 0.348 f1 0.329\n",
            "[W3] RNN Epoch 05 | train_loss 1.0369 acc 0.465 f1 0.462 || val_loss 1.0984 acc 0.362 f1 0.344\n",
            "[W3] RNN Epoch 06 | train_loss 1.0173 acc 0.482 f1 0.477 || val_loss 1.0911 acc 0.352 f1 0.331\n",
            "[W3] RNN Epoch 07 | train_loss 0.9980 acc 0.500 f1 0.495 || val_loss 1.0794 acc 0.364 f1 0.344\n",
            "[W3] RNN Epoch 08 | train_loss 0.9748 acc 0.534 f1 0.528 || val_loss 1.0780 acc 0.372 f1 0.348\n",
            "[W3] RNN Epoch 09 | train_loss 0.9603 acc 0.530 f1 0.522 || val_loss 1.0853 acc 0.372 f1 0.356\n",
            "[W3] RNN Epoch 10 | train_loss 0.9331 acc 0.554 f1 0.545 || val_loss 1.0828 acc 0.381 f1 0.353\n",
            "[W3] RNN Epoch 11 | train_loss 0.9065 acc 0.581 f1 0.572 || val_loss 1.0752 acc 0.403 f1 0.370\n",
            "[W3] RNN Epoch 12 | train_loss 0.8841 acc 0.584 f1 0.576 || val_loss 1.0790 acc 0.422 f1 0.383\n",
            "[W3] RNN Epoch 13 | train_loss 0.8630 acc 0.601 f1 0.593 || val_loss 1.0744 acc 0.414 f1 0.370\n",
            "[W3] RNN Epoch 14 | train_loss 0.8432 acc 0.612 f1 0.605 || val_loss 1.1184 acc 0.391 f1 0.360\n",
            "[W3] RNN Epoch 15 | train_loss 0.8264 acc 0.617 f1 0.609 || val_loss 1.1040 acc 0.405 f1 0.369\n",
            "[W3] RNN Epoch 16 | train_loss 0.7916 acc 0.641 f1 0.633 || val_loss 1.0901 acc 0.412 f1 0.361\n",
            "[W3] RNN Epoch 17 | train_loss 0.7807 acc 0.639 f1 0.631 || val_loss 1.1258 acc 0.412 f1 0.364\n",
            "[W3] RNN Epoch 18 | train_loss 0.7534 acc 0.654 f1 0.647 || val_loss 1.1255 acc 0.403 f1 0.356\n",
            "[W3] RNN Epoch 19 | train_loss 0.7331 acc 0.662 f1 0.655 || val_loss 1.1271 acc 0.422 f1 0.382\n",
            "[W3] RNN Epoch 20 | train_loss 0.7186 acc 0.662 f1 0.656 || val_loss 1.1417 acc 0.430 f1 0.366\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=15\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 932, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1008 acc 0.348 f1 0.232 || val_loss 1.1041 acc 0.292 f1 0.236\n",
            "[W3] GRU Epoch 02 | train_loss 1.0906 acc 0.405 f1 0.371 || val_loss 1.0976 acc 0.323 f1 0.306\n",
            "[W3] GRU Epoch 03 | train_loss 1.0809 acc 0.435 f1 0.422 || val_loss 1.0912 acc 0.327 f1 0.298\n",
            "[W3] GRU Epoch 04 | train_loss 1.0648 acc 0.461 f1 0.453 || val_loss 1.0979 acc 0.321 f1 0.308\n",
            "[W3] GRU Epoch 05 | train_loss 1.0395 acc 0.473 f1 0.465 || val_loss 1.0829 acc 0.354 f1 0.323\n",
            "[W3] GRU Epoch 06 | train_loss 0.9972 acc 0.514 f1 0.499 || val_loss 1.0889 acc 0.362 f1 0.325\n",
            "[W3] GRU Epoch 07 | train_loss 0.9272 acc 0.557 f1 0.544 || val_loss 1.1034 acc 0.350 f1 0.309\n",
            "[W3] GRU Epoch 08 | train_loss 0.8502 acc 0.593 f1 0.582 || val_loss 1.1025 acc 0.387 f1 0.315\n",
            "[W3] GRU Epoch 09 | train_loss 0.7946 acc 0.616 f1 0.609 || val_loss 1.1496 acc 0.389 f1 0.323\n",
            "[W3] GRU Epoch 10 | train_loss 0.7477 acc 0.631 f1 0.619 || val_loss 1.1732 acc 0.405 f1 0.324\n",
            "[W3] GRU Epoch 11 | train_loss 0.7115 acc 0.638 f1 0.631 || val_loss 1.2143 acc 0.389 f1 0.321\n",
            "[W3] GRU Epoch 12 | train_loss 0.6816 acc 0.664 f1 0.658 || val_loss 1.2207 acc 0.399 f1 0.335\n",
            "[W3] GRU Epoch 13 | train_loss 0.6470 acc 0.667 f1 0.662 || val_loss 1.2443 acc 0.393 f1 0.323\n",
            "[W3] GRU Epoch 14 | train_loss 0.6105 acc 0.697 f1 0.692 || val_loss 1.2880 acc 0.391 f1 0.318\n",
            "[W3] GRU Epoch 15 | train_loss 0.5778 acc 0.708 f1 0.704 || val_loss 1.3475 acc 0.372 f1 0.312\n",
            "[W3] GRU Epoch 16 | train_loss 0.5628 acc 0.724 f1 0.722 || val_loss 1.3554 acc 0.393 f1 0.319\n",
            "[W3] GRU Epoch 17 | train_loss 0.5497 acc 0.722 f1 0.720 || val_loss 1.3890 acc 0.381 f1 0.315\n",
            "[W3] GRU Epoch 18 | train_loss 0.5349 acc 0.729 f1 0.726 || val_loss 1.4151 acc 0.395 f1 0.325\n",
            "[W3] GRU Epoch 19 | train_loss 0.5033 acc 0.746 f1 0.744 || val_loss 1.4439 acc 0.397 f1 0.330\n",
            "[W3] GRU Epoch 20 | train_loss 0.4849 acc 0.749 f1 0.748 || val_loss 1.4728 acc 0.405 f1 0.336\n",
            "[W3] GRU Epoch 21 | train_loss 0.4676 acc 0.761 f1 0.760 || val_loss 1.5006 acc 0.422 f1 0.344\n",
            "[W3] GRU Epoch 22 | train_loss 0.4498 acc 0.772 f1 0.771 || val_loss 1.5604 acc 0.414 f1 0.331\n",
            "[W3] GRU Epoch 23 | train_loss 0.4392 acc 0.778 f1 0.778 || val_loss 1.5591 acc 0.418 f1 0.335\n",
            "[W3] GRU Epoch 24 | train_loss 0.4199 acc 0.791 f1 0.790 || val_loss 1.6008 acc 0.432 f1 0.342\n",
            "[W3] GRU Epoch 25 | train_loss 0.4063 acc 0.802 f1 0.802 || val_loss 1.6361 acc 0.395 f1 0.319\n",
            "[W3] GRU Epoch 26 | train_loss 0.3985 acc 0.803 f1 0.803 || val_loss 1.6964 acc 0.418 f1 0.333\n",
            "[W3] GRU Epoch 27 | train_loss 0.3831 acc 0.819 f1 0.819 || val_loss 1.7182 acc 0.420 f1 0.338\n",
            "[W3] GRU Epoch 28 | train_loss 0.3654 acc 0.834 f1 0.834 || val_loss 1.7925 acc 0.407 f1 0.342\n",
            "[W3] GRU Epoch 29 | train_loss 0.3505 acc 0.842 f1 0.842 || val_loss 1.7868 acc 0.434 f1 0.349\n",
            "[W3] GRU Epoch 30 | train_loss 0.3386 acc 0.854 f1 0.854 || val_loss 1.8577 acc 0.418 f1 0.341\n",
            "[W3] GRU Epoch 31 | train_loss 0.3191 acc 0.863 f1 0.863 || val_loss 1.9038 acc 0.418 f1 0.327\n",
            "[W3] GRU Epoch 32 | train_loss 0.3079 acc 0.868 f1 0.868 || val_loss 1.9915 acc 0.409 f1 0.335\n",
            "[W3] GRU Epoch 33 | train_loss 0.2874 acc 0.879 f1 0.879 || val_loss 2.0541 acc 0.420 f1 0.334\n",
            "[W3] GRU Epoch 34 | train_loss 0.2732 acc 0.883 f1 0.883 || val_loss 2.0527 acc 0.424 f1 0.341\n",
            "[W3] GRU Epoch 35 | train_loss 0.2562 acc 0.895 f1 0.895 || val_loss 2.1289 acc 0.412 f1 0.332\n",
            "[W3] GRU Epoch 36 | train_loss 0.2414 acc 0.896 f1 0.896 || val_loss 2.2319 acc 0.422 f1 0.345\n",
            "[W3] GRU Epoch 37 | train_loss 0.2197 acc 0.917 f1 0.917 || val_loss 2.2729 acc 0.428 f1 0.339\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=15\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 932, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1019 acc 0.324 f1 0.197 || val_loss 1.0863 acc 0.455 f1 0.230\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0949 acc 0.349 f1 0.291 || val_loss 1.0972 acc 0.366 f1 0.318\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0884 acc 0.425 f1 0.414 || val_loss 1.1037 acc 0.315 f1 0.296\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0762 acc 0.436 f1 0.422 || val_loss 1.1042 acc 0.315 f1 0.300\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0388 acc 0.471 f1 0.453 || val_loss 1.0616 acc 0.374 f1 0.326\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9713 acc 0.510 f1 0.499 || val_loss 1.0812 acc 0.362 f1 0.297\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8898 acc 0.566 f1 0.559 || val_loss 1.1155 acc 0.372 f1 0.315\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8226 acc 0.590 f1 0.585 || val_loss 1.1080 acc 0.395 f1 0.319\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7712 acc 0.621 f1 0.616 || val_loss 1.1462 acc 0.395 f1 0.307\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7241 acc 0.646 f1 0.641 || val_loss 1.1791 acc 0.395 f1 0.304\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6841 acc 0.661 f1 0.657 || val_loss 1.2081 acc 0.401 f1 0.320\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6597 acc 0.682 f1 0.679 || val_loss 1.2518 acc 0.391 f1 0.322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  15%|        | 15/100 [07:12<41:02, 28.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 13 | train_loss 0.6376 acc 0.694 f1 0.690 || val_loss 1.2834 acc 0.401 f1 0.318\n",
            "Early stopping.\n",
            "LOSO: Test subject=16 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=16\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1224 acc 0.414 f1 0.410 || val_loss 1.1924 acc 0.261 f1 0.257\n",
            "[W3] ANN Epoch 02 | train_loss 0.9715 acc 0.519 f1 0.511 || val_loss 1.1816 acc 0.274 f1 0.261\n",
            "[W3] ANN Epoch 03 | train_loss 0.8840 acc 0.550 f1 0.540 || val_loss 1.1619 acc 0.317 f1 0.299\n",
            "[W3] ANN Epoch 04 | train_loss 0.8224 acc 0.601 f1 0.595 || val_loss 1.1650 acc 0.321 f1 0.297\n",
            "[W3] ANN Epoch 05 | train_loss 0.7633 acc 0.634 f1 0.629 || val_loss 1.1898 acc 0.313 f1 0.292\n",
            "[W3] ANN Epoch 06 | train_loss 0.7270 acc 0.641 f1 0.637 || val_loss 1.1513 acc 0.331 f1 0.292\n",
            "[W3] ANN Epoch 07 | train_loss 0.6685 acc 0.685 f1 0.681 || val_loss 1.1432 acc 0.372 f1 0.307\n",
            "[W3] ANN Epoch 08 | train_loss 0.6492 acc 0.686 f1 0.684 || val_loss 1.1761 acc 0.389 f1 0.328\n",
            "[W3] ANN Epoch 09 | train_loss 0.6420 acc 0.683 f1 0.681 || val_loss 1.1876 acc 0.409 f1 0.348\n",
            "[W3] ANN Epoch 10 | train_loss 0.5924 acc 0.717 f1 0.715 || val_loss 1.2170 acc 0.372 f1 0.315\n",
            "[W3] ANN Epoch 11 | train_loss 0.5655 acc 0.724 f1 0.723 || val_loss 1.2166 acc 0.389 f1 0.319\n",
            "[W3] ANN Epoch 12 | train_loss 0.5904 acc 0.727 f1 0.725 || val_loss 1.2276 acc 0.391 f1 0.321\n",
            "[W3] ANN Epoch 13 | train_loss 0.5723 acc 0.734 f1 0.732 || val_loss 1.1949 acc 0.428 f1 0.358\n",
            "[W3] ANN Epoch 14 | train_loss 0.5545 acc 0.741 f1 0.740 || val_loss 1.2342 acc 0.399 f1 0.340\n",
            "[W3] ANN Epoch 15 | train_loss 0.5295 acc 0.771 f1 0.769 || val_loss 1.2348 acc 0.424 f1 0.358\n",
            "[W3] ANN Epoch 16 | train_loss 0.5032 acc 0.766 f1 0.765 || val_loss 1.2656 acc 0.424 f1 0.345\n",
            "[W3] ANN Epoch 17 | train_loss 0.5126 acc 0.770 f1 0.769 || val_loss 1.2696 acc 0.426 f1 0.364\n",
            "[W3] ANN Epoch 18 | train_loss 0.5024 acc 0.774 f1 0.773 || val_loss 1.2671 acc 0.426 f1 0.360\n",
            "[W3] ANN Epoch 19 | train_loss 0.5092 acc 0.764 f1 0.763 || val_loss 1.2627 acc 0.424 f1 0.356\n",
            "[W3] ANN Epoch 20 | train_loss 0.4661 acc 0.792 f1 0.792 || val_loss 1.2921 acc 0.418 f1 0.361\n",
            "[W3] ANN Epoch 21 | train_loss 0.4445 acc 0.801 f1 0.800 || val_loss 1.3049 acc 0.428 f1 0.353\n",
            "[W3] ANN Epoch 22 | train_loss 0.4292 acc 0.809 f1 0.809 || val_loss 1.3265 acc 0.412 f1 0.349\n",
            "[W3] ANN Epoch 23 | train_loss 0.4439 acc 0.800 f1 0.799 || val_loss 1.3496 acc 0.414 f1 0.347\n",
            "[W3] ANN Epoch 24 | train_loss 0.4139 acc 0.818 f1 0.817 || val_loss 1.3707 acc 0.403 f1 0.332\n",
            "[W3] ANN Epoch 25 | train_loss 0.3794 acc 0.834 f1 0.833 || val_loss 1.4043 acc 0.422 f1 0.344\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=16\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0747 acc 0.405 f1 0.407 || val_loss 1.0457 acc 0.420 f1 0.307\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9630 acc 0.526 f1 0.521 || val_loss 1.0277 acc 0.403 f1 0.328\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8334 acc 0.598 f1 0.594 || val_loss 1.1034 acc 0.393 f1 0.338\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7546 acc 0.628 f1 0.622 || val_loss 1.1131 acc 0.374 f1 0.320\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6767 acc 0.675 f1 0.671 || val_loss 1.1843 acc 0.346 f1 0.285\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6136 acc 0.705 f1 0.704 || val_loss 1.1975 acc 0.395 f1 0.313\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5696 acc 0.733 f1 0.731 || val_loss 1.2553 acc 0.395 f1 0.331\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5196 acc 0.760 f1 0.760 || val_loss 1.3156 acc 0.372 f1 0.324\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4674 acc 0.788 f1 0.787 || val_loss 1.3743 acc 0.391 f1 0.325\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4261 acc 0.806 f1 0.806 || val_loss 1.4663 acc 0.405 f1 0.346\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.4135 acc 0.816 f1 0.815 || val_loss 1.4982 acc 0.387 f1 0.314\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3615 acc 0.848 f1 0.848 || val_loss 1.6118 acc 0.385 f1 0.307\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3317 acc 0.872 f1 0.872 || val_loss 1.7257 acc 0.407 f1 0.338\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3641 acc 0.854 f1 0.853 || val_loss 1.6380 acc 0.434 f1 0.362\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.3137 acc 0.872 f1 0.872 || val_loss 1.7107 acc 0.420 f1 0.347\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2828 acc 0.891 f1 0.890 || val_loss 1.8179 acc 0.391 f1 0.317\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.2734 acc 0.894 f1 0.894 || val_loss 1.8359 acc 0.422 f1 0.349\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.2460 acc 0.905 f1 0.905 || val_loss 1.8526 acc 0.412 f1 0.336\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.2193 acc 0.922 f1 0.922 || val_loss 2.0302 acc 0.405 f1 0.332\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1977 acc 0.932 f1 0.931 || val_loss 2.0981 acc 0.397 f1 0.322\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.2170 acc 0.923 f1 0.923 || val_loss 2.1580 acc 0.418 f1 0.340\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1995 acc 0.934 f1 0.934 || val_loss 2.1296 acc 0.414 f1 0.338\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=16\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0979 acc 0.343 f1 0.285 || val_loss 1.0826 acc 0.420 f1 0.369\n",
            "[W3] RNN Epoch 02 | train_loss 1.0793 acc 0.413 f1 0.412 || val_loss 1.0861 acc 0.364 f1 0.348\n",
            "[W3] RNN Epoch 03 | train_loss 1.0635 acc 0.439 f1 0.432 || val_loss 1.0854 acc 0.317 f1 0.299\n",
            "[W3] RNN Epoch 04 | train_loss 1.0489 acc 0.453 f1 0.443 || val_loss 1.0895 acc 0.340 f1 0.327\n",
            "[W3] RNN Epoch 05 | train_loss 1.0321 acc 0.469 f1 0.464 || val_loss 1.0759 acc 0.358 f1 0.332\n",
            "[W3] RNN Epoch 06 | train_loss 1.0115 acc 0.489 f1 0.484 || val_loss 1.0662 acc 0.387 f1 0.348\n",
            "[W3] RNN Epoch 07 | train_loss 0.9892 acc 0.513 f1 0.505 || val_loss 1.0706 acc 0.389 f1 0.348\n",
            "[W3] RNN Epoch 08 | train_loss 0.9631 acc 0.531 f1 0.524 || val_loss 1.0684 acc 0.399 f1 0.360\n",
            "[W3] RNN Epoch 09 | train_loss 0.9418 acc 0.544 f1 0.533 || val_loss 1.0703 acc 0.387 f1 0.345\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=16\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0975 acc 0.360 f1 0.284 || val_loss 1.1020 acc 0.267 f1 0.266\n",
            "[W3] GRU Epoch 02 | train_loss 1.0890 acc 0.395 f1 0.379 || val_loss 1.0919 acc 0.331 f1 0.312\n",
            "[W3] GRU Epoch 03 | train_loss 1.0798 acc 0.418 f1 0.408 || val_loss 1.0876 acc 0.329 f1 0.317\n",
            "[W3] GRU Epoch 04 | train_loss 1.0667 acc 0.445 f1 0.437 || val_loss 1.0786 acc 0.358 f1 0.336\n",
            "[W3] GRU Epoch 05 | train_loss 1.0461 acc 0.470 f1 0.462 || val_loss 1.0921 acc 0.333 f1 0.322\n",
            "[W3] GRU Epoch 06 | train_loss 1.0108 acc 0.512 f1 0.500 || val_loss 1.0552 acc 0.368 f1 0.312\n",
            "[W3] GRU Epoch 07 | train_loss 0.9522 acc 0.545 f1 0.538 || val_loss 1.0701 acc 0.387 f1 0.327\n",
            "[W3] GRU Epoch 08 | train_loss 0.8852 acc 0.568 f1 0.558 || val_loss 1.0623 acc 0.416 f1 0.335\n",
            "[W3] GRU Epoch 09 | train_loss 0.8383 acc 0.593 f1 0.588 || val_loss 1.1414 acc 0.360 f1 0.310\n",
            "[W3] GRU Epoch 10 | train_loss 0.7827 acc 0.613 f1 0.606 || val_loss 1.1189 acc 0.399 f1 0.328\n",
            "[W3] GRU Epoch 11 | train_loss 0.7573 acc 0.627 f1 0.621 || val_loss 1.1402 acc 0.401 f1 0.341\n",
            "[W3] GRU Epoch 12 | train_loss 0.7052 acc 0.656 f1 0.652 || val_loss 1.1982 acc 0.374 f1 0.324\n",
            "[W3] GRU Epoch 13 | train_loss 0.6825 acc 0.665 f1 0.661 || val_loss 1.2408 acc 0.389 f1 0.331\n",
            "[W3] GRU Epoch 14 | train_loss 0.6444 acc 0.687 f1 0.683 || val_loss 1.2561 acc 0.393 f1 0.328\n",
            "[W3] GRU Epoch 15 | train_loss 0.6308 acc 0.688 f1 0.684 || val_loss 1.2750 acc 0.399 f1 0.331\n",
            "[W3] GRU Epoch 16 | train_loss 0.5983 acc 0.708 f1 0.705 || val_loss 1.3222 acc 0.393 f1 0.336\n",
            "[W3] GRU Epoch 17 | train_loss 0.5676 acc 0.719 f1 0.716 || val_loss 1.3516 acc 0.412 f1 0.341\n",
            "[W3] GRU Epoch 18 | train_loss 0.5526 acc 0.724 f1 0.722 || val_loss 1.3829 acc 0.420 f1 0.350\n",
            "[W3] GRU Epoch 19 | train_loss 0.5254 acc 0.747 f1 0.745 || val_loss 1.4437 acc 0.395 f1 0.345\n",
            "[W3] GRU Epoch 20 | train_loss 0.4992 acc 0.748 f1 0.746 || val_loss 1.4550 acc 0.418 f1 0.353\n",
            "[W3] GRU Epoch 21 | train_loss 0.4829 acc 0.771 f1 0.770 || val_loss 1.5140 acc 0.397 f1 0.338\n",
            "[W3] GRU Epoch 22 | train_loss 0.4639 acc 0.780 f1 0.779 || val_loss 1.5524 acc 0.405 f1 0.347\n",
            "[W3] GRU Epoch 23 | train_loss 0.4390 acc 0.790 f1 0.789 || val_loss 1.6211 acc 0.403 f1 0.342\n",
            "[W3] GRU Epoch 24 | train_loss 0.4312 acc 0.800 f1 0.799 || val_loss 1.6420 acc 0.407 f1 0.344\n",
            "[W3] GRU Epoch 25 | train_loss 0.4134 acc 0.799 f1 0.799 || val_loss 1.7534 acc 0.389 f1 0.337\n",
            "[W3] GRU Epoch 26 | train_loss 0.3871 acc 0.822 f1 0.822 || val_loss 1.7550 acc 0.403 f1 0.342\n",
            "[W3] GRU Epoch 27 | train_loss 0.3799 acc 0.825 f1 0.824 || val_loss 1.8254 acc 0.401 f1 0.341\n",
            "[W3] GRU Epoch 28 | train_loss 0.3651 acc 0.829 f1 0.829 || val_loss 1.9136 acc 0.377 f1 0.322\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=16\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0996 acc 0.344 f1 0.261 || val_loss 1.1038 acc 0.261 f1 0.217\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0942 acc 0.385 f1 0.342 || val_loss 1.0965 acc 0.313 f1 0.296\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0884 acc 0.396 f1 0.384 || val_loss 1.0929 acc 0.305 f1 0.286\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0767 acc 0.424 f1 0.418 || val_loss 1.0715 acc 0.362 f1 0.339\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0547 acc 0.468 f1 0.459 || val_loss 1.0710 acc 0.362 f1 0.335\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9979 acc 0.509 f1 0.504 || val_loss 1.0938 acc 0.335 f1 0.309\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9093 acc 0.554 f1 0.545 || val_loss 1.0995 acc 0.364 f1 0.306\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8437 acc 0.585 f1 0.577 || val_loss 1.1508 acc 0.337 f1 0.290\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7932 acc 0.608 f1 0.601 || val_loss 1.1376 acc 0.377 f1 0.303\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7477 acc 0.630 f1 0.623 || val_loss 1.1736 acc 0.395 f1 0.309\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7254 acc 0.649 f1 0.645 || val_loss 1.1900 acc 0.385 f1 0.308\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6842 acc 0.663 f1 0.658 || val_loss 1.2426 acc 0.381 f1 0.318\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  16%|        | 16/100 [07:42<40:48, 29.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=17 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=17\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 931, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1400 acc 0.394 f1 0.392 || val_loss 1.1399 acc 0.317 f1 0.288\n",
            "[W3] ANN Epoch 02 | train_loss 0.9506 acc 0.530 f1 0.521 || val_loss 1.1150 acc 0.364 f1 0.314\n",
            "[W3] ANN Epoch 03 | train_loss 0.8414 acc 0.586 f1 0.580 || val_loss 1.1140 acc 0.399 f1 0.347\n",
            "[W3] ANN Epoch 04 | train_loss 0.7701 acc 0.620 f1 0.616 || val_loss 1.1127 acc 0.436 f1 0.360\n",
            "[W3] ANN Epoch 05 | train_loss 0.7256 acc 0.656 f1 0.652 || val_loss 1.0955 acc 0.453 f1 0.361\n",
            "[W3] ANN Epoch 06 | train_loss 0.6694 acc 0.680 f1 0.678 || val_loss 1.1313 acc 0.426 f1 0.321\n",
            "[W3] ANN Epoch 07 | train_loss 0.6374 acc 0.690 f1 0.687 || val_loss 1.1619 acc 0.416 f1 0.332\n",
            "[W3] ANN Epoch 08 | train_loss 0.5806 acc 0.716 f1 0.715 || val_loss 1.1871 acc 0.434 f1 0.319\n",
            "[W3] ANN Epoch 09 | train_loss 0.5486 acc 0.743 f1 0.741 || val_loss 1.2139 acc 0.434 f1 0.342\n",
            "[W3] ANN Epoch 10 | train_loss 0.5326 acc 0.754 f1 0.754 || val_loss 1.2723 acc 0.428 f1 0.332\n",
            "[W3] ANN Epoch 11 | train_loss 0.5338 acc 0.755 f1 0.755 || val_loss 1.2862 acc 0.420 f1 0.335\n",
            "[W3] ANN Epoch 12 | train_loss 0.5044 acc 0.777 f1 0.776 || val_loss 1.2952 acc 0.436 f1 0.354\n",
            "[W3] ANN Epoch 13 | train_loss 0.4688 acc 0.798 f1 0.798 || val_loss 1.3111 acc 0.451 f1 0.340\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=17\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 931, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0781 acc 0.393 f1 0.392 || val_loss 1.0369 acc 0.416 f1 0.312\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9324 acc 0.543 f1 0.538 || val_loss 1.0396 acc 0.414 f1 0.323\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8009 acc 0.611 f1 0.608 || val_loss 1.1119 acc 0.383 f1 0.312\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.6986 acc 0.680 f1 0.677 || val_loss 1.1954 acc 0.391 f1 0.319\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6202 acc 0.717 f1 0.715 || val_loss 1.2544 acc 0.393 f1 0.311\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5624 acc 0.738 f1 0.738 || val_loss 1.3215 acc 0.385 f1 0.309\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.4969 acc 0.785 f1 0.784 || val_loss 1.3823 acc 0.403 f1 0.320\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4479 acc 0.806 f1 0.806 || val_loss 1.4415 acc 0.405 f1 0.317\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.3926 acc 0.833 f1 0.832 || val_loss 1.5516 acc 0.395 f1 0.304\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3460 acc 0.862 f1 0.862 || val_loss 1.6401 acc 0.401 f1 0.315\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=17\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 931, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.1008 acc 0.342 f1 0.304 || val_loss 1.0988 acc 0.305 f1 0.268\n",
            "[W3] RNN Epoch 02 | train_loss 1.0834 acc 0.419 f1 0.419 || val_loss 1.0970 acc 0.360 f1 0.333\n",
            "[W3] RNN Epoch 03 | train_loss 1.0690 acc 0.439 f1 0.437 || val_loss 1.0807 acc 0.395 f1 0.361\n",
            "[W3] RNN Epoch 04 | train_loss 1.0477 acc 0.469 f1 0.469 || val_loss 1.0932 acc 0.325 f1 0.299\n",
            "[W3] RNN Epoch 05 | train_loss 1.0240 acc 0.486 f1 0.485 || val_loss 1.0840 acc 0.360 f1 0.329\n",
            "[W3] RNN Epoch 06 | train_loss 0.9975 acc 0.506 f1 0.502 || val_loss 1.1058 acc 0.335 f1 0.310\n",
            "[W3] RNN Epoch 07 | train_loss 0.9727 acc 0.524 f1 0.518 || val_loss 1.0993 acc 0.342 f1 0.313\n",
            "[W3] RNN Epoch 08 | train_loss 0.9506 acc 0.530 f1 0.522 || val_loss 1.0910 acc 0.362 f1 0.329\n",
            "[W3] RNN Epoch 09 | train_loss 0.9250 acc 0.557 f1 0.550 || val_loss 1.0925 acc 0.364 f1 0.328\n",
            "[W3] RNN Epoch 10 | train_loss 0.9060 acc 0.565 f1 0.554 || val_loss 1.0896 acc 0.372 f1 0.323\n",
            "[W3] RNN Epoch 11 | train_loss 0.8789 acc 0.585 f1 0.575 || val_loss 1.0883 acc 0.362 f1 0.298\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=17\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 931, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1012 acc 0.338 f1 0.270 || val_loss 1.1059 acc 0.255 f1 0.232\n",
            "[W3] GRU Epoch 02 | train_loss 1.0919 acc 0.383 f1 0.368 || val_loss 1.0976 acc 0.319 f1 0.306\n",
            "[W3] GRU Epoch 03 | train_loss 1.0829 acc 0.420 f1 0.418 || val_loss 1.0944 acc 0.358 f1 0.333\n",
            "[W3] GRU Epoch 04 | train_loss 1.0690 acc 0.446 f1 0.442 || val_loss 1.0952 acc 0.364 f1 0.340\n",
            "[W3] GRU Epoch 05 | train_loss 1.0521 acc 0.450 f1 0.448 || val_loss 1.1006 acc 0.360 f1 0.335\n",
            "[W3] GRU Epoch 06 | train_loss 1.0087 acc 0.502 f1 0.492 || val_loss 1.0568 acc 0.397 f1 0.341\n",
            "[W3] GRU Epoch 07 | train_loss 0.9393 acc 0.549 f1 0.543 || val_loss 1.0665 acc 0.395 f1 0.350\n",
            "[W3] GRU Epoch 08 | train_loss 0.8583 acc 0.600 f1 0.594 || val_loss 1.1031 acc 0.379 f1 0.325\n",
            "[W3] GRU Epoch 09 | train_loss 0.7946 acc 0.611 f1 0.604 || val_loss 1.1115 acc 0.405 f1 0.336\n",
            "[W3] GRU Epoch 10 | train_loss 0.7485 acc 0.635 f1 0.632 || val_loss 1.1716 acc 0.391 f1 0.332\n",
            "[W3] GRU Epoch 11 | train_loss 0.7209 acc 0.647 f1 0.643 || val_loss 1.1570 acc 0.407 f1 0.333\n",
            "[W3] GRU Epoch 12 | train_loss 0.6866 acc 0.669 f1 0.665 || val_loss 1.1597 acc 0.416 f1 0.334\n",
            "[W3] GRU Epoch 13 | train_loss 0.6519 acc 0.682 f1 0.678 || val_loss 1.2096 acc 0.418 f1 0.339\n",
            "[W3] GRU Epoch 14 | train_loss 0.6203 acc 0.706 f1 0.704 || val_loss 1.2430 acc 0.426 f1 0.347\n",
            "[W3] GRU Epoch 15 | train_loss 0.5813 acc 0.717 f1 0.715 || val_loss 1.3179 acc 0.401 f1 0.336\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=17\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 931, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0986 acc 0.344 f1 0.282 || val_loss 1.1015 acc 0.288 f1 0.256\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0930 acc 0.388 f1 0.364 || val_loss 1.0979 acc 0.331 f1 0.316\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0849 acc 0.420 f1 0.414 || val_loss 1.0925 acc 0.350 f1 0.335\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0635 acc 0.447 f1 0.437 || val_loss 1.0908 acc 0.354 f1 0.342\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0072 acc 0.513 f1 0.497 || val_loss 1.0702 acc 0.366 f1 0.332\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9165 acc 0.554 f1 0.545 || val_loss 1.0780 acc 0.381 f1 0.343\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8297 acc 0.590 f1 0.582 || val_loss 1.1261 acc 0.372 f1 0.339\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7861 acc 0.606 f1 0.597 || val_loss 1.1070 acc 0.399 f1 0.343\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7469 acc 0.634 f1 0.629 || val_loss 1.1093 acc 0.399 f1 0.342\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7064 acc 0.650 f1 0.645 || val_loss 1.1498 acc 0.389 f1 0.348\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6802 acc 0.665 f1 0.661 || val_loss 1.1993 acc 0.401 f1 0.351\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6472 acc 0.676 f1 0.672 || val_loss 1.2125 acc 0.377 f1 0.328\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6168 acc 0.693 f1 0.689 || val_loss 1.2245 acc 0.405 f1 0.347\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5916 acc 0.710 f1 0.707 || val_loss 1.2951 acc 0.370 f1 0.314\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5731 acc 0.715 f1 0.713 || val_loss 1.3088 acc 0.397 f1 0.337\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5521 acc 0.727 f1 0.724 || val_loss 1.3322 acc 0.401 f1 0.337\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5190 acc 0.753 f1 0.752 || val_loss 1.3694 acc 0.434 f1 0.358\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5030 acc 0.749 f1 0.748 || val_loss 1.4609 acc 0.401 f1 0.344\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4799 acc 0.762 f1 0.761 || val_loss 1.4856 acc 0.416 f1 0.348\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4629 acc 0.765 f1 0.765 || val_loss 1.5509 acc 0.387 f1 0.323\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4445 acc 0.785 f1 0.785 || val_loss 1.6073 acc 0.391 f1 0.326\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4239 acc 0.800 f1 0.799 || val_loss 1.6291 acc 0.407 f1 0.333\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4086 acc 0.811 f1 0.810 || val_loss 1.6801 acc 0.387 f1 0.316\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3900 acc 0.815 f1 0.814 || val_loss 1.7437 acc 0.412 f1 0.341\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3762 acc 0.827 f1 0.827 || val_loss 1.8153 acc 0.412 f1 0.337\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  17%|        | 17/100 [08:15<42:15, 30.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=18 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=18\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 930, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1460 acc 0.402 f1 0.399 || val_loss 1.1225 acc 0.335 f1 0.310\n",
            "[W3] ANN Epoch 02 | train_loss 0.9640 acc 0.521 f1 0.515 || val_loss 1.1092 acc 0.362 f1 0.329\n",
            "[W3] ANN Epoch 03 | train_loss 0.8782 acc 0.571 f1 0.564 || val_loss 1.0918 acc 0.379 f1 0.330\n",
            "[W3] ANN Epoch 04 | train_loss 0.8099 acc 0.604 f1 0.599 || val_loss 1.0656 acc 0.393 f1 0.344\n",
            "[W3] ANN Epoch 05 | train_loss 0.7465 acc 0.643 f1 0.642 || val_loss 1.0913 acc 0.416 f1 0.350\n",
            "[W3] ANN Epoch 06 | train_loss 0.6936 acc 0.687 f1 0.686 || val_loss 1.1033 acc 0.430 f1 0.373\n",
            "[W3] ANN Epoch 07 | train_loss 0.6395 acc 0.694 f1 0.692 || val_loss 1.1230 acc 0.422 f1 0.353\n",
            "[W3] ANN Epoch 08 | train_loss 0.6032 acc 0.722 f1 0.720 || val_loss 1.1650 acc 0.414 f1 0.342\n",
            "[W3] ANN Epoch 09 | train_loss 0.5763 acc 0.730 f1 0.729 || val_loss 1.1997 acc 0.420 f1 0.337\n",
            "[W3] ANN Epoch 10 | train_loss 0.5573 acc 0.748 f1 0.748 || val_loss 1.2057 acc 0.444 f1 0.365\n",
            "[W3] ANN Epoch 11 | train_loss 0.5306 acc 0.761 f1 0.760 || val_loss 1.2349 acc 0.436 f1 0.347\n",
            "[W3] ANN Epoch 12 | train_loss 0.4974 acc 0.776 f1 0.775 || val_loss 1.2697 acc 0.426 f1 0.346\n",
            "[W3] ANN Epoch 13 | train_loss 0.4923 acc 0.784 f1 0.784 || val_loss 1.3210 acc 0.430 f1 0.345\n",
            "[W3] ANN Epoch 14 | train_loss 0.4730 acc 0.797 f1 0.796 || val_loss 1.3209 acc 0.434 f1 0.346\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=18\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 930, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0721 acc 0.420 f1 0.422 || val_loss 1.0170 acc 0.428 f1 0.304\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9397 acc 0.553 f1 0.550 || val_loss 1.0313 acc 0.426 f1 0.341\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8125 acc 0.609 f1 0.606 || val_loss 1.1180 acc 0.399 f1 0.351\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7186 acc 0.664 f1 0.661 || val_loss 1.1348 acc 0.414 f1 0.357\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6213 acc 0.711 f1 0.709 || val_loss 1.1678 acc 0.405 f1 0.340\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5665 acc 0.744 f1 0.743 || val_loss 1.3025 acc 0.368 f1 0.328\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.4951 acc 0.776 f1 0.775 || val_loss 1.2849 acc 0.436 f1 0.376\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4408 acc 0.811 f1 0.810 || val_loss 1.3965 acc 0.420 f1 0.363\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.3900 acc 0.843 f1 0.843 || val_loss 1.3985 acc 0.455 f1 0.370\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3572 acc 0.853 f1 0.853 || val_loss 1.5229 acc 0.449 f1 0.367\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3142 acc 0.874 f1 0.874 || val_loss 1.5156 acc 0.461 f1 0.361\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2598 acc 0.902 f1 0.902 || val_loss 1.6164 acc 0.481 f1 0.408\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2092 acc 0.926 f1 0.926 || val_loss 1.7383 acc 0.451 f1 0.387\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.1986 acc 0.930 f1 0.930 || val_loss 1.8655 acc 0.451 f1 0.345\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1998 acc 0.926 f1 0.926 || val_loss 1.7931 acc 0.469 f1 0.384\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1540 acc 0.949 f1 0.949 || val_loss 1.9754 acc 0.471 f1 0.386\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1458 acc 0.948 f1 0.948 || val_loss 1.9458 acc 0.444 f1 0.364\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1315 acc 0.953 f1 0.953 || val_loss 2.1298 acc 0.449 f1 0.366\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1172 acc 0.962 f1 0.962 || val_loss 2.1707 acc 0.477 f1 0.392\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1060 acc 0.966 f1 0.966 || val_loss 2.1314 acc 0.473 f1 0.398\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=18\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 930, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.1016 acc 0.354 f1 0.270 || val_loss 1.1106 acc 0.247 f1 0.245\n",
            "[W3] RNN Epoch 02 | train_loss 1.0811 acc 0.412 f1 0.395 || val_loss 1.0937 acc 0.331 f1 0.307\n",
            "[W3] RNN Epoch 03 | train_loss 1.0671 acc 0.444 f1 0.439 || val_loss 1.0869 acc 0.346 f1 0.329\n",
            "[W3] RNN Epoch 04 | train_loss 1.0489 acc 0.470 f1 0.461 || val_loss 1.0862 acc 0.346 f1 0.327\n",
            "[W3] RNN Epoch 05 | train_loss 1.0327 acc 0.483 f1 0.473 || val_loss 1.0825 acc 0.360 f1 0.342\n",
            "[W3] RNN Epoch 06 | train_loss 1.0132 acc 0.498 f1 0.489 || val_loss 1.0693 acc 0.370 f1 0.339\n",
            "[W3] RNN Epoch 07 | train_loss 0.9859 acc 0.531 f1 0.521 || val_loss 1.0700 acc 0.366 f1 0.332\n",
            "[W3] RNN Epoch 08 | train_loss 0.9612 acc 0.546 f1 0.539 || val_loss 1.0865 acc 0.360 f1 0.335\n",
            "[W3] RNN Epoch 09 | train_loss 0.9335 acc 0.567 f1 0.558 || val_loss 1.0686 acc 0.383 f1 0.338\n",
            "[W3] RNN Epoch 10 | train_loss 0.9124 acc 0.574 f1 0.564 || val_loss 1.0738 acc 0.387 f1 0.340\n",
            "[W3] RNN Epoch 11 | train_loss 0.8897 acc 0.583 f1 0.574 || val_loss 1.0876 acc 0.399 f1 0.357\n",
            "[W3] RNN Epoch 12 | train_loss 0.8642 acc 0.596 f1 0.586 || val_loss 1.0892 acc 0.393 f1 0.342\n",
            "[W3] RNN Epoch 13 | train_loss 0.8458 acc 0.599 f1 0.590 || val_loss 1.0937 acc 0.401 f1 0.346\n",
            "[W3] RNN Epoch 14 | train_loss 0.8219 acc 0.622 f1 0.613 || val_loss 1.1017 acc 0.389 f1 0.330\n",
            "[W3] RNN Epoch 15 | train_loss 0.8062 acc 0.626 f1 0.619 || val_loss 1.1206 acc 0.412 f1 0.365\n",
            "[W3] RNN Epoch 16 | train_loss 0.7758 acc 0.640 f1 0.633 || val_loss 1.1151 acc 0.422 f1 0.358\n",
            "[W3] RNN Epoch 17 | train_loss 0.7573 acc 0.654 f1 0.647 || val_loss 1.1290 acc 0.424 f1 0.360\n",
            "[W3] RNN Epoch 18 | train_loss 0.7387 acc 0.653 f1 0.645 || val_loss 1.1508 acc 0.412 f1 0.356\n",
            "[W3] RNN Epoch 19 | train_loss 0.7242 acc 0.664 f1 0.656 || val_loss 1.1378 acc 0.426 f1 0.360\n",
            "[W3] RNN Epoch 20 | train_loss 0.6914 acc 0.688 f1 0.682 || val_loss 1.1702 acc 0.424 f1 0.361\n",
            "[W3] RNN Epoch 21 | train_loss 0.6778 acc 0.691 f1 0.685 || val_loss 1.1728 acc 0.418 f1 0.361\n",
            "[W3] RNN Epoch 22 | train_loss 0.6578 acc 0.700 f1 0.694 || val_loss 1.1982 acc 0.426 f1 0.368\n",
            "[W3] RNN Epoch 23 | train_loss 0.6355 acc 0.715 f1 0.708 || val_loss 1.1989 acc 0.434 f1 0.369\n",
            "[W3] RNN Epoch 24 | train_loss 0.6191 acc 0.718 f1 0.712 || val_loss 1.2232 acc 0.436 f1 0.368\n",
            "[W3] RNN Epoch 25 | train_loss 0.5924 acc 0.728 f1 0.723 || val_loss 1.2407 acc 0.432 f1 0.361\n",
            "[W3] RNN Epoch 26 | train_loss 0.5821 acc 0.728 f1 0.722 || val_loss 1.2592 acc 0.428 f1 0.357\n",
            "[W3] RNN Epoch 27 | train_loss 0.5612 acc 0.758 f1 0.754 || val_loss 1.2745 acc 0.451 f1 0.374\n",
            "[W3] RNN Epoch 28 | train_loss 0.5461 acc 0.755 f1 0.752 || val_loss 1.3019 acc 0.418 f1 0.353\n",
            "[W3] RNN Epoch 29 | train_loss 0.5434 acc 0.746 f1 0.742 || val_loss 1.3164 acc 0.451 f1 0.383\n",
            "[W3] RNN Epoch 30 | train_loss 0.5074 acc 0.762 f1 0.759 || val_loss 1.3435 acc 0.453 f1 0.376\n",
            "[W3] RNN Epoch 31 | train_loss 0.5203 acc 0.757 f1 0.753 || val_loss 1.3702 acc 0.436 f1 0.367\n",
            "[W3] RNN Epoch 32 | train_loss 0.5015 acc 0.771 f1 0.768 || val_loss 1.4020 acc 0.461 f1 0.381\n",
            "[W3] RNN Epoch 33 | train_loss 0.4817 acc 0.781 f1 0.778 || val_loss 1.4262 acc 0.461 f1 0.382\n",
            "[W3] RNN Epoch 34 | train_loss 0.4775 acc 0.779 f1 0.776 || val_loss 1.4442 acc 0.438 f1 0.361\n",
            "[W3] RNN Epoch 35 | train_loss 0.4633 acc 0.785 f1 0.783 || val_loss 1.4624 acc 0.442 f1 0.372\n",
            "[W3] RNN Epoch 36 | train_loss 0.4539 acc 0.795 f1 0.793 || val_loss 1.5014 acc 0.457 f1 0.384\n",
            "[W3] RNN Epoch 37 | train_loss 0.4446 acc 0.796 f1 0.794 || val_loss 1.5182 acc 0.461 f1 0.386\n",
            "[W3] RNN Epoch 38 | train_loss 0.4312 acc 0.803 f1 0.800 || val_loss 1.5255 acc 0.436 f1 0.368\n",
            "[W3] RNN Epoch 39 | train_loss 0.4289 acc 0.804 f1 0.803 || val_loss 1.5614 acc 0.459 f1 0.376\n",
            "[W3] RNN Epoch 40 | train_loss 0.4148 acc 0.811 f1 0.810 || val_loss 1.5875 acc 0.442 f1 0.373\n",
            "[W3] RNN Epoch 41 | train_loss 0.4064 acc 0.813 f1 0.812 || val_loss 1.6046 acc 0.467 f1 0.395\n",
            "[W3] RNN Epoch 42 | train_loss 0.3937 acc 0.827 f1 0.826 || val_loss 1.6342 acc 0.434 f1 0.362\n",
            "[W3] RNN Epoch 43 | train_loss 0.3868 acc 0.819 f1 0.818 || val_loss 1.6622 acc 0.424 f1 0.350\n",
            "[W3] RNN Epoch 44 | train_loss 0.3869 acc 0.821 f1 0.820 || val_loss 1.6732 acc 0.453 f1 0.385\n",
            "[W3] RNN Epoch 45 | train_loss 0.3764 acc 0.834 f1 0.833 || val_loss 1.7077 acc 0.447 f1 0.380\n",
            "[W3] RNN Epoch 46 | train_loss 0.3663 acc 0.844 f1 0.843 || val_loss 1.7280 acc 0.455 f1 0.382\n",
            "[W3] RNN Epoch 47 | train_loss 0.3565 acc 0.844 f1 0.843 || val_loss 1.7289 acc 0.449 f1 0.381\n",
            "[W3] RNN Epoch 48 | train_loss 0.3375 acc 0.853 f1 0.853 || val_loss 1.7753 acc 0.447 f1 0.376\n",
            "[W3] RNN Epoch 49 | train_loss 0.3358 acc 0.851 f1 0.850 || val_loss 1.8008 acc 0.451 f1 0.379\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=18\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 930, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0992 acc 0.345 f1 0.304 || val_loss 1.1106 acc 0.202 f1 0.202\n",
            "[W3] GRU Epoch 02 | train_loss 1.0905 acc 0.389 f1 0.383 || val_loss 1.1023 acc 0.286 f1 0.273\n",
            "[W3] GRU Epoch 03 | train_loss 1.0834 acc 0.409 f1 0.405 || val_loss 1.0885 acc 0.315 f1 0.287\n",
            "[W3] GRU Epoch 04 | train_loss 1.0715 acc 0.432 f1 0.427 || val_loss 1.0958 acc 0.315 f1 0.298\n",
            "[W3] GRU Epoch 05 | train_loss 1.0456 acc 0.481 f1 0.473 || val_loss 1.0909 acc 0.348 f1 0.320\n",
            "[W3] GRU Epoch 06 | train_loss 1.0088 acc 0.511 f1 0.505 || val_loss 1.0945 acc 0.342 f1 0.312\n",
            "[W3] GRU Epoch 07 | train_loss 0.9502 acc 0.543 f1 0.535 || val_loss 1.1644 acc 0.288 f1 0.275\n",
            "[W3] GRU Epoch 08 | train_loss 0.8736 acc 0.578 f1 0.571 || val_loss 1.1336 acc 0.358 f1 0.314\n",
            "[W3] GRU Epoch 09 | train_loss 0.8105 acc 0.613 f1 0.608 || val_loss 1.1919 acc 0.337 f1 0.308\n",
            "[W3] GRU Epoch 10 | train_loss 0.7559 acc 0.628 f1 0.621 || val_loss 1.1435 acc 0.412 f1 0.336\n",
            "[W3] GRU Epoch 11 | train_loss 0.7142 acc 0.658 f1 0.652 || val_loss 1.2037 acc 0.391 f1 0.333\n",
            "[W3] GRU Epoch 12 | train_loss 0.6872 acc 0.663 f1 0.659 || val_loss 1.2192 acc 0.389 f1 0.324\n",
            "[W3] GRU Epoch 13 | train_loss 0.6504 acc 0.682 f1 0.679 || val_loss 1.2618 acc 0.385 f1 0.313\n",
            "[W3] GRU Epoch 14 | train_loss 0.6124 acc 0.708 f1 0.705 || val_loss 1.2792 acc 0.391 f1 0.321\n",
            "[W3] GRU Epoch 15 | train_loss 0.5810 acc 0.714 f1 0.712 || val_loss 1.3464 acc 0.383 f1 0.318\n",
            "[W3] GRU Epoch 16 | train_loss 0.5569 acc 0.726 f1 0.723 || val_loss 1.3578 acc 0.393 f1 0.314\n",
            "[W3] GRU Epoch 17 | train_loss 0.5396 acc 0.739 f1 0.738 || val_loss 1.4181 acc 0.401 f1 0.341\n",
            "[W3] GRU Epoch 18 | train_loss 0.5087 acc 0.755 f1 0.754 || val_loss 1.4480 acc 0.401 f1 0.321\n",
            "[W3] GRU Epoch 19 | train_loss 0.4915 acc 0.759 f1 0.757 || val_loss 1.4698 acc 0.409 f1 0.331\n",
            "[W3] GRU Epoch 20 | train_loss 0.4688 acc 0.769 f1 0.767 || val_loss 1.5121 acc 0.397 f1 0.318\n",
            "[W3] GRU Epoch 21 | train_loss 0.4432 acc 0.793 f1 0.792 || val_loss 1.5782 acc 0.397 f1 0.325\n",
            "[W3] GRU Epoch 22 | train_loss 0.4355 acc 0.788 f1 0.787 || val_loss 1.5884 acc 0.412 f1 0.328\n",
            "[W3] GRU Epoch 23 | train_loss 0.4148 acc 0.798 f1 0.797 || val_loss 1.6494 acc 0.403 f1 0.327\n",
            "[W3] GRU Epoch 24 | train_loss 0.4024 acc 0.804 f1 0.804 || val_loss 1.6931 acc 0.418 f1 0.333\n",
            "[W3] GRU Epoch 25 | train_loss 0.3783 acc 0.825 f1 0.825 || val_loss 1.7622 acc 0.412 f1 0.328\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=18\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 930, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0987 acc 0.334 f1 0.251 || val_loss 1.1010 acc 0.286 f1 0.242\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0943 acc 0.382 f1 0.362 || val_loss 1.1009 acc 0.288 f1 0.276\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0873 acc 0.411 f1 0.399 || val_loss 1.0981 acc 0.292 f1 0.274\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0733 acc 0.433 f1 0.423 || val_loss 1.1101 acc 0.292 f1 0.283\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0411 acc 0.472 f1 0.461 || val_loss 1.1046 acc 0.364 f1 0.342\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9665 acc 0.525 f1 0.513 || val_loss 1.0824 acc 0.374 f1 0.314\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8823 acc 0.567 f1 0.559 || val_loss 1.1171 acc 0.370 f1 0.303\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8153 acc 0.594 f1 0.586 || val_loss 1.1125 acc 0.405 f1 0.307\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7656 acc 0.623 f1 0.617 || val_loss 1.1233 acc 0.397 f1 0.305\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7232 acc 0.652 f1 0.647 || val_loss 1.1411 acc 0.403 f1 0.322\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6793 acc 0.665 f1 0.661 || val_loss 1.1764 acc 0.403 f1 0.317\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6516 acc 0.675 f1 0.672 || val_loss 1.2395 acc 0.393 f1 0.312\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6171 acc 0.695 f1 0.692 || val_loss 1.3173 acc 0.372 f1 0.310\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  18%|        | 18/100 [08:55<45:36, 33.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=19 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=19\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1287 acc 0.402 f1 0.397 || val_loss 1.1254 acc 0.344 f1 0.323\n",
            "[W3] ANN Epoch 02 | train_loss 0.9667 acc 0.518 f1 0.508 || val_loss 1.1139 acc 0.350 f1 0.319\n",
            "[W3] ANN Epoch 03 | train_loss 0.8677 acc 0.574 f1 0.566 || val_loss 1.0888 acc 0.370 f1 0.329\n",
            "[W3] ANN Epoch 04 | train_loss 0.7815 acc 0.625 f1 0.619 || val_loss 1.1010 acc 0.389 f1 0.337\n",
            "[W3] ANN Epoch 05 | train_loss 0.7225 acc 0.657 f1 0.654 || val_loss 1.1264 acc 0.412 f1 0.361\n",
            "[W3] ANN Epoch 06 | train_loss 0.6865 acc 0.670 f1 0.667 || val_loss 1.0988 acc 0.442 f1 0.380\n",
            "[W3] ANN Epoch 07 | train_loss 0.6411 acc 0.696 f1 0.693 || val_loss 1.1146 acc 0.424 f1 0.358\n",
            "[W3] ANN Epoch 08 | train_loss 0.6087 acc 0.716 f1 0.715 || val_loss 1.1184 acc 0.449 f1 0.378\n",
            "[W3] ANN Epoch 09 | train_loss 0.5749 acc 0.736 f1 0.735 || val_loss 1.1592 acc 0.451 f1 0.370\n",
            "[W3] ANN Epoch 10 | train_loss 0.5728 acc 0.732 f1 0.730 || val_loss 1.1838 acc 0.453 f1 0.362\n",
            "[W3] ANN Epoch 11 | train_loss 0.5335 acc 0.761 f1 0.760 || val_loss 1.1752 acc 0.453 f1 0.372\n",
            "[W3] ANN Epoch 12 | train_loss 0.5086 acc 0.771 f1 0.770 || val_loss 1.1978 acc 0.453 f1 0.368\n",
            "[W3] ANN Epoch 13 | train_loss 0.5000 acc 0.775 f1 0.773 || val_loss 1.2357 acc 0.432 f1 0.357\n",
            "[W3] ANN Epoch 14 | train_loss 0.4718 acc 0.786 f1 0.786 || val_loss 1.3035 acc 0.457 f1 0.365\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=19\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0682 acc 0.411 f1 0.410 || val_loss 1.0323 acc 0.397 f1 0.321\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9334 acc 0.554 f1 0.550 || val_loss 1.0552 acc 0.366 f1 0.298\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8144 acc 0.606 f1 0.603 || val_loss 1.1525 acc 0.344 f1 0.273\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7339 acc 0.647 f1 0.643 || val_loss 1.2055 acc 0.335 f1 0.298\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6633 acc 0.687 f1 0.683 || val_loss 1.2487 acc 0.356 f1 0.315\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6137 acc 0.717 f1 0.716 || val_loss 1.2646 acc 0.352 f1 0.311\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5537 acc 0.740 f1 0.739 || val_loss 1.2845 acc 0.383 f1 0.323\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5139 acc 0.766 f1 0.765 || val_loss 1.3777 acc 0.372 f1 0.321\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4775 acc 0.788 f1 0.786 || val_loss 1.4297 acc 0.356 f1 0.301\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4446 acc 0.802 f1 0.802 || val_loss 1.5102 acc 0.403 f1 0.334\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.4145 acc 0.822 f1 0.821 || val_loss 1.5000 acc 0.383 f1 0.330\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.4139 acc 0.827 f1 0.827 || val_loss 1.4538 acc 0.397 f1 0.335\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.4042 acc 0.828 f1 0.828 || val_loss 1.5256 acc 0.362 f1 0.311\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3735 acc 0.851 f1 0.851 || val_loss 1.5993 acc 0.383 f1 0.309\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.3391 acc 0.862 f1 0.861 || val_loss 1.7101 acc 0.385 f1 0.326\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.3025 acc 0.876 f1 0.876 || val_loss 1.6945 acc 0.391 f1 0.329\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.3089 acc 0.870 f1 0.870 || val_loss 1.7078 acc 0.416 f1 0.345\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.2791 acc 0.893 f1 0.893 || val_loss 1.7408 acc 0.416 f1 0.348\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.2318 acc 0.912 f1 0.912 || val_loss 1.8805 acc 0.409 f1 0.343\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.2610 acc 0.899 f1 0.898 || val_loss 1.8573 acc 0.420 f1 0.351\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.2327 acc 0.909 f1 0.909 || val_loss 1.9458 acc 0.409 f1 0.349\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.2426 acc 0.902 f1 0.902 || val_loss 1.8077 acc 0.420 f1 0.355\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.2152 acc 0.917 f1 0.917 || val_loss 1.8950 acc 0.436 f1 0.366\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.2343 acc 0.917 f1 0.917 || val_loss 1.8966 acc 0.407 f1 0.337\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.2025 acc 0.927 f1 0.927 || val_loss 1.9676 acc 0.416 f1 0.348\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.2114 acc 0.920 f1 0.919 || val_loss 1.9985 acc 0.407 f1 0.333\n",
            "[W3] CNN1D Epoch 27 | train_loss 0.1901 acc 0.930 f1 0.930 || val_loss 2.0317 acc 0.385 f1 0.308\n",
            "[W3] CNN1D Epoch 28 | train_loss 0.2043 acc 0.920 f1 0.920 || val_loss 2.0446 acc 0.395 f1 0.324\n",
            "[W3] CNN1D Epoch 29 | train_loss 0.1948 acc 0.931 f1 0.930 || val_loss 2.0994 acc 0.420 f1 0.342\n",
            "[W3] CNN1D Epoch 30 | train_loss 0.1904 acc 0.933 f1 0.933 || val_loss 2.0296 acc 0.409 f1 0.330\n",
            "[W3] CNN1D Epoch 31 | train_loss 0.1487 acc 0.951 f1 0.952 || val_loss 2.2401 acc 0.393 f1 0.343\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=19\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0987 acc 0.346 f1 0.295 || val_loss 1.0942 acc 0.397 f1 0.356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0840 acc 0.414 f1 0.413 || val_loss 1.0911 acc 0.383 f1 0.353\n",
            "[W3] RNN Epoch 03 | train_loss 1.0663 acc 0.430 f1 0.429 || val_loss 1.0965 acc 0.366 f1 0.352\n",
            "[W3] RNN Epoch 04 | train_loss 1.0531 acc 0.460 f1 0.452 || val_loss 1.0729 acc 0.397 f1 0.367\n",
            "[W3] RNN Epoch 05 | train_loss 1.0369 acc 0.468 f1 0.464 || val_loss 1.0942 acc 0.366 f1 0.351\n",
            "[W3] RNN Epoch 06 | train_loss 1.0254 acc 0.490 f1 0.487 || val_loss 1.0755 acc 0.409 f1 0.377\n",
            "[W3] RNN Epoch 07 | train_loss 1.0018 acc 0.514 f1 0.511 || val_loss 1.0790 acc 0.385 f1 0.363\n",
            "[W3] RNN Epoch 08 | train_loss 0.9861 acc 0.518 f1 0.512 || val_loss 1.0642 acc 0.405 f1 0.374\n",
            "[W3] RNN Epoch 09 | train_loss 0.9642 acc 0.538 f1 0.535 || val_loss 1.0739 acc 0.399 f1 0.375\n",
            "[W3] RNN Epoch 10 | train_loss 0.9425 acc 0.561 f1 0.558 || val_loss 1.0691 acc 0.379 f1 0.352\n",
            "[W3] RNN Epoch 11 | train_loss 0.9239 acc 0.558 f1 0.553 || val_loss 1.0887 acc 0.346 f1 0.327\n",
            "[W3] RNN Epoch 12 | train_loss 0.9064 acc 0.578 f1 0.572 || val_loss 1.0847 acc 0.356 f1 0.327\n",
            "[W3] RNN Epoch 13 | train_loss 0.8810 acc 0.587 f1 0.577 || val_loss 1.0810 acc 0.362 f1 0.329\n",
            "[W3] RNN Epoch 14 | train_loss 0.8637 acc 0.595 f1 0.590 || val_loss 1.1075 acc 0.333 f1 0.310\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=19\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0972 acc 0.355 f1 0.346 || val_loss 1.1001 acc 0.307 f1 0.298\n",
            "[W3] GRU Epoch 02 | train_loss 1.0920 acc 0.375 f1 0.364 || val_loss 1.1031 acc 0.302 f1 0.293\n",
            "[W3] GRU Epoch 03 | train_loss 1.0856 acc 0.397 f1 0.387 || val_loss 1.0964 acc 0.354 f1 0.339\n",
            "[W3] GRU Epoch 04 | train_loss 1.0757 acc 0.427 f1 0.416 || val_loss 1.0997 acc 0.340 f1 0.326\n",
            "[W3] GRU Epoch 05 | train_loss 1.0634 acc 0.447 f1 0.445 || val_loss 1.1018 acc 0.327 f1 0.315\n",
            "[W3] GRU Epoch 06 | train_loss 1.0398 acc 0.477 f1 0.465 || val_loss 1.0864 acc 0.346 f1 0.321\n",
            "[W3] GRU Epoch 07 | train_loss 1.0093 acc 0.492 f1 0.482 || val_loss 1.0843 acc 0.362 f1 0.334\n",
            "[W3] GRU Epoch 08 | train_loss 0.9446 acc 0.551 f1 0.543 || val_loss 1.0782 acc 0.372 f1 0.322\n",
            "[W3] GRU Epoch 09 | train_loss 0.8849 acc 0.577 f1 0.573 || val_loss 1.0760 acc 0.374 f1 0.326\n",
            "[W3] GRU Epoch 10 | train_loss 0.8474 acc 0.588 f1 0.582 || val_loss 1.1246 acc 0.362 f1 0.322\n",
            "[W3] GRU Epoch 11 | train_loss 0.7968 acc 0.620 f1 0.616 || val_loss 1.0913 acc 0.391 f1 0.336\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=19\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0982 acc 0.353 f1 0.273 || val_loss 1.1002 acc 0.280 f1 0.263\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0941 acc 0.368 f1 0.336 || val_loss 1.0879 acc 0.364 f1 0.315\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0890 acc 0.380 f1 0.348 || val_loss 1.0830 acc 0.335 f1 0.294\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0792 acc 0.402 f1 0.397 || val_loss 1.0869 acc 0.327 f1 0.318\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0648 acc 0.447 f1 0.429 || val_loss 1.0731 acc 0.362 f1 0.344\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0401 acc 0.475 f1 0.465 || val_loss 1.0815 acc 0.340 f1 0.324\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9840 acc 0.512 f1 0.499 || val_loss 1.0532 acc 0.391 f1 0.342\n",
            "[W3] LSTM Epoch 08 | train_loss 0.9097 acc 0.556 f1 0.547 || val_loss 1.1575 acc 0.337 f1 0.307\n",
            "[W3] LSTM Epoch 09 | train_loss 0.8467 acc 0.588 f1 0.579 || val_loss 1.0929 acc 0.395 f1 0.324\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7990 acc 0.605 f1 0.599 || val_loss 1.1434 acc 0.360 f1 0.306\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7519 acc 0.641 f1 0.634 || val_loss 1.1773 acc 0.383 f1 0.329\n",
            "[W3] LSTM Epoch 12 | train_loss 0.7340 acc 0.648 f1 0.641 || val_loss 1.1719 acc 0.389 f1 0.331\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6871 acc 0.654 f1 0.650 || val_loss 1.2067 acc 0.381 f1 0.321\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  19%|        | 19/100 [09:34<46:57, 34.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=20 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=20\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 927, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1266 acc 0.397 f1 0.398 || val_loss 1.1037 acc 0.354 f1 0.319\n",
            "[W3] ANN Epoch 02 | train_loss 0.9589 acc 0.516 f1 0.508 || val_loss 1.1417 acc 0.350 f1 0.312\n",
            "[W3] ANN Epoch 03 | train_loss 0.8617 acc 0.583 f1 0.576 || val_loss 1.1231 acc 0.399 f1 0.344\n",
            "[W3] ANN Epoch 04 | train_loss 0.7923 acc 0.617 f1 0.611 || val_loss 1.1171 acc 0.397 f1 0.335\n",
            "[W3] ANN Epoch 05 | train_loss 0.7280 acc 0.646 f1 0.642 || val_loss 1.1301 acc 0.449 f1 0.391\n",
            "[W3] ANN Epoch 06 | train_loss 0.6913 acc 0.662 f1 0.660 || val_loss 1.1481 acc 0.387 f1 0.325\n",
            "[W3] ANN Epoch 07 | train_loss 0.6465 acc 0.698 f1 0.696 || val_loss 1.1415 acc 0.405 f1 0.342\n",
            "[W3] ANN Epoch 08 | train_loss 0.6204 acc 0.700 f1 0.698 || val_loss 1.1996 acc 0.422 f1 0.353\n",
            "[W3] ANN Epoch 09 | train_loss 0.5878 acc 0.730 f1 0.728 || val_loss 1.2092 acc 0.424 f1 0.341\n",
            "[W3] ANN Epoch 10 | train_loss 0.5707 acc 0.734 f1 0.732 || val_loss 1.2232 acc 0.430 f1 0.350\n",
            "[W3] ANN Epoch 11 | train_loss 0.5587 acc 0.747 f1 0.746 || val_loss 1.2138 acc 0.430 f1 0.346\n",
            "[W3] ANN Epoch 12 | train_loss 0.5200 acc 0.767 f1 0.765 || val_loss 1.2642 acc 0.416 f1 0.343\n",
            "[W3] ANN Epoch 13 | train_loss 0.4974 acc 0.771 f1 0.770 || val_loss 1.2861 acc 0.407 f1 0.323\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=20\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 927, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0729 acc 0.410 f1 0.410 || val_loss 1.0221 acc 0.428 f1 0.342\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9336 acc 0.561 f1 0.559 || val_loss 1.0258 acc 0.399 f1 0.306\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.7989 acc 0.608 f1 0.604 || val_loss 1.1033 acc 0.366 f1 0.297\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7185 acc 0.663 f1 0.662 || val_loss 1.1497 acc 0.383 f1 0.314\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6437 acc 0.696 f1 0.694 || val_loss 1.2332 acc 0.385 f1 0.315\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5936 acc 0.734 f1 0.732 || val_loss 1.2714 acc 0.412 f1 0.338\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5360 acc 0.750 f1 0.749 || val_loss 1.3588 acc 0.372 f1 0.307\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4803 acc 0.789 f1 0.788 || val_loss 1.4241 acc 0.399 f1 0.333\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4415 acc 0.812 f1 0.810 || val_loss 1.5202 acc 0.399 f1 0.322\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=20\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 927, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0970 acc 0.359 f1 0.351 || val_loss 1.0931 acc 0.356 f1 0.315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0819 acc 0.415 f1 0.413 || val_loss 1.0839 acc 0.372 f1 0.329\n",
            "[W3] RNN Epoch 03 | train_loss 1.0669 acc 0.450 f1 0.447 || val_loss 1.0773 acc 0.356 f1 0.323\n",
            "[W3] RNN Epoch 04 | train_loss 1.0488 acc 0.464 f1 0.460 || val_loss 1.0742 acc 0.358 f1 0.328\n",
            "[W3] RNN Epoch 05 | train_loss 1.0336 acc 0.480 f1 0.473 || val_loss 1.0712 acc 0.372 f1 0.335\n",
            "[W3] RNN Epoch 06 | train_loss 1.0101 acc 0.498 f1 0.493 || val_loss 1.0739 acc 0.358 f1 0.328\n",
            "[W3] RNN Epoch 07 | train_loss 0.9815 acc 0.525 f1 0.519 || val_loss 1.0736 acc 0.342 f1 0.310\n",
            "[W3] RNN Epoch 08 | train_loss 0.9552 acc 0.544 f1 0.537 || val_loss 1.0916 acc 0.350 f1 0.306\n",
            "[W3] RNN Epoch 09 | train_loss 0.9300 acc 0.555 f1 0.548 || val_loss 1.0741 acc 0.350 f1 0.313\n",
            "[W3] RNN Epoch 10 | train_loss 0.8988 acc 0.568 f1 0.560 || val_loss 1.0866 acc 0.370 f1 0.325\n",
            "[W3] RNN Epoch 11 | train_loss 0.8770 acc 0.580 f1 0.572 || val_loss 1.0938 acc 0.368 f1 0.327\n",
            "[W3] RNN Epoch 12 | train_loss 0.8551 acc 0.597 f1 0.590 || val_loss 1.1325 acc 0.325 f1 0.297\n",
            "[W3] RNN Epoch 13 | train_loss 0.8264 acc 0.603 f1 0.594 || val_loss 1.1147 acc 0.358 f1 0.314\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=20\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 927, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0977 acc 0.347 f1 0.329 || val_loss 1.0987 acc 0.313 f1 0.296\n",
            "[W3] GRU Epoch 02 | train_loss 1.0889 acc 0.407 f1 0.399 || val_loss 1.0957 acc 0.331 f1 0.311\n",
            "[W3] GRU Epoch 03 | train_loss 1.0775 acc 0.425 f1 0.422 || val_loss 1.1017 acc 0.288 f1 0.281\n",
            "[W3] GRU Epoch 04 | train_loss 1.0584 acc 0.452 f1 0.444 || val_loss 1.0844 acc 0.340 f1 0.312\n",
            "[W3] GRU Epoch 05 | train_loss 1.0286 acc 0.486 f1 0.477 || val_loss 1.1036 acc 0.335 f1 0.317\n",
            "[W3] GRU Epoch 06 | train_loss 0.9735 acc 0.523 f1 0.513 || val_loss 1.1294 acc 0.340 f1 0.315\n",
            "[W3] GRU Epoch 07 | train_loss 0.9137 acc 0.557 f1 0.546 || val_loss 1.0912 acc 0.354 f1 0.304\n",
            "[W3] GRU Epoch 08 | train_loss 0.8545 acc 0.582 f1 0.575 || val_loss 1.1005 acc 0.379 f1 0.316\n",
            "[W3] GRU Epoch 09 | train_loss 0.8009 acc 0.613 f1 0.606 || val_loss 1.1374 acc 0.362 f1 0.303\n",
            "[W3] GRU Epoch 10 | train_loss 0.7641 acc 0.631 f1 0.624 || val_loss 1.1678 acc 0.354 f1 0.300\n",
            "[W3] GRU Epoch 11 | train_loss 0.7284 acc 0.650 f1 0.644 || val_loss 1.1590 acc 0.381 f1 0.298\n",
            "[W3] GRU Epoch 12 | train_loss 0.6977 acc 0.667 f1 0.661 || val_loss 1.1842 acc 0.383 f1 0.315\n",
            "[W3] GRU Epoch 13 | train_loss 0.6592 acc 0.687 f1 0.683 || val_loss 1.2344 acc 0.391 f1 0.331\n",
            "[W3] GRU Epoch 14 | train_loss 0.6371 acc 0.682 f1 0.678 || val_loss 1.2744 acc 0.370 f1 0.313\n",
            "[W3] GRU Epoch 15 | train_loss 0.6043 acc 0.704 f1 0.702 || val_loss 1.2836 acc 0.372 f1 0.310\n",
            "[W3] GRU Epoch 16 | train_loss 0.5798 acc 0.725 f1 0.723 || val_loss 1.2943 acc 0.377 f1 0.309\n",
            "[W3] GRU Epoch 17 | train_loss 0.5560 acc 0.733 f1 0.731 || val_loss 1.3431 acc 0.379 f1 0.326\n",
            "[W3] GRU Epoch 18 | train_loss 0.5258 acc 0.738 f1 0.736 || val_loss 1.3717 acc 0.379 f1 0.315\n",
            "[W3] GRU Epoch 19 | train_loss 0.5125 acc 0.753 f1 0.751 || val_loss 1.4310 acc 0.364 f1 0.305\n",
            "[W3] GRU Epoch 20 | train_loss 0.4936 acc 0.762 f1 0.760 || val_loss 1.4438 acc 0.368 f1 0.304\n",
            "[W3] GRU Epoch 21 | train_loss 0.4664 acc 0.779 f1 0.778 || val_loss 1.4871 acc 0.381 f1 0.317\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=20\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 927, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0987 acc 0.330 f1 0.318 || val_loss 1.1016 acc 0.257 f1 0.257\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0938 acc 0.383 f1 0.349 || val_loss 1.1015 acc 0.276 f1 0.273\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0869 acc 0.423 f1 0.413 || val_loss 1.0905 acc 0.342 f1 0.317\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0733 acc 0.437 f1 0.433 || val_loss 1.1097 acc 0.302 f1 0.300\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0384 acc 0.480 f1 0.465 || val_loss 1.0924 acc 0.329 f1 0.310\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9536 acc 0.536 f1 0.524 || val_loss 1.0549 acc 0.393 f1 0.334\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8545 acc 0.566 f1 0.559 || val_loss 1.0895 acc 0.381 f1 0.339\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7957 acc 0.605 f1 0.597 || val_loss 1.1165 acc 0.379 f1 0.323\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7598 acc 0.617 f1 0.611 || val_loss 1.1351 acc 0.393 f1 0.346\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7234 acc 0.639 f1 0.634 || val_loss 1.1429 acc 0.381 f1 0.328\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6903 acc 0.661 f1 0.656 || val_loss 1.1396 acc 0.418 f1 0.336\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6621 acc 0.667 f1 0.663 || val_loss 1.1759 acc 0.407 f1 0.339\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6354 acc 0.676 f1 0.672 || val_loss 1.2191 acc 0.403 f1 0.338\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6053 acc 0.693 f1 0.690 || val_loss 1.2424 acc 0.418 f1 0.349\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5753 acc 0.707 f1 0.704 || val_loss 1.2811 acc 0.409 f1 0.331\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5560 acc 0.718 f1 0.716 || val_loss 1.3255 acc 0.391 f1 0.321\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5216 acc 0.738 f1 0.737 || val_loss 1.3847 acc 0.405 f1 0.332\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5061 acc 0.748 f1 0.747 || val_loss 1.4134 acc 0.407 f1 0.338\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4914 acc 0.749 f1 0.748 || val_loss 1.4443 acc 0.416 f1 0.343\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4693 acc 0.766 f1 0.766 || val_loss 1.5381 acc 0.414 f1 0.350\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4504 acc 0.780 f1 0.780 || val_loss 1.5436 acc 0.401 f1 0.333\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4361 acc 0.786 f1 0.786 || val_loss 1.5704 acc 0.418 f1 0.349\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4172 acc 0.809 f1 0.809 || val_loss 1.6305 acc 0.414 f1 0.343\n",
            "[W3] LSTM Epoch 24 | train_loss 0.4019 acc 0.807 f1 0.807 || val_loss 1.7210 acc 0.414 f1 0.346\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3911 acc 0.812 f1 0.812 || val_loss 1.7205 acc 0.409 f1 0.338\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3744 acc 0.824 f1 0.824 || val_loss 1.7420 acc 0.420 f1 0.340\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3528 acc 0.842 f1 0.842 || val_loss 1.8010 acc 0.412 f1 0.333\n",
            "[W3] LSTM Epoch 28 | train_loss 0.3375 acc 0.846 f1 0.846 || val_loss 1.8520 acc 0.422 f1 0.349\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  20%|        | 20/100 [09:58<42:23, 31.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=21 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=21\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 931, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1416 acc 0.401 f1 0.401 || val_loss 1.1500 acc 0.327 f1 0.308\n",
            "[W3] ANN Epoch 02 | train_loss 0.9757 acc 0.512 f1 0.505 || val_loss 1.1685 acc 0.302 f1 0.284\n",
            "[W3] ANN Epoch 03 | train_loss 0.8970 acc 0.561 f1 0.550 || val_loss 1.1387 acc 0.337 f1 0.308\n",
            "[W3] ANN Epoch 04 | train_loss 0.8061 acc 0.614 f1 0.605 || val_loss 1.1243 acc 0.377 f1 0.333\n",
            "[W3] ANN Epoch 05 | train_loss 0.7226 acc 0.650 f1 0.644 || val_loss 1.1488 acc 0.385 f1 0.345\n",
            "[W3] ANN Epoch 06 | train_loss 0.6754 acc 0.679 f1 0.675 || val_loss 1.1489 acc 0.393 f1 0.334\n",
            "[W3] ANN Epoch 07 | train_loss 0.6255 acc 0.704 f1 0.701 || val_loss 1.1194 acc 0.391 f1 0.341\n",
            "[W3] ANN Epoch 08 | train_loss 0.5816 acc 0.725 f1 0.723 || val_loss 1.1646 acc 0.414 f1 0.346\n",
            "[W3] ANN Epoch 09 | train_loss 0.5625 acc 0.740 f1 0.738 || val_loss 1.1891 acc 0.401 f1 0.337\n",
            "[W3] ANN Epoch 10 | train_loss 0.5349 acc 0.760 f1 0.759 || val_loss 1.1847 acc 0.424 f1 0.357\n",
            "[W3] ANN Epoch 11 | train_loss 0.5258 acc 0.757 f1 0.756 || val_loss 1.2327 acc 0.409 f1 0.339\n",
            "[W3] ANN Epoch 12 | train_loss 0.4932 acc 0.781 f1 0.780 || val_loss 1.2369 acc 0.432 f1 0.381\n",
            "[W3] ANN Epoch 13 | train_loss 0.4644 acc 0.790 f1 0.789 || val_loss 1.3559 acc 0.407 f1 0.346\n",
            "[W3] ANN Epoch 14 | train_loss 0.4608 acc 0.802 f1 0.801 || val_loss 1.3563 acc 0.409 f1 0.351\n",
            "[W3] ANN Epoch 15 | train_loss 0.4246 acc 0.814 f1 0.814 || val_loss 1.3582 acc 0.395 f1 0.338\n",
            "[W3] ANN Epoch 16 | train_loss 0.4041 acc 0.827 f1 0.826 || val_loss 1.3961 acc 0.399 f1 0.336\n",
            "[W3] ANN Epoch 17 | train_loss 0.3887 acc 0.845 f1 0.845 || val_loss 1.4319 acc 0.414 f1 0.341\n",
            "[W3] ANN Epoch 18 | train_loss 0.3659 acc 0.844 f1 0.844 || val_loss 1.4795 acc 0.432 f1 0.362\n",
            "[W3] ANN Epoch 19 | train_loss 0.3600 acc 0.848 f1 0.847 || val_loss 1.4462 acc 0.430 f1 0.365\n",
            "[W3] ANN Epoch 20 | train_loss 0.3494 acc 0.853 f1 0.852 || val_loss 1.4605 acc 0.449 f1 0.369\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=21\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 931, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0760 acc 0.404 f1 0.405 || val_loss 1.0248 acc 0.409 f1 0.280\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9416 acc 0.554 f1 0.552 || val_loss 1.0620 acc 0.383 f1 0.287\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8139 acc 0.617 f1 0.612 || val_loss 1.0932 acc 0.393 f1 0.334\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7109 acc 0.669 f1 0.666 || val_loss 1.1503 acc 0.414 f1 0.357\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6269 acc 0.705 f1 0.704 || val_loss 1.2203 acc 0.391 f1 0.354\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5637 acc 0.750 f1 0.748 || val_loss 1.2374 acc 0.405 f1 0.361\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5136 acc 0.774 f1 0.774 || val_loss 1.3023 acc 0.416 f1 0.344\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4399 acc 0.808 f1 0.808 || val_loss 1.4250 acc 0.399 f1 0.336\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.3937 acc 0.832 f1 0.831 || val_loss 1.4979 acc 0.416 f1 0.347\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3447 acc 0.860 f1 0.860 || val_loss 1.6131 acc 0.428 f1 0.353\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3030 acc 0.879 f1 0.879 || val_loss 1.7129 acc 0.426 f1 0.351\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2695 acc 0.898 f1 0.898 || val_loss 1.8138 acc 0.395 f1 0.343\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2470 acc 0.905 f1 0.905 || val_loss 1.8447 acc 0.424 f1 0.360\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2052 acc 0.921 f1 0.921 || val_loss 2.0048 acc 0.403 f1 0.350\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=21\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 931, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0991 acc 0.362 f1 0.306 || val_loss 1.1091 acc 0.317 f1 0.298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0826 acc 0.407 f1 0.397 || val_loss 1.0977 acc 0.346 f1 0.314\n",
            "[W3] RNN Epoch 03 | train_loss 1.0698 acc 0.433 f1 0.430 || val_loss 1.0952 acc 0.342 f1 0.316\n",
            "[W3] RNN Epoch 04 | train_loss 1.0530 acc 0.462 f1 0.459 || val_loss 1.0918 acc 0.321 f1 0.301\n",
            "[W3] RNN Epoch 05 | train_loss 1.0356 acc 0.473 f1 0.466 || val_loss 1.0824 acc 0.337 f1 0.308\n",
            "[W3] RNN Epoch 06 | train_loss 1.0159 acc 0.488 f1 0.482 || val_loss 1.0743 acc 0.344 f1 0.312\n",
            "[W3] RNN Epoch 07 | train_loss 0.9947 acc 0.516 f1 0.512 || val_loss 1.0809 acc 0.358 f1 0.326\n",
            "[W3] RNN Epoch 08 | train_loss 0.9721 acc 0.527 f1 0.519 || val_loss 1.0854 acc 0.348 f1 0.319\n",
            "[W3] RNN Epoch 09 | train_loss 0.9451 acc 0.553 f1 0.546 || val_loss 1.0925 acc 0.354 f1 0.319\n",
            "[W3] RNN Epoch 10 | train_loss 0.9228 acc 0.558 f1 0.550 || val_loss 1.0935 acc 0.366 f1 0.322\n",
            "[W3] RNN Epoch 11 | train_loss 0.9066 acc 0.573 f1 0.565 || val_loss 1.1010 acc 0.379 f1 0.340\n",
            "[W3] RNN Epoch 12 | train_loss 0.8757 acc 0.584 f1 0.574 || val_loss 1.1056 acc 0.379 f1 0.337\n",
            "[W3] RNN Epoch 13 | train_loss 0.8641 acc 0.589 f1 0.581 || val_loss 1.1206 acc 0.393 f1 0.352\n",
            "[W3] RNN Epoch 14 | train_loss 0.8406 acc 0.600 f1 0.592 || val_loss 1.1161 acc 0.374 f1 0.334\n",
            "[W3] RNN Epoch 15 | train_loss 0.8166 acc 0.633 f1 0.626 || val_loss 1.1325 acc 0.379 f1 0.341\n",
            "[W3] RNN Epoch 16 | train_loss 0.8007 acc 0.634 f1 0.626 || val_loss 1.1413 acc 0.389 f1 0.348\n",
            "[W3] RNN Epoch 17 | train_loss 0.7723 acc 0.648 f1 0.640 || val_loss 1.1681 acc 0.379 f1 0.344\n",
            "[W3] RNN Epoch 18 | train_loss 0.7518 acc 0.662 f1 0.654 || val_loss 1.1667 acc 0.401 f1 0.363\n",
            "[W3] RNN Epoch 19 | train_loss 0.7374 acc 0.663 f1 0.656 || val_loss 1.1842 acc 0.438 f1 0.393\n",
            "[W3] RNN Epoch 20 | train_loss 0.7130 acc 0.678 f1 0.672 || val_loss 1.2047 acc 0.422 f1 0.372\n",
            "[W3] RNN Epoch 21 | train_loss 0.6907 acc 0.695 f1 0.688 || val_loss 1.2111 acc 0.407 f1 0.367\n",
            "[W3] RNN Epoch 22 | train_loss 0.6613 acc 0.696 f1 0.690 || val_loss 1.2437 acc 0.414 f1 0.374\n",
            "[W3] RNN Epoch 23 | train_loss 0.6547 acc 0.696 f1 0.690 || val_loss 1.2340 acc 0.428 f1 0.382\n",
            "[W3] RNN Epoch 24 | train_loss 0.6273 acc 0.715 f1 0.710 || val_loss 1.2531 acc 0.418 f1 0.373\n",
            "[W3] RNN Epoch 25 | train_loss 0.6116 acc 0.723 f1 0.718 || val_loss 1.2760 acc 0.438 f1 0.389\n",
            "[W3] RNN Epoch 26 | train_loss 0.5853 acc 0.719 f1 0.715 || val_loss 1.3048 acc 0.444 f1 0.384\n",
            "[W3] RNN Epoch 27 | train_loss 0.5726 acc 0.735 f1 0.731 || val_loss 1.3358 acc 0.426 f1 0.382\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=21\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 931, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1023 acc 0.334 f1 0.210 || val_loss 1.0892 acc 0.407 f1 0.291\n",
            "[W3] GRU Epoch 02 | train_loss 1.0923 acc 0.363 f1 0.352 || val_loss 1.0987 acc 0.337 f1 0.326\n",
            "[W3] GRU Epoch 03 | train_loss 1.0826 acc 0.416 f1 0.405 || val_loss 1.0894 acc 0.360 f1 0.343\n",
            "[W3] GRU Epoch 04 | train_loss 1.0702 acc 0.431 f1 0.421 || val_loss 1.0736 acc 0.362 f1 0.324\n",
            "[W3] GRU Epoch 05 | train_loss 1.0429 acc 0.466 f1 0.459 || val_loss 1.0987 acc 0.348 f1 0.326\n",
            "[W3] GRU Epoch 06 | train_loss 0.9900 acc 0.515 f1 0.502 || val_loss 1.0875 acc 0.342 f1 0.304\n",
            "[W3] GRU Epoch 07 | train_loss 0.9179 acc 0.555 f1 0.547 || val_loss 1.1262 acc 0.354 f1 0.311\n",
            "[W3] GRU Epoch 08 | train_loss 0.8498 acc 0.583 f1 0.578 || val_loss 1.1777 acc 0.325 f1 0.292\n",
            "[W3] GRU Epoch 09 | train_loss 0.7962 acc 0.615 f1 0.610 || val_loss 1.1539 acc 0.360 f1 0.312\n",
            "[W3] GRU Epoch 10 | train_loss 0.7620 acc 0.624 f1 0.621 || val_loss 1.1572 acc 0.354 f1 0.307\n",
            "[W3] GRU Epoch 11 | train_loss 0.7232 acc 0.649 f1 0.645 || val_loss 1.1468 acc 0.393 f1 0.323\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=21\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 931, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1017 acc 0.348 f1 0.233 || val_loss 1.1016 acc 0.420 f1 0.264\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0940 acc 0.380 f1 0.374 || val_loss 1.0943 acc 0.356 f1 0.318\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0863 acc 0.414 f1 0.405 || val_loss 1.0897 acc 0.372 f1 0.345\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0784 acc 0.415 f1 0.414 || val_loss 1.0892 acc 0.368 f1 0.351\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0556 acc 0.468 f1 0.467 || val_loss 1.0938 acc 0.354 f1 0.339\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0174 acc 0.507 f1 0.502 || val_loss 1.0709 acc 0.391 f1 0.337\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9296 acc 0.563 f1 0.557 || val_loss 1.0838 acc 0.383 f1 0.318\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8430 acc 0.602 f1 0.597 || val_loss 1.1170 acc 0.401 f1 0.320\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7810 acc 0.633 f1 0.628 || val_loss 1.1405 acc 0.379 f1 0.313\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7324 acc 0.652 f1 0.647 || val_loss 1.1767 acc 0.395 f1 0.299\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7001 acc 0.668 f1 0.665 || val_loss 1.1854 acc 0.393 f1 0.312\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6611 acc 0.694 f1 0.691 || val_loss 1.2164 acc 0.405 f1 0.322\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  21%|        | 21/100 [10:30<41:59, 31.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=22 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=22\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 986, np.int64(1): 928, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 986, np.int64(2): 986, np.int64(0): 986})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1362 acc 0.387 f1 0.382 || val_loss 1.1626 acc 0.294 f1 0.287\n",
            "[W3] ANN Epoch 02 | train_loss 0.9691 acc 0.506 f1 0.493 || val_loss 1.1257 acc 0.319 f1 0.295\n",
            "[W3] ANN Epoch 03 | train_loss 0.8471 acc 0.591 f1 0.582 || val_loss 1.1204 acc 0.356 f1 0.319\n",
            "[W3] ANN Epoch 04 | train_loss 0.7853 acc 0.621 f1 0.613 || val_loss 1.1366 acc 0.364 f1 0.324\n",
            "[W3] ANN Epoch 05 | train_loss 0.7227 acc 0.646 f1 0.641 || val_loss 1.1196 acc 0.381 f1 0.318\n",
            "[W3] ANN Epoch 06 | train_loss 0.6590 acc 0.693 f1 0.690 || val_loss 1.1461 acc 0.377 f1 0.321\n",
            "[W3] ANN Epoch 07 | train_loss 0.6358 acc 0.703 f1 0.701 || val_loss 1.1492 acc 0.409 f1 0.325\n",
            "[W3] ANN Epoch 08 | train_loss 0.5831 acc 0.715 f1 0.714 || val_loss 1.1878 acc 0.418 f1 0.353\n",
            "[W3] ANN Epoch 09 | train_loss 0.5705 acc 0.730 f1 0.727 || val_loss 1.2161 acc 0.414 f1 0.346\n",
            "[W3] ANN Epoch 10 | train_loss 0.5475 acc 0.746 f1 0.745 || val_loss 1.2168 acc 0.422 f1 0.343\n",
            "[W3] ANN Epoch 11 | train_loss 0.4961 acc 0.768 f1 0.768 || val_loss 1.2511 acc 0.449 f1 0.354\n",
            "[W3] ANN Epoch 12 | train_loss 0.4815 acc 0.783 f1 0.782 || val_loss 1.2660 acc 0.426 f1 0.342\n",
            "[W3] ANN Epoch 13 | train_loss 0.4753 acc 0.787 f1 0.787 || val_loss 1.2853 acc 0.440 f1 0.356\n",
            "[W3] ANN Epoch 14 | train_loss 0.4448 acc 0.813 f1 0.813 || val_loss 1.3490 acc 0.436 f1 0.355\n",
            "[W3] ANN Epoch 15 | train_loss 0.4465 acc 0.807 f1 0.807 || val_loss 1.3862 acc 0.434 f1 0.370\n",
            "[W3] ANN Epoch 16 | train_loss 0.4326 acc 0.818 f1 0.818 || val_loss 1.3927 acc 0.438 f1 0.365\n",
            "[W3] ANN Epoch 17 | train_loss 0.4231 acc 0.816 f1 0.816 || val_loss 1.3597 acc 0.461 f1 0.373\n",
            "[W3] ANN Epoch 18 | train_loss 0.3912 acc 0.827 f1 0.827 || val_loss 1.4022 acc 0.434 f1 0.366\n",
            "[W3] ANN Epoch 19 | train_loss 0.3736 acc 0.848 f1 0.848 || val_loss 1.4177 acc 0.455 f1 0.380\n",
            "[W3] ANN Epoch 20 | train_loss 0.3830 acc 0.832 f1 0.832 || val_loss 1.4892 acc 0.438 f1 0.355\n",
            "[W3] ANN Epoch 21 | train_loss 0.3622 acc 0.847 f1 0.847 || val_loss 1.5034 acc 0.438 f1 0.369\n",
            "[W3] ANN Epoch 22 | train_loss 0.3581 acc 0.857 f1 0.857 || val_loss 1.4887 acc 0.432 f1 0.351\n",
            "[W3] ANN Epoch 23 | train_loss 0.3175 acc 0.871 f1 0.871 || val_loss 1.5820 acc 0.414 f1 0.338\n",
            "[W3] ANN Epoch 24 | train_loss 0.3252 acc 0.861 f1 0.861 || val_loss 1.5915 acc 0.434 f1 0.351\n",
            "[W3] ANN Epoch 25 | train_loss 0.3173 acc 0.869 f1 0.869 || val_loss 1.6030 acc 0.418 f1 0.341\n",
            "[W3] ANN Epoch 26 | train_loss 0.3151 acc 0.876 f1 0.876 || val_loss 1.5984 acc 0.424 f1 0.340\n",
            "[W3] ANN Epoch 27 | train_loss 0.2954 acc 0.881 f1 0.881 || val_loss 1.6273 acc 0.438 f1 0.355\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=22\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 986, np.int64(1): 928, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 986, np.int64(2): 986, np.int64(0): 986})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0719 acc 0.392 f1 0.394 || val_loss 1.0421 acc 0.387 f1 0.288\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9338 acc 0.541 f1 0.536 || val_loss 1.0842 acc 0.374 f1 0.306\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8060 acc 0.608 f1 0.606 || val_loss 1.1566 acc 0.364 f1 0.308\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7199 acc 0.670 f1 0.668 || val_loss 1.1810 acc 0.370 f1 0.309\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6374 acc 0.715 f1 0.713 || val_loss 1.2172 acc 0.403 f1 0.334\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5739 acc 0.739 f1 0.738 || val_loss 1.2954 acc 0.383 f1 0.330\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5040 acc 0.782 f1 0.781 || val_loss 1.3591 acc 0.403 f1 0.338\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4440 acc 0.816 f1 0.815 || val_loss 1.4663 acc 0.389 f1 0.321\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4248 acc 0.815 f1 0.814 || val_loss 1.5049 acc 0.379 f1 0.311\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3673 acc 0.849 f1 0.849 || val_loss 1.5958 acc 0.374 f1 0.305\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3246 acc 0.869 f1 0.869 || val_loss 1.7377 acc 0.387 f1 0.322\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2925 acc 0.886 f1 0.886 || val_loss 1.7893 acc 0.370 f1 0.301\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2415 acc 0.907 f1 0.907 || val_loss 1.8938 acc 0.387 f1 0.311\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2186 acc 0.919 f1 0.919 || val_loss 1.9941 acc 0.407 f1 0.351\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1936 acc 0.931 f1 0.931 || val_loss 2.1471 acc 0.409 f1 0.347\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1860 acc 0.931 f1 0.931 || val_loss 2.1841 acc 0.385 f1 0.316\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1445 acc 0.950 f1 0.950 || val_loss 2.3650 acc 0.377 f1 0.303\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1314 acc 0.956 f1 0.956 || val_loss 2.3335 acc 0.389 f1 0.324\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1357 acc 0.953 f1 0.953 || val_loss 2.5398 acc 0.387 f1 0.328\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1367 acc 0.954 f1 0.954 || val_loss 2.5224 acc 0.405 f1 0.332\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.1130 acc 0.963 f1 0.963 || val_loss 2.5557 acc 0.393 f1 0.327\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.0878 acc 0.973 f1 0.973 || val_loss 2.7109 acc 0.403 f1 0.320\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=22\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 986, np.int64(1): 928, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 986, np.int64(2): 986, np.int64(0): 986})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1013 acc 0.357 f1 0.316 || val_loss 1.1025 acc 0.296 f1 0.278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0822 acc 0.416 f1 0.397 || val_loss 1.0946 acc 0.323 f1 0.310\n",
            "[W3] RNN Epoch 03 | train_loss 1.0690 acc 0.437 f1 0.430 || val_loss 1.0957 acc 0.333 f1 0.321\n",
            "[W3] RNN Epoch 04 | train_loss 1.0529 acc 0.455 f1 0.446 || val_loss 1.0924 acc 0.333 f1 0.314\n",
            "[W3] RNN Epoch 05 | train_loss 1.0379 acc 0.476 f1 0.468 || val_loss 1.0888 acc 0.342 f1 0.316\n",
            "[W3] RNN Epoch 06 | train_loss 1.0156 acc 0.511 f1 0.506 || val_loss 1.0921 acc 0.331 f1 0.309\n",
            "[W3] RNN Epoch 07 | train_loss 0.9969 acc 0.515 f1 0.508 || val_loss 1.0630 acc 0.381 f1 0.345\n",
            "[W3] RNN Epoch 08 | train_loss 0.9779 acc 0.525 f1 0.521 || val_loss 1.0705 acc 0.385 f1 0.347\n",
            "[W3] RNN Epoch 09 | train_loss 0.9624 acc 0.536 f1 0.528 || val_loss 1.0790 acc 0.379 f1 0.345\n",
            "[W3] RNN Epoch 10 | train_loss 0.9306 acc 0.559 f1 0.552 || val_loss 1.0862 acc 0.354 f1 0.323\n",
            "[W3] RNN Epoch 11 | train_loss 0.9137 acc 0.565 f1 0.556 || val_loss 1.0765 acc 0.366 f1 0.321\n",
            "[W3] RNN Epoch 12 | train_loss 0.8904 acc 0.580 f1 0.573 || val_loss 1.0973 acc 0.368 f1 0.337\n",
            "[W3] RNN Epoch 13 | train_loss 0.8689 acc 0.589 f1 0.578 || val_loss 1.1058 acc 0.350 f1 0.316\n",
            "[W3] RNN Epoch 14 | train_loss 0.8465 acc 0.600 f1 0.590 || val_loss 1.1000 acc 0.366 f1 0.325\n",
            "[W3] RNN Epoch 15 | train_loss 0.8245 acc 0.611 f1 0.602 || val_loss 1.0984 acc 0.364 f1 0.316\n",
            "[W3] RNN Epoch 16 | train_loss 0.8044 acc 0.615 f1 0.606 || val_loss 1.1124 acc 0.370 f1 0.329\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=22\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 986, np.int64(1): 928, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 986, np.int64(2): 986, np.int64(0): 986})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1009 acc 0.325 f1 0.274 || val_loss 1.1021 acc 0.296 f1 0.263\n",
            "[W3] GRU Epoch 02 | train_loss 1.0926 acc 0.394 f1 0.394 || val_loss 1.0991 acc 0.329 f1 0.296\n",
            "[W3] GRU Epoch 03 | train_loss 1.0841 acc 0.433 f1 0.419 || val_loss 1.1066 acc 0.309 f1 0.300\n",
            "[W3] GRU Epoch 04 | train_loss 1.0700 acc 0.459 f1 0.451 || val_loss 1.1058 acc 0.311 f1 0.299\n",
            "[W3] GRU Epoch 05 | train_loss 1.0487 acc 0.479 f1 0.473 || val_loss 1.0983 acc 0.315 f1 0.286\n",
            "[W3] GRU Epoch 06 | train_loss 1.0046 acc 0.523 f1 0.518 || val_loss 1.0955 acc 0.358 f1 0.324\n",
            "[W3] GRU Epoch 07 | train_loss 0.9398 acc 0.556 f1 0.550 || val_loss 1.1255 acc 0.358 f1 0.310\n",
            "[W3] GRU Epoch 08 | train_loss 0.8710 acc 0.584 f1 0.578 || val_loss 1.1118 acc 0.389 f1 0.328\n",
            "[W3] GRU Epoch 09 | train_loss 0.8092 acc 0.618 f1 0.615 || val_loss 1.1644 acc 0.387 f1 0.331\n",
            "[W3] GRU Epoch 10 | train_loss 0.7606 acc 0.649 f1 0.647 || val_loss 1.1649 acc 0.397 f1 0.326\n",
            "[W3] GRU Epoch 11 | train_loss 0.7225 acc 0.667 f1 0.666 || val_loss 1.1608 acc 0.414 f1 0.333\n",
            "[W3] GRU Epoch 12 | train_loss 0.6863 acc 0.677 f1 0.676 || val_loss 1.1791 acc 0.409 f1 0.338\n",
            "[W3] GRU Epoch 13 | train_loss 0.6504 acc 0.707 f1 0.705 || val_loss 1.1863 acc 0.412 f1 0.339\n",
            "[W3] GRU Epoch 14 | train_loss 0.6201 acc 0.723 f1 0.722 || val_loss 1.2429 acc 0.401 f1 0.335\n",
            "[W3] GRU Epoch 15 | train_loss 0.5997 acc 0.722 f1 0.720 || val_loss 1.2605 acc 0.428 f1 0.357\n",
            "[W3] GRU Epoch 16 | train_loss 0.5664 acc 0.737 f1 0.737 || val_loss 1.3087 acc 0.424 f1 0.348\n",
            "[W3] GRU Epoch 17 | train_loss 0.5420 acc 0.752 f1 0.751 || val_loss 1.3100 acc 0.409 f1 0.339\n",
            "[W3] GRU Epoch 18 | train_loss 0.5125 acc 0.776 f1 0.775 || val_loss 1.3576 acc 0.407 f1 0.337\n",
            "[W3] GRU Epoch 19 | train_loss 0.4907 acc 0.775 f1 0.775 || val_loss 1.3993 acc 0.403 f1 0.334\n",
            "[W3] GRU Epoch 20 | train_loss 0.4690 acc 0.791 f1 0.791 || val_loss 1.4493 acc 0.399 f1 0.331\n",
            "[W3] GRU Epoch 21 | train_loss 0.4443 acc 0.803 f1 0.802 || val_loss 1.4287 acc 0.434 f1 0.358\n",
            "[W3] GRU Epoch 22 | train_loss 0.4316 acc 0.803 f1 0.803 || val_loss 1.4811 acc 0.420 f1 0.347\n",
            "[W3] GRU Epoch 23 | train_loss 0.4099 acc 0.816 f1 0.815 || val_loss 1.5387 acc 0.422 f1 0.340\n",
            "[W3] GRU Epoch 24 | train_loss 0.3862 acc 0.830 f1 0.830 || val_loss 1.5905 acc 0.407 f1 0.329\n",
            "[W3] GRU Epoch 25 | train_loss 0.3651 acc 0.839 f1 0.840 || val_loss 1.6500 acc 0.414 f1 0.346\n",
            "[W3] GRU Epoch 26 | train_loss 0.3487 acc 0.847 f1 0.847 || val_loss 1.7320 acc 0.409 f1 0.339\n",
            "[W3] GRU Epoch 27 | train_loss 0.3368 acc 0.855 f1 0.855 || val_loss 1.7232 acc 0.414 f1 0.328\n",
            "[W3] GRU Epoch 28 | train_loss 0.3202 acc 0.861 f1 0.861 || val_loss 1.8318 acc 0.391 f1 0.326\n",
            "[W3] GRU Epoch 29 | train_loss 0.3014 acc 0.874 f1 0.874 || val_loss 1.8361 acc 0.405 f1 0.322\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=22\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 986, np.int64(1): 928, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 986, np.int64(2): 986, np.int64(0): 986})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0989 acc 0.353 f1 0.283 || val_loss 1.1002 acc 0.335 f1 0.268\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0918 acc 0.385 f1 0.378 || val_loss 1.0929 acc 0.313 f1 0.298\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0813 acc 0.415 f1 0.409 || val_loss 1.0842 acc 0.335 f1 0.307\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0650 acc 0.442 f1 0.429 || val_loss 1.0943 acc 0.300 f1 0.291\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0404 acc 0.471 f1 0.460 || val_loss 1.0908 acc 0.335 f1 0.315\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9924 acc 0.520 f1 0.511 || val_loss 1.0906 acc 0.356 f1 0.322\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9070 acc 0.560 f1 0.550 || val_loss 1.1345 acc 0.358 f1 0.327\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8271 acc 0.591 f1 0.583 || val_loss 1.1239 acc 0.403 f1 0.334\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7783 acc 0.614 f1 0.609 || val_loss 1.1639 acc 0.383 f1 0.331\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7386 acc 0.645 f1 0.641 || val_loss 1.2514 acc 0.358 f1 0.319\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7074 acc 0.656 f1 0.649 || val_loss 1.2080 acc 0.379 f1 0.309\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6672 acc 0.675 f1 0.671 || val_loss 1.2277 acc 0.399 f1 0.334\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6324 acc 0.691 f1 0.686 || val_loss 1.2670 acc 0.401 f1 0.329\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6080 acc 0.703 f1 0.700 || val_loss 1.3044 acc 0.407 f1 0.334\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5745 acc 0.725 f1 0.723 || val_loss 1.3611 acc 0.399 f1 0.336\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5506 acc 0.738 f1 0.736 || val_loss 1.4019 acc 0.418 f1 0.348\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5274 acc 0.757 f1 0.755 || val_loss 1.4450 acc 0.428 f1 0.333\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5011 acc 0.761 f1 0.759 || val_loss 1.4911 acc 0.416 f1 0.355\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4789 acc 0.775 f1 0.773 || val_loss 1.5337 acc 0.412 f1 0.345\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4557 acc 0.783 f1 0.781 || val_loss 1.5974 acc 0.403 f1 0.335\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4396 acc 0.793 f1 0.793 || val_loss 1.6439 acc 0.426 f1 0.358\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4196 acc 0.810 f1 0.809 || val_loss 1.6991 acc 0.416 f1 0.348\n",
            "[W3] LSTM Epoch 23 | train_loss 0.3961 acc 0.817 f1 0.817 || val_loss 1.7706 acc 0.422 f1 0.355\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3791 acc 0.835 f1 0.834 || val_loss 1.8365 acc 0.426 f1 0.352\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3624 acc 0.841 f1 0.841 || val_loss 1.8417 acc 0.424 f1 0.350\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3488 acc 0.839 f1 0.839 || val_loss 1.9218 acc 0.447 f1 0.371\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3283 acc 0.864 f1 0.864 || val_loss 1.9839 acc 0.444 f1 0.363\n",
            "[W3] LSTM Epoch 28 | train_loss 0.3200 acc 0.854 f1 0.854 || val_loss 2.0556 acc 0.407 f1 0.330\n",
            "[W3] LSTM Epoch 29 | train_loss 0.2993 acc 0.874 f1 0.874 || val_loss 2.1116 acc 0.424 f1 0.350\n",
            "[W3] LSTM Epoch 30 | train_loss 0.2840 acc 0.880 f1 0.880 || val_loss 2.1559 acc 0.434 f1 0.361\n",
            "[W3] LSTM Epoch 31 | train_loss 0.2665 acc 0.883 f1 0.883 || val_loss 2.2432 acc 0.444 f1 0.364\n",
            "[W3] LSTM Epoch 32 | train_loss 0.2541 acc 0.900 f1 0.900 || val_loss 2.3071 acc 0.438 f1 0.360\n",
            "[W3] LSTM Epoch 33 | train_loss 0.2314 acc 0.913 f1 0.913 || val_loss 2.3782 acc 0.418 f1 0.346\n",
            "[W3] LSTM Epoch 34 | train_loss 0.2219 acc 0.912 f1 0.912 || val_loss 2.4250 acc 0.436 f1 0.368\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  22%|       | 22/100 [11:08<43:36, 33.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=23 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=23\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1503 acc 0.394 f1 0.390 || val_loss 1.1729 acc 0.276 f1 0.270\n",
            "[W3] ANN Epoch 02 | train_loss 0.9596 acc 0.525 f1 0.514 || val_loss 1.1674 acc 0.284 f1 0.264\n",
            "[W3] ANN Epoch 03 | train_loss 0.8919 acc 0.562 f1 0.551 || val_loss 1.1521 acc 0.350 f1 0.315\n",
            "[W3] ANN Epoch 04 | train_loss 0.8312 acc 0.596 f1 0.586 || val_loss 1.1788 acc 0.342 f1 0.308\n",
            "[W3] ANN Epoch 05 | train_loss 0.7534 acc 0.643 f1 0.637 || val_loss 1.1558 acc 0.346 f1 0.298\n",
            "[W3] ANN Epoch 06 | train_loss 0.7240 acc 0.642 f1 0.636 || val_loss 1.1550 acc 0.366 f1 0.312\n",
            "[W3] ANN Epoch 07 | train_loss 0.6986 acc 0.658 f1 0.655 || val_loss 1.1367 acc 0.397 f1 0.324\n",
            "[W3] ANN Epoch 08 | train_loss 0.6747 acc 0.678 f1 0.674 || val_loss 1.1668 acc 0.399 f1 0.335\n",
            "[W3] ANN Epoch 09 | train_loss 0.6298 acc 0.706 f1 0.705 || val_loss 1.2187 acc 0.385 f1 0.310\n",
            "[W3] ANN Epoch 10 | train_loss 0.6357 acc 0.696 f1 0.695 || val_loss 1.1747 acc 0.442 f1 0.359\n",
            "[W3] ANN Epoch 11 | train_loss 0.5946 acc 0.713 f1 0.711 || val_loss 1.2145 acc 0.418 f1 0.345\n",
            "[W3] ANN Epoch 12 | train_loss 0.6273 acc 0.709 f1 0.708 || val_loss 1.2162 acc 0.409 f1 0.331\n",
            "[W3] ANN Epoch 13 | train_loss 0.5685 acc 0.734 f1 0.733 || val_loss 1.1815 acc 0.414 f1 0.339\n",
            "[W3] ANN Epoch 14 | train_loss 0.5552 acc 0.744 f1 0.743 || val_loss 1.2116 acc 0.407 f1 0.333\n",
            "[W3] ANN Epoch 15 | train_loss 0.5529 acc 0.738 f1 0.737 || val_loss 1.2714 acc 0.416 f1 0.341\n",
            "[W3] ANN Epoch 16 | train_loss 0.5241 acc 0.754 f1 0.752 || val_loss 1.2672 acc 0.416 f1 0.326\n",
            "[W3] ANN Epoch 17 | train_loss 0.4997 acc 0.769 f1 0.768 || val_loss 1.2948 acc 0.416 f1 0.347\n",
            "[W3] ANN Epoch 18 | train_loss 0.5053 acc 0.776 f1 0.776 || val_loss 1.2945 acc 0.397 f1 0.325\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=23\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0715 acc 0.402 f1 0.404 || val_loss 1.0146 acc 0.397 f1 0.293\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9333 acc 0.550 f1 0.542 || val_loss 1.0500 acc 0.389 f1 0.319\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8088 acc 0.600 f1 0.595 || val_loss 1.1368 acc 0.368 f1 0.306\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7102 acc 0.655 f1 0.653 || val_loss 1.1875 acc 0.379 f1 0.311\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6418 acc 0.693 f1 0.690 || val_loss 1.2457 acc 0.366 f1 0.316\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5949 acc 0.719 f1 0.718 || val_loss 1.2842 acc 0.393 f1 0.318\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5252 acc 0.753 f1 0.753 || val_loss 1.3029 acc 0.407 f1 0.337\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4897 acc 0.768 f1 0.768 || val_loss 1.4054 acc 0.405 f1 0.323\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4748 acc 0.779 f1 0.777 || val_loss 1.4062 acc 0.414 f1 0.341\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4024 acc 0.822 f1 0.821 || val_loss 1.4876 acc 0.432 f1 0.350\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3719 acc 0.833 f1 0.833 || val_loss 1.5654 acc 0.409 f1 0.342\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3346 acc 0.852 f1 0.852 || val_loss 1.6450 acc 0.403 f1 0.326\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3568 acc 0.857 f1 0.856 || val_loss 1.6894 acc 0.432 f1 0.352\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3171 acc 0.869 f1 0.868 || val_loss 1.7461 acc 0.424 f1 0.362\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.3009 acc 0.882 f1 0.881 || val_loss 1.8030 acc 0.434 f1 0.352\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2624 acc 0.903 f1 0.903 || val_loss 1.8853 acc 0.420 f1 0.341\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.2410 acc 0.909 f1 0.909 || val_loss 1.9394 acc 0.422 f1 0.328\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1840 acc 0.935 f1 0.935 || val_loss 2.0372 acc 0.414 f1 0.330\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.2175 acc 0.916 f1 0.915 || val_loss 2.0404 acc 0.409 f1 0.330\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1810 acc 0.940 f1 0.940 || val_loss 2.1704 acc 0.399 f1 0.317\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.1854 acc 0.934 f1 0.934 || val_loss 2.1760 acc 0.405 f1 0.332\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1420 acc 0.953 f1 0.953 || val_loss 2.2801 acc 0.409 f1 0.317\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=23\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0958 acc 0.368 f1 0.319 || val_loss 1.1030 acc 0.302 f1 0.297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0764 acc 0.424 f1 0.419 || val_loss 1.0986 acc 0.325 f1 0.316\n",
            "[W3] RNN Epoch 03 | train_loss 1.0579 acc 0.450 f1 0.444 || val_loss 1.0867 acc 0.344 f1 0.332\n",
            "[W3] RNN Epoch 04 | train_loss 1.0451 acc 0.460 f1 0.457 || val_loss 1.0874 acc 0.348 f1 0.336\n",
            "[W3] RNN Epoch 05 | train_loss 1.0248 acc 0.494 f1 0.489 || val_loss 1.0861 acc 0.356 f1 0.338\n",
            "[W3] RNN Epoch 06 | train_loss 1.0070 acc 0.509 f1 0.505 || val_loss 1.0898 acc 0.342 f1 0.325\n",
            "[W3] RNN Epoch 07 | train_loss 0.9891 acc 0.513 f1 0.505 || val_loss 1.0564 acc 0.381 f1 0.352\n",
            "[W3] RNN Epoch 08 | train_loss 0.9660 acc 0.535 f1 0.528 || val_loss 1.0652 acc 0.364 f1 0.330\n",
            "[W3] RNN Epoch 09 | train_loss 0.9448 acc 0.555 f1 0.545 || val_loss 1.0803 acc 0.350 f1 0.323\n",
            "[W3] RNN Epoch 10 | train_loss 0.9267 acc 0.563 f1 0.556 || val_loss 1.0570 acc 0.393 f1 0.348\n",
            "[W3] RNN Epoch 11 | train_loss 0.9059 acc 0.569 f1 0.561 || val_loss 1.0748 acc 0.393 f1 0.349\n",
            "[W3] RNN Epoch 12 | train_loss 0.8795 acc 0.580 f1 0.571 || val_loss 1.0650 acc 0.395 f1 0.342\n",
            "[W3] RNN Epoch 13 | train_loss 0.8569 acc 0.606 f1 0.599 || val_loss 1.0927 acc 0.374 f1 0.332\n",
            "[W3] RNN Epoch 14 | train_loss 0.8296 acc 0.616 f1 0.609 || val_loss 1.0953 acc 0.393 f1 0.342\n",
            "[W3] RNN Epoch 15 | train_loss 0.8095 acc 0.642 f1 0.636 || val_loss 1.1021 acc 0.397 f1 0.355\n",
            "[W3] RNN Epoch 16 | train_loss 0.7898 acc 0.642 f1 0.635 || val_loss 1.1014 acc 0.414 f1 0.360\n",
            "[W3] RNN Epoch 17 | train_loss 0.7696 acc 0.641 f1 0.633 || val_loss 1.1122 acc 0.414 f1 0.360\n",
            "[W3] RNN Epoch 18 | train_loss 0.7423 acc 0.652 f1 0.646 || val_loss 1.1026 acc 0.424 f1 0.354\n",
            "[W3] RNN Epoch 19 | train_loss 0.7352 acc 0.656 f1 0.651 || val_loss 1.1235 acc 0.405 f1 0.346\n",
            "[W3] RNN Epoch 20 | train_loss 0.7069 acc 0.673 f1 0.665 || val_loss 1.1363 acc 0.436 f1 0.364\n",
            "[W3] RNN Epoch 21 | train_loss 0.6857 acc 0.680 f1 0.674 || val_loss 1.1501 acc 0.428 f1 0.349\n",
            "[W3] RNN Epoch 22 | train_loss 0.6731 acc 0.688 f1 0.683 || val_loss 1.1762 acc 0.428 f1 0.350\n",
            "[W3] RNN Epoch 23 | train_loss 0.6581 acc 0.704 f1 0.699 || val_loss 1.1645 acc 0.442 f1 0.371\n",
            "[W3] RNN Epoch 24 | train_loss 0.6330 acc 0.714 f1 0.709 || val_loss 1.1745 acc 0.453 f1 0.367\n",
            "[W3] RNN Epoch 25 | train_loss 0.6287 acc 0.710 f1 0.707 || val_loss 1.1986 acc 0.438 f1 0.369\n",
            "[W3] RNN Epoch 26 | train_loss 0.5992 acc 0.724 f1 0.720 || val_loss 1.2096 acc 0.422 f1 0.360\n",
            "[W3] RNN Epoch 27 | train_loss 0.5845 acc 0.731 f1 0.727 || val_loss 1.2201 acc 0.440 f1 0.362\n",
            "[W3] RNN Epoch 28 | train_loss 0.5631 acc 0.742 f1 0.739 || val_loss 1.2342 acc 0.438 f1 0.369\n",
            "[W3] RNN Epoch 29 | train_loss 0.5532 acc 0.753 f1 0.750 || val_loss 1.2681 acc 0.436 f1 0.371\n",
            "[W3] RNN Epoch 30 | train_loss 0.5458 acc 0.745 f1 0.741 || val_loss 1.2709 acc 0.442 f1 0.363\n",
            "[W3] RNN Epoch 31 | train_loss 0.5262 acc 0.759 f1 0.756 || val_loss 1.2737 acc 0.424 f1 0.349\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=23\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0990 acc 0.332 f1 0.297 || val_loss 1.0957 acc 0.370 f1 0.331\n",
            "[W3] GRU Epoch 02 | train_loss 1.0923 acc 0.385 f1 0.378 || val_loss 1.0926 acc 0.368 f1 0.331\n",
            "[W3] GRU Epoch 03 | train_loss 1.0852 acc 0.397 f1 0.394 || val_loss 1.0860 acc 0.372 f1 0.332\n",
            "[W3] GRU Epoch 04 | train_loss 1.0738 acc 0.417 f1 0.417 || val_loss 1.0809 acc 0.385 f1 0.344\n",
            "[W3] GRU Epoch 05 | train_loss 1.0544 acc 0.449 f1 0.445 || val_loss 1.0653 acc 0.389 f1 0.344\n",
            "[W3] GRU Epoch 06 | train_loss 1.0182 acc 0.484 f1 0.481 || val_loss 1.0826 acc 0.333 f1 0.297\n",
            "[W3] GRU Epoch 07 | train_loss 0.9558 acc 0.541 f1 0.533 || val_loss 1.0521 acc 0.414 f1 0.318\n",
            "[W3] GRU Epoch 08 | train_loss 0.8789 acc 0.580 f1 0.575 || val_loss 1.1175 acc 0.350 f1 0.290\n",
            "[W3] GRU Epoch 09 | train_loss 0.8315 acc 0.599 f1 0.592 || val_loss 1.1145 acc 0.377 f1 0.301\n",
            "[W3] GRU Epoch 10 | train_loss 0.7876 acc 0.625 f1 0.621 || val_loss 1.1469 acc 0.385 f1 0.308\n",
            "[W3] GRU Epoch 11 | train_loss 0.7423 acc 0.634 f1 0.630 || val_loss 1.1634 acc 0.416 f1 0.328\n",
            "[W3] GRU Epoch 12 | train_loss 0.7279 acc 0.651 f1 0.648 || val_loss 1.1526 acc 0.418 f1 0.320\n",
            "[W3] GRU Epoch 13 | train_loss 0.6851 acc 0.668 f1 0.666 || val_loss 1.2340 acc 0.383 f1 0.316\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=23\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0988 acc 0.337 f1 0.282 || val_loss 1.0903 acc 0.449 f1 0.331\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0945 acc 0.376 f1 0.355 || val_loss 1.0930 acc 0.372 f1 0.341\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0880 acc 0.414 f1 0.406 || val_loss 1.0980 acc 0.313 f1 0.303\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0743 acc 0.445 f1 0.425 || val_loss 1.0864 acc 0.321 f1 0.287\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0432 acc 0.468 f1 0.449 || val_loss 1.0669 acc 0.348 f1 0.302\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9688 acc 0.529 f1 0.512 || val_loss 1.0709 acc 0.366 f1 0.314\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8868 acc 0.556 f1 0.549 || val_loss 1.0939 acc 0.385 f1 0.340\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8242 acc 0.603 f1 0.597 || val_loss 1.1606 acc 0.362 f1 0.315\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7764 acc 0.610 f1 0.603 || val_loss 1.1420 acc 0.387 f1 0.311\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7513 acc 0.628 f1 0.625 || val_loss 1.1727 acc 0.385 f1 0.309\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  23%|       | 23/100 [11:34<40:17, 31.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=24 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=24\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1337 acc 0.399 f1 0.398 || val_loss 1.1310 acc 0.294 f1 0.278\n",
            "[W3] ANN Epoch 02 | train_loss 0.9765 acc 0.509 f1 0.502 || val_loss 1.1411 acc 0.342 f1 0.317\n",
            "[W3] ANN Epoch 03 | train_loss 0.8725 acc 0.569 f1 0.563 || val_loss 1.1148 acc 0.370 f1 0.327\n",
            "[W3] ANN Epoch 04 | train_loss 0.7934 acc 0.625 f1 0.619 || val_loss 1.1220 acc 0.387 f1 0.340\n",
            "[W3] ANN Epoch 05 | train_loss 0.7551 acc 0.626 f1 0.622 || val_loss 1.1090 acc 0.397 f1 0.336\n",
            "[W3] ANN Epoch 06 | train_loss 0.6886 acc 0.673 f1 0.671 || val_loss 1.1123 acc 0.405 f1 0.340\n",
            "[W3] ANN Epoch 07 | train_loss 0.6466 acc 0.695 f1 0.693 || val_loss 1.1430 acc 0.436 f1 0.349\n",
            "[W3] ANN Epoch 08 | train_loss 0.6258 acc 0.712 f1 0.710 || val_loss 1.1754 acc 0.416 f1 0.339\n",
            "[W3] ANN Epoch 09 | train_loss 0.5929 acc 0.723 f1 0.721 || val_loss 1.1724 acc 0.426 f1 0.336\n",
            "[W3] ANN Epoch 10 | train_loss 0.5777 acc 0.741 f1 0.740 || val_loss 1.1823 acc 0.442 f1 0.365\n",
            "[W3] ANN Epoch 11 | train_loss 0.5490 acc 0.746 f1 0.744 || val_loss 1.2170 acc 0.426 f1 0.347\n",
            "[W3] ANN Epoch 12 | train_loss 0.5109 acc 0.774 f1 0.773 || val_loss 1.2705 acc 0.436 f1 0.344\n",
            "[W3] ANN Epoch 13 | train_loss 0.5095 acc 0.778 f1 0.777 || val_loss 1.2933 acc 0.428 f1 0.349\n",
            "[W3] ANN Epoch 14 | train_loss 0.4864 acc 0.786 f1 0.785 || val_loss 1.3198 acc 0.418 f1 0.331\n",
            "[W3] ANN Epoch 15 | train_loss 0.4798 acc 0.779 f1 0.778 || val_loss 1.3271 acc 0.409 f1 0.325\n",
            "[W3] ANN Epoch 16 | train_loss 0.4808 acc 0.791 f1 0.791 || val_loss 1.2918 acc 0.401 f1 0.324\n",
            "[W3] ANN Epoch 17 | train_loss 0.4276 acc 0.813 f1 0.813 || val_loss 1.3553 acc 0.444 f1 0.361\n",
            "[W3] ANN Epoch 18 | train_loss 0.4354 acc 0.821 f1 0.820 || val_loss 1.4026 acc 0.438 f1 0.346\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=24\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0757 acc 0.396 f1 0.393 || val_loss 1.0243 acc 0.440 f1 0.288\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9369 acc 0.541 f1 0.529 || val_loss 1.0420 acc 0.405 f1 0.331\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8109 acc 0.618 f1 0.613 || val_loss 1.0901 acc 0.381 f1 0.323\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7312 acc 0.648 f1 0.647 || val_loss 1.2124 acc 0.354 f1 0.298\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6647 acc 0.681 f1 0.678 || val_loss 1.2316 acc 0.368 f1 0.318\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6072 acc 0.718 f1 0.715 || val_loss 1.2920 acc 0.370 f1 0.308\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5404 acc 0.753 f1 0.753 || val_loss 1.3270 acc 0.370 f1 0.307\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4919 acc 0.778 f1 0.778 || val_loss 1.3546 acc 0.379 f1 0.316\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4362 acc 0.812 f1 0.811 || val_loss 1.4169 acc 0.391 f1 0.329\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4130 acc 0.825 f1 0.824 || val_loss 1.4596 acc 0.391 f1 0.340\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3779 acc 0.846 f1 0.845 || val_loss 1.5072 acc 0.409 f1 0.330\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3382 acc 0.863 f1 0.863 || val_loss 1.5815 acc 0.409 f1 0.330\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2888 acc 0.897 f1 0.896 || val_loss 1.7433 acc 0.399 f1 0.311\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2880 acc 0.888 f1 0.888 || val_loss 1.7815 acc 0.403 f1 0.327\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2437 acc 0.910 f1 0.910 || val_loss 1.8921 acc 0.381 f1 0.301\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2119 acc 0.921 f1 0.921 || val_loss 1.9698 acc 0.393 f1 0.304\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1968 acc 0.929 f1 0.929 || val_loss 2.0041 acc 0.395 f1 0.328\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1650 acc 0.941 f1 0.941 || val_loss 2.0997 acc 0.407 f1 0.333\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=24\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0937 acc 0.374 f1 0.352 || val_loss 1.0960 acc 0.331 f1 0.309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0746 acc 0.427 f1 0.424 || val_loss 1.0999 acc 0.331 f1 0.315\n",
            "[W3] RNN Epoch 03 | train_loss 1.0550 acc 0.440 f1 0.434 || val_loss 1.0796 acc 0.374 f1 0.343\n",
            "[W3] RNN Epoch 04 | train_loss 1.0343 acc 0.464 f1 0.462 || val_loss 1.0981 acc 0.368 f1 0.349\n",
            "[W3] RNN Epoch 05 | train_loss 1.0158 acc 0.496 f1 0.492 || val_loss 1.1083 acc 0.362 f1 0.341\n",
            "[W3] RNN Epoch 06 | train_loss 0.9970 acc 0.502 f1 0.488 || val_loss 1.0957 acc 0.381 f1 0.353\n",
            "[W3] RNN Epoch 07 | train_loss 0.9717 acc 0.522 f1 0.516 || val_loss 1.0990 acc 0.381 f1 0.350\n",
            "[W3] RNN Epoch 08 | train_loss 0.9564 acc 0.535 f1 0.527 || val_loss 1.0865 acc 0.393 f1 0.358\n",
            "[W3] RNN Epoch 09 | train_loss 0.9236 acc 0.554 f1 0.546 || val_loss 1.1020 acc 0.397 f1 0.362\n",
            "[W3] RNN Epoch 10 | train_loss 0.9035 acc 0.562 f1 0.553 || val_loss 1.1069 acc 0.389 f1 0.351\n",
            "[W3] RNN Epoch 11 | train_loss 0.8803 acc 0.579 f1 0.569 || val_loss 1.1095 acc 0.391 f1 0.348\n",
            "[W3] RNN Epoch 12 | train_loss 0.8566 acc 0.601 f1 0.592 || val_loss 1.1098 acc 0.393 f1 0.354\n",
            "[W3] RNN Epoch 13 | train_loss 0.8376 acc 0.603 f1 0.594 || val_loss 1.1180 acc 0.372 f1 0.331\n",
            "[W3] RNN Epoch 14 | train_loss 0.8116 acc 0.620 f1 0.611 || val_loss 1.1261 acc 0.399 f1 0.358\n",
            "[W3] RNN Epoch 15 | train_loss 0.7907 acc 0.632 f1 0.623 || val_loss 1.1279 acc 0.397 f1 0.344\n",
            "[W3] RNN Epoch 16 | train_loss 0.7669 acc 0.653 f1 0.645 || val_loss 1.1315 acc 0.389 f1 0.332\n",
            "[W3] RNN Epoch 17 | train_loss 0.7525 acc 0.643 f1 0.636 || val_loss 1.1431 acc 0.409 f1 0.350\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=24\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0979 acc 0.346 f1 0.281 || val_loss 1.0995 acc 0.327 f1 0.309\n",
            "[W3] GRU Epoch 02 | train_loss 1.0881 acc 0.402 f1 0.399 || val_loss 1.0893 acc 0.354 f1 0.323\n",
            "[W3] GRU Epoch 03 | train_loss 1.0797 acc 0.411 f1 0.411 || val_loss 1.0905 acc 0.356 f1 0.330\n",
            "[W3] GRU Epoch 04 | train_loss 1.0670 acc 0.455 f1 0.449 || val_loss 1.0712 acc 0.391 f1 0.334\n",
            "[W3] GRU Epoch 05 | train_loss 1.0486 acc 0.470 f1 0.469 || val_loss 1.0905 acc 0.366 f1 0.339\n",
            "[W3] GRU Epoch 06 | train_loss 1.0131 acc 0.513 f1 0.508 || val_loss 1.0913 acc 0.362 f1 0.319\n",
            "[W3] GRU Epoch 07 | train_loss 0.9517 acc 0.560 f1 0.552 || val_loss 1.0955 acc 0.364 f1 0.314\n",
            "[W3] GRU Epoch 08 | train_loss 0.8793 acc 0.577 f1 0.570 || val_loss 1.1337 acc 0.372 f1 0.324\n",
            "[W3] GRU Epoch 09 | train_loss 0.8243 acc 0.607 f1 0.601 || val_loss 1.1610 acc 0.372 f1 0.330\n",
            "[W3] GRU Epoch 10 | train_loss 0.7841 acc 0.630 f1 0.625 || val_loss 1.1314 acc 0.412 f1 0.352\n",
            "[W3] GRU Epoch 11 | train_loss 0.7420 acc 0.640 f1 0.637 || val_loss 1.1915 acc 0.379 f1 0.329\n",
            "[W3] GRU Epoch 12 | train_loss 0.7045 acc 0.667 f1 0.663 || val_loss 1.1977 acc 0.401 f1 0.340\n",
            "[W3] GRU Epoch 13 | train_loss 0.6762 acc 0.674 f1 0.671 || val_loss 1.1856 acc 0.416 f1 0.355\n",
            "[W3] GRU Epoch 14 | train_loss 0.6469 acc 0.701 f1 0.699 || val_loss 1.2370 acc 0.399 f1 0.339\n",
            "[W3] GRU Epoch 15 | train_loss 0.6167 acc 0.710 f1 0.708 || val_loss 1.2378 acc 0.412 f1 0.341\n",
            "[W3] GRU Epoch 16 | train_loss 0.5889 acc 0.725 f1 0.723 || val_loss 1.3204 acc 0.383 f1 0.329\n",
            "[W3] GRU Epoch 17 | train_loss 0.5648 acc 0.737 f1 0.736 || val_loss 1.3169 acc 0.401 f1 0.340\n",
            "[W3] GRU Epoch 18 | train_loss 0.5399 acc 0.741 f1 0.740 || val_loss 1.3513 acc 0.395 f1 0.336\n",
            "[W3] GRU Epoch 19 | train_loss 0.5122 acc 0.764 f1 0.763 || val_loss 1.3939 acc 0.399 f1 0.335\n",
            "[W3] GRU Epoch 20 | train_loss 0.4921 acc 0.775 f1 0.773 || val_loss 1.4055 acc 0.412 f1 0.345\n",
            "[W3] GRU Epoch 21 | train_loss 0.4711 acc 0.779 f1 0.778 || val_loss 1.4457 acc 0.414 f1 0.350\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=24\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0991 acc 0.332 f1 0.256 || val_loss 1.1001 acc 0.315 f1 0.273\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0955 acc 0.384 f1 0.362 || val_loss 1.0988 acc 0.309 f1 0.292\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0900 acc 0.419 f1 0.412 || val_loss 1.0960 acc 0.342 f1 0.324\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0794 acc 0.435 f1 0.424 || val_loss 1.0877 acc 0.374 f1 0.355\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0534 acc 0.467 f1 0.457 || val_loss 1.0693 acc 0.387 f1 0.354\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9951 acc 0.520 f1 0.514 || val_loss 1.1055 acc 0.342 f1 0.318\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9036 acc 0.554 f1 0.543 || val_loss 1.1084 acc 0.377 f1 0.326\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8471 acc 0.586 f1 0.580 || val_loss 1.0837 acc 0.405 f1 0.324\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7963 acc 0.614 f1 0.610 || val_loss 1.1354 acc 0.403 f1 0.343\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7463 acc 0.653 f1 0.648 || val_loss 1.1502 acc 0.412 f1 0.339\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7187 acc 0.651 f1 0.648 || val_loss 1.1731 acc 0.405 f1 0.334\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6782 acc 0.678 f1 0.674 || val_loss 1.1857 acc 0.395 f1 0.322\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  24%|       | 24/100 [12:00<37:39, 29.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=25 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=25\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1278 acc 0.416 f1 0.414 || val_loss 1.1685 acc 0.284 f1 0.267\n",
            "[W3] ANN Epoch 02 | train_loss 0.9832 acc 0.515 f1 0.507 || val_loss 1.1847 acc 0.315 f1 0.284\n",
            "[W3] ANN Epoch 03 | train_loss 0.8936 acc 0.561 f1 0.553 || val_loss 1.1655 acc 0.364 f1 0.320\n",
            "[W3] ANN Epoch 04 | train_loss 0.8272 acc 0.601 f1 0.594 || val_loss 1.1446 acc 0.381 f1 0.330\n",
            "[W3] ANN Epoch 05 | train_loss 0.7507 acc 0.640 f1 0.634 || val_loss 1.1547 acc 0.393 f1 0.327\n",
            "[W3] ANN Epoch 06 | train_loss 0.7238 acc 0.648 f1 0.645 || val_loss 1.1836 acc 0.389 f1 0.320\n",
            "[W3] ANN Epoch 07 | train_loss 0.7045 acc 0.661 f1 0.658 || val_loss 1.1674 acc 0.409 f1 0.329\n",
            "[W3] ANN Epoch 08 | train_loss 0.6543 acc 0.692 f1 0.690 || val_loss 1.1819 acc 0.387 f1 0.317\n",
            "[W3] ANN Epoch 09 | train_loss 0.6220 acc 0.708 f1 0.706 || val_loss 1.2367 acc 0.370 f1 0.313\n",
            "[W3] ANN Epoch 10 | train_loss 0.6224 acc 0.700 f1 0.699 || val_loss 1.1949 acc 0.397 f1 0.328\n",
            "[W3] ANN Epoch 11 | train_loss 0.5928 acc 0.726 f1 0.724 || val_loss 1.2420 acc 0.393 f1 0.321\n",
            "[W3] ANN Epoch 12 | train_loss 0.5722 acc 0.731 f1 0.730 || val_loss 1.2472 acc 0.407 f1 0.344\n",
            "[W3] ANN Epoch 13 | train_loss 0.5586 acc 0.742 f1 0.741 || val_loss 1.2620 acc 0.401 f1 0.331\n",
            "[W3] ANN Epoch 14 | train_loss 0.5645 acc 0.737 f1 0.736 || val_loss 1.2463 acc 0.405 f1 0.329\n",
            "[W3] ANN Epoch 15 | train_loss 0.5354 acc 0.748 f1 0.747 || val_loss 1.2819 acc 0.412 f1 0.336\n",
            "[W3] ANN Epoch 16 | train_loss 0.5031 acc 0.760 f1 0.759 || val_loss 1.3172 acc 0.401 f1 0.338\n",
            "[W3] ANN Epoch 17 | train_loss 0.4712 acc 0.789 f1 0.788 || val_loss 1.3615 acc 0.397 f1 0.333\n",
            "[W3] ANN Epoch 18 | train_loss 0.4885 acc 0.780 f1 0.779 || val_loss 1.3604 acc 0.409 f1 0.335\n",
            "[W3] ANN Epoch 19 | train_loss 0.5210 acc 0.768 f1 0.767 || val_loss 1.3501 acc 0.360 f1 0.300\n",
            "[W3] ANN Epoch 20 | train_loss 0.5135 acc 0.775 f1 0.775 || val_loss 1.3734 acc 0.395 f1 0.330\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=25\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0825 acc 0.385 f1 0.387 || val_loss 1.0367 acc 0.395 f1 0.309\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9447 acc 0.542 f1 0.536 || val_loss 1.0327 acc 0.383 f1 0.303\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8192 acc 0.594 f1 0.590 || val_loss 1.1015 acc 0.370 f1 0.297\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7157 acc 0.654 f1 0.651 || val_loss 1.1268 acc 0.368 f1 0.296\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6508 acc 0.699 f1 0.698 || val_loss 1.2431 acc 0.354 f1 0.303\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5798 acc 0.736 f1 0.735 || val_loss 1.2881 acc 0.346 f1 0.291\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5334 acc 0.763 f1 0.761 || val_loss 1.3310 acc 0.360 f1 0.305\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5095 acc 0.774 f1 0.773 || val_loss 1.3427 acc 0.374 f1 0.313\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4498 acc 0.797 f1 0.796 || val_loss 1.4181 acc 0.362 f1 0.308\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4079 acc 0.826 f1 0.825 || val_loss 1.4875 acc 0.364 f1 0.312\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3898 acc 0.840 f1 0.839 || val_loss 1.5224 acc 0.381 f1 0.309\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3817 acc 0.832 f1 0.831 || val_loss 1.5563 acc 0.370 f1 0.294\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3699 acc 0.845 f1 0.845 || val_loss 1.6298 acc 0.372 f1 0.307\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3039 acc 0.876 f1 0.875 || val_loss 1.7040 acc 0.358 f1 0.308\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2614 acc 0.897 f1 0.897 || val_loss 1.7931 acc 0.389 f1 0.320\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2295 acc 0.908 f1 0.908 || val_loss 1.9302 acc 0.383 f1 0.325\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.2283 acc 0.907 f1 0.907 || val_loss 1.9633 acc 0.409 f1 0.344\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1881 acc 0.933 f1 0.933 || val_loss 2.0511 acc 0.395 f1 0.314\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.2339 acc 0.919 f1 0.919 || val_loss 2.0659 acc 0.393 f1 0.308\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.2371 acc 0.907 f1 0.908 || val_loss 2.0487 acc 0.383 f1 0.319\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.1835 acc 0.942 f1 0.942 || val_loss 2.2082 acc 0.385 f1 0.301\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1905 acc 0.933 f1 0.933 || val_loss 2.2119 acc 0.377 f1 0.300\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.1675 acc 0.944 f1 0.944 || val_loss 2.2284 acc 0.383 f1 0.299\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.1084 acc 0.968 f1 0.968 || val_loss 2.3812 acc 0.383 f1 0.293\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.1609 acc 0.942 f1 0.942 || val_loss 2.4170 acc 0.377 f1 0.302\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=25\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0952 acc 0.368 f1 0.344 || val_loss 1.1006 acc 0.331 f1 0.311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0783 acc 0.427 f1 0.422 || val_loss 1.0917 acc 0.364 f1 0.337\n",
            "[W3] RNN Epoch 03 | train_loss 1.0587 acc 0.461 f1 0.455 || val_loss 1.0977 acc 0.360 f1 0.346\n",
            "[W3] RNN Epoch 04 | train_loss 1.0363 acc 0.472 f1 0.461 || val_loss 1.0627 acc 0.401 f1 0.363\n",
            "[W3] RNN Epoch 05 | train_loss 1.0178 acc 0.485 f1 0.478 || val_loss 1.0817 acc 0.385 f1 0.353\n",
            "[W3] RNN Epoch 06 | train_loss 0.9998 acc 0.503 f1 0.497 || val_loss 1.0735 acc 0.360 f1 0.331\n",
            "[W3] RNN Epoch 07 | train_loss 0.9722 acc 0.525 f1 0.518 || val_loss 1.0523 acc 0.409 f1 0.350\n",
            "[W3] RNN Epoch 08 | train_loss 0.9498 acc 0.543 f1 0.536 || val_loss 1.0495 acc 0.416 f1 0.353\n",
            "[W3] RNN Epoch 09 | train_loss 0.9268 acc 0.551 f1 0.544 || val_loss 1.0831 acc 0.389 f1 0.345\n",
            "[W3] RNN Epoch 10 | train_loss 0.9105 acc 0.558 f1 0.550 || val_loss 1.0659 acc 0.397 f1 0.341\n",
            "[W3] RNN Epoch 11 | train_loss 0.8896 acc 0.566 f1 0.556 || val_loss 1.0633 acc 0.430 f1 0.369\n",
            "[W3] RNN Epoch 12 | train_loss 0.8690 acc 0.579 f1 0.572 || val_loss 1.1121 acc 0.370 f1 0.341\n",
            "[W3] RNN Epoch 13 | train_loss 0.8456 acc 0.598 f1 0.589 || val_loss 1.1012 acc 0.407 f1 0.360\n",
            "[W3] RNN Epoch 14 | train_loss 0.8266 acc 0.602 f1 0.592 || val_loss 1.1089 acc 0.374 f1 0.321\n",
            "[W3] RNN Epoch 15 | train_loss 0.8119 acc 0.612 f1 0.605 || val_loss 1.1065 acc 0.414 f1 0.363\n",
            "[W3] RNN Epoch 16 | train_loss 0.7837 acc 0.647 f1 0.640 || val_loss 1.1074 acc 0.414 f1 0.363\n",
            "[W3] RNN Epoch 17 | train_loss 0.7752 acc 0.628 f1 0.621 || val_loss 1.1157 acc 0.414 f1 0.363\n",
            "[W3] RNN Epoch 18 | train_loss 0.7408 acc 0.664 f1 0.659 || val_loss 1.1191 acc 0.407 f1 0.361\n",
            "[W3] RNN Epoch 19 | train_loss 0.7301 acc 0.650 f1 0.643 || val_loss 1.1204 acc 0.409 f1 0.359\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=25\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1004 acc 0.337 f1 0.237 || val_loss 1.0857 acc 0.418 f1 0.320\n",
            "[W3] GRU Epoch 02 | train_loss 1.0894 acc 0.400 f1 0.396 || val_loss 1.0916 acc 0.342 f1 0.315\n",
            "[W3] GRU Epoch 03 | train_loss 1.0806 acc 0.416 f1 0.416 || val_loss 1.0928 acc 0.305 f1 0.288\n",
            "[W3] GRU Epoch 04 | train_loss 1.0688 acc 0.435 f1 0.432 || val_loss 1.0965 acc 0.305 f1 0.294\n",
            "[W3] GRU Epoch 05 | train_loss 1.0518 acc 0.468 f1 0.458 || val_loss 1.0773 acc 0.346 f1 0.320\n",
            "[W3] GRU Epoch 06 | train_loss 1.0296 acc 0.480 f1 0.474 || val_loss 1.0844 acc 0.346 f1 0.317\n",
            "[W3] GRU Epoch 07 | train_loss 0.9896 acc 0.514 f1 0.507 || val_loss 1.0826 acc 0.350 f1 0.311\n",
            "[W3] GRU Epoch 08 | train_loss 0.9313 acc 0.559 f1 0.551 || val_loss 1.0814 acc 0.383 f1 0.342\n",
            "[W3] GRU Epoch 09 | train_loss 0.8712 acc 0.590 f1 0.583 || val_loss 1.1076 acc 0.399 f1 0.348\n",
            "[W3] GRU Epoch 10 | train_loss 0.8248 acc 0.597 f1 0.591 || val_loss 1.1515 acc 0.356 f1 0.321\n",
            "[W3] GRU Epoch 11 | train_loss 0.7747 acc 0.644 f1 0.639 || val_loss 1.1192 acc 0.407 f1 0.342\n",
            "[W3] GRU Epoch 12 | train_loss 0.7392 acc 0.655 f1 0.651 || val_loss 1.1448 acc 0.412 f1 0.349\n",
            "[W3] GRU Epoch 13 | train_loss 0.7060 acc 0.669 f1 0.665 || val_loss 1.1714 acc 0.418 f1 0.350\n",
            "[W3] GRU Epoch 14 | train_loss 0.6674 acc 0.688 f1 0.684 || val_loss 1.2289 acc 0.420 f1 0.358\n",
            "[W3] GRU Epoch 15 | train_loss 0.6457 acc 0.703 f1 0.699 || val_loss 1.2216 acc 0.403 f1 0.333\n",
            "[W3] GRU Epoch 16 | train_loss 0.6089 acc 0.720 f1 0.718 || val_loss 1.2590 acc 0.412 f1 0.350\n",
            "[W3] GRU Epoch 17 | train_loss 0.5784 acc 0.742 f1 0.740 || val_loss 1.2874 acc 0.403 f1 0.330\n",
            "[W3] GRU Epoch 18 | train_loss 0.5527 acc 0.750 f1 0.748 || val_loss 1.3226 acc 0.403 f1 0.342\n",
            "[W3] GRU Epoch 19 | train_loss 0.5257 acc 0.759 f1 0.757 || val_loss 1.3572 acc 0.407 f1 0.359\n",
            "[W3] GRU Epoch 20 | train_loss 0.4984 acc 0.773 f1 0.772 || val_loss 1.4122 acc 0.397 f1 0.341\n",
            "[W3] GRU Epoch 21 | train_loss 0.4856 acc 0.784 f1 0.782 || val_loss 1.4421 acc 0.401 f1 0.346\n",
            "[W3] GRU Epoch 22 | train_loss 0.4662 acc 0.792 f1 0.792 || val_loss 1.4628 acc 0.412 f1 0.360\n",
            "[W3] GRU Epoch 23 | train_loss 0.4381 acc 0.795 f1 0.794 || val_loss 1.4969 acc 0.401 f1 0.349\n",
            "[W3] GRU Epoch 24 | train_loss 0.4144 acc 0.817 f1 0.817 || val_loss 1.5374 acc 0.405 f1 0.348\n",
            "[W3] GRU Epoch 25 | train_loss 0.3945 acc 0.830 f1 0.830 || val_loss 1.5861 acc 0.399 f1 0.347\n",
            "[W3] GRU Epoch 26 | train_loss 0.3800 acc 0.839 f1 0.838 || val_loss 1.6055 acc 0.414 f1 0.355\n",
            "[W3] GRU Epoch 27 | train_loss 0.3592 acc 0.846 f1 0.846 || val_loss 1.6837 acc 0.403 f1 0.350\n",
            "[W3] GRU Epoch 28 | train_loss 0.3414 acc 0.858 f1 0.858 || val_loss 1.7244 acc 0.405 f1 0.348\n",
            "[W3] GRU Epoch 29 | train_loss 0.3274 acc 0.863 f1 0.862 || val_loss 1.7542 acc 0.422 f1 0.353\n",
            "[W3] GRU Epoch 30 | train_loss 0.3090 acc 0.873 f1 0.873 || val_loss 1.8310 acc 0.401 f1 0.333\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=25\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0999 acc 0.334 f1 0.183 || val_loss 1.1094 acc 0.185 f1 0.177\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0932 acc 0.387 f1 0.366 || val_loss 1.0935 acc 0.333 f1 0.304\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0856 acc 0.404 f1 0.403 || val_loss 1.0895 acc 0.315 f1 0.298\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0717 acc 0.425 f1 0.411 || val_loss 1.0808 acc 0.346 f1 0.320\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0465 acc 0.465 f1 0.458 || val_loss 1.0925 acc 0.317 f1 0.294\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9873 acc 0.505 f1 0.493 || val_loss 1.0849 acc 0.362 f1 0.324\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9057 acc 0.545 f1 0.537 || val_loss 1.1292 acc 0.374 f1 0.342\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8391 acc 0.583 f1 0.575 || val_loss 1.1066 acc 0.389 f1 0.327\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7933 acc 0.607 f1 0.601 || val_loss 1.1086 acc 0.391 f1 0.317\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7618 acc 0.629 f1 0.625 || val_loss 1.1213 acc 0.393 f1 0.333\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7334 acc 0.641 f1 0.637 || val_loss 1.1448 acc 0.391 f1 0.316\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6964 acc 0.653 f1 0.649 || val_loss 1.2019 acc 0.383 f1 0.315\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6671 acc 0.659 f1 0.656 || val_loss 1.2096 acc 0.387 f1 0.313\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6497 acc 0.678 f1 0.675 || val_loss 1.2584 acc 0.389 f1 0.332\n",
            "[W3] LSTM Epoch 15 | train_loss 0.6260 acc 0.687 f1 0.684 || val_loss 1.2483 acc 0.393 f1 0.327\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  25%|       | 25/100 [12:31<37:41, 30.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=26 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=26\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1137 acc 0.397 f1 0.394 || val_loss 1.1259 acc 0.315 f1 0.298\n",
            "[W3] ANN Epoch 02 | train_loss 0.9499 acc 0.517 f1 0.508 || val_loss 1.1783 acc 0.309 f1 0.288\n",
            "[W3] ANN Epoch 03 | train_loss 0.8682 acc 0.566 f1 0.558 || val_loss 1.1191 acc 0.370 f1 0.328\n",
            "[W3] ANN Epoch 04 | train_loss 0.7889 acc 0.609 f1 0.600 || val_loss 1.1591 acc 0.374 f1 0.330\n",
            "[W3] ANN Epoch 05 | train_loss 0.7119 acc 0.653 f1 0.648 || val_loss 1.1804 acc 0.389 f1 0.337\n",
            "[W3] ANN Epoch 06 | train_loss 0.6689 acc 0.674 f1 0.671 || val_loss 1.1737 acc 0.374 f1 0.320\n",
            "[W3] ANN Epoch 07 | train_loss 0.6391 acc 0.697 f1 0.694 || val_loss 1.1985 acc 0.385 f1 0.335\n",
            "[W3] ANN Epoch 08 | train_loss 0.6042 acc 0.704 f1 0.700 || val_loss 1.2112 acc 0.401 f1 0.341\n",
            "[W3] ANN Epoch 09 | train_loss 0.5704 acc 0.725 f1 0.723 || val_loss 1.2223 acc 0.407 f1 0.329\n",
            "[W3] ANN Epoch 10 | train_loss 0.5548 acc 0.748 f1 0.747 || val_loss 1.2492 acc 0.381 f1 0.316\n",
            "[W3] ANN Epoch 11 | train_loss 0.5287 acc 0.758 f1 0.757 || val_loss 1.2845 acc 0.414 f1 0.343\n",
            "[W3] ANN Epoch 12 | train_loss 0.5107 acc 0.761 f1 0.760 || val_loss 1.3054 acc 0.407 f1 0.341\n",
            "[W3] ANN Epoch 13 | train_loss 0.4732 acc 0.787 f1 0.787 || val_loss 1.3270 acc 0.401 f1 0.341\n",
            "[W3] ANN Epoch 14 | train_loss 0.4736 acc 0.788 f1 0.787 || val_loss 1.3427 acc 0.407 f1 0.350\n",
            "[W3] ANN Epoch 15 | train_loss 0.4496 acc 0.805 f1 0.804 || val_loss 1.3121 acc 0.420 f1 0.347\n",
            "[W3] ANN Epoch 16 | train_loss 0.4580 acc 0.802 f1 0.801 || val_loss 1.3253 acc 0.442 f1 0.376\n",
            "[W3] ANN Epoch 17 | train_loss 0.4531 acc 0.795 f1 0.793 || val_loss 1.3217 acc 0.424 f1 0.351\n",
            "[W3] ANN Epoch 18 | train_loss 0.4167 acc 0.820 f1 0.820 || val_loss 1.3798 acc 0.424 f1 0.351\n",
            "[W3] ANN Epoch 19 | train_loss 0.4089 acc 0.818 f1 0.818 || val_loss 1.4311 acc 0.440 f1 0.352\n",
            "[W3] ANN Epoch 20 | train_loss 0.3920 acc 0.830 f1 0.830 || val_loss 1.4448 acc 0.412 f1 0.349\n",
            "[W3] ANN Epoch 21 | train_loss 0.3868 acc 0.829 f1 0.829 || val_loss 1.4714 acc 0.420 f1 0.352\n",
            "[W3] ANN Epoch 22 | train_loss 0.3616 acc 0.846 f1 0.846 || val_loss 1.5157 acc 0.418 f1 0.346\n",
            "[W3] ANN Epoch 23 | train_loss 0.3413 acc 0.857 f1 0.857 || val_loss 1.5783 acc 0.416 f1 0.355\n",
            "[W3] ANN Epoch 24 | train_loss 0.3348 acc 0.869 f1 0.869 || val_loss 1.5287 acc 0.455 f1 0.369\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=26\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0727 acc 0.397 f1 0.398 || val_loss 1.0293 acc 0.395 f1 0.300\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9375 acc 0.542 f1 0.537 || val_loss 1.0649 acc 0.372 f1 0.300\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8168 acc 0.617 f1 0.615 || val_loss 1.0908 acc 0.391 f1 0.321\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7252 acc 0.665 f1 0.664 || val_loss 1.1571 acc 0.383 f1 0.321\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6582 acc 0.703 f1 0.701 || val_loss 1.1566 acc 0.412 f1 0.335\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6393 acc 0.717 f1 0.714 || val_loss 1.1978 acc 0.381 f1 0.308\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5715 acc 0.756 f1 0.755 || val_loss 1.2947 acc 0.377 f1 0.304\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5458 acc 0.769 f1 0.766 || val_loss 1.2725 acc 0.385 f1 0.304\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4771 acc 0.810 f1 0.809 || val_loss 1.3589 acc 0.383 f1 0.331\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4498 acc 0.815 f1 0.815 || val_loss 1.3949 acc 0.385 f1 0.316\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.4315 acc 0.820 f1 0.819 || val_loss 1.3373 acc 0.414 f1 0.326\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.4012 acc 0.836 f1 0.835 || val_loss 1.4358 acc 0.389 f1 0.325\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3763 acc 0.851 f1 0.850 || val_loss 1.4257 acc 0.391 f1 0.311\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=26\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0962 acc 0.370 f1 0.354 || val_loss 1.1013 acc 0.317 f1 0.303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0830 acc 0.406 f1 0.399 || val_loss 1.1029 acc 0.298 f1 0.287\n",
            "[W3] RNN Epoch 03 | train_loss 1.0698 acc 0.423 f1 0.416 || val_loss 1.1006 acc 0.317 f1 0.305\n",
            "[W3] RNN Epoch 04 | train_loss 1.0556 acc 0.441 f1 0.429 || val_loss 1.0864 acc 0.350 f1 0.331\n",
            "[W3] RNN Epoch 05 | train_loss 1.0423 acc 0.450 f1 0.442 || val_loss 1.0763 acc 0.372 f1 0.343\n",
            "[W3] RNN Epoch 06 | train_loss 1.0243 acc 0.485 f1 0.483 || val_loss 1.0893 acc 0.358 f1 0.332\n",
            "[W3] RNN Epoch 07 | train_loss 1.0046 acc 0.499 f1 0.494 || val_loss 1.0998 acc 0.331 f1 0.314\n",
            "[W3] RNN Epoch 08 | train_loss 0.9868 acc 0.520 f1 0.513 || val_loss 1.0788 acc 0.356 f1 0.326\n",
            "[W3] RNN Epoch 09 | train_loss 0.9686 acc 0.537 f1 0.529 || val_loss 1.0780 acc 0.344 f1 0.311\n",
            "[W3] RNN Epoch 10 | train_loss 0.9474 acc 0.549 f1 0.540 || val_loss 1.1020 acc 0.350 f1 0.317\n",
            "[W3] RNN Epoch 11 | train_loss 0.9260 acc 0.550 f1 0.540 || val_loss 1.0911 acc 0.342 f1 0.306\n",
            "[W3] RNN Epoch 12 | train_loss 0.9021 acc 0.564 f1 0.557 || val_loss 1.1147 acc 0.342 f1 0.309\n",
            "[W3] RNN Epoch 13 | train_loss 0.8815 acc 0.584 f1 0.572 || val_loss 1.1040 acc 0.362 f1 0.326\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=26\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1000 acc 0.336 f1 0.196 || val_loss 1.0911 acc 0.416 f1 0.244\n",
            "[W3] GRU Epoch 02 | train_loss 1.0912 acc 0.375 f1 0.365 || val_loss 1.0902 acc 0.360 f1 0.319\n",
            "[W3] GRU Epoch 03 | train_loss 1.0810 acc 0.413 f1 0.408 || val_loss 1.0839 acc 0.348 f1 0.329\n",
            "[W3] GRU Epoch 04 | train_loss 1.0701 acc 0.430 f1 0.421 || val_loss 1.0825 acc 0.342 f1 0.321\n",
            "[W3] GRU Epoch 05 | train_loss 1.0497 acc 0.452 f1 0.447 || val_loss 1.0915 acc 0.333 f1 0.317\n",
            "[W3] GRU Epoch 06 | train_loss 1.0228 acc 0.482 f1 0.477 || val_loss 1.0955 acc 0.340 f1 0.319\n",
            "[W3] GRU Epoch 07 | train_loss 0.9779 acc 0.523 f1 0.511 || val_loss 1.1012 acc 0.350 f1 0.324\n",
            "[W3] GRU Epoch 08 | train_loss 0.9113 acc 0.558 f1 0.548 || val_loss 1.1116 acc 0.348 f1 0.311\n",
            "[W3] GRU Epoch 09 | train_loss 0.8576 acc 0.580 f1 0.570 || val_loss 1.1003 acc 0.372 f1 0.316\n",
            "[W3] GRU Epoch 10 | train_loss 0.8092 acc 0.611 f1 0.603 || val_loss 1.1270 acc 0.383 f1 0.317\n",
            "[W3] GRU Epoch 11 | train_loss 0.7796 acc 0.626 f1 0.622 || val_loss 1.1554 acc 0.370 f1 0.317\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=26\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1012 acc 0.341 f1 0.201 || val_loss 1.1132 acc 0.144 f1 0.115\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0944 acc 0.383 f1 0.347 || val_loss 1.1022 acc 0.315 f1 0.304\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0894 acc 0.414 f1 0.402 || val_loss 1.1021 acc 0.307 f1 0.297\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0788 acc 0.436 f1 0.428 || val_loss 1.0842 acc 0.352 f1 0.327\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0570 acc 0.458 f1 0.448 || val_loss 1.0863 acc 0.335 f1 0.314\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0175 acc 0.495 f1 0.490 || val_loss 1.0760 acc 0.370 f1 0.332\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9352 acc 0.553 f1 0.542 || val_loss 1.0995 acc 0.344 f1 0.302\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8551 acc 0.574 f1 0.566 || val_loss 1.0901 acc 0.389 f1 0.305\n",
            "[W3] LSTM Epoch 09 | train_loss 0.8095 acc 0.614 f1 0.608 || val_loss 1.1132 acc 0.372 f1 0.312\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7616 acc 0.631 f1 0.627 || val_loss 1.1941 acc 0.335 f1 0.288\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7512 acc 0.635 f1 0.627 || val_loss 1.1628 acc 0.393 f1 0.318\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6987 acc 0.663 f1 0.658 || val_loss 1.1868 acc 0.372 f1 0.307\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6621 acc 0.687 f1 0.683 || val_loss 1.2480 acc 0.344 f1 0.280\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6368 acc 0.692 f1 0.687 || val_loss 1.2498 acc 0.391 f1 0.312\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  26%|       | 26/100 [12:54<34:35, 28.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=27 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=27\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 931, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1238 acc 0.421 f1 0.413 || val_loss 1.1523 acc 0.315 f1 0.308\n",
            "[W3] ANN Epoch 02 | train_loss 0.9536 acc 0.525 f1 0.514 || val_loss 1.1364 acc 0.335 f1 0.310\n",
            "[W3] ANN Epoch 03 | train_loss 0.8496 acc 0.577 f1 0.569 || val_loss 1.1550 acc 0.352 f1 0.314\n",
            "[W3] ANN Epoch 04 | train_loss 0.7803 acc 0.622 f1 0.615 || val_loss 1.1120 acc 0.374 f1 0.326\n",
            "[W3] ANN Epoch 05 | train_loss 0.7336 acc 0.653 f1 0.649 || val_loss 1.1389 acc 0.366 f1 0.316\n",
            "[W3] ANN Epoch 06 | train_loss 0.6842 acc 0.673 f1 0.670 || val_loss 1.1507 acc 0.356 f1 0.296\n",
            "[W3] ANN Epoch 07 | train_loss 0.6470 acc 0.688 f1 0.686 || val_loss 1.1665 acc 0.358 f1 0.296\n",
            "[W3] ANN Epoch 08 | train_loss 0.6098 acc 0.706 f1 0.706 || val_loss 1.2191 acc 0.387 f1 0.318\n",
            "[W3] ANN Epoch 09 | train_loss 0.5993 acc 0.716 f1 0.714 || val_loss 1.1943 acc 0.412 f1 0.349\n",
            "[W3] ANN Epoch 10 | train_loss 0.5445 acc 0.742 f1 0.740 || val_loss 1.2496 acc 0.395 f1 0.336\n",
            "[W3] ANN Epoch 11 | train_loss 0.5363 acc 0.752 f1 0.750 || val_loss 1.2267 acc 0.409 f1 0.324\n",
            "[W3] ANN Epoch 12 | train_loss 0.5225 acc 0.760 f1 0.759 || val_loss 1.2606 acc 0.405 f1 0.332\n",
            "[W3] ANN Epoch 13 | train_loss 0.5056 acc 0.775 f1 0.775 || val_loss 1.2745 acc 0.399 f1 0.322\n",
            "[W3] ANN Epoch 14 | train_loss 0.4657 acc 0.780 f1 0.779 || val_loss 1.3172 acc 0.424 f1 0.341\n",
            "[W3] ANN Epoch 15 | train_loss 0.4970 acc 0.785 f1 0.784 || val_loss 1.3398 acc 0.420 f1 0.327\n",
            "[W3] ANN Epoch 16 | train_loss 0.4559 acc 0.801 f1 0.801 || val_loss 1.3577 acc 0.407 f1 0.317\n",
            "[W3] ANN Epoch 17 | train_loss 0.4352 acc 0.809 f1 0.808 || val_loss 1.3818 acc 0.436 f1 0.355\n",
            "[W3] ANN Epoch 18 | train_loss 0.4054 acc 0.823 f1 0.823 || val_loss 1.4103 acc 0.436 f1 0.344\n",
            "[W3] ANN Epoch 19 | train_loss 0.4080 acc 0.824 f1 0.824 || val_loss 1.4272 acc 0.409 f1 0.341\n",
            "[W3] ANN Epoch 20 | train_loss 0.3883 acc 0.841 f1 0.841 || val_loss 1.5172 acc 0.407 f1 0.333\n",
            "[W3] ANN Epoch 21 | train_loss 0.3727 acc 0.843 f1 0.842 || val_loss 1.4753 acc 0.412 f1 0.341\n",
            "[W3] ANN Epoch 22 | train_loss 0.3784 acc 0.843 f1 0.842 || val_loss 1.4883 acc 0.409 f1 0.336\n",
            "[W3] ANN Epoch 23 | train_loss 0.3373 acc 0.862 f1 0.862 || val_loss 1.5227 acc 0.403 f1 0.330\n",
            "[W3] ANN Epoch 24 | train_loss 0.3551 acc 0.854 f1 0.854 || val_loss 1.5298 acc 0.418 f1 0.361\n",
            "[W3] ANN Epoch 25 | train_loss 0.3487 acc 0.853 f1 0.853 || val_loss 1.5611 acc 0.416 f1 0.353\n",
            "[W3] ANN Epoch 26 | train_loss 0.3572 acc 0.854 f1 0.854 || val_loss 1.5279 acc 0.422 f1 0.344\n",
            "[W3] ANN Epoch 27 | train_loss 0.3449 acc 0.859 f1 0.860 || val_loss 1.5903 acc 0.422 f1 0.344\n",
            "[W3] ANN Epoch 28 | train_loss 0.3257 acc 0.865 f1 0.864 || val_loss 1.6761 acc 0.412 f1 0.331\n",
            "[W3] ANN Epoch 29 | train_loss 0.3147 acc 0.871 f1 0.871 || val_loss 1.6099 acc 0.416 f1 0.340\n",
            "[W3] ANN Epoch 30 | train_loss 0.3072 acc 0.876 f1 0.876 || val_loss 1.6532 acc 0.409 f1 0.335\n",
            "[W3] ANN Epoch 31 | train_loss 0.2879 acc 0.885 f1 0.885 || val_loss 1.6495 acc 0.409 f1 0.344\n",
            "[W3] ANN Epoch 32 | train_loss 0.2675 acc 0.894 f1 0.894 || val_loss 1.7176 acc 0.422 f1 0.345\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=27\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 931, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0820 acc 0.397 f1 0.398 || val_loss 1.0261 acc 0.430 f1 0.298\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9549 acc 0.528 f1 0.523 || val_loss 1.0434 acc 0.387 f1 0.296\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8131 acc 0.615 f1 0.610 || val_loss 1.0954 acc 0.401 f1 0.303\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7103 acc 0.665 f1 0.663 || val_loss 1.2689 acc 0.358 f1 0.314\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6509 acc 0.697 f1 0.694 || val_loss 1.2045 acc 0.401 f1 0.320\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5761 acc 0.728 f1 0.727 || val_loss 1.2850 acc 0.385 f1 0.299\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5137 acc 0.765 f1 0.765 || val_loss 1.3258 acc 0.395 f1 0.324\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4620 acc 0.791 f1 0.790 || val_loss 1.4256 acc 0.401 f1 0.336\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4154 acc 0.817 f1 0.816 || val_loss 1.5121 acc 0.409 f1 0.327\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3756 acc 0.839 f1 0.838 || val_loss 1.5444 acc 0.420 f1 0.332\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3558 acc 0.854 f1 0.854 || val_loss 1.6122 acc 0.420 f1 0.321\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3217 acc 0.863 f1 0.862 || val_loss 1.6689 acc 0.442 f1 0.333\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2742 acc 0.895 f1 0.895 || val_loss 1.7520 acc 0.403 f1 0.325\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2465 acc 0.903 f1 0.903 || val_loss 1.8407 acc 0.426 f1 0.333\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2141 acc 0.921 f1 0.921 || val_loss 1.9536 acc 0.422 f1 0.333\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1821 acc 0.932 f1 0.932 || val_loss 2.0337 acc 0.436 f1 0.339\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1534 acc 0.948 f1 0.948 || val_loss 2.1077 acc 0.420 f1 0.338\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1538 acc 0.943 f1 0.943 || val_loss 2.0996 acc 0.457 f1 0.359\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1425 acc 0.952 f1 0.952 || val_loss 2.2613 acc 0.432 f1 0.341\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1204 acc 0.957 f1 0.957 || val_loss 2.3917 acc 0.432 f1 0.351\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.1087 acc 0.960 f1 0.960 || val_loss 2.4410 acc 0.426 f1 0.342\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1074 acc 0.963 f1 0.963 || val_loss 2.5445 acc 0.428 f1 0.335\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.1042 acc 0.968 f1 0.968 || val_loss 2.5100 acc 0.447 f1 0.356\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.0985 acc 0.967 f1 0.967 || val_loss 2.5997 acc 0.412 f1 0.334\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.0931 acc 0.970 f1 0.970 || val_loss 2.6268 acc 0.403 f1 0.330\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.0921 acc 0.971 f1 0.971 || val_loss 2.6695 acc 0.424 f1 0.331\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=27\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 931, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0963 acc 0.342 f1 0.254 || val_loss 1.0846 acc 0.391 f1 0.313\n",
            "[W3] RNN Epoch 02 | train_loss 1.0799 acc 0.415 f1 0.414 || val_loss 1.0860 acc 0.344 f1 0.312\n",
            "[W3] RNN Epoch 03 | train_loss 1.0582 acc 0.456 f1 0.450 || val_loss 1.0626 acc 0.381 f1 0.341\n",
            "[W3] RNN Epoch 04 | train_loss 1.0383 acc 0.475 f1 0.470 || val_loss 1.0763 acc 0.372 f1 0.344\n",
            "[W3] RNN Epoch 05 | train_loss 1.0104 acc 0.500 f1 0.495 || val_loss 1.0968 acc 0.350 f1 0.335\n",
            "[W3] RNN Epoch 06 | train_loss 0.9845 acc 0.515 f1 0.505 || val_loss 1.0485 acc 0.401 f1 0.351\n",
            "[W3] RNN Epoch 07 | train_loss 0.9644 acc 0.541 f1 0.536 || val_loss 1.0622 acc 0.389 f1 0.348\n",
            "[W3] RNN Epoch 08 | train_loss 0.9308 acc 0.557 f1 0.548 || val_loss 1.0731 acc 0.381 f1 0.343\n",
            "[W3] RNN Epoch 09 | train_loss 0.9056 acc 0.572 f1 0.565 || val_loss 1.0853 acc 0.372 f1 0.336\n",
            "[W3] RNN Epoch 10 | train_loss 0.8764 acc 0.580 f1 0.570 || val_loss 1.0759 acc 0.389 f1 0.345\n",
            "[W3] RNN Epoch 11 | train_loss 0.8425 acc 0.610 f1 0.602 || val_loss 1.0785 acc 0.403 f1 0.352\n",
            "[W3] RNN Epoch 12 | train_loss 0.8163 acc 0.619 f1 0.609 || val_loss 1.1007 acc 0.377 f1 0.332\n",
            "[W3] RNN Epoch 13 | train_loss 0.7891 acc 0.635 f1 0.626 || val_loss 1.1048 acc 0.407 f1 0.349\n",
            "[W3] RNN Epoch 14 | train_loss 0.7697 acc 0.640 f1 0.631 || val_loss 1.1079 acc 0.407 f1 0.346\n",
            "[W3] RNN Epoch 15 | train_loss 0.7424 acc 0.642 f1 0.635 || val_loss 1.1307 acc 0.399 f1 0.335\n",
            "[W3] RNN Epoch 16 | train_loss 0.7205 acc 0.661 f1 0.654 || val_loss 1.1319 acc 0.407 f1 0.344\n",
            "[W3] RNN Epoch 17 | train_loss 0.6907 acc 0.677 f1 0.670 || val_loss 1.1564 acc 0.416 f1 0.352\n",
            "[W3] RNN Epoch 18 | train_loss 0.6713 acc 0.683 f1 0.677 || val_loss 1.1782 acc 0.426 f1 0.356\n",
            "[W3] RNN Epoch 19 | train_loss 0.6636 acc 0.688 f1 0.681 || val_loss 1.1931 acc 0.418 f1 0.360\n",
            "[W3] RNN Epoch 20 | train_loss 0.6465 acc 0.691 f1 0.686 || val_loss 1.2261 acc 0.397 f1 0.346\n",
            "[W3] RNN Epoch 21 | train_loss 0.6243 acc 0.699 f1 0.694 || val_loss 1.2301 acc 0.407 f1 0.342\n",
            "[W3] RNN Epoch 22 | train_loss 0.6118 acc 0.703 f1 0.699 || val_loss 1.2599 acc 0.397 f1 0.340\n",
            "[W3] RNN Epoch 23 | train_loss 0.5878 acc 0.727 f1 0.722 || val_loss 1.2651 acc 0.397 f1 0.321\n",
            "[W3] RNN Epoch 24 | train_loss 0.5726 acc 0.725 f1 0.722 || val_loss 1.3088 acc 0.416 f1 0.348\n",
            "[W3] RNN Epoch 25 | train_loss 0.5602 acc 0.727 f1 0.723 || val_loss 1.3246 acc 0.407 f1 0.341\n",
            "[W3] RNN Epoch 26 | train_loss 0.5427 acc 0.739 f1 0.736 || val_loss 1.3332 acc 0.424 f1 0.354\n",
            "[W3] RNN Epoch 27 | train_loss 0.5350 acc 0.745 f1 0.742 || val_loss 1.3670 acc 0.416 f1 0.348\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=27\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 931, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0974 acc 0.349 f1 0.288 || val_loss 1.0977 acc 0.387 f1 0.321\n",
            "[W3] GRU Epoch 02 | train_loss 1.0919 acc 0.379 f1 0.373 || val_loss 1.0913 acc 0.383 f1 0.318\n",
            "[W3] GRU Epoch 03 | train_loss 1.0841 acc 0.400 f1 0.397 || val_loss 1.0882 acc 0.350 f1 0.310\n",
            "[W3] GRU Epoch 04 | train_loss 1.0726 acc 0.434 f1 0.430 || val_loss 1.0838 acc 0.352 f1 0.311\n",
            "[W3] GRU Epoch 05 | train_loss 1.0562 acc 0.461 f1 0.457 || val_loss 1.0818 acc 0.354 f1 0.314\n",
            "[W3] GRU Epoch 06 | train_loss 1.0309 acc 0.476 f1 0.474 || val_loss 1.1044 acc 0.340 f1 0.320\n",
            "[W3] GRU Epoch 07 | train_loss 0.9842 acc 0.525 f1 0.521 || val_loss 1.0941 acc 0.366 f1 0.325\n",
            "[W3] GRU Epoch 08 | train_loss 0.9141 acc 0.560 f1 0.554 || val_loss 1.1559 acc 0.362 f1 0.332\n",
            "[W3] GRU Epoch 09 | train_loss 0.8471 acc 0.597 f1 0.593 || val_loss 1.1102 acc 0.395 f1 0.328\n",
            "[W3] GRU Epoch 10 | train_loss 0.7937 acc 0.617 f1 0.613 || val_loss 1.1559 acc 0.385 f1 0.328\n",
            "[W3] GRU Epoch 11 | train_loss 0.7468 acc 0.652 f1 0.648 || val_loss 1.1617 acc 0.383 f1 0.314\n",
            "[W3] GRU Epoch 12 | train_loss 0.7068 acc 0.658 f1 0.655 || val_loss 1.2048 acc 0.383 f1 0.320\n",
            "[W3] GRU Epoch 13 | train_loss 0.6748 acc 0.679 f1 0.676 || val_loss 1.2254 acc 0.395 f1 0.320\n",
            "[W3] GRU Epoch 14 | train_loss 0.6382 acc 0.695 f1 0.693 || val_loss 1.2247 acc 0.405 f1 0.324\n",
            "[W3] GRU Epoch 15 | train_loss 0.6044 acc 0.721 f1 0.720 || val_loss 1.2548 acc 0.399 f1 0.324\n",
            "[W3] GRU Epoch 16 | train_loss 0.5764 acc 0.727 f1 0.725 || val_loss 1.3003 acc 0.397 f1 0.322\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=27\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 931, np.int64(0): 271})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0982 acc 0.343 f1 0.281 || val_loss 1.0948 acc 0.372 f1 0.328\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0917 acc 0.392 f1 0.387 || val_loss 1.1031 acc 0.294 f1 0.284\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0822 acc 0.413 f1 0.384 || val_loss 1.0879 acc 0.329 f1 0.294\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0614 acc 0.452 f1 0.441 || val_loss 1.1038 acc 0.307 f1 0.296\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0145 acc 0.494 f1 0.469 || val_loss 1.0840 acc 0.337 f1 0.314\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9239 acc 0.556 f1 0.542 || val_loss 1.0478 acc 0.377 f1 0.320\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8316 acc 0.592 f1 0.584 || val_loss 1.0646 acc 0.381 f1 0.325\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7853 acc 0.602 f1 0.597 || val_loss 1.1288 acc 0.340 f1 0.304\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7326 acc 0.629 f1 0.624 || val_loss 1.1499 acc 0.354 f1 0.307\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  27%|       | 27/100 [13:26<35:20, 29.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=28 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=28\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 929, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1622 acc 0.393 f1 0.392 || val_loss 1.1345 acc 0.309 f1 0.289\n",
            "[W3] ANN Epoch 02 | train_loss 1.0004 acc 0.489 f1 0.480 || val_loss 1.1171 acc 0.356 f1 0.325\n",
            "[W3] ANN Epoch 03 | train_loss 0.8868 acc 0.561 f1 0.553 || val_loss 1.1103 acc 0.354 f1 0.320\n",
            "[W3] ANN Epoch 04 | train_loss 0.8162 acc 0.607 f1 0.599 || val_loss 1.1025 acc 0.381 f1 0.331\n",
            "[W3] ANN Epoch 05 | train_loss 0.7470 acc 0.632 f1 0.628 || val_loss 1.1171 acc 0.372 f1 0.315\n",
            "[W3] ANN Epoch 06 | train_loss 0.6856 acc 0.674 f1 0.670 || val_loss 1.1084 acc 0.377 f1 0.306\n",
            "[W3] ANN Epoch 07 | train_loss 0.6436 acc 0.683 f1 0.680 || val_loss 1.1429 acc 0.420 f1 0.332\n",
            "[W3] ANN Epoch 08 | train_loss 0.6335 acc 0.700 f1 0.698 || val_loss 1.1360 acc 0.436 f1 0.330\n",
            "[W3] ANN Epoch 09 | train_loss 0.6058 acc 0.711 f1 0.709 || val_loss 1.1817 acc 0.370 f1 0.297\n",
            "[W3] ANN Epoch 10 | train_loss 0.5786 acc 0.732 f1 0.730 || val_loss 1.1705 acc 0.438 f1 0.338\n",
            "[W3] ANN Epoch 11 | train_loss 0.5632 acc 0.733 f1 0.732 || val_loss 1.1600 acc 0.434 f1 0.336\n",
            "[W3] ANN Epoch 12 | train_loss 0.5321 acc 0.767 f1 0.766 || val_loss 1.2361 acc 0.432 f1 0.348\n",
            "[W3] ANN Epoch 13 | train_loss 0.5129 acc 0.770 f1 0.770 || val_loss 1.2080 acc 0.432 f1 0.340\n",
            "[W3] ANN Epoch 14 | train_loss 0.4885 acc 0.785 f1 0.785 || val_loss 1.2644 acc 0.440 f1 0.347\n",
            "[W3] ANN Epoch 15 | train_loss 0.4744 acc 0.794 f1 0.793 || val_loss 1.3163 acc 0.430 f1 0.337\n",
            "[W3] ANN Epoch 16 | train_loss 0.4629 acc 0.794 f1 0.793 || val_loss 1.3547 acc 0.409 f1 0.327\n",
            "[W3] ANN Epoch 17 | train_loss 0.4525 acc 0.801 f1 0.800 || val_loss 1.3425 acc 0.424 f1 0.346\n",
            "[W3] ANN Epoch 18 | train_loss 0.4370 acc 0.821 f1 0.821 || val_loss 1.3204 acc 0.420 f1 0.327\n",
            "[W3] ANN Epoch 19 | train_loss 0.4392 acc 0.814 f1 0.814 || val_loss 1.3226 acc 0.453 f1 0.334\n",
            "[W3] ANN Epoch 20 | train_loss 0.4165 acc 0.822 f1 0.822 || val_loss 1.3093 acc 0.463 f1 0.355\n",
            "[W3] ANN Epoch 21 | train_loss 0.3939 acc 0.830 f1 0.829 || val_loss 1.3569 acc 0.438 f1 0.345\n",
            "[W3] ANN Epoch 22 | train_loss 0.3926 acc 0.834 f1 0.834 || val_loss 1.3482 acc 0.442 f1 0.348\n",
            "[W3] ANN Epoch 23 | train_loss 0.3942 acc 0.835 f1 0.835 || val_loss 1.3854 acc 0.465 f1 0.358\n",
            "[W3] ANN Epoch 24 | train_loss 0.3838 acc 0.834 f1 0.834 || val_loss 1.4169 acc 0.467 f1 0.346\n",
            "[W3] ANN Epoch 25 | train_loss 0.3882 acc 0.848 f1 0.848 || val_loss 1.4778 acc 0.416 f1 0.324\n",
            "[W3] ANN Epoch 26 | train_loss 0.3427 acc 0.862 f1 0.862 || val_loss 1.4976 acc 0.447 f1 0.338\n",
            "[W3] ANN Epoch 27 | train_loss 0.3624 acc 0.843 f1 0.843 || val_loss 1.4529 acc 0.422 f1 0.321\n",
            "[W3] ANN Epoch 28 | train_loss 0.3645 acc 0.839 f1 0.839 || val_loss 1.4675 acc 0.430 f1 0.339\n",
            "[W3] ANN Epoch 29 | train_loss 0.3521 acc 0.857 f1 0.857 || val_loss 1.4762 acc 0.436 f1 0.339\n",
            "[W3] ANN Epoch 30 | train_loss 0.3282 acc 0.863 f1 0.862 || val_loss 1.5189 acc 0.440 f1 0.346\n",
            "[W3] ANN Epoch 31 | train_loss 0.3222 acc 0.863 f1 0.863 || val_loss 1.5560 acc 0.451 f1 0.350\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=28\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 929, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0794 acc 0.397 f1 0.398 || val_loss 1.0316 acc 0.444 f1 0.333\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9586 acc 0.539 f1 0.534 || val_loss 1.0474 acc 0.364 f1 0.302\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8274 acc 0.604 f1 0.600 || val_loss 1.0969 acc 0.393 f1 0.319\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7425 acc 0.640 f1 0.639 || val_loss 1.1380 acc 0.391 f1 0.342\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6672 acc 0.687 f1 0.685 || val_loss 1.1884 acc 0.356 f1 0.299\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6212 acc 0.717 f1 0.714 || val_loss 1.2217 acc 0.362 f1 0.315\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5777 acc 0.737 f1 0.736 || val_loss 1.2303 acc 0.372 f1 0.309\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5289 acc 0.760 f1 0.759 || val_loss 1.2797 acc 0.391 f1 0.314\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4883 acc 0.781 f1 0.780 || val_loss 1.3241 acc 0.379 f1 0.313\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4511 acc 0.801 f1 0.800 || val_loss 1.3884 acc 0.379 f1 0.311\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.4485 acc 0.805 f1 0.803 || val_loss 1.4069 acc 0.395 f1 0.328\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3942 acc 0.838 f1 0.837 || val_loss 1.4592 acc 0.399 f1 0.327\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=28\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 929, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1014 acc 0.339 f1 0.303 || val_loss 1.0898 acc 0.366 f1 0.320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0839 acc 0.387 f1 0.386 || val_loss 1.0821 acc 0.383 f1 0.324\n",
            "[W3] RNN Epoch 03 | train_loss 1.0708 acc 0.426 f1 0.420 || val_loss 1.0953 acc 0.352 f1 0.324\n",
            "[W3] RNN Epoch 04 | train_loss 1.0540 acc 0.447 f1 0.441 || val_loss 1.0858 acc 0.350 f1 0.316\n",
            "[W3] RNN Epoch 05 | train_loss 1.0397 acc 0.461 f1 0.454 || val_loss 1.0845 acc 0.346 f1 0.319\n",
            "[W3] RNN Epoch 06 | train_loss 1.0182 acc 0.495 f1 0.489 || val_loss 1.0985 acc 0.331 f1 0.307\n",
            "[W3] RNN Epoch 07 | train_loss 1.0006 acc 0.511 f1 0.505 || val_loss 1.1051 acc 0.335 f1 0.310\n",
            "[W3] RNN Epoch 08 | train_loss 0.9712 acc 0.529 f1 0.520 || val_loss 1.0913 acc 0.366 f1 0.325\n",
            "[W3] RNN Epoch 09 | train_loss 0.9546 acc 0.538 f1 0.532 || val_loss 1.1203 acc 0.329 f1 0.302\n",
            "[W3] RNN Epoch 10 | train_loss 0.9356 acc 0.553 f1 0.543 || val_loss 1.1147 acc 0.329 f1 0.295\n",
            "[W3] RNN Epoch 11 | train_loss 0.9132 acc 0.564 f1 0.557 || val_loss 1.1204 acc 0.337 f1 0.305\n",
            "[W3] RNN Epoch 12 | train_loss 0.9055 acc 0.570 f1 0.560 || val_loss 1.1339 acc 0.346 f1 0.316\n",
            "[W3] RNN Epoch 13 | train_loss 0.8794 acc 0.581 f1 0.572 || val_loss 1.1247 acc 0.340 f1 0.302\n",
            "[W3] RNN Epoch 14 | train_loss 0.8551 acc 0.592 f1 0.582 || val_loss 1.1355 acc 0.346 f1 0.299\n",
            "[W3] RNN Epoch 15 | train_loss 0.8365 acc 0.604 f1 0.595 || val_loss 1.1454 acc 0.346 f1 0.298\n",
            "[W3] RNN Epoch 16 | train_loss 0.8304 acc 0.610 f1 0.602 || val_loss 1.1586 acc 0.335 f1 0.301\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=28\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 929, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0999 acc 0.331 f1 0.276 || val_loss 1.0947 acc 0.385 f1 0.315\n",
            "[W3] GRU Epoch 02 | train_loss 1.0918 acc 0.382 f1 0.375 || val_loss 1.0901 acc 0.403 f1 0.360\n",
            "[W3] GRU Epoch 03 | train_loss 1.0843 acc 0.402 f1 0.398 || val_loss 1.0876 acc 0.391 f1 0.362\n",
            "[W3] GRU Epoch 04 | train_loss 1.0718 acc 0.442 f1 0.433 || val_loss 1.0801 acc 0.372 f1 0.341\n",
            "[W3] GRU Epoch 05 | train_loss 1.0559 acc 0.465 f1 0.464 || val_loss 1.0764 acc 0.381 f1 0.354\n",
            "[W3] GRU Epoch 06 | train_loss 1.0268 acc 0.494 f1 0.488 || val_loss 1.0834 acc 0.362 f1 0.344\n",
            "[W3] GRU Epoch 07 | train_loss 0.9814 acc 0.525 f1 0.516 || val_loss 1.0739 acc 0.358 f1 0.328\n",
            "[W3] GRU Epoch 08 | train_loss 0.9128 acc 0.567 f1 0.558 || val_loss 1.0844 acc 0.348 f1 0.296\n",
            "[W3] GRU Epoch 09 | train_loss 0.8456 acc 0.593 f1 0.588 || val_loss 1.0915 acc 0.366 f1 0.309\n",
            "[W3] GRU Epoch 10 | train_loss 0.7948 acc 0.608 f1 0.601 || val_loss 1.0981 acc 0.379 f1 0.307\n",
            "[W3] GRU Epoch 11 | train_loss 0.7684 acc 0.624 f1 0.618 || val_loss 1.1237 acc 0.370 f1 0.313\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=28\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 929, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0988 acc 0.354 f1 0.290 || val_loss 1.1010 acc 0.292 f1 0.250\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0946 acc 0.371 f1 0.354 || val_loss 1.0953 acc 0.352 f1 0.320\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0874 acc 0.418 f1 0.409 || val_loss 1.0861 acc 0.366 f1 0.340\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0776 acc 0.436 f1 0.423 || val_loss 1.0759 acc 0.377 f1 0.348\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0626 acc 0.460 f1 0.456 || val_loss 1.0711 acc 0.368 f1 0.335\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0354 acc 0.484 f1 0.471 || val_loss 1.0698 acc 0.358 f1 0.330\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9835 acc 0.523 f1 0.512 || val_loss 1.0487 acc 0.397 f1 0.323\n",
            "[W3] LSTM Epoch 08 | train_loss 0.9034 acc 0.563 f1 0.557 || val_loss 1.1018 acc 0.356 f1 0.315\n",
            "[W3] LSTM Epoch 09 | train_loss 0.8484 acc 0.584 f1 0.577 || val_loss 1.1204 acc 0.366 f1 0.302\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7967 acc 0.611 f1 0.604 || val_loss 1.1135 acc 0.414 f1 0.319\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7422 acc 0.633 f1 0.629 || val_loss 1.1438 acc 0.412 f1 0.333\n",
            "[W3] LSTM Epoch 12 | train_loss 0.7292 acc 0.640 f1 0.637 || val_loss 1.1377 acc 0.418 f1 0.341\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  28%|       | 28/100 [13:51<33:30, 27.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=29 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=29\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1250 acc 0.400 f1 0.395 || val_loss 1.1131 acc 0.327 f1 0.299\n",
            "[W3] ANN Epoch 02 | train_loss 0.9599 acc 0.518 f1 0.510 || val_loss 1.0883 acc 0.395 f1 0.356\n",
            "[W3] ANN Epoch 03 | train_loss 0.8743 acc 0.560 f1 0.552 || val_loss 1.0860 acc 0.403 f1 0.345\n",
            "[W3] ANN Epoch 04 | train_loss 0.7951 acc 0.616 f1 0.610 || val_loss 1.0791 acc 0.399 f1 0.345\n",
            "[W3] ANN Epoch 05 | train_loss 0.7298 acc 0.649 f1 0.645 || val_loss 1.1379 acc 0.381 f1 0.329\n",
            "[W3] ANN Epoch 06 | train_loss 0.6975 acc 0.668 f1 0.664 || val_loss 1.1158 acc 0.391 f1 0.321\n",
            "[W3] ANN Epoch 07 | train_loss 0.6404 acc 0.683 f1 0.679 || val_loss 1.1602 acc 0.366 f1 0.321\n",
            "[W3] ANN Epoch 08 | train_loss 0.6095 acc 0.715 f1 0.714 || val_loss 1.1579 acc 0.401 f1 0.337\n",
            "[W3] ANN Epoch 09 | train_loss 0.5816 acc 0.725 f1 0.723 || val_loss 1.1711 acc 0.424 f1 0.355\n",
            "[W3] ANN Epoch 10 | train_loss 0.5652 acc 0.750 f1 0.749 || val_loss 1.1963 acc 0.414 f1 0.339\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=29\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0677 acc 0.418 f1 0.420 || val_loss 1.0132 acc 0.449 f1 0.331\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9335 acc 0.535 f1 0.531 || val_loss 1.0374 acc 0.409 f1 0.335\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8229 acc 0.599 f1 0.591 || val_loss 1.1075 acc 0.370 f1 0.324\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7616 acc 0.640 f1 0.634 || val_loss 1.1214 acc 0.379 f1 0.324\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6926 acc 0.657 f1 0.652 || val_loss 1.1863 acc 0.325 f1 0.287\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6242 acc 0.711 f1 0.709 || val_loss 1.2010 acc 0.370 f1 0.306\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5735 acc 0.743 f1 0.741 || val_loss 1.2859 acc 0.350 f1 0.300\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5282 acc 0.764 f1 0.761 || val_loss 1.3353 acc 0.377 f1 0.331\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.5239 acc 0.766 f1 0.764 || val_loss 1.3475 acc 0.391 f1 0.335\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4781 acc 0.781 f1 0.780 || val_loss 1.3612 acc 0.403 f1 0.345\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.4200 acc 0.822 f1 0.822 || val_loss 1.4273 acc 0.418 f1 0.362\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.4039 acc 0.836 f1 0.836 || val_loss 1.4889 acc 0.401 f1 0.338\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3685 acc 0.841 f1 0.840 || val_loss 1.5708 acc 0.393 f1 0.348\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3712 acc 0.845 f1 0.844 || val_loss 1.5721 acc 0.374 f1 0.308\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.3467 acc 0.859 f1 0.858 || val_loss 1.5781 acc 0.397 f1 0.358\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.3255 acc 0.871 f1 0.871 || val_loss 1.6533 acc 0.393 f1 0.338\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.3074 acc 0.889 f1 0.888 || val_loss 1.6606 acc 0.391 f1 0.321\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.2704 acc 0.903 f1 0.903 || val_loss 1.7935 acc 0.395 f1 0.339\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.2498 acc 0.907 f1 0.907 || val_loss 1.7816 acc 0.409 f1 0.349\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=29\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0999 acc 0.345 f1 0.307 || val_loss 1.0959 acc 0.362 f1 0.317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0798 acc 0.420 f1 0.415 || val_loss 1.0807 acc 0.358 f1 0.321\n",
            "[W3] RNN Epoch 03 | train_loss 1.0642 acc 0.439 f1 0.440 || val_loss 1.0768 acc 0.342 f1 0.308\n",
            "[W3] RNN Epoch 04 | train_loss 1.0465 acc 0.457 f1 0.455 || val_loss 1.0740 acc 0.366 f1 0.333\n",
            "[W3] RNN Epoch 05 | train_loss 1.0254 acc 0.474 f1 0.472 || val_loss 1.0859 acc 0.346 f1 0.325\n",
            "[W3] RNN Epoch 06 | train_loss 1.0038 acc 0.500 f1 0.495 || val_loss 1.0732 acc 0.366 f1 0.332\n",
            "[W3] RNN Epoch 07 | train_loss 0.9830 acc 0.523 f1 0.515 || val_loss 1.0818 acc 0.352 f1 0.318\n",
            "[W3] RNN Epoch 08 | train_loss 0.9557 acc 0.531 f1 0.519 || val_loss 1.0839 acc 0.356 f1 0.322\n",
            "[W3] RNN Epoch 09 | train_loss 0.9301 acc 0.553 f1 0.541 || val_loss 1.0782 acc 0.372 f1 0.332\n",
            "[W3] RNN Epoch 10 | train_loss 0.9003 acc 0.585 f1 0.576 || val_loss 1.1109 acc 0.372 f1 0.336\n",
            "[W3] RNN Epoch 11 | train_loss 0.8847 acc 0.582 f1 0.571 || val_loss 1.1309 acc 0.362 f1 0.329\n",
            "[W3] RNN Epoch 12 | train_loss 0.8639 acc 0.586 f1 0.575 || val_loss 1.1044 acc 0.399 f1 0.346\n",
            "[W3] RNN Epoch 13 | train_loss 0.8495 acc 0.597 f1 0.589 || val_loss 1.1262 acc 0.372 f1 0.330\n",
            "[W3] RNN Epoch 14 | train_loss 0.8291 acc 0.613 f1 0.602 || val_loss 1.1325 acc 0.368 f1 0.319\n",
            "[W3] RNN Epoch 15 | train_loss 0.8160 acc 0.612 f1 0.603 || val_loss 1.1312 acc 0.381 f1 0.322\n",
            "[W3] RNN Epoch 16 | train_loss 0.7874 acc 0.637 f1 0.628 || val_loss 1.1541 acc 0.389 f1 0.338\n",
            "[W3] RNN Epoch 17 | train_loss 0.7708 acc 0.634 f1 0.624 || val_loss 1.1568 acc 0.385 f1 0.332\n",
            "[W3] RNN Epoch 18 | train_loss 0.7411 acc 0.657 f1 0.648 || val_loss 1.1632 acc 0.387 f1 0.330\n",
            "[W3] RNN Epoch 19 | train_loss 0.7294 acc 0.658 f1 0.649 || val_loss 1.1836 acc 0.379 f1 0.326\n",
            "[W3] RNN Epoch 20 | train_loss 0.7429 acc 0.642 f1 0.635 || val_loss 1.1825 acc 0.397 f1 0.334\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=29\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0985 acc 0.341 f1 0.338 || val_loss 1.0968 acc 0.358 f1 0.309\n",
            "[W3] GRU Epoch 02 | train_loss 1.0899 acc 0.395 f1 0.392 || val_loss 1.0943 acc 0.354 f1 0.317\n",
            "[W3] GRU Epoch 03 | train_loss 1.0813 acc 0.423 f1 0.419 || val_loss 1.0938 acc 0.319 f1 0.288\n",
            "[W3] GRU Epoch 04 | train_loss 1.0682 acc 0.441 f1 0.441 || val_loss 1.0999 acc 0.329 f1 0.312\n",
            "[W3] GRU Epoch 05 | train_loss 1.0533 acc 0.457 f1 0.449 || val_loss 1.0943 acc 0.360 f1 0.332\n",
            "[W3] GRU Epoch 06 | train_loss 1.0239 acc 0.497 f1 0.493 || val_loss 1.1146 acc 0.325 f1 0.311\n",
            "[W3] GRU Epoch 07 | train_loss 0.9817 acc 0.533 f1 0.527 || val_loss 1.0792 acc 0.368 f1 0.324\n",
            "[W3] GRU Epoch 08 | train_loss 0.9181 acc 0.555 f1 0.545 || val_loss 1.1165 acc 0.327 f1 0.296\n",
            "[W3] GRU Epoch 09 | train_loss 0.8690 acc 0.582 f1 0.576 || val_loss 1.0792 acc 0.362 f1 0.292\n",
            "[W3] GRU Epoch 10 | train_loss 0.8341 acc 0.601 f1 0.597 || val_loss 1.0845 acc 0.364 f1 0.290\n",
            "[W3] GRU Epoch 11 | train_loss 0.8110 acc 0.614 f1 0.611 || val_loss 1.1670 acc 0.337 f1 0.284\n",
            "[W3] GRU Epoch 12 | train_loss 0.7554 acc 0.629 f1 0.623 || val_loss 1.1481 acc 0.364 f1 0.290\n",
            "[W3] GRU Epoch 13 | train_loss 0.7429 acc 0.635 f1 0.630 || val_loss 1.1665 acc 0.356 f1 0.282\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=29\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0993 acc 0.332 f1 0.212 || val_loss 1.0926 acc 0.403 f1 0.308\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0944 acc 0.362 f1 0.291 || val_loss 1.0933 acc 0.354 f1 0.255\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0881 acc 0.399 f1 0.378 || val_loss 1.0909 acc 0.302 f1 0.275\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0753 acc 0.420 f1 0.397 || val_loss 1.0982 acc 0.298 f1 0.287\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0485 acc 0.468 f1 0.447 || val_loss 1.1084 acc 0.296 f1 0.287\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9956 acc 0.521 f1 0.506 || val_loss 1.1275 acc 0.292 f1 0.280\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9222 acc 0.540 f1 0.526 || val_loss 1.1231 acc 0.337 f1 0.302\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8542 acc 0.580 f1 0.572 || val_loss 1.1966 acc 0.319 f1 0.295\n",
            "[W3] LSTM Epoch 09 | train_loss 0.8145 acc 0.595 f1 0.588 || val_loss 1.1568 acc 0.360 f1 0.309\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7784 acc 0.621 f1 0.614 || val_loss 1.1551 acc 0.364 f1 0.305\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7300 acc 0.633 f1 0.628 || val_loss 1.1757 acc 0.360 f1 0.301\n",
            "[W3] LSTM Epoch 12 | train_loss 0.7151 acc 0.651 f1 0.647 || val_loss 1.1978 acc 0.356 f1 0.298\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6773 acc 0.666 f1 0.662 || val_loss 1.2592 acc 0.360 f1 0.314\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6482 acc 0.678 f1 0.674 || val_loss 1.2775 acc 0.370 f1 0.308\n",
            "[W3] LSTM Epoch 15 | train_loss 0.6317 acc 0.687 f1 0.683 || val_loss 1.3135 acc 0.354 f1 0.302\n",
            "[W3] LSTM Epoch 16 | train_loss 0.6192 acc 0.688 f1 0.685 || val_loss 1.2877 acc 0.370 f1 0.305\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5831 acc 0.707 f1 0.704 || val_loss 1.3301 acc 0.377 f1 0.314\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5646 acc 0.716 f1 0.714 || val_loss 1.3822 acc 0.370 f1 0.310\n",
            "[W3] LSTM Epoch 19 | train_loss 0.5332 acc 0.732 f1 0.730 || val_loss 1.3859 acc 0.399 f1 0.327\n",
            "[W3] LSTM Epoch 20 | train_loss 0.5136 acc 0.733 f1 0.732 || val_loss 1.4647 acc 0.385 f1 0.320\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4907 acc 0.759 f1 0.758 || val_loss 1.5213 acc 0.391 f1 0.320\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4786 acc 0.761 f1 0.760 || val_loss 1.5361 acc 0.399 f1 0.327\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4588 acc 0.775 f1 0.774 || val_loss 1.6240 acc 0.389 f1 0.324\n",
            "[W3] LSTM Epoch 24 | train_loss 0.4435 acc 0.777 f1 0.776 || val_loss 1.6557 acc 0.391 f1 0.319\n",
            "[W3] LSTM Epoch 25 | train_loss 0.4222 acc 0.796 f1 0.795 || val_loss 1.6917 acc 0.401 f1 0.329\n",
            "[W3] LSTM Epoch 26 | train_loss 0.4172 acc 0.795 f1 0.795 || val_loss 1.7709 acc 0.387 f1 0.319\n",
            "[W3] LSTM Epoch 27 | train_loss 0.4072 acc 0.795 f1 0.795 || val_loss 1.7680 acc 0.401 f1 0.324\n",
            "[W3] LSTM Epoch 28 | train_loss 0.3925 acc 0.813 f1 0.813 || val_loss 1.8099 acc 0.405 f1 0.326\n",
            "[W3] LSTM Epoch 29 | train_loss 0.3695 acc 0.828 f1 0.828 || val_loss 1.8877 acc 0.414 f1 0.337\n",
            "[W3] LSTM Epoch 30 | train_loss 0.3567 acc 0.829 f1 0.829 || val_loss 1.9653 acc 0.412 f1 0.335\n",
            "[W3] LSTM Epoch 31 | train_loss 0.3483 acc 0.840 f1 0.840 || val_loss 1.9878 acc 0.377 f1 0.314\n",
            "[W3] LSTM Epoch 32 | train_loss 0.3353 acc 0.843 f1 0.843 || val_loss 2.0192 acc 0.403 f1 0.330\n",
            "[W3] LSTM Epoch 33 | train_loss 0.3140 acc 0.863 f1 0.863 || val_loss 2.1249 acc 0.385 f1 0.321\n",
            "[W3] LSTM Epoch 34 | train_loss 0.3094 acc 0.866 f1 0.866 || val_loss 2.1421 acc 0.399 f1 0.328\n",
            "[W3] LSTM Epoch 35 | train_loss 0.2960 acc 0.874 f1 0.874 || val_loss 2.1608 acc 0.393 f1 0.323\n",
            "[W3] LSTM Epoch 36 | train_loss 0.2796 acc 0.879 f1 0.879 || val_loss 2.2451 acc 0.407 f1 0.334\n",
            "[W3] LSTM Epoch 37 | train_loss 0.2689 acc 0.888 f1 0.888 || val_loss 2.2502 acc 0.414 f1 0.336\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  29%|       | 29/100 [14:21<33:45, 28.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=30 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=30\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 932, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1440 acc 0.397 f1 0.396 || val_loss 1.1588 acc 0.282 f1 0.273\n",
            "[W3] ANN Epoch 02 | train_loss 0.9692 acc 0.522 f1 0.515 || val_loss 1.1435 acc 0.362 f1 0.346\n",
            "[W3] ANN Epoch 03 | train_loss 0.8817 acc 0.571 f1 0.562 || val_loss 1.1112 acc 0.379 f1 0.349\n",
            "[W3] ANN Epoch 04 | train_loss 0.7859 acc 0.622 f1 0.616 || val_loss 1.0969 acc 0.370 f1 0.328\n",
            "[W3] ANN Epoch 05 | train_loss 0.7201 acc 0.644 f1 0.637 || val_loss 1.1091 acc 0.387 f1 0.346\n",
            "[W3] ANN Epoch 06 | train_loss 0.6691 acc 0.676 f1 0.673 || val_loss 1.1095 acc 0.422 f1 0.367\n",
            "[W3] ANN Epoch 07 | train_loss 0.6252 acc 0.718 f1 0.718 || val_loss 1.1435 acc 0.434 f1 0.381\n",
            "[W3] ANN Epoch 08 | train_loss 0.5896 acc 0.723 f1 0.722 || val_loss 1.1880 acc 0.426 f1 0.367\n",
            "[W3] ANN Epoch 09 | train_loss 0.5594 acc 0.736 f1 0.735 || val_loss 1.1976 acc 0.430 f1 0.374\n",
            "[W3] ANN Epoch 10 | train_loss 0.5381 acc 0.759 f1 0.758 || val_loss 1.2335 acc 0.422 f1 0.360\n",
            "[W3] ANN Epoch 11 | train_loss 0.5163 acc 0.767 f1 0.766 || val_loss 1.2588 acc 0.436 f1 0.359\n",
            "[W3] ANN Epoch 12 | train_loss 0.4919 acc 0.782 f1 0.782 || val_loss 1.2691 acc 0.424 f1 0.355\n",
            "[W3] ANN Epoch 13 | train_loss 0.4730 acc 0.784 f1 0.784 || val_loss 1.2819 acc 0.420 f1 0.347\n",
            "[W3] ANN Epoch 14 | train_loss 0.4668 acc 0.791 f1 0.791 || val_loss 1.3155 acc 0.459 f1 0.381\n",
            "[W3] ANN Epoch 15 | train_loss 0.4190 acc 0.819 f1 0.819 || val_loss 1.3448 acc 0.475 f1 0.395\n",
            "[W3] ANN Epoch 16 | train_loss 0.3904 acc 0.827 f1 0.826 || val_loss 1.4007 acc 0.440 f1 0.347\n",
            "[W3] ANN Epoch 17 | train_loss 0.3821 acc 0.830 f1 0.830 || val_loss 1.4770 acc 0.453 f1 0.362\n",
            "[W3] ANN Epoch 18 | train_loss 0.3746 acc 0.835 f1 0.834 || val_loss 1.4332 acc 0.436 f1 0.344\n",
            "[W3] ANN Epoch 19 | train_loss 0.3906 acc 0.838 f1 0.838 || val_loss 1.4395 acc 0.422 f1 0.344\n",
            "[W3] ANN Epoch 20 | train_loss 0.3674 acc 0.845 f1 0.844 || val_loss 1.4691 acc 0.447 f1 0.345\n",
            "[W3] ANN Epoch 21 | train_loss 0.3493 acc 0.855 f1 0.855 || val_loss 1.4588 acc 0.463 f1 0.379\n",
            "[W3] ANN Epoch 22 | train_loss 0.3375 acc 0.861 f1 0.861 || val_loss 1.5369 acc 0.449 f1 0.364\n",
            "[W3] ANN Epoch 23 | train_loss 0.3230 acc 0.869 f1 0.869 || val_loss 1.5394 acc 0.449 f1 0.364\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=30\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 932, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0753 acc 0.402 f1 0.403 || val_loss 1.0409 acc 0.426 f1 0.295\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9530 acc 0.537 f1 0.530 || val_loss 1.0468 acc 0.409 f1 0.309\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8176 acc 0.615 f1 0.611 || val_loss 1.1318 acc 0.377 f1 0.304\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7080 acc 0.680 f1 0.677 || val_loss 1.2583 acc 0.335 f1 0.297\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6285 acc 0.711 f1 0.708 || val_loss 1.2412 acc 0.370 f1 0.309\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5582 acc 0.750 f1 0.748 || val_loss 1.3098 acc 0.383 f1 0.320\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5028 acc 0.769 f1 0.768 || val_loss 1.3479 acc 0.418 f1 0.346\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4418 acc 0.810 f1 0.809 || val_loss 1.4482 acc 0.387 f1 0.292\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.3935 acc 0.830 f1 0.830 || val_loss 1.5331 acc 0.399 f1 0.322\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3564 acc 0.846 f1 0.846 || val_loss 1.5883 acc 0.393 f1 0.317\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3212 acc 0.870 f1 0.870 || val_loss 1.6754 acc 0.383 f1 0.310\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2853 acc 0.889 f1 0.889 || val_loss 1.8719 acc 0.403 f1 0.301\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2600 acc 0.894 f1 0.894 || val_loss 1.9593 acc 0.374 f1 0.298\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2289 acc 0.912 f1 0.912 || val_loss 1.9511 acc 0.405 f1 0.333\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1889 acc 0.928 f1 0.928 || val_loss 2.0996 acc 0.377 f1 0.300\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=30\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 932, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1000 acc 0.343 f1 0.302 || val_loss 1.1098 acc 0.288 f1 0.276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0873 acc 0.405 f1 0.401 || val_loss 1.1033 acc 0.331 f1 0.305\n",
            "[W3] RNN Epoch 03 | train_loss 1.0757 acc 0.426 f1 0.425 || val_loss 1.1152 acc 0.315 f1 0.298\n",
            "[W3] RNN Epoch 04 | train_loss 1.0586 acc 0.456 f1 0.453 || val_loss 1.1040 acc 0.344 f1 0.318\n",
            "[W3] RNN Epoch 05 | train_loss 1.0388 acc 0.475 f1 0.473 || val_loss 1.1078 acc 0.335 f1 0.317\n",
            "[W3] RNN Epoch 06 | train_loss 1.0111 acc 0.504 f1 0.502 || val_loss 1.0962 acc 0.340 f1 0.312\n",
            "[W3] RNN Epoch 07 | train_loss 0.9837 acc 0.529 f1 0.524 || val_loss 1.0926 acc 0.335 f1 0.305\n",
            "[W3] RNN Epoch 08 | train_loss 0.9525 acc 0.532 f1 0.527 || val_loss 1.0929 acc 0.350 f1 0.322\n",
            "[W3] RNN Epoch 09 | train_loss 0.9229 acc 0.565 f1 0.558 || val_loss 1.0852 acc 0.362 f1 0.321\n",
            "[W3] RNN Epoch 10 | train_loss 0.8958 acc 0.564 f1 0.557 || val_loss 1.1033 acc 0.362 f1 0.323\n",
            "[W3] RNN Epoch 11 | train_loss 0.8673 acc 0.605 f1 0.598 || val_loss 1.0913 acc 0.364 f1 0.313\n",
            "[W3] RNN Epoch 12 | train_loss 0.8325 acc 0.615 f1 0.607 || val_loss 1.0970 acc 0.370 f1 0.317\n",
            "[W3] RNN Epoch 13 | train_loss 0.8159 acc 0.610 f1 0.604 || val_loss 1.1124 acc 0.358 f1 0.303\n",
            "[W3] RNN Epoch 14 | train_loss 0.7916 acc 0.624 f1 0.616 || val_loss 1.1297 acc 0.362 f1 0.317\n",
            "[W3] RNN Epoch 15 | train_loss 0.7639 acc 0.638 f1 0.630 || val_loss 1.1424 acc 0.368 f1 0.316\n",
            "[W3] RNN Epoch 16 | train_loss 0.7343 acc 0.657 f1 0.650 || val_loss 1.1333 acc 0.370 f1 0.309\n",
            "[W3] RNN Epoch 17 | train_loss 0.7178 acc 0.665 f1 0.658 || val_loss 1.1466 acc 0.381 f1 0.325\n",
            "[W3] RNN Epoch 18 | train_loss 0.6858 acc 0.677 f1 0.671 || val_loss 1.1726 acc 0.393 f1 0.334\n",
            "[W3] RNN Epoch 19 | train_loss 0.6820 acc 0.684 f1 0.677 || val_loss 1.1872 acc 0.385 f1 0.328\n",
            "[W3] RNN Epoch 20 | train_loss 0.6613 acc 0.695 f1 0.689 || val_loss 1.1969 acc 0.372 f1 0.318\n",
            "[W3] RNN Epoch 21 | train_loss 0.6312 acc 0.718 f1 0.713 || val_loss 1.2270 acc 0.391 f1 0.342\n",
            "[W3] RNN Epoch 22 | train_loss 0.6156 acc 0.711 f1 0.707 || val_loss 1.2343 acc 0.387 f1 0.322\n",
            "[W3] RNN Epoch 23 | train_loss 0.5915 acc 0.721 f1 0.717 || val_loss 1.2669 acc 0.372 f1 0.315\n",
            "[W3] RNN Epoch 24 | train_loss 0.5782 acc 0.728 f1 0.724 || val_loss 1.2656 acc 0.399 f1 0.332\n",
            "[W3] RNN Epoch 25 | train_loss 0.5657 acc 0.734 f1 0.731 || val_loss 1.3055 acc 0.379 f1 0.320\n",
            "[W3] RNN Epoch 26 | train_loss 0.5461 acc 0.735 f1 0.732 || val_loss 1.3245 acc 0.389 f1 0.331\n",
            "[W3] RNN Epoch 27 | train_loss 0.5333 acc 0.752 f1 0.749 || val_loss 1.3169 acc 0.401 f1 0.345\n",
            "[W3] RNN Epoch 28 | train_loss 0.5187 acc 0.752 f1 0.749 || val_loss 1.3529 acc 0.387 f1 0.317\n",
            "[W3] RNN Epoch 29 | train_loss 0.5059 acc 0.748 f1 0.745 || val_loss 1.3571 acc 0.385 f1 0.312\n",
            "[W3] RNN Epoch 30 | train_loss 0.5015 acc 0.763 f1 0.760 || val_loss 1.3849 acc 0.401 f1 0.328\n",
            "[W3] RNN Epoch 31 | train_loss 0.4795 acc 0.772 f1 0.770 || val_loss 1.4240 acc 0.403 f1 0.330\n",
            "[W3] RNN Epoch 32 | train_loss 0.4731 acc 0.781 f1 0.780 || val_loss 1.4361 acc 0.403 f1 0.334\n",
            "[W3] RNN Epoch 33 | train_loss 0.4648 acc 0.784 f1 0.781 || val_loss 1.4602 acc 0.414 f1 0.343\n",
            "[W3] RNN Epoch 34 | train_loss 0.4564 acc 0.790 f1 0.789 || val_loss 1.4851 acc 0.393 f1 0.335\n",
            "[W3] RNN Epoch 35 | train_loss 0.4413 acc 0.794 f1 0.792 || val_loss 1.5392 acc 0.420 f1 0.355\n",
            "[W3] RNN Epoch 36 | train_loss 0.4346 acc 0.793 f1 0.791 || val_loss 1.5276 acc 0.403 f1 0.335\n",
            "[W3] RNN Epoch 37 | train_loss 0.4176 acc 0.811 f1 0.810 || val_loss 1.5468 acc 0.412 f1 0.341\n",
            "[W3] RNN Epoch 38 | train_loss 0.4189 acc 0.801 f1 0.800 || val_loss 1.5658 acc 0.422 f1 0.358\n",
            "[W3] RNN Epoch 39 | train_loss 0.4090 acc 0.811 f1 0.810 || val_loss 1.5666 acc 0.436 f1 0.364\n",
            "[W3] RNN Epoch 40 | train_loss 0.3945 acc 0.822 f1 0.821 || val_loss 1.6062 acc 0.416 f1 0.348\n",
            "[W3] RNN Epoch 41 | train_loss 0.3886 acc 0.822 f1 0.821 || val_loss 1.6261 acc 0.444 f1 0.364\n",
            "[W3] RNN Epoch 42 | train_loss 0.3713 acc 0.828 f1 0.827 || val_loss 1.6421 acc 0.444 f1 0.376\n",
            "[W3] RNN Epoch 43 | train_loss 0.3715 acc 0.827 f1 0.826 || val_loss 1.6792 acc 0.432 f1 0.356\n",
            "[W3] RNN Epoch 44 | train_loss 0.3605 acc 0.844 f1 0.844 || val_loss 1.7336 acc 0.428 f1 0.353\n",
            "[W3] RNN Epoch 45 | train_loss 0.3588 acc 0.840 f1 0.839 || val_loss 1.7136 acc 0.426 f1 0.342\n",
            "[W3] RNN Epoch 46 | train_loss 0.3510 acc 0.854 f1 0.853 || val_loss 1.7500 acc 0.426 f1 0.352\n",
            "[W3] RNN Epoch 47 | train_loss 0.3421 acc 0.853 f1 0.852 || val_loss 1.7475 acc 0.453 f1 0.376\n",
            "[W3] RNN Epoch 48 | train_loss 0.3275 acc 0.861 f1 0.861 || val_loss 1.7828 acc 0.438 f1 0.361\n",
            "[W3] RNN Epoch 49 | train_loss 0.3126 acc 0.871 f1 0.871 || val_loss 1.8205 acc 0.434 f1 0.362\n",
            "[W3] RNN Epoch 50 | train_loss 0.3180 acc 0.868 f1 0.867 || val_loss 1.8118 acc 0.426 f1 0.356\n",
            "[W3] RNN Epoch 51 | train_loss 0.3038 acc 0.874 f1 0.874 || val_loss 1.8726 acc 0.432 f1 0.361\n",
            "[W3] RNN Epoch 52 | train_loss 0.2928 acc 0.880 f1 0.879 || val_loss 1.8840 acc 0.430 f1 0.349\n",
            "[W3] RNN Epoch 53 | train_loss 0.2843 acc 0.883 f1 0.883 || val_loss 1.9368 acc 0.418 f1 0.336\n",
            "[W3] RNN Epoch 54 | train_loss 0.2776 acc 0.885 f1 0.885 || val_loss 1.9572 acc 0.451 f1 0.378\n",
            "[W3] RNN Epoch 55 | train_loss 0.2641 acc 0.901 f1 0.900 || val_loss 1.9696 acc 0.430 f1 0.341\n",
            "[W3] RNN Epoch 56 | train_loss 0.2655 acc 0.897 f1 0.896 || val_loss 2.0014 acc 0.422 f1 0.334\n",
            "[W3] RNN Epoch 57 | train_loss 0.2490 acc 0.902 f1 0.902 || val_loss 2.0078 acc 0.424 f1 0.354\n",
            "[W3] RNN Epoch 58 | train_loss 0.2507 acc 0.897 f1 0.897 || val_loss 2.0350 acc 0.444 f1 0.360\n",
            "[W3] RNN Epoch 59 | train_loss 0.2318 acc 0.909 f1 0.909 || val_loss 2.0547 acc 0.416 f1 0.348\n",
            "[W3] RNN Epoch 60 | train_loss 0.2262 acc 0.915 f1 0.915 || val_loss 2.0931 acc 0.428 f1 0.344\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=30\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 932, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0982 acc 0.345 f1 0.323 || val_loss 1.0967 acc 0.356 f1 0.334\n",
            "[W3] GRU Epoch 02 | train_loss 1.0900 acc 0.398 f1 0.397 || val_loss 1.0893 acc 0.362 f1 0.326\n",
            "[W3] GRU Epoch 03 | train_loss 1.0808 acc 0.417 f1 0.415 || val_loss 1.0901 acc 0.344 f1 0.319\n",
            "[W3] GRU Epoch 04 | train_loss 1.0643 acc 0.450 f1 0.448 || val_loss 1.0994 acc 0.333 f1 0.320\n",
            "[W3] GRU Epoch 05 | train_loss 1.0395 acc 0.486 f1 0.479 || val_loss 1.0900 acc 0.340 f1 0.313\n",
            "[W3] GRU Epoch 06 | train_loss 0.9895 acc 0.519 f1 0.512 || val_loss 1.0838 acc 0.342 f1 0.309\n",
            "[W3] GRU Epoch 07 | train_loss 0.9200 acc 0.556 f1 0.549 || val_loss 1.0977 acc 0.358 f1 0.310\n",
            "[W3] GRU Epoch 08 | train_loss 0.8566 acc 0.580 f1 0.574 || val_loss 1.1116 acc 0.360 f1 0.322\n",
            "[W3] GRU Epoch 09 | train_loss 0.7999 acc 0.609 f1 0.604 || val_loss 1.0833 acc 0.387 f1 0.308\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=30\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 932, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0996 acc 0.343 f1 0.222 || val_loss 1.0917 acc 0.424 f1 0.254\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0945 acc 0.378 f1 0.343 || val_loss 1.0934 acc 0.387 f1 0.317\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0874 acc 0.402 f1 0.397 || val_loss 1.0916 acc 0.360 f1 0.315\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0726 acc 0.439 f1 0.428 || val_loss 1.0993 acc 0.317 f1 0.297\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0441 acc 0.482 f1 0.475 || val_loss 1.1066 acc 0.340 f1 0.307\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9601 acc 0.544 f1 0.536 || val_loss 1.0745 acc 0.379 f1 0.326\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8659 acc 0.580 f1 0.573 || val_loss 1.1077 acc 0.362 f1 0.305\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7994 acc 0.620 f1 0.614 || val_loss 1.1312 acc 0.364 f1 0.306\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7472 acc 0.644 f1 0.639 || val_loss 1.1534 acc 0.372 f1 0.308\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7129 acc 0.653 f1 0.648 || val_loss 1.1727 acc 0.377 f1 0.305\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6783 acc 0.671 f1 0.668 || val_loss 1.1884 acc 0.379 f1 0.303\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6406 acc 0.690 f1 0.687 || val_loss 1.2437 acc 0.387 f1 0.325\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6173 acc 0.705 f1 0.702 || val_loss 1.2489 acc 0.383 f1 0.310\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5901 acc 0.718 f1 0.715 || val_loss 1.2838 acc 0.377 f1 0.301\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  30%|       | 30/100 [14:52<33:59, 29.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=31 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=31\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1575 acc 0.386 f1 0.382 || val_loss 1.1278 acc 0.340 f1 0.325\n",
            "[W3] ANN Epoch 02 | train_loss 0.9744 acc 0.505 f1 0.496 || val_loss 1.0918 acc 0.391 f1 0.354\n",
            "[W3] ANN Epoch 03 | train_loss 0.8848 acc 0.549 f1 0.542 || val_loss 1.0871 acc 0.403 f1 0.357\n",
            "[W3] ANN Epoch 04 | train_loss 0.8109 acc 0.601 f1 0.593 || val_loss 1.0704 acc 0.405 f1 0.327\n",
            "[W3] ANN Epoch 05 | train_loss 0.7479 acc 0.630 f1 0.625 || val_loss 1.1153 acc 0.397 f1 0.337\n",
            "[W3] ANN Epoch 06 | train_loss 0.7047 acc 0.653 f1 0.650 || val_loss 1.1014 acc 0.414 f1 0.343\n",
            "[W3] ANN Epoch 07 | train_loss 0.6717 acc 0.669 f1 0.667 || val_loss 1.1460 acc 0.409 f1 0.325\n",
            "[W3] ANN Epoch 08 | train_loss 0.6447 acc 0.685 f1 0.684 || val_loss 1.1477 acc 0.418 f1 0.331\n",
            "[W3] ANN Epoch 09 | train_loss 0.6134 acc 0.714 f1 0.713 || val_loss 1.1878 acc 0.434 f1 0.343\n",
            "[W3] ANN Epoch 10 | train_loss 0.5891 acc 0.716 f1 0.714 || val_loss 1.1696 acc 0.416 f1 0.334\n",
            "[W3] ANN Epoch 11 | train_loss 0.5632 acc 0.734 f1 0.732 || val_loss 1.2284 acc 0.416 f1 0.329\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=31\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0659 acc 0.411 f1 0.413 || val_loss 1.0145 acc 0.409 f1 0.318\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9337 acc 0.544 f1 0.538 || val_loss 1.0248 acc 0.405 f1 0.297\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8141 acc 0.605 f1 0.600 || val_loss 1.0788 acc 0.399 f1 0.338\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7134 acc 0.669 f1 0.666 || val_loss 1.1254 acc 0.379 f1 0.314\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6503 acc 0.693 f1 0.690 || val_loss 1.2051 acc 0.370 f1 0.301\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5848 acc 0.726 f1 0.726 || val_loss 1.2697 acc 0.362 f1 0.301\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5306 acc 0.755 f1 0.753 || val_loss 1.3067 acc 0.387 f1 0.306\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4653 acc 0.796 f1 0.795 || val_loss 1.4303 acc 0.377 f1 0.313\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4331 acc 0.808 f1 0.807 || val_loss 1.4603 acc 0.370 f1 0.329\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3905 acc 0.836 f1 0.836 || val_loss 1.4868 acc 0.401 f1 0.336\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3505 acc 0.855 f1 0.855 || val_loss 1.6003 acc 0.377 f1 0.323\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=31\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0987 acc 0.363 f1 0.319 || val_loss 1.1029 acc 0.319 f1 0.298\n",
            "[W3] RNN Epoch 02 | train_loss 1.0826 acc 0.422 f1 0.414 || val_loss 1.0983 acc 0.325 f1 0.303\n",
            "[W3] RNN Epoch 03 | train_loss 1.0658 acc 0.440 f1 0.433 || val_loss 1.0866 acc 0.354 f1 0.324\n",
            "[W3] RNN Epoch 04 | train_loss 1.0459 acc 0.454 f1 0.450 || val_loss 1.0763 acc 0.360 f1 0.318\n",
            "[W3] RNN Epoch 05 | train_loss 1.0221 acc 0.472 f1 0.465 || val_loss 1.0844 acc 0.346 f1 0.319\n",
            "[W3] RNN Epoch 06 | train_loss 0.9980 acc 0.500 f1 0.494 || val_loss 1.0823 acc 0.356 f1 0.323\n",
            "[W3] RNN Epoch 07 | train_loss 0.9721 acc 0.523 f1 0.514 || val_loss 1.1122 acc 0.333 f1 0.319\n",
            "[W3] RNN Epoch 08 | train_loss 0.9460 acc 0.544 f1 0.538 || val_loss 1.1030 acc 0.356 f1 0.332\n",
            "[W3] RNN Epoch 09 | train_loss 0.9266 acc 0.553 f1 0.545 || val_loss 1.0783 acc 0.374 f1 0.340\n",
            "[W3] RNN Epoch 10 | train_loss 0.9001 acc 0.572 f1 0.566 || val_loss 1.0780 acc 0.379 f1 0.340\n",
            "[W3] RNN Epoch 11 | train_loss 0.8661 acc 0.599 f1 0.589 || val_loss 1.0972 acc 0.370 f1 0.336\n",
            "[W3] RNN Epoch 12 | train_loss 0.8519 acc 0.606 f1 0.599 || val_loss 1.0851 acc 0.379 f1 0.340\n",
            "[W3] RNN Epoch 13 | train_loss 0.8301 acc 0.608 f1 0.599 || val_loss 1.0982 acc 0.395 f1 0.361\n",
            "[W3] RNN Epoch 14 | train_loss 0.8079 acc 0.604 f1 0.594 || val_loss 1.1304 acc 0.372 f1 0.342\n",
            "[W3] RNN Epoch 15 | train_loss 0.7867 acc 0.639 f1 0.631 || val_loss 1.1222 acc 0.372 f1 0.342\n",
            "[W3] RNN Epoch 16 | train_loss 0.7513 acc 0.652 f1 0.644 || val_loss 1.1235 acc 0.389 f1 0.337\n",
            "[W3] RNN Epoch 17 | train_loss 0.7368 acc 0.659 f1 0.653 || val_loss 1.1462 acc 0.383 f1 0.333\n",
            "[W3] RNN Epoch 18 | train_loss 0.7174 acc 0.664 f1 0.658 || val_loss 1.1570 acc 0.381 f1 0.337\n",
            "[W3] RNN Epoch 19 | train_loss 0.6917 acc 0.676 f1 0.669 || val_loss 1.1689 acc 0.397 f1 0.348\n",
            "[W3] RNN Epoch 20 | train_loss 0.6776 acc 0.678 f1 0.670 || val_loss 1.1718 acc 0.381 f1 0.317\n",
            "[W3] RNN Epoch 21 | train_loss 0.6569 acc 0.687 f1 0.680 || val_loss 1.1968 acc 0.383 f1 0.326\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=31\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0967 acc 0.336 f1 0.280 || val_loss 1.0960 acc 0.302 f1 0.281\n",
            "[W3] GRU Epoch 02 | train_loss 1.0867 acc 0.398 f1 0.398 || val_loss 1.0985 acc 0.302 f1 0.292\n",
            "[W3] GRU Epoch 03 | train_loss 1.0755 acc 0.432 f1 0.430 || val_loss 1.0974 acc 0.290 f1 0.283\n",
            "[W3] GRU Epoch 04 | train_loss 1.0606 acc 0.445 f1 0.435 || val_loss 1.1031 acc 0.298 f1 0.291\n",
            "[W3] GRU Epoch 05 | train_loss 1.0302 acc 0.485 f1 0.476 || val_loss 1.1108 acc 0.319 f1 0.308\n",
            "[W3] GRU Epoch 06 | train_loss 0.9870 acc 0.527 f1 0.521 || val_loss 1.1048 acc 0.327 f1 0.297\n",
            "[W3] GRU Epoch 07 | train_loss 0.9118 acc 0.557 f1 0.549 || val_loss 1.1170 acc 0.364 f1 0.310\n",
            "[W3] GRU Epoch 08 | train_loss 0.8548 acc 0.580 f1 0.573 || val_loss 1.1408 acc 0.360 f1 0.312\n",
            "[W3] GRU Epoch 09 | train_loss 0.8063 acc 0.610 f1 0.604 || val_loss 1.1576 acc 0.385 f1 0.317\n",
            "[W3] GRU Epoch 10 | train_loss 0.7606 acc 0.631 f1 0.626 || val_loss 1.1427 acc 0.403 f1 0.326\n",
            "[W3] GRU Epoch 11 | train_loss 0.7321 acc 0.645 f1 0.641 || val_loss 1.1706 acc 0.412 f1 0.341\n",
            "[W3] GRU Epoch 12 | train_loss 0.7021 acc 0.665 f1 0.663 || val_loss 1.2061 acc 0.405 f1 0.347\n",
            "[W3] GRU Epoch 13 | train_loss 0.6661 acc 0.684 f1 0.681 || val_loss 1.2049 acc 0.385 f1 0.313\n",
            "[W3] GRU Epoch 14 | train_loss 0.6491 acc 0.686 f1 0.684 || val_loss 1.2177 acc 0.407 f1 0.342\n",
            "[W3] GRU Epoch 15 | train_loss 0.6229 acc 0.700 f1 0.697 || val_loss 1.2465 acc 0.412 f1 0.352\n",
            "[W3] GRU Epoch 16 | train_loss 0.5964 acc 0.710 f1 0.706 || val_loss 1.2694 acc 0.412 f1 0.341\n",
            "[W3] GRU Epoch 17 | train_loss 0.5726 acc 0.731 f1 0.729 || val_loss 1.3283 acc 0.409 f1 0.354\n",
            "[W3] GRU Epoch 18 | train_loss 0.5497 acc 0.738 f1 0.736 || val_loss 1.3503 acc 0.397 f1 0.337\n",
            "[W3] GRU Epoch 19 | train_loss 0.5307 acc 0.748 f1 0.747 || val_loss 1.3647 acc 0.399 f1 0.334\n",
            "[W3] GRU Epoch 20 | train_loss 0.5042 acc 0.762 f1 0.761 || val_loss 1.4288 acc 0.403 f1 0.346\n",
            "[W3] GRU Epoch 21 | train_loss 0.4924 acc 0.767 f1 0.766 || val_loss 1.4517 acc 0.399 f1 0.339\n",
            "[W3] GRU Epoch 22 | train_loss 0.4676 acc 0.776 f1 0.776 || val_loss 1.4841 acc 0.397 f1 0.341\n",
            "[W3] GRU Epoch 23 | train_loss 0.4591 acc 0.788 f1 0.787 || val_loss 1.4742 acc 0.414 f1 0.338\n",
            "[W3] GRU Epoch 24 | train_loss 0.4342 acc 0.799 f1 0.799 || val_loss 1.5206 acc 0.414 f1 0.351\n",
            "[W3] GRU Epoch 25 | train_loss 0.4152 acc 0.816 f1 0.816 || val_loss 1.5844 acc 0.405 f1 0.344\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=31\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1003 acc 0.332 f1 0.246 || val_loss 1.1031 acc 0.300 f1 0.233\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0946 acc 0.372 f1 0.336 || val_loss 1.0969 acc 0.313 f1 0.283\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0878 acc 0.408 f1 0.407 || val_loss 1.0938 acc 0.325 f1 0.308\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0736 acc 0.440 f1 0.436 || val_loss 1.0869 acc 0.327 f1 0.299\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0441 acc 0.480 f1 0.478 || val_loss 1.1097 acc 0.321 f1 0.311\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9820 acc 0.535 f1 0.528 || val_loss 1.0799 acc 0.374 f1 0.330\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8811 acc 0.573 f1 0.566 || val_loss 1.0964 acc 0.377 f1 0.324\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8268 acc 0.593 f1 0.587 || val_loss 1.1113 acc 0.391 f1 0.304\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7683 acc 0.633 f1 0.629 || val_loss 1.1401 acc 0.393 f1 0.321\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7322 acc 0.655 f1 0.652 || val_loss 1.1599 acc 0.383 f1 0.296\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6964 acc 0.661 f1 0.658 || val_loss 1.2218 acc 0.395 f1 0.337\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6602 acc 0.679 f1 0.676 || val_loss 1.2420 acc 0.401 f1 0.324\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6379 acc 0.690 f1 0.687 || val_loss 1.2443 acc 0.436 f1 0.332\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6113 acc 0.705 f1 0.702 || val_loss 1.2812 acc 0.430 f1 0.329\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5678 acc 0.730 f1 0.727 || val_loss 1.3289 acc 0.428 f1 0.335\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5455 acc 0.735 f1 0.732 || val_loss 1.3499 acc 0.430 f1 0.335\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5224 acc 0.750 f1 0.748 || val_loss 1.4117 acc 0.426 f1 0.342\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5063 acc 0.754 f1 0.752 || val_loss 1.4254 acc 0.438 f1 0.349\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4776 acc 0.771 f1 0.769 || val_loss 1.4830 acc 0.414 f1 0.338\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4477 acc 0.796 f1 0.796 || val_loss 1.5247 acc 0.416 f1 0.335\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4313 acc 0.803 f1 0.802 || val_loss 1.5601 acc 0.416 f1 0.335\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4122 acc 0.805 f1 0.805 || val_loss 1.6205 acc 0.430 f1 0.340\n",
            "[W3] LSTM Epoch 23 | train_loss 0.3937 acc 0.813 f1 0.813 || val_loss 1.6669 acc 0.430 f1 0.342\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3850 acc 0.821 f1 0.821 || val_loss 1.7261 acc 0.424 f1 0.341\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3602 acc 0.841 f1 0.841 || val_loss 1.7718 acc 0.430 f1 0.340\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3528 acc 0.846 f1 0.846 || val_loss 1.8318 acc 0.422 f1 0.340\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  31%|       | 31/100 [15:19<32:54, 28.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=32 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=32\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 979, np.int64(1): 935, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 979, np.int64(2): 979, np.int64(0): 979})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1262 acc 0.417 f1 0.413 || val_loss 1.1998 acc 0.267 f1 0.266\n",
            "[W3] ANN Epoch 02 | train_loss 0.9607 acc 0.512 f1 0.504 || val_loss 1.2043 acc 0.296 f1 0.285\n",
            "[W3] ANN Epoch 03 | train_loss 0.8671 acc 0.569 f1 0.561 || val_loss 1.1643 acc 0.323 f1 0.291\n",
            "[W3] ANN Epoch 04 | train_loss 0.7836 acc 0.629 f1 0.621 || val_loss 1.1462 acc 0.342 f1 0.300\n",
            "[W3] ANN Epoch 05 | train_loss 0.7177 acc 0.652 f1 0.647 || val_loss 1.1279 acc 0.387 f1 0.339\n",
            "[W3] ANN Epoch 06 | train_loss 0.6737 acc 0.685 f1 0.682 || val_loss 1.1452 acc 0.399 f1 0.355\n",
            "[W3] ANN Epoch 07 | train_loss 0.6368 acc 0.693 f1 0.691 || val_loss 1.1417 acc 0.395 f1 0.340\n",
            "[W3] ANN Epoch 08 | train_loss 0.6025 acc 0.724 f1 0.723 || val_loss 1.1644 acc 0.416 f1 0.348\n",
            "[W3] ANN Epoch 09 | train_loss 0.5502 acc 0.741 f1 0.740 || val_loss 1.2149 acc 0.407 f1 0.349\n",
            "[W3] ANN Epoch 10 | train_loss 0.5293 acc 0.767 f1 0.767 || val_loss 1.1975 acc 0.420 f1 0.358\n",
            "[W3] ANN Epoch 11 | train_loss 0.5224 acc 0.757 f1 0.756 || val_loss 1.2479 acc 0.422 f1 0.354\n",
            "[W3] ANN Epoch 12 | train_loss 0.4985 acc 0.772 f1 0.771 || val_loss 1.2989 acc 0.407 f1 0.338\n",
            "[W3] ANN Epoch 13 | train_loss 0.4611 acc 0.798 f1 0.798 || val_loss 1.3103 acc 0.422 f1 0.357\n",
            "[W3] ANN Epoch 14 | train_loss 0.4533 acc 0.796 f1 0.796 || val_loss 1.3441 acc 0.422 f1 0.361\n",
            "[W3] ANN Epoch 15 | train_loss 0.4307 acc 0.818 f1 0.818 || val_loss 1.3803 acc 0.416 f1 0.353\n",
            "[W3] ANN Epoch 16 | train_loss 0.4175 acc 0.822 f1 0.821 || val_loss 1.3698 acc 0.451 f1 0.376\n",
            "[W3] ANN Epoch 17 | train_loss 0.4005 acc 0.832 f1 0.831 || val_loss 1.4179 acc 0.444 f1 0.375\n",
            "[W3] ANN Epoch 18 | train_loss 0.3899 acc 0.832 f1 0.832 || val_loss 1.4678 acc 0.432 f1 0.352\n",
            "[W3] ANN Epoch 19 | train_loss 0.3922 acc 0.838 f1 0.838 || val_loss 1.4436 acc 0.430 f1 0.366\n",
            "[W3] ANN Epoch 20 | train_loss 0.3752 acc 0.833 f1 0.833 || val_loss 1.4673 acc 0.432 f1 0.361\n",
            "[W3] ANN Epoch 21 | train_loss 0.3563 acc 0.849 f1 0.849 || val_loss 1.4751 acc 0.432 f1 0.366\n",
            "[W3] ANN Epoch 22 | train_loss 0.3325 acc 0.864 f1 0.864 || val_loss 1.5158 acc 0.449 f1 0.359\n",
            "[W3] ANN Epoch 23 | train_loss 0.3304 acc 0.863 f1 0.863 || val_loss 1.5872 acc 0.444 f1 0.356\n",
            "[W3] ANN Epoch 24 | train_loss 0.2985 acc 0.889 f1 0.889 || val_loss 1.6399 acc 0.436 f1 0.370\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=32\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 979, np.int64(1): 935, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 979, np.int64(2): 979, np.int64(0): 979})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0740 acc 0.412 f1 0.416 || val_loss 1.0129 acc 0.424 f1 0.305\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9432 acc 0.540 f1 0.538 || val_loss 1.0446 acc 0.416 f1 0.304\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8069 acc 0.612 f1 0.606 || val_loss 1.1052 acc 0.391 f1 0.311\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7145 acc 0.653 f1 0.650 || val_loss 1.1530 acc 0.405 f1 0.332\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6337 acc 0.708 f1 0.705 || val_loss 1.2425 acc 0.383 f1 0.330\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5620 acc 0.731 f1 0.729 || val_loss 1.2777 acc 0.422 f1 0.353\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5082 acc 0.769 f1 0.768 || val_loss 1.3409 acc 0.395 f1 0.313\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4597 acc 0.795 f1 0.793 || val_loss 1.4241 acc 0.401 f1 0.330\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4014 acc 0.836 f1 0.835 || val_loss 1.5099 acc 0.416 f1 0.333\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3658 acc 0.836 f1 0.835 || val_loss 1.5541 acc 0.389 f1 0.324\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3254 acc 0.871 f1 0.871 || val_loss 1.6997 acc 0.414 f1 0.326\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2883 acc 0.888 f1 0.888 || val_loss 1.7818 acc 0.405 f1 0.340\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2487 acc 0.908 f1 0.908 || val_loss 1.8623 acc 0.407 f1 0.323\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2190 acc 0.919 f1 0.919 || val_loss 1.9911 acc 0.424 f1 0.333\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=32\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 979, np.int64(1): 935, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 979, np.int64(2): 979, np.int64(0): 979})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0986 acc 0.346 f1 0.323 || val_loss 1.0956 acc 0.305 f1 0.264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0820 acc 0.412 f1 0.393 || val_loss 1.0915 acc 0.315 f1 0.299\n",
            "[W3] RNN Epoch 03 | train_loss 1.0651 acc 0.442 f1 0.438 || val_loss 1.0799 acc 0.368 f1 0.331\n",
            "[W3] RNN Epoch 04 | train_loss 1.0482 acc 0.460 f1 0.454 || val_loss 1.0801 acc 0.362 f1 0.337\n",
            "[W3] RNN Epoch 05 | train_loss 1.0287 acc 0.483 f1 0.477 || val_loss 1.0706 acc 0.348 f1 0.330\n",
            "[W3] RNN Epoch 06 | train_loss 0.9996 acc 0.511 f1 0.504 || val_loss 1.0597 acc 0.379 f1 0.350\n",
            "[W3] RNN Epoch 07 | train_loss 0.9699 acc 0.529 f1 0.522 || val_loss 1.0596 acc 0.374 f1 0.342\n",
            "[W3] RNN Epoch 08 | train_loss 0.9482 acc 0.535 f1 0.526 || val_loss 1.1020 acc 0.352 f1 0.335\n",
            "[W3] RNN Epoch 09 | train_loss 0.9176 acc 0.551 f1 0.539 || val_loss 1.0667 acc 0.395 f1 0.357\n",
            "[W3] RNN Epoch 10 | train_loss 0.8984 acc 0.566 f1 0.555 || val_loss 1.0980 acc 0.356 f1 0.327\n",
            "[W3] RNN Epoch 11 | train_loss 0.8797 acc 0.583 f1 0.570 || val_loss 1.0972 acc 0.381 f1 0.345\n",
            "[W3] RNN Epoch 12 | train_loss 0.8527 acc 0.596 f1 0.586 || val_loss 1.1080 acc 0.372 f1 0.335\n",
            "[W3] RNN Epoch 13 | train_loss 0.8331 acc 0.601 f1 0.590 || val_loss 1.1206 acc 0.387 f1 0.355\n",
            "[W3] RNN Epoch 14 | train_loss 0.8156 acc 0.616 f1 0.606 || val_loss 1.1218 acc 0.395 f1 0.358\n",
            "[W3] RNN Epoch 15 | train_loss 0.7890 acc 0.628 f1 0.619 || val_loss 1.1249 acc 0.397 f1 0.357\n",
            "[W3] RNN Epoch 16 | train_loss 0.7695 acc 0.635 f1 0.624 || val_loss 1.1392 acc 0.418 f1 0.369\n",
            "[W3] RNN Epoch 17 | train_loss 0.7550 acc 0.634 f1 0.626 || val_loss 1.1755 acc 0.405 f1 0.366\n",
            "[W3] RNN Epoch 18 | train_loss 0.7281 acc 0.653 f1 0.644 || val_loss 1.1611 acc 0.405 f1 0.359\n",
            "[W3] RNN Epoch 19 | train_loss 0.7026 acc 0.666 f1 0.658 || val_loss 1.1918 acc 0.409 f1 0.364\n",
            "[W3] RNN Epoch 20 | train_loss 0.6996 acc 0.654 f1 0.646 || val_loss 1.1909 acc 0.428 f1 0.380\n",
            "[W3] RNN Epoch 21 | train_loss 0.6779 acc 0.678 f1 0.670 || val_loss 1.2090 acc 0.424 f1 0.374\n",
            "[W3] RNN Epoch 22 | train_loss 0.6597 acc 0.686 f1 0.680 || val_loss 1.2113 acc 0.430 f1 0.382\n",
            "[W3] RNN Epoch 23 | train_loss 0.6309 acc 0.704 f1 0.698 || val_loss 1.2514 acc 0.403 f1 0.354\n",
            "[W3] RNN Epoch 24 | train_loss 0.6253 acc 0.704 f1 0.698 || val_loss 1.2628 acc 0.401 f1 0.350\n",
            "[W3] RNN Epoch 25 | train_loss 0.6187 acc 0.704 f1 0.698 || val_loss 1.2733 acc 0.407 f1 0.349\n",
            "[W3] RNN Epoch 26 | train_loss 0.5867 acc 0.713 f1 0.709 || val_loss 1.2907 acc 0.430 f1 0.370\n",
            "[W3] RNN Epoch 27 | train_loss 0.5862 acc 0.711 f1 0.704 || val_loss 1.3093 acc 0.405 f1 0.344\n",
            "[W3] RNN Epoch 28 | train_loss 0.5660 acc 0.725 f1 0.720 || val_loss 1.3205 acc 0.422 f1 0.344\n",
            "[W3] RNN Epoch 29 | train_loss 0.5484 acc 0.732 f1 0.729 || val_loss 1.3635 acc 0.424 f1 0.365\n",
            "[W3] RNN Epoch 30 | train_loss 0.5401 acc 0.746 f1 0.743 || val_loss 1.3659 acc 0.430 f1 0.362\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=32\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 979, np.int64(1): 935, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 979, np.int64(2): 979, np.int64(0): 979})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1011 acc 0.339 f1 0.264 || val_loss 1.0857 acc 0.350 f1 0.304\n",
            "[W3] GRU Epoch 02 | train_loss 1.0893 acc 0.383 f1 0.377 || val_loss 1.0949 acc 0.298 f1 0.290\n",
            "[W3] GRU Epoch 03 | train_loss 1.0772 acc 0.427 f1 0.419 || val_loss 1.0942 acc 0.319 f1 0.309\n",
            "[W3] GRU Epoch 04 | train_loss 1.0532 acc 0.463 f1 0.452 || val_loss 1.0887 acc 0.315 f1 0.296\n",
            "[W3] GRU Epoch 05 | train_loss 1.0093 acc 0.507 f1 0.496 || val_loss 1.0623 acc 0.354 f1 0.320\n",
            "[W3] GRU Epoch 06 | train_loss 0.9414 acc 0.537 f1 0.522 || val_loss 1.1066 acc 0.333 f1 0.306\n",
            "[W3] GRU Epoch 07 | train_loss 0.8685 acc 0.580 f1 0.571 || val_loss 1.0873 acc 0.356 f1 0.299\n",
            "[W3] GRU Epoch 08 | train_loss 0.8159 acc 0.603 f1 0.598 || val_loss 1.1036 acc 0.370 f1 0.316\n",
            "[W3] GRU Epoch 09 | train_loss 0.7710 acc 0.622 f1 0.616 || val_loss 1.0958 acc 0.377 f1 0.306\n",
            "[W3] GRU Epoch 10 | train_loss 0.7439 acc 0.640 f1 0.635 || val_loss 1.1119 acc 0.391 f1 0.325\n",
            "[W3] GRU Epoch 11 | train_loss 0.7067 acc 0.657 f1 0.655 || val_loss 1.1445 acc 0.387 f1 0.322\n",
            "[W3] GRU Epoch 12 | train_loss 0.6834 acc 0.663 f1 0.659 || val_loss 1.1650 acc 0.395 f1 0.332\n",
            "[W3] GRU Epoch 13 | train_loss 0.6606 acc 0.671 f1 0.668 || val_loss 1.1942 acc 0.403 f1 0.342\n",
            "[W3] GRU Epoch 14 | train_loss 0.6237 acc 0.691 f1 0.689 || val_loss 1.2149 acc 0.407 f1 0.333\n",
            "[W3] GRU Epoch 15 | train_loss 0.6042 acc 0.700 f1 0.698 || val_loss 1.2336 acc 0.397 f1 0.330\n",
            "[W3] GRU Epoch 16 | train_loss 0.5841 acc 0.714 f1 0.712 || val_loss 1.2575 acc 0.416 f1 0.339\n",
            "[W3] GRU Epoch 17 | train_loss 0.5557 acc 0.723 f1 0.721 || val_loss 1.2989 acc 0.393 f1 0.327\n",
            "[W3] GRU Epoch 18 | train_loss 0.5365 acc 0.731 f1 0.730 || val_loss 1.3230 acc 0.401 f1 0.332\n",
            "[W3] GRU Epoch 19 | train_loss 0.5195 acc 0.743 f1 0.741 || val_loss 1.3583 acc 0.399 f1 0.331\n",
            "[W3] GRU Epoch 20 | train_loss 0.4971 acc 0.762 f1 0.762 || val_loss 1.3925 acc 0.420 f1 0.343\n",
            "[W3] GRU Epoch 21 | train_loss 0.4782 acc 0.771 f1 0.769 || val_loss 1.4106 acc 0.420 f1 0.345\n",
            "[W3] GRU Epoch 22 | train_loss 0.4561 acc 0.782 f1 0.782 || val_loss 1.4557 acc 0.438 f1 0.361\n",
            "[W3] GRU Epoch 23 | train_loss 0.4428 acc 0.788 f1 0.787 || val_loss 1.4924 acc 0.434 f1 0.357\n",
            "[W3] GRU Epoch 24 | train_loss 0.4351 acc 0.790 f1 0.790 || val_loss 1.5227 acc 0.428 f1 0.353\n",
            "[W3] GRU Epoch 25 | train_loss 0.4163 acc 0.797 f1 0.797 || val_loss 1.5396 acc 0.440 f1 0.358\n",
            "[W3] GRU Epoch 26 | train_loss 0.4073 acc 0.805 f1 0.805 || val_loss 1.5900 acc 0.432 f1 0.360\n",
            "[W3] GRU Epoch 27 | train_loss 0.3868 acc 0.822 f1 0.822 || val_loss 1.6436 acc 0.424 f1 0.350\n",
            "[W3] GRU Epoch 28 | train_loss 0.3754 acc 0.827 f1 0.827 || val_loss 1.6497 acc 0.430 f1 0.350\n",
            "[W3] GRU Epoch 29 | train_loss 0.3641 acc 0.837 f1 0.837 || val_loss 1.6931 acc 0.438 f1 0.364\n",
            "[W3] GRU Epoch 30 | train_loss 0.3506 acc 0.841 f1 0.841 || val_loss 1.7297 acc 0.422 f1 0.353\n",
            "[W3] GRU Epoch 31 | train_loss 0.3365 acc 0.848 f1 0.848 || val_loss 1.7796 acc 0.420 f1 0.347\n",
            "[W3] GRU Epoch 32 | train_loss 0.3124 acc 0.864 f1 0.864 || val_loss 1.7873 acc 0.442 f1 0.364\n",
            "[W3] GRU Epoch 33 | train_loss 0.3125 acc 0.860 f1 0.860 || val_loss 1.8693 acc 0.430 f1 0.359\n",
            "[W3] GRU Epoch 34 | train_loss 0.2866 acc 0.882 f1 0.882 || val_loss 1.9133 acc 0.424 f1 0.346\n",
            "[W3] GRU Epoch 35 | train_loss 0.2724 acc 0.886 f1 0.886 || val_loss 1.9286 acc 0.438 f1 0.365\n",
            "[W3] GRU Epoch 36 | train_loss 0.2573 acc 0.890 f1 0.890 || val_loss 2.0192 acc 0.434 f1 0.358\n",
            "[W3] GRU Epoch 37 | train_loss 0.2369 acc 0.907 f1 0.907 || val_loss 2.0960 acc 0.426 f1 0.356\n",
            "[W3] GRU Epoch 38 | train_loss 0.2184 acc 0.919 f1 0.919 || val_loss 2.1335 acc 0.432 f1 0.356\n",
            "[W3] GRU Epoch 39 | train_loss 0.2051 acc 0.923 f1 0.923 || val_loss 2.2221 acc 0.432 f1 0.356\n",
            "[W3] GRU Epoch 40 | train_loss 0.1921 acc 0.930 f1 0.930 || val_loss 2.2413 acc 0.428 f1 0.349\n",
            "[W3] GRU Epoch 41 | train_loss 0.1771 acc 0.934 f1 0.934 || val_loss 2.3609 acc 0.430 f1 0.354\n",
            "[W3] GRU Epoch 42 | train_loss 0.1523 acc 0.948 f1 0.948 || val_loss 2.4036 acc 0.432 f1 0.352\n",
            "[W3] GRU Epoch 43 | train_loss 0.1425 acc 0.957 f1 0.957 || val_loss 2.5030 acc 0.426 f1 0.356\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=32\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 979, np.int64(1): 935, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 979, np.int64(2): 979, np.int64(0): 979})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1014 acc 0.349 f1 0.268 || val_loss 1.1102 acc 0.243 f1 0.200\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0946 acc 0.387 f1 0.335 || val_loss 1.1033 acc 0.294 f1 0.257\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0875 acc 0.403 f1 0.360 || val_loss 1.1035 acc 0.319 f1 0.305\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0731 acc 0.434 f1 0.423 || val_loss 1.1035 acc 0.337 f1 0.326\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0418 acc 0.474 f1 0.462 || val_loss 1.0904 acc 0.348 f1 0.325\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9690 acc 0.524 f1 0.513 || val_loss 1.1063 acc 0.381 f1 0.339\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8683 acc 0.581 f1 0.569 || val_loss 1.1069 acc 0.370 f1 0.303\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8030 acc 0.596 f1 0.585 || val_loss 1.1948 acc 0.354 f1 0.302\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7533 acc 0.627 f1 0.621 || val_loss 1.2032 acc 0.348 f1 0.291\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7173 acc 0.645 f1 0.640 || val_loss 1.2010 acc 0.366 f1 0.306\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6770 acc 0.671 f1 0.666 || val_loss 1.2385 acc 0.385 f1 0.313\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6506 acc 0.683 f1 0.680 || val_loss 1.2637 acc 0.377 f1 0.302\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6163 acc 0.696 f1 0.693 || val_loss 1.3278 acc 0.358 f1 0.289\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5909 acc 0.705 f1 0.701 || val_loss 1.3523 acc 0.370 f1 0.310\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  32%|      | 32/100 [15:54<34:37, 30.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=33 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=33\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 928, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1545 acc 0.387 f1 0.386 || val_loss 1.1481 acc 0.307 f1 0.291\n",
            "[W3] ANN Epoch 02 | train_loss 0.9945 acc 0.494 f1 0.487 || val_loss 1.1391 acc 0.321 f1 0.301\n",
            "[W3] ANN Epoch 03 | train_loss 0.8936 acc 0.562 f1 0.553 || val_loss 1.1377 acc 0.340 f1 0.309\n",
            "[W3] ANN Epoch 04 | train_loss 0.7921 acc 0.613 f1 0.605 || val_loss 1.1354 acc 0.393 f1 0.341\n",
            "[W3] ANN Epoch 05 | train_loss 0.7403 acc 0.650 f1 0.644 || val_loss 1.1682 acc 0.358 f1 0.304\n",
            "[W3] ANN Epoch 06 | train_loss 0.6927 acc 0.661 f1 0.657 || val_loss 1.1730 acc 0.374 f1 0.319\n",
            "[W3] ANN Epoch 07 | train_loss 0.6496 acc 0.694 f1 0.691 || val_loss 1.2184 acc 0.395 f1 0.337\n",
            "[W3] ANN Epoch 08 | train_loss 0.6207 acc 0.704 f1 0.702 || val_loss 1.1934 acc 0.409 f1 0.334\n",
            "[W3] ANN Epoch 09 | train_loss 0.6062 acc 0.712 f1 0.710 || val_loss 1.2107 acc 0.407 f1 0.346\n",
            "[W3] ANN Epoch 10 | train_loss 0.5777 acc 0.732 f1 0.730 || val_loss 1.2284 acc 0.430 f1 0.347\n",
            "[W3] ANN Epoch 11 | train_loss 0.5652 acc 0.741 f1 0.740 || val_loss 1.2637 acc 0.403 f1 0.335\n",
            "[W3] ANN Epoch 12 | train_loss 0.5386 acc 0.760 f1 0.759 || val_loss 1.2488 acc 0.393 f1 0.322\n",
            "[W3] ANN Epoch 13 | train_loss 0.4871 acc 0.776 f1 0.776 || val_loss 1.2813 acc 0.395 f1 0.323\n",
            "[W3] ANN Epoch 14 | train_loss 0.4955 acc 0.783 f1 0.782 || val_loss 1.3130 acc 0.420 f1 0.338\n",
            "[W3] ANN Epoch 15 | train_loss 0.4887 acc 0.783 f1 0.782 || val_loss 1.3419 acc 0.393 f1 0.320\n",
            "[W3] ANN Epoch 16 | train_loss 0.4683 acc 0.789 f1 0.788 || val_loss 1.3480 acc 0.414 f1 0.336\n",
            "[W3] ANN Epoch 17 | train_loss 0.4410 acc 0.806 f1 0.806 || val_loss 1.3718 acc 0.416 f1 0.348\n",
            "[W3] ANN Epoch 18 | train_loss 0.4319 acc 0.813 f1 0.813 || val_loss 1.4193 acc 0.405 f1 0.327\n",
            "[W3] ANN Epoch 19 | train_loss 0.4119 acc 0.831 f1 0.830 || val_loss 1.4537 acc 0.407 f1 0.342\n",
            "[W3] ANN Epoch 20 | train_loss 0.4147 acc 0.823 f1 0.823 || val_loss 1.4254 acc 0.422 f1 0.343\n",
            "[W3] ANN Epoch 21 | train_loss 0.3802 acc 0.846 f1 0.846 || val_loss 1.4777 acc 0.397 f1 0.330\n",
            "[W3] ANN Epoch 22 | train_loss 0.3915 acc 0.829 f1 0.829 || val_loss 1.5336 acc 0.405 f1 0.340\n",
            "[W3] ANN Epoch 23 | train_loss 0.3882 acc 0.835 f1 0.835 || val_loss 1.4639 acc 0.428 f1 0.348\n",
            "[W3] ANN Epoch 24 | train_loss 0.3880 acc 0.835 f1 0.835 || val_loss 1.5088 acc 0.430 f1 0.348\n",
            "[W3] ANN Epoch 25 | train_loss 0.3585 acc 0.850 f1 0.850 || val_loss 1.5238 acc 0.422 f1 0.336\n",
            "[W3] ANN Epoch 26 | train_loss 0.3426 acc 0.863 f1 0.863 || val_loss 1.5239 acc 0.424 f1 0.333\n",
            "[W3] ANN Epoch 27 | train_loss 0.3311 acc 0.866 f1 0.866 || val_loss 1.5447 acc 0.434 f1 0.348\n",
            "[W3] ANN Epoch 28 | train_loss 0.3059 acc 0.872 f1 0.872 || val_loss 1.5674 acc 0.438 f1 0.361\n",
            "[W3] ANN Epoch 29 | train_loss 0.3355 acc 0.858 f1 0.858 || val_loss 1.5264 acc 0.434 f1 0.342\n",
            "[W3] ANN Epoch 30 | train_loss 0.3275 acc 0.868 f1 0.868 || val_loss 1.5736 acc 0.420 f1 0.342\n",
            "[W3] ANN Epoch 31 | train_loss 0.3250 acc 0.867 f1 0.867 || val_loss 1.6226 acc 0.418 f1 0.330\n",
            "[W3] ANN Epoch 32 | train_loss 0.3012 acc 0.875 f1 0.875 || val_loss 1.6593 acc 0.455 f1 0.364\n",
            "[W3] ANN Epoch 33 | train_loss 0.2833 acc 0.888 f1 0.888 || val_loss 1.6681 acc 0.416 f1 0.329\n",
            "[W3] ANN Epoch 34 | train_loss 0.3096 acc 0.873 f1 0.873 || val_loss 1.7415 acc 0.428 f1 0.341\n",
            "[W3] ANN Epoch 35 | train_loss 0.3078 acc 0.883 f1 0.884 || val_loss 1.7098 acc 0.414 f1 0.327\n",
            "[W3] ANN Epoch 36 | train_loss 0.2876 acc 0.881 f1 0.881 || val_loss 1.7449 acc 0.416 f1 0.335\n",
            "[W3] ANN Epoch 37 | train_loss 0.2515 acc 0.906 f1 0.906 || val_loss 1.7842 acc 0.438 f1 0.356\n",
            "[W3] ANN Epoch 38 | train_loss 0.2348 acc 0.911 f1 0.911 || val_loss 1.8611 acc 0.416 f1 0.327\n",
            "[W3] ANN Epoch 39 | train_loss 0.2794 acc 0.882 f1 0.882 || val_loss 1.8333 acc 0.409 f1 0.324\n",
            "[W3] ANN Epoch 40 | train_loss 0.2816 acc 0.883 f1 0.884 || val_loss 1.8162 acc 0.416 f1 0.349\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=33\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 928, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0667 acc 0.417 f1 0.418 || val_loss 1.0395 acc 0.401 f1 0.306\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9245 acc 0.568 f1 0.559 || val_loss 1.0658 acc 0.395 f1 0.302\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8109 acc 0.599 f1 0.593 || val_loss 1.1389 acc 0.381 f1 0.310\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7102 acc 0.658 f1 0.655 || val_loss 1.1926 acc 0.405 f1 0.331\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6406 acc 0.697 f1 0.695 || val_loss 1.2127 acc 0.391 f1 0.323\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5705 acc 0.729 f1 0.727 || val_loss 1.2631 acc 0.399 f1 0.322\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5331 acc 0.768 f1 0.767 || val_loss 1.3274 acc 0.395 f1 0.331\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4677 acc 0.791 f1 0.790 || val_loss 1.3916 acc 0.397 f1 0.321\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4375 acc 0.809 f1 0.809 || val_loss 1.4674 acc 0.403 f1 0.338\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4286 acc 0.820 f1 0.819 || val_loss 1.5043 acc 0.395 f1 0.306\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3610 acc 0.852 f1 0.852 || val_loss 1.6186 acc 0.374 f1 0.299\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3527 acc 0.857 f1 0.856 || val_loss 1.7041 acc 0.370 f1 0.313\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2964 acc 0.882 f1 0.882 || val_loss 1.7483 acc 0.409 f1 0.330\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2651 acc 0.898 f1 0.898 || val_loss 1.8110 acc 0.399 f1 0.326\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2736 acc 0.893 f1 0.893 || val_loss 1.8176 acc 0.416 f1 0.339\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2344 acc 0.909 f1 0.909 || val_loss 1.9184 acc 0.412 f1 0.328\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.2140 acc 0.918 f1 0.918 || val_loss 1.9856 acc 0.407 f1 0.333\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1786 acc 0.931 f1 0.931 || val_loss 2.0475 acc 0.409 f1 0.339\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1736 acc 0.943 f1 0.943 || val_loss 2.1129 acc 0.407 f1 0.327\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1267 acc 0.956 f1 0.956 || val_loss 2.2164 acc 0.414 f1 0.328\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.1017 acc 0.966 f1 0.966 || val_loss 2.3072 acc 0.414 f1 0.333\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.0952 acc 0.970 f1 0.970 || val_loss 2.4134 acc 0.432 f1 0.352\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.0906 acc 0.970 f1 0.970 || val_loss 2.5188 acc 0.412 f1 0.331\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.0808 acc 0.975 f1 0.975 || val_loss 2.4748 acc 0.420 f1 0.343\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.0894 acc 0.970 f1 0.970 || val_loss 2.5980 acc 0.407 f1 0.333\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.0729 acc 0.976 f1 0.976 || val_loss 2.6581 acc 0.407 f1 0.323\n",
            "[W3] CNN1D Epoch 27 | train_loss 0.0653 acc 0.981 f1 0.981 || val_loss 2.7291 acc 0.412 f1 0.332\n",
            "[W3] CNN1D Epoch 28 | train_loss 0.0515 acc 0.987 f1 0.987 || val_loss 2.8794 acc 0.426 f1 0.337\n",
            "[W3] CNN1D Epoch 29 | train_loss 0.0551 acc 0.983 f1 0.983 || val_loss 3.0154 acc 0.409 f1 0.342\n",
            "[W3] CNN1D Epoch 30 | train_loss 0.1027 acc 0.960 f1 0.960 || val_loss 2.8421 acc 0.428 f1 0.342\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=33\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 928, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1028 acc 0.324 f1 0.320 || val_loss 1.0909 acc 0.401 f1 0.353\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0877 acc 0.401 f1 0.400 || val_loss 1.0914 acc 0.366 f1 0.332\n",
            "[W3] RNN Epoch 03 | train_loss 1.0718 acc 0.442 f1 0.438 || val_loss 1.0962 acc 0.337 f1 0.320\n",
            "[W3] RNN Epoch 04 | train_loss 1.0562 acc 0.455 f1 0.450 || val_loss 1.0838 acc 0.352 f1 0.322\n",
            "[W3] RNN Epoch 05 | train_loss 1.0351 acc 0.476 f1 0.470 || val_loss 1.0847 acc 0.354 f1 0.324\n",
            "[W3] RNN Epoch 06 | train_loss 1.0132 acc 0.497 f1 0.494 || val_loss 1.0774 acc 0.346 f1 0.315\n",
            "[W3] RNN Epoch 07 | train_loss 0.9915 acc 0.524 f1 0.518 || val_loss 1.0722 acc 0.354 f1 0.318\n",
            "[W3] RNN Epoch 08 | train_loss 0.9632 acc 0.554 f1 0.549 || val_loss 1.0702 acc 0.383 f1 0.347\n",
            "[W3] RNN Epoch 09 | train_loss 0.9329 acc 0.546 f1 0.538 || val_loss 1.0769 acc 0.383 f1 0.345\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=33\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 928, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0994 acc 0.332 f1 0.284 || val_loss 1.0951 acc 0.379 f1 0.333\n",
            "[W3] GRU Epoch 02 | train_loss 1.0930 acc 0.381 f1 0.374 || val_loss 1.0993 acc 0.344 f1 0.316\n",
            "[W3] GRU Epoch 03 | train_loss 1.0864 acc 0.408 f1 0.400 || val_loss 1.1014 acc 0.331 f1 0.297\n",
            "[W3] GRU Epoch 04 | train_loss 1.0768 acc 0.432 f1 0.422 || val_loss 1.1011 acc 0.335 f1 0.303\n",
            "[W3] GRU Epoch 05 | train_loss 1.0572 acc 0.471 f1 0.468 || val_loss 1.1203 acc 0.294 f1 0.281\n",
            "[W3] GRU Epoch 06 | train_loss 1.0228 acc 0.500 f1 0.496 || val_loss 1.1159 acc 0.313 f1 0.296\n",
            "[W3] GRU Epoch 07 | train_loss 0.9572 acc 0.539 f1 0.531 || val_loss 1.0860 acc 0.360 f1 0.306\n",
            "[W3] GRU Epoch 08 | train_loss 0.8804 acc 0.581 f1 0.573 || val_loss 1.0960 acc 0.389 f1 0.328\n",
            "[W3] GRU Epoch 09 | train_loss 0.8172 acc 0.600 f1 0.594 || val_loss 1.1120 acc 0.385 f1 0.319\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=33\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 928, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1008 acc 0.334 f1 0.171 || val_loss 1.0876 acc 0.457 f1 0.229\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0943 acc 0.373 f1 0.368 || val_loss 1.0966 acc 0.342 f1 0.327\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0861 acc 0.419 f1 0.417 || val_loss 1.0897 acc 0.331 f1 0.312\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0675 acc 0.440 f1 0.432 || val_loss 1.0920 acc 0.331 f1 0.313\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0169 acc 0.503 f1 0.495 || val_loss 1.0920 acc 0.346 f1 0.307\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9221 acc 0.551 f1 0.541 || val_loss 1.1098 acc 0.374 f1 0.342\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8525 acc 0.577 f1 0.569 || val_loss 1.0766 acc 0.399 f1 0.324\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7939 acc 0.610 f1 0.601 || val_loss 1.1642 acc 0.387 f1 0.342\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7604 acc 0.636 f1 0.630 || val_loss 1.1178 acc 0.420 f1 0.343\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7285 acc 0.638 f1 0.633 || val_loss 1.1297 acc 0.414 f1 0.325\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7060 acc 0.653 f1 0.651 || val_loss 1.1584 acc 0.407 f1 0.335\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6651 acc 0.677 f1 0.673 || val_loss 1.2023 acc 0.409 f1 0.327\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6397 acc 0.691 f1 0.688 || val_loss 1.2195 acc 0.397 f1 0.318\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6195 acc 0.690 f1 0.687 || val_loss 1.2604 acc 0.409 f1 0.340\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5917 acc 0.711 f1 0.708 || val_loss 1.3008 acc 0.405 f1 0.326\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5675 acc 0.724 f1 0.721 || val_loss 1.3387 acc 0.399 f1 0.321\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5443 acc 0.730 f1 0.728 || val_loss 1.3634 acc 0.403 f1 0.325\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  33%|      | 33/100 [16:28<35:16, 31.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=34 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=34\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 928, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1240 acc 0.400 f1 0.398 || val_loss 1.1291 acc 0.327 f1 0.308\n",
            "[W3] ANN Epoch 02 | train_loss 0.9596 acc 0.511 f1 0.501 || val_loss 1.1009 acc 0.364 f1 0.333\n",
            "[W3] ANN Epoch 03 | train_loss 0.8613 acc 0.569 f1 0.561 || val_loss 1.0907 acc 0.383 f1 0.342\n",
            "[W3] ANN Epoch 04 | train_loss 0.7823 acc 0.616 f1 0.608 || val_loss 1.0851 acc 0.414 f1 0.353\n",
            "[W3] ANN Epoch 05 | train_loss 0.7320 acc 0.646 f1 0.642 || val_loss 1.0797 acc 0.424 f1 0.358\n",
            "[W3] ANN Epoch 06 | train_loss 0.6931 acc 0.662 f1 0.658 || val_loss 1.1179 acc 0.418 f1 0.348\n",
            "[W3] ANN Epoch 07 | train_loss 0.6563 acc 0.685 f1 0.683 || val_loss 1.1249 acc 0.434 f1 0.354\n",
            "[W3] ANN Epoch 08 | train_loss 0.6284 acc 0.699 f1 0.695 || val_loss 1.1372 acc 0.428 f1 0.361\n",
            "[W3] ANN Epoch 09 | train_loss 0.5956 acc 0.720 f1 0.718 || val_loss 1.1346 acc 0.432 f1 0.360\n",
            "[W3] ANN Epoch 10 | train_loss 0.5908 acc 0.720 f1 0.717 || val_loss 1.1564 acc 0.442 f1 0.378\n",
            "[W3] ANN Epoch 11 | train_loss 0.5495 acc 0.739 f1 0.739 || val_loss 1.1556 acc 0.436 f1 0.355\n",
            "[W3] ANN Epoch 12 | train_loss 0.5263 acc 0.746 f1 0.745 || val_loss 1.2327 acc 0.416 f1 0.334\n",
            "[W3] ANN Epoch 13 | train_loss 0.5050 acc 0.766 f1 0.765 || val_loss 1.2306 acc 0.457 f1 0.359\n",
            "[W3] ANN Epoch 14 | train_loss 0.5015 acc 0.769 f1 0.768 || val_loss 1.2410 acc 0.455 f1 0.357\n",
            "[W3] ANN Epoch 15 | train_loss 0.4749 acc 0.781 f1 0.781 || val_loss 1.3148 acc 0.436 f1 0.359\n",
            "[W3] ANN Epoch 16 | train_loss 0.4666 acc 0.792 f1 0.791 || val_loss 1.3088 acc 0.424 f1 0.341\n",
            "[W3] ANN Epoch 17 | train_loss 0.4545 acc 0.793 f1 0.793 || val_loss 1.3290 acc 0.436 f1 0.339\n",
            "[W3] ANN Epoch 18 | train_loss 0.4577 acc 0.796 f1 0.796 || val_loss 1.3411 acc 0.449 f1 0.347\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=34\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 928, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0774 acc 0.400 f1 0.402 || val_loss 1.0412 acc 0.438 f1 0.338\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9551 acc 0.542 f1 0.540 || val_loss 1.0509 acc 0.405 f1 0.320\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8281 acc 0.606 f1 0.601 || val_loss 1.1324 acc 0.360 f1 0.308\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7367 acc 0.660 f1 0.657 || val_loss 1.1698 acc 0.374 f1 0.328\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6635 acc 0.697 f1 0.697 || val_loss 1.1955 acc 0.372 f1 0.311\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5993 acc 0.731 f1 0.729 || val_loss 1.2397 acc 0.397 f1 0.336\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5345 acc 0.767 f1 0.767 || val_loss 1.3265 acc 0.389 f1 0.319\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5066 acc 0.787 f1 0.786 || val_loss 1.3289 acc 0.379 f1 0.305\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4437 acc 0.808 f1 0.807 || val_loss 1.4135 acc 0.399 f1 0.331\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=34\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 928, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0974 acc 0.368 f1 0.339 || val_loss 1.0889 acc 0.358 f1 0.318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0817 acc 0.418 f1 0.414 || val_loss 1.0985 acc 0.323 f1 0.313\n",
            "[W3] RNN Epoch 03 | train_loss 1.0663 acc 0.449 f1 0.446 || val_loss 1.0910 acc 0.344 f1 0.324\n",
            "[W3] RNN Epoch 04 | train_loss 1.0484 acc 0.461 f1 0.456 || val_loss 1.0925 acc 0.325 f1 0.309\n",
            "[W3] RNN Epoch 05 | train_loss 1.0276 acc 0.477 f1 0.474 || val_loss 1.0882 acc 0.346 f1 0.312\n",
            "[W3] RNN Epoch 06 | train_loss 1.0088 acc 0.502 f1 0.498 || val_loss 1.1051 acc 0.331 f1 0.310\n",
            "[W3] RNN Epoch 07 | train_loss 0.9823 acc 0.522 f1 0.515 || val_loss 1.0858 acc 0.350 f1 0.313\n",
            "[W3] RNN Epoch 08 | train_loss 0.9581 acc 0.544 f1 0.535 || val_loss 1.0971 acc 0.333 f1 0.304\n",
            "[W3] RNN Epoch 09 | train_loss 0.9325 acc 0.563 f1 0.553 || val_loss 1.0855 acc 0.370 f1 0.335\n",
            "[W3] RNN Epoch 10 | train_loss 0.9071 acc 0.581 f1 0.572 || val_loss 1.0886 acc 0.383 f1 0.337\n",
            "[W3] RNN Epoch 11 | train_loss 0.8879 acc 0.581 f1 0.573 || val_loss 1.0954 acc 0.364 f1 0.318\n",
            "[W3] RNN Epoch 12 | train_loss 0.8564 acc 0.594 f1 0.584 || val_loss 1.0877 acc 0.391 f1 0.337\n",
            "[W3] RNN Epoch 13 | train_loss 0.8326 acc 0.621 f1 0.614 || val_loss 1.0993 acc 0.374 f1 0.323\n",
            "[W3] RNN Epoch 14 | train_loss 0.8136 acc 0.621 f1 0.612 || val_loss 1.1381 acc 0.372 f1 0.335\n",
            "[W3] RNN Epoch 15 | train_loss 0.7836 acc 0.627 f1 0.619 || val_loss 1.1174 acc 0.391 f1 0.347\n",
            "[W3] RNN Epoch 16 | train_loss 0.7677 acc 0.646 f1 0.639 || val_loss 1.1299 acc 0.374 f1 0.320\n",
            "[W3] RNN Epoch 17 | train_loss 0.7373 acc 0.659 f1 0.652 || val_loss 1.1410 acc 0.393 f1 0.338\n",
            "[W3] RNN Epoch 18 | train_loss 0.7196 acc 0.670 f1 0.664 || val_loss 1.1584 acc 0.395 f1 0.343\n",
            "[W3] RNN Epoch 19 | train_loss 0.6906 acc 0.680 f1 0.673 || val_loss 1.1714 acc 0.370 f1 0.308\n",
            "[W3] RNN Epoch 20 | train_loss 0.6862 acc 0.682 f1 0.676 || val_loss 1.1806 acc 0.395 f1 0.336\n",
            "[W3] RNN Epoch 21 | train_loss 0.6573 acc 0.686 f1 0.680 || val_loss 1.1907 acc 0.389 f1 0.313\n",
            "[W3] RNN Epoch 22 | train_loss 0.6374 acc 0.704 f1 0.699 || val_loss 1.2251 acc 0.387 f1 0.333\n",
            "[W3] RNN Epoch 23 | train_loss 0.6305 acc 0.703 f1 0.697 || val_loss 1.2249 acc 0.387 f1 0.311\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=34\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 928, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0987 acc 0.351 f1 0.263 || val_loss 1.1040 acc 0.255 f1 0.235\n",
            "[W3] GRU Epoch 02 | train_loss 1.0891 acc 0.404 f1 0.392 || val_loss 1.0918 acc 0.333 f1 0.307\n",
            "[W3] GRU Epoch 03 | train_loss 1.0777 acc 0.443 f1 0.440 || val_loss 1.0835 acc 0.348 f1 0.313\n",
            "[W3] GRU Epoch 04 | train_loss 1.0625 acc 0.457 f1 0.450 || val_loss 1.0883 acc 0.335 f1 0.310\n",
            "[W3] GRU Epoch 05 | train_loss 1.0356 acc 0.484 f1 0.480 || val_loss 1.0756 acc 0.346 f1 0.303\n",
            "[W3] GRU Epoch 06 | train_loss 0.9939 acc 0.517 f1 0.509 || val_loss 1.0648 acc 0.372 f1 0.322\n",
            "[W3] GRU Epoch 07 | train_loss 0.9282 acc 0.553 f1 0.544 || val_loss 1.0850 acc 0.366 f1 0.317\n",
            "[W3] GRU Epoch 08 | train_loss 0.8549 acc 0.588 f1 0.580 || val_loss 1.0790 acc 0.389 f1 0.327\n",
            "[W3] GRU Epoch 09 | train_loss 0.8018 acc 0.620 f1 0.616 || val_loss 1.1102 acc 0.385 f1 0.315\n",
            "[W3] GRU Epoch 10 | train_loss 0.7623 acc 0.633 f1 0.628 || val_loss 1.1203 acc 0.401 f1 0.344\n",
            "[W3] GRU Epoch 11 | train_loss 0.7177 acc 0.654 f1 0.650 || val_loss 1.1568 acc 0.381 f1 0.319\n",
            "[W3] GRU Epoch 12 | train_loss 0.6855 acc 0.670 f1 0.666 || val_loss 1.1502 acc 0.403 f1 0.339\n",
            "[W3] GRU Epoch 13 | train_loss 0.6518 acc 0.677 f1 0.675 || val_loss 1.1883 acc 0.405 f1 0.360\n",
            "[W3] GRU Epoch 14 | train_loss 0.6235 acc 0.701 f1 0.697 || val_loss 1.2219 acc 0.399 f1 0.331\n",
            "[W3] GRU Epoch 15 | train_loss 0.5994 acc 0.712 f1 0.709 || val_loss 1.2403 acc 0.405 f1 0.321\n",
            "[W3] GRU Epoch 16 | train_loss 0.5775 acc 0.727 f1 0.724 || val_loss 1.2621 acc 0.407 f1 0.328\n",
            "[W3] GRU Epoch 17 | train_loss 0.5503 acc 0.744 f1 0.742 || val_loss 1.3069 acc 0.391 f1 0.325\n",
            "[W3] GRU Epoch 18 | train_loss 0.5209 acc 0.753 f1 0.751 || val_loss 1.3549 acc 0.385 f1 0.311\n",
            "[W3] GRU Epoch 19 | train_loss 0.5031 acc 0.760 f1 0.760 || val_loss 1.3800 acc 0.405 f1 0.336\n",
            "[W3] GRU Epoch 20 | train_loss 0.4903 acc 0.759 f1 0.757 || val_loss 1.4215 acc 0.405 f1 0.336\n",
            "[W3] GRU Epoch 21 | train_loss 0.4592 acc 0.785 f1 0.784 || val_loss 1.4674 acc 0.416 f1 0.339\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=34\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 928, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1003 acc 0.336 f1 0.217 || val_loss 1.0981 acc 0.385 f1 0.300\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0956 acc 0.371 f1 0.362 || val_loss 1.0992 acc 0.391 f1 0.350\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0920 acc 0.377 f1 0.363 || val_loss 1.0931 acc 0.412 f1 0.365\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0845 acc 0.408 f1 0.404 || val_loss 1.0859 acc 0.424 f1 0.377\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0698 acc 0.425 f1 0.423 || val_loss 1.0862 acc 0.381 f1 0.334\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0448 acc 0.472 f1 0.470 || val_loss 1.0819 acc 0.399 f1 0.356\n",
            "[W3] LSTM Epoch 07 | train_loss 1.0021 acc 0.521 f1 0.520 || val_loss 1.0703 acc 0.389 f1 0.330\n",
            "[W3] LSTM Epoch 08 | train_loss 0.9255 acc 0.561 f1 0.556 || val_loss 1.1112 acc 0.350 f1 0.291\n",
            "[W3] LSTM Epoch 09 | train_loss 0.8486 acc 0.600 f1 0.594 || val_loss 1.1405 acc 0.372 f1 0.316\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7878 acc 0.638 f1 0.632 || val_loss 1.1641 acc 0.374 f1 0.310\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7398 acc 0.661 f1 0.657 || val_loss 1.2052 acc 0.370 f1 0.320\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6886 acc 0.680 f1 0.675 || val_loss 1.2029 acc 0.418 f1 0.330\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  34%|      | 34/100 [16:53<32:27, 29.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=35 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=35\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 979, np.int64(1): 933, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 979, np.int64(2): 979, np.int64(0): 979})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1402 acc 0.398 f1 0.392 || val_loss 1.1281 acc 0.340 f1 0.317\n",
            "[W3] ANN Epoch 02 | train_loss 0.9611 acc 0.505 f1 0.494 || val_loss 1.1098 acc 0.393 f1 0.363\n",
            "[W3] ANN Epoch 03 | train_loss 0.8670 acc 0.581 f1 0.573 || val_loss 1.1183 acc 0.379 f1 0.328\n",
            "[W3] ANN Epoch 04 | train_loss 0.7648 acc 0.634 f1 0.629 || val_loss 1.1158 acc 0.403 f1 0.339\n",
            "[W3] ANN Epoch 05 | train_loss 0.7278 acc 0.652 f1 0.649 || val_loss 1.1107 acc 0.422 f1 0.348\n",
            "[W3] ANN Epoch 06 | train_loss 0.6557 acc 0.686 f1 0.683 || val_loss 1.1469 acc 0.426 f1 0.346\n",
            "[W3] ANN Epoch 07 | train_loss 0.6135 acc 0.708 f1 0.705 || val_loss 1.1490 acc 0.434 f1 0.361\n",
            "[W3] ANN Epoch 08 | train_loss 0.5839 acc 0.733 f1 0.731 || val_loss 1.1794 acc 0.405 f1 0.349\n",
            "[W3] ANN Epoch 09 | train_loss 0.5689 acc 0.737 f1 0.736 || val_loss 1.1839 acc 0.447 f1 0.366\n",
            "[W3] ANN Epoch 10 | train_loss 0.5232 acc 0.757 f1 0.756 || val_loss 1.2190 acc 0.430 f1 0.343\n",
            "[W3] ANN Epoch 11 | train_loss 0.5190 acc 0.759 f1 0.758 || val_loss 1.2623 acc 0.424 f1 0.334\n",
            "[W3] ANN Epoch 12 | train_loss 0.4727 acc 0.793 f1 0.793 || val_loss 1.2880 acc 0.416 f1 0.350\n",
            "[W3] ANN Epoch 13 | train_loss 0.4809 acc 0.792 f1 0.792 || val_loss 1.2785 acc 0.432 f1 0.352\n",
            "[W3] ANN Epoch 14 | train_loss 0.4537 acc 0.800 f1 0.800 || val_loss 1.3086 acc 0.409 f1 0.336\n",
            "[W3] ANN Epoch 15 | train_loss 0.4348 acc 0.807 f1 0.807 || val_loss 1.3363 acc 0.438 f1 0.357\n",
            "[W3] ANN Epoch 16 | train_loss 0.4138 acc 0.815 f1 0.814 || val_loss 1.3676 acc 0.434 f1 0.364\n",
            "[W3] ANN Epoch 17 | train_loss 0.3909 acc 0.830 f1 0.830 || val_loss 1.3892 acc 0.442 f1 0.375\n",
            "[W3] ANN Epoch 18 | train_loss 0.3708 acc 0.851 f1 0.851 || val_loss 1.3955 acc 0.444 f1 0.373\n",
            "[W3] ANN Epoch 19 | train_loss 0.3820 acc 0.838 f1 0.838 || val_loss 1.3817 acc 0.444 f1 0.368\n",
            "[W3] ANN Epoch 20 | train_loss 0.3747 acc 0.842 f1 0.841 || val_loss 1.4347 acc 0.440 f1 0.347\n",
            "[W3] ANN Epoch 21 | train_loss 0.3689 acc 0.849 f1 0.849 || val_loss 1.4495 acc 0.436 f1 0.356\n",
            "[W3] ANN Epoch 22 | train_loss 0.3290 acc 0.868 f1 0.867 || val_loss 1.4972 acc 0.426 f1 0.355\n",
            "[W3] ANN Epoch 23 | train_loss 0.3317 acc 0.856 f1 0.856 || val_loss 1.4934 acc 0.440 f1 0.359\n",
            "[W3] ANN Epoch 24 | train_loss 0.3056 acc 0.876 f1 0.876 || val_loss 1.5557 acc 0.444 f1 0.367\n",
            "[W3] ANN Epoch 25 | train_loss 0.3004 acc 0.877 f1 0.877 || val_loss 1.5669 acc 0.422 f1 0.339\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=35\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 979, np.int64(1): 933, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 979, np.int64(2): 979, np.int64(0): 979})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0808 acc 0.395 f1 0.396 || val_loss 1.0325 acc 0.385 f1 0.279\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9468 acc 0.546 f1 0.539 || val_loss 1.0335 acc 0.391 f1 0.286\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8142 acc 0.613 f1 0.610 || val_loss 1.1187 acc 0.368 f1 0.319\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7072 acc 0.670 f1 0.666 || val_loss 1.1362 acc 0.385 f1 0.316\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6297 acc 0.706 f1 0.705 || val_loss 1.1957 acc 0.393 f1 0.332\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5572 acc 0.744 f1 0.743 || val_loss 1.2883 acc 0.393 f1 0.336\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5002 acc 0.773 f1 0.771 || val_loss 1.2989 acc 0.409 f1 0.337\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4520 acc 0.801 f1 0.800 || val_loss 1.4272 acc 0.391 f1 0.321\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4167 acc 0.818 f1 0.817 || val_loss 1.5118 acc 0.409 f1 0.328\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3533 acc 0.850 f1 0.850 || val_loss 1.5845 acc 0.391 f1 0.327\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3153 acc 0.875 f1 0.875 || val_loss 1.7269 acc 0.389 f1 0.297\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2908 acc 0.881 f1 0.881 || val_loss 1.7837 acc 0.395 f1 0.304\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2691 acc 0.891 f1 0.891 || val_loss 1.8905 acc 0.397 f1 0.312\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2337 acc 0.918 f1 0.918 || val_loss 1.9935 acc 0.374 f1 0.294\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1965 acc 0.927 f1 0.927 || val_loss 2.0850 acc 0.370 f1 0.291\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=35\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 979, np.int64(1): 933, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 979, np.int64(2): 979, np.int64(0): 979})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0970 acc 0.346 f1 0.341 || val_loss 1.0954 acc 0.323 f1 0.295\n",
            "[W3] RNN Epoch 02 | train_loss 1.0812 acc 0.406 f1 0.403 || val_loss 1.1010 acc 0.315 f1 0.289\n",
            "[W3] RNN Epoch 03 | train_loss 1.0673 acc 0.426 f1 0.416 || val_loss 1.1013 acc 0.311 f1 0.295\n",
            "[W3] RNN Epoch 04 | train_loss 1.0482 acc 0.447 f1 0.440 || val_loss 1.0947 acc 0.327 f1 0.308\n",
            "[W3] RNN Epoch 05 | train_loss 1.0284 acc 0.464 f1 0.453 || val_loss 1.0883 acc 0.327 f1 0.301\n",
            "[W3] RNN Epoch 06 | train_loss 1.0074 acc 0.491 f1 0.482 || val_loss 1.1184 acc 0.325 f1 0.309\n",
            "[W3] RNN Epoch 07 | train_loss 0.9867 acc 0.525 f1 0.517 || val_loss 1.1155 acc 0.333 f1 0.310\n",
            "[W3] RNN Epoch 08 | train_loss 0.9718 acc 0.520 f1 0.510 || val_loss 1.0967 acc 0.331 f1 0.304\n",
            "[W3] RNN Epoch 09 | train_loss 0.9442 acc 0.543 f1 0.534 || val_loss 1.1161 acc 0.352 f1 0.318\n",
            "[W3] RNN Epoch 10 | train_loss 0.9166 acc 0.562 f1 0.550 || val_loss 1.1144 acc 0.362 f1 0.323\n",
            "[W3] RNN Epoch 11 | train_loss 0.9039 acc 0.568 f1 0.560 || val_loss 1.1237 acc 0.362 f1 0.331\n",
            "[W3] RNN Epoch 12 | train_loss 0.8764 acc 0.579 f1 0.571 || val_loss 1.1216 acc 0.337 f1 0.304\n",
            "[W3] RNN Epoch 13 | train_loss 0.8580 acc 0.597 f1 0.587 || val_loss 1.1329 acc 0.377 f1 0.336\n",
            "[W3] RNN Epoch 14 | train_loss 0.8329 acc 0.600 f1 0.592 || val_loss 1.1332 acc 0.333 f1 0.296\n",
            "[W3] RNN Epoch 15 | train_loss 0.8116 acc 0.618 f1 0.610 || val_loss 1.1122 acc 0.368 f1 0.321\n",
            "[W3] RNN Epoch 16 | train_loss 0.7941 acc 0.629 f1 0.621 || val_loss 1.1336 acc 0.374 f1 0.331\n",
            "[W3] RNN Epoch 17 | train_loss 0.7607 acc 0.649 f1 0.642 || val_loss 1.1325 acc 0.377 f1 0.319\n",
            "[W3] RNN Epoch 18 | train_loss 0.7413 acc 0.647 f1 0.640 || val_loss 1.1475 acc 0.372 f1 0.322\n",
            "[W3] RNN Epoch 19 | train_loss 0.7282 acc 0.657 f1 0.650 || val_loss 1.1443 acc 0.391 f1 0.339\n",
            "[W3] RNN Epoch 20 | train_loss 0.6987 acc 0.676 f1 0.669 || val_loss 1.1606 acc 0.381 f1 0.337\n",
            "[W3] RNN Epoch 21 | train_loss 0.6669 acc 0.697 f1 0.691 || val_loss 1.1630 acc 0.389 f1 0.334\n",
            "[W3] RNN Epoch 22 | train_loss 0.6610 acc 0.691 f1 0.685 || val_loss 1.1809 acc 0.393 f1 0.329\n",
            "[W3] RNN Epoch 23 | train_loss 0.6236 acc 0.702 f1 0.696 || val_loss 1.1926 acc 0.366 f1 0.307\n",
            "[W3] RNN Epoch 24 | train_loss 0.6095 acc 0.718 f1 0.714 || val_loss 1.2160 acc 0.391 f1 0.321\n",
            "[W3] RNN Epoch 25 | train_loss 0.5904 acc 0.721 f1 0.716 || val_loss 1.2276 acc 0.393 f1 0.325\n",
            "[W3] RNN Epoch 26 | train_loss 0.5742 acc 0.735 f1 0.731 || val_loss 1.2406 acc 0.393 f1 0.327\n",
            "[W3] RNN Epoch 27 | train_loss 0.5532 acc 0.744 f1 0.740 || val_loss 1.2710 acc 0.397 f1 0.322\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=35\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 979, np.int64(1): 933, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 979, np.int64(2): 979, np.int64(0): 979})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0983 acc 0.334 f1 0.327 || val_loss 1.0910 acc 0.426 f1 0.369\n",
            "[W3] GRU Epoch 02 | train_loss 1.0920 acc 0.384 f1 0.377 || val_loss 1.0904 acc 0.391 f1 0.359\n",
            "[W3] GRU Epoch 03 | train_loss 1.0821 acc 0.421 f1 0.420 || val_loss 1.0843 acc 0.370 f1 0.337\n",
            "[W3] GRU Epoch 04 | train_loss 1.0679 acc 0.452 f1 0.452 || val_loss 1.0846 acc 0.350 f1 0.330\n",
            "[W3] GRU Epoch 05 | train_loss 1.0391 acc 0.490 f1 0.486 || val_loss 1.0826 acc 0.368 f1 0.347\n",
            "[W3] GRU Epoch 06 | train_loss 0.9960 acc 0.526 f1 0.517 || val_loss 1.0766 acc 0.354 f1 0.308\n",
            "[W3] GRU Epoch 07 | train_loss 0.9214 acc 0.559 f1 0.549 || val_loss 1.0901 acc 0.366 f1 0.318\n",
            "[W3] GRU Epoch 08 | train_loss 0.8425 acc 0.593 f1 0.585 || val_loss 1.1204 acc 0.344 f1 0.296\n",
            "[W3] GRU Epoch 09 | train_loss 0.7831 acc 0.627 f1 0.622 || val_loss 1.1686 acc 0.344 f1 0.311\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=35\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 979, np.int64(1): 933, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 979, np.int64(2): 979, np.int64(0): 979})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1004 acc 0.331 f1 0.176 || val_loss 1.0916 acc 0.399 f1 0.201\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0928 acc 0.374 f1 0.326 || val_loss 1.0918 acc 0.315 f1 0.256\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0830 acc 0.415 f1 0.381 || val_loss 1.0960 acc 0.282 f1 0.268\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0609 acc 0.446 f1 0.429 || val_loss 1.0903 acc 0.337 f1 0.311\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0069 acc 0.503 f1 0.486 || val_loss 1.0552 acc 0.381 f1 0.337\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9118 acc 0.538 f1 0.526 || val_loss 1.0815 acc 0.379 f1 0.322\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8323 acc 0.586 f1 0.579 || val_loss 1.1027 acc 0.397 f1 0.328\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7832 acc 0.605 f1 0.599 || val_loss 1.1277 acc 0.374 f1 0.309\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7483 acc 0.623 f1 0.615 || val_loss 1.1341 acc 0.387 f1 0.314\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7122 acc 0.646 f1 0.641 || val_loss 1.1553 acc 0.389 f1 0.305\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6783 acc 0.653 f1 0.646 || val_loss 1.2118 acc 0.393 f1 0.328\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6504 acc 0.655 f1 0.650 || val_loss 1.2384 acc 0.381 f1 0.307\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6260 acc 0.687 f1 0.684 || val_loss 1.2870 acc 0.385 f1 0.318\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  35%|      | 35/100 [17:18<30:42, 28.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=36 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=36\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 931, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1335 acc 0.406 f1 0.402 || val_loss 1.1813 acc 0.300 f1 0.288\n",
            "[W3] ANN Epoch 02 | train_loss 0.9789 acc 0.504 f1 0.497 || val_loss 1.1638 acc 0.333 f1 0.312\n",
            "[W3] ANN Epoch 03 | train_loss 0.8841 acc 0.579 f1 0.571 || val_loss 1.1576 acc 0.333 f1 0.301\n",
            "[W3] ANN Epoch 04 | train_loss 0.8070 acc 0.600 f1 0.592 || val_loss 1.1490 acc 0.370 f1 0.329\n",
            "[W3] ANN Epoch 05 | train_loss 0.7523 acc 0.650 f1 0.645 || val_loss 1.1491 acc 0.399 f1 0.347\n",
            "[W3] ANN Epoch 06 | train_loss 0.6961 acc 0.668 f1 0.663 || val_loss 1.1634 acc 0.391 f1 0.332\n",
            "[W3] ANN Epoch 07 | train_loss 0.6669 acc 0.682 f1 0.679 || val_loss 1.1657 acc 0.385 f1 0.321\n",
            "[W3] ANN Epoch 08 | train_loss 0.6370 acc 0.698 f1 0.695 || val_loss 1.1621 acc 0.409 f1 0.347\n",
            "[W3] ANN Epoch 09 | train_loss 0.5983 acc 0.721 f1 0.719 || val_loss 1.1919 acc 0.407 f1 0.325\n",
            "[W3] ANN Epoch 10 | train_loss 0.5517 acc 0.741 f1 0.739 || val_loss 1.2199 acc 0.418 f1 0.345\n",
            "[W3] ANN Epoch 11 | train_loss 0.5398 acc 0.753 f1 0.751 || val_loss 1.2344 acc 0.418 f1 0.336\n",
            "[W3] ANN Epoch 12 | train_loss 0.5279 acc 0.763 f1 0.761 || val_loss 1.2995 acc 0.407 f1 0.337\n",
            "[W3] ANN Epoch 13 | train_loss 0.5005 acc 0.769 f1 0.768 || val_loss 1.3175 acc 0.395 f1 0.315\n",
            "[W3] ANN Epoch 14 | train_loss 0.4821 acc 0.791 f1 0.790 || val_loss 1.3319 acc 0.407 f1 0.328\n",
            "[W3] ANN Epoch 15 | train_loss 0.4618 acc 0.791 f1 0.791 || val_loss 1.3323 acc 0.426 f1 0.342\n",
            "[W3] ANN Epoch 16 | train_loss 0.4438 acc 0.803 f1 0.802 || val_loss 1.3577 acc 0.409 f1 0.334\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=36\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 931, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0661 acc 0.395 f1 0.397 || val_loss 1.0376 acc 0.401 f1 0.337\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9139 acc 0.560 f1 0.554 || val_loss 1.0625 acc 0.420 f1 0.341\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.7967 acc 0.615 f1 0.611 || val_loss 1.1620 acc 0.385 f1 0.338\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7049 acc 0.657 f1 0.653 || val_loss 1.1965 acc 0.389 f1 0.329\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6268 acc 0.705 f1 0.703 || val_loss 1.2676 acc 0.401 f1 0.341\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5639 acc 0.733 f1 0.731 || val_loss 1.3072 acc 0.412 f1 0.330\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5262 acc 0.758 f1 0.757 || val_loss 1.2962 acc 0.428 f1 0.343\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4822 acc 0.788 f1 0.787 || val_loss 1.3859 acc 0.422 f1 0.328\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4396 acc 0.808 f1 0.807 || val_loss 1.4833 acc 0.393 f1 0.304\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4060 acc 0.821 f1 0.820 || val_loss 1.5008 acc 0.438 f1 0.346\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3575 acc 0.850 f1 0.850 || val_loss 1.6265 acc 0.414 f1 0.334\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3435 acc 0.856 f1 0.856 || val_loss 1.6194 acc 0.426 f1 0.351\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2968 acc 0.881 f1 0.880 || val_loss 1.7864 acc 0.426 f1 0.337\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2720 acc 0.898 f1 0.898 || val_loss 1.8108 acc 0.424 f1 0.329\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2465 acc 0.910 f1 0.910 || val_loss 1.9268 acc 0.397 f1 0.313\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2391 acc 0.908 f1 0.907 || val_loss 1.9687 acc 0.436 f1 0.339\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.2034 acc 0.926 f1 0.926 || val_loss 2.0885 acc 0.395 f1 0.302\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1875 acc 0.935 f1 0.935 || val_loss 2.1657 acc 0.399 f1 0.327\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1808 acc 0.938 f1 0.938 || val_loss 2.1905 acc 0.391 f1 0.316\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1721 acc 0.941 f1 0.941 || val_loss 2.1509 acc 0.409 f1 0.334\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=36\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 931, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0994 acc 0.333 f1 0.332 || val_loss 1.0979 acc 0.305 f1 0.281\n",
            "[W3] RNN Epoch 02 | train_loss 1.0828 acc 0.411 f1 0.408 || val_loss 1.0824 acc 0.340 f1 0.314\n",
            "[W3] RNN Epoch 03 | train_loss 1.0641 acc 0.439 f1 0.434 || val_loss 1.0927 acc 0.331 f1 0.311\n",
            "[W3] RNN Epoch 04 | train_loss 1.0479 acc 0.459 f1 0.442 || val_loss 1.0816 acc 0.344 f1 0.321\n",
            "[W3] RNN Epoch 05 | train_loss 1.0258 acc 0.472 f1 0.466 || val_loss 1.0912 acc 0.348 f1 0.329\n",
            "[W3] RNN Epoch 06 | train_loss 1.0075 acc 0.498 f1 0.490 || val_loss 1.0895 acc 0.335 f1 0.313\n",
            "[W3] RNN Epoch 07 | train_loss 0.9833 acc 0.523 f1 0.515 || val_loss 1.0716 acc 0.374 f1 0.334\n",
            "[W3] RNN Epoch 08 | train_loss 0.9565 acc 0.544 f1 0.537 || val_loss 1.1021 acc 0.346 f1 0.324\n",
            "[W3] RNN Epoch 09 | train_loss 0.9356 acc 0.559 f1 0.549 || val_loss 1.0818 acc 0.350 f1 0.316\n",
            "[W3] RNN Epoch 10 | train_loss 0.9105 acc 0.578 f1 0.570 || val_loss 1.0723 acc 0.387 f1 0.344\n",
            "[W3] RNN Epoch 11 | train_loss 0.8806 acc 0.590 f1 0.581 || val_loss 1.0769 acc 0.370 f1 0.333\n",
            "[W3] RNN Epoch 12 | train_loss 0.8671 acc 0.589 f1 0.580 || val_loss 1.0968 acc 0.362 f1 0.322\n",
            "[W3] RNN Epoch 13 | train_loss 0.8427 acc 0.601 f1 0.591 || val_loss 1.0801 acc 0.381 f1 0.331\n",
            "[W3] RNN Epoch 14 | train_loss 0.8218 acc 0.616 f1 0.608 || val_loss 1.1133 acc 0.364 f1 0.328\n",
            "[W3] RNN Epoch 15 | train_loss 0.7978 acc 0.631 f1 0.624 || val_loss 1.1315 acc 0.352 f1 0.318\n",
            "[W3] RNN Epoch 16 | train_loss 0.7761 acc 0.644 f1 0.635 || val_loss 1.1354 acc 0.385 f1 0.334\n",
            "[W3] RNN Epoch 17 | train_loss 0.7562 acc 0.650 f1 0.644 || val_loss 1.1217 acc 0.387 f1 0.331\n",
            "[W3] RNN Epoch 18 | train_loss 0.7306 acc 0.662 f1 0.654 || val_loss 1.1468 acc 0.397 f1 0.339\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=36\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 931, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0990 acc 0.327 f1 0.261 || val_loss 1.0950 acc 0.416 f1 0.322\n",
            "[W3] GRU Epoch 02 | train_loss 1.0914 acc 0.387 f1 0.386 || val_loss 1.0961 acc 0.385 f1 0.327\n",
            "[W3] GRU Epoch 03 | train_loss 1.0845 acc 0.412 f1 0.410 || val_loss 1.0958 acc 0.362 f1 0.322\n",
            "[W3] GRU Epoch 04 | train_loss 1.0701 acc 0.447 f1 0.442 || val_loss 1.0921 acc 0.368 f1 0.331\n",
            "[W3] GRU Epoch 05 | train_loss 1.0458 acc 0.479 f1 0.473 || val_loss 1.0880 acc 0.370 f1 0.330\n",
            "[W3] GRU Epoch 06 | train_loss 1.0058 acc 0.516 f1 0.510 || val_loss 1.0917 acc 0.377 f1 0.337\n",
            "[W3] GRU Epoch 07 | train_loss 0.9397 acc 0.548 f1 0.541 || val_loss 1.1460 acc 0.344 f1 0.314\n",
            "[W3] GRU Epoch 08 | train_loss 0.8732 acc 0.577 f1 0.569 || val_loss 1.1086 acc 0.366 f1 0.314\n",
            "[W3] GRU Epoch 09 | train_loss 0.8201 acc 0.604 f1 0.597 || val_loss 1.1182 acc 0.389 f1 0.331\n",
            "[W3] GRU Epoch 10 | train_loss 0.7692 acc 0.638 f1 0.633 || val_loss 1.1195 acc 0.383 f1 0.314\n",
            "[W3] GRU Epoch 11 | train_loss 0.7319 acc 0.651 f1 0.647 || val_loss 1.1942 acc 0.364 f1 0.317\n",
            "[W3] GRU Epoch 12 | train_loss 0.6924 acc 0.674 f1 0.672 || val_loss 1.2192 acc 0.383 f1 0.329\n",
            "[W3] GRU Epoch 13 | train_loss 0.6672 acc 0.686 f1 0.682 || val_loss 1.2200 acc 0.389 f1 0.333\n",
            "[W3] GRU Epoch 14 | train_loss 0.6387 acc 0.700 f1 0.697 || val_loss 1.2709 acc 0.399 f1 0.328\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=36\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 931, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0998 acc 0.336 f1 0.278 || val_loss 1.0917 acc 0.416 f1 0.355\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0937 acc 0.385 f1 0.385 || val_loss 1.0943 acc 0.337 f1 0.314\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0835 acc 0.431 f1 0.427 || val_loss 1.0994 acc 0.311 f1 0.301\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0652 acc 0.452 f1 0.433 || val_loss 1.0925 acc 0.323 f1 0.307\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0157 acc 0.501 f1 0.488 || val_loss 1.0586 acc 0.403 f1 0.342\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9196 acc 0.565 f1 0.551 || val_loss 1.0892 acc 0.368 f1 0.322\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8422 acc 0.593 f1 0.582 || val_loss 1.1209 acc 0.366 f1 0.323\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7886 acc 0.629 f1 0.624 || val_loss 1.1638 acc 0.379 f1 0.327\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7599 acc 0.631 f1 0.626 || val_loss 1.1469 acc 0.395 f1 0.317\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  36%|      | 36/100 [17:43<29:00, 27.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=37 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=37\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1253 acc 0.407 f1 0.408 || val_loss 1.1429 acc 0.325 f1 0.306\n",
            "[W3] ANN Epoch 02 | train_loss 0.9658 acc 0.514 f1 0.506 || val_loss 1.1510 acc 0.333 f1 0.301\n",
            "[W3] ANN Epoch 03 | train_loss 0.8747 acc 0.574 f1 0.567 || val_loss 1.1388 acc 0.352 f1 0.320\n",
            "[W3] ANN Epoch 04 | train_loss 0.8138 acc 0.609 f1 0.601 || val_loss 1.1049 acc 0.401 f1 0.355\n",
            "[W3] ANN Epoch 05 | train_loss 0.7491 acc 0.641 f1 0.635 || val_loss 1.1439 acc 0.387 f1 0.346\n",
            "[W3] ANN Epoch 06 | train_loss 0.7231 acc 0.651 f1 0.646 || val_loss 1.1463 acc 0.383 f1 0.323\n",
            "[W3] ANN Epoch 07 | train_loss 0.6687 acc 0.689 f1 0.686 || val_loss 1.1561 acc 0.412 f1 0.360\n",
            "[W3] ANN Epoch 08 | train_loss 0.6451 acc 0.689 f1 0.686 || val_loss 1.1604 acc 0.412 f1 0.357\n",
            "[W3] ANN Epoch 09 | train_loss 0.6161 acc 0.714 f1 0.712 || val_loss 1.1800 acc 0.412 f1 0.362\n",
            "[W3] ANN Epoch 10 | train_loss 0.6085 acc 0.725 f1 0.723 || val_loss 1.1818 acc 0.453 f1 0.399\n",
            "[W3] ANN Epoch 11 | train_loss 0.5754 acc 0.736 f1 0.734 || val_loss 1.2184 acc 0.397 f1 0.331\n",
            "[W3] ANN Epoch 12 | train_loss 0.5697 acc 0.747 f1 0.747 || val_loss 1.2390 acc 0.403 f1 0.352\n",
            "[W3] ANN Epoch 13 | train_loss 0.5536 acc 0.748 f1 0.746 || val_loss 1.2324 acc 0.414 f1 0.348\n",
            "[W3] ANN Epoch 14 | train_loss 0.5338 acc 0.758 f1 0.757 || val_loss 1.2520 acc 0.430 f1 0.371\n",
            "[W3] ANN Epoch 15 | train_loss 0.4914 acc 0.769 f1 0.769 || val_loss 1.2829 acc 0.407 f1 0.351\n",
            "[W3] ANN Epoch 16 | train_loss 0.4992 acc 0.772 f1 0.771 || val_loss 1.2854 acc 0.424 f1 0.354\n",
            "[W3] ANN Epoch 17 | train_loss 0.5004 acc 0.776 f1 0.775 || val_loss 1.3097 acc 0.422 f1 0.360\n",
            "[W3] ANN Epoch 18 | train_loss 0.4844 acc 0.784 f1 0.784 || val_loss 1.3114 acc 0.436 f1 0.365\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=37\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0758 acc 0.415 f1 0.416 || val_loss 1.0148 acc 0.418 f1 0.314\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9596 acc 0.535 f1 0.533 || val_loss 1.0505 acc 0.368 f1 0.293\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8448 acc 0.591 f1 0.588 || val_loss 1.0797 acc 0.397 f1 0.312\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7492 acc 0.649 f1 0.646 || val_loss 1.1541 acc 0.356 f1 0.309\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6861 acc 0.685 f1 0.682 || val_loss 1.1840 acc 0.358 f1 0.309\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6160 acc 0.722 f1 0.719 || val_loss 1.2245 acc 0.389 f1 0.313\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5544 acc 0.755 f1 0.754 || val_loss 1.3049 acc 0.387 f1 0.323\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5019 acc 0.783 f1 0.782 || val_loss 1.3801 acc 0.385 f1 0.324\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4611 acc 0.799 f1 0.798 || val_loss 1.3934 acc 0.430 f1 0.345\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4282 acc 0.824 f1 0.824 || val_loss 1.4650 acc 0.414 f1 0.339\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3775 acc 0.845 f1 0.845 || val_loss 1.5610 acc 0.401 f1 0.324\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3296 acc 0.869 f1 0.869 || val_loss 1.6240 acc 0.440 f1 0.357\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3119 acc 0.874 f1 0.873 || val_loss 1.7475 acc 0.416 f1 0.331\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3283 acc 0.872 f1 0.872 || val_loss 1.7338 acc 0.430 f1 0.350\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2743 acc 0.900 f1 0.899 || val_loss 1.8235 acc 0.426 f1 0.355\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2462 acc 0.909 f1 0.909 || val_loss 1.8881 acc 0.426 f1 0.346\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1975 acc 0.930 f1 0.930 || val_loss 2.0059 acc 0.426 f1 0.347\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.2015 acc 0.926 f1 0.926 || val_loss 2.0995 acc 0.407 f1 0.353\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1909 acc 0.932 f1 0.932 || val_loss 2.1765 acc 0.436 f1 0.358\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1504 acc 0.947 f1 0.947 || val_loss 2.2222 acc 0.416 f1 0.344\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.1309 acc 0.958 f1 0.958 || val_loss 2.3607 acc 0.424 f1 0.346\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1731 acc 0.940 f1 0.940 || val_loss 2.4275 acc 0.418 f1 0.323\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.1572 acc 0.946 f1 0.946 || val_loss 2.4294 acc 0.420 f1 0.339\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.1290 acc 0.955 f1 0.955 || val_loss 2.4061 acc 0.416 f1 0.340\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.1233 acc 0.958 f1 0.958 || val_loss 2.4951 acc 0.405 f1 0.317\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.1407 acc 0.953 f1 0.953 || val_loss 2.4750 acc 0.436 f1 0.345\n",
            "[W3] CNN1D Epoch 27 | train_loss 0.0914 acc 0.974 f1 0.974 || val_loss 2.6131 acc 0.430 f1 0.340\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=37\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0966 acc 0.356 f1 0.328 || val_loss 1.0857 acc 0.377 f1 0.324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0779 acc 0.407 f1 0.406 || val_loss 1.0931 acc 0.337 f1 0.320\n",
            "[W3] RNN Epoch 03 | train_loss 1.0661 acc 0.426 f1 0.418 || val_loss 1.0787 acc 0.366 f1 0.342\n",
            "[W3] RNN Epoch 04 | train_loss 1.0478 acc 0.454 f1 0.451 || val_loss 1.0787 acc 0.364 f1 0.345\n",
            "[W3] RNN Epoch 05 | train_loss 1.0298 acc 0.474 f1 0.469 || val_loss 1.0714 acc 0.393 f1 0.366\n",
            "[W3] RNN Epoch 06 | train_loss 1.0077 acc 0.501 f1 0.496 || val_loss 1.0809 acc 0.352 f1 0.333\n",
            "[W3] RNN Epoch 07 | train_loss 0.9839 acc 0.517 f1 0.509 || val_loss 1.0696 acc 0.366 f1 0.333\n",
            "[W3] RNN Epoch 08 | train_loss 0.9579 acc 0.531 f1 0.523 || val_loss 1.0745 acc 0.389 f1 0.360\n",
            "[W3] RNN Epoch 09 | train_loss 0.9254 acc 0.560 f1 0.550 || val_loss 1.1052 acc 0.354 f1 0.333\n",
            "[W3] RNN Epoch 10 | train_loss 0.8985 acc 0.565 f1 0.555 || val_loss 1.0765 acc 0.379 f1 0.331\n",
            "[W3] RNN Epoch 11 | train_loss 0.8682 acc 0.595 f1 0.589 || val_loss 1.0999 acc 0.350 f1 0.316\n",
            "[W3] RNN Epoch 12 | train_loss 0.8514 acc 0.596 f1 0.586 || val_loss 1.0814 acc 0.399 f1 0.352\n",
            "[W3] RNN Epoch 13 | train_loss 0.8241 acc 0.613 f1 0.606 || val_loss 1.1032 acc 0.393 f1 0.352\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=37\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0998 acc 0.333 f1 0.285 || val_loss 1.0908 acc 0.399 f1 0.331\n",
            "[W3] GRU Epoch 02 | train_loss 1.0940 acc 0.383 f1 0.378 || val_loss 1.0961 acc 0.356 f1 0.335\n",
            "[W3] GRU Epoch 03 | train_loss 1.0848 acc 0.407 f1 0.407 || val_loss 1.0870 acc 0.370 f1 0.331\n",
            "[W3] GRU Epoch 04 | train_loss 1.0744 acc 0.422 f1 0.420 || val_loss 1.0966 acc 0.352 f1 0.329\n",
            "[W3] GRU Epoch 05 | train_loss 1.0538 acc 0.459 f1 0.455 || val_loss 1.0872 acc 0.362 f1 0.316\n",
            "[W3] GRU Epoch 06 | train_loss 1.0169 acc 0.490 f1 0.487 || val_loss 1.0730 acc 0.372 f1 0.321\n",
            "[W3] GRU Epoch 07 | train_loss 0.9546 acc 0.537 f1 0.529 || val_loss 1.0861 acc 0.368 f1 0.305\n",
            "[W3] GRU Epoch 08 | train_loss 0.8856 acc 0.565 f1 0.558 || val_loss 1.0878 acc 0.356 f1 0.295\n",
            "[W3] GRU Epoch 09 | train_loss 0.8278 acc 0.604 f1 0.599 || val_loss 1.1187 acc 0.389 f1 0.323\n",
            "[W3] GRU Epoch 10 | train_loss 0.7870 acc 0.614 f1 0.608 || val_loss 1.1385 acc 0.381 f1 0.306\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=37\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1007 acc 0.329 f1 0.189 || val_loss 1.0957 acc 0.405 f1 0.242\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0950 acc 0.365 f1 0.307 || val_loss 1.0985 acc 0.321 f1 0.282\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0901 acc 0.393 f1 0.377 || val_loss 1.0940 acc 0.323 f1 0.286\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0795 acc 0.418 f1 0.400 || val_loss 1.1046 acc 0.300 f1 0.282\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0555 acc 0.446 f1 0.429 || val_loss 1.0927 acc 0.325 f1 0.305\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9970 acc 0.503 f1 0.489 || val_loss 1.1295 acc 0.315 f1 0.298\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9070 acc 0.556 f1 0.549 || val_loss 1.1433 acc 0.352 f1 0.312\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8451 acc 0.588 f1 0.581 || val_loss 1.1200 acc 0.379 f1 0.317\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7928 acc 0.614 f1 0.608 || val_loss 1.1732 acc 0.372 f1 0.316\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7607 acc 0.634 f1 0.631 || val_loss 1.1512 acc 0.395 f1 0.315\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7209 acc 0.648 f1 0.644 || val_loss 1.1882 acc 0.407 f1 0.322\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6927 acc 0.661 f1 0.658 || val_loss 1.2275 acc 0.372 f1 0.299\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6626 acc 0.676 f1 0.675 || val_loss 1.2710 acc 0.377 f1 0.314\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6419 acc 0.685 f1 0.683 || val_loss 1.2468 acc 0.377 f1 0.297\n",
            "[W3] LSTM Epoch 15 | train_loss 0.6078 acc 0.710 f1 0.709 || val_loss 1.3233 acc 0.362 f1 0.303\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5882 acc 0.716 f1 0.713 || val_loss 1.3020 acc 0.403 f1 0.323\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5636 acc 0.720 f1 0.718 || val_loss 1.3823 acc 0.372 f1 0.303\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5401 acc 0.736 f1 0.734 || val_loss 1.3598 acc 0.389 f1 0.313\n",
            "[W3] LSTM Epoch 19 | train_loss 0.5091 acc 0.763 f1 0.762 || val_loss 1.4133 acc 0.381 f1 0.305\n",
            "[W3] LSTM Epoch 20 | train_loss 0.5081 acc 0.757 f1 0.756 || val_loss 1.4030 acc 0.397 f1 0.313\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4697 acc 0.779 f1 0.778 || val_loss 1.4586 acc 0.393 f1 0.310\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4548 acc 0.779 f1 0.779 || val_loss 1.5213 acc 0.407 f1 0.321\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4411 acc 0.793 f1 0.793 || val_loss 1.5374 acc 0.401 f1 0.311\n",
            "[W3] LSTM Epoch 24 | train_loss 0.4165 acc 0.807 f1 0.807 || val_loss 1.5691 acc 0.418 f1 0.328\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3966 acc 0.815 f1 0.815 || val_loss 1.6538 acc 0.412 f1 0.324\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3843 acc 0.823 f1 0.823 || val_loss 1.6516 acc 0.412 f1 0.324\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3705 acc 0.833 f1 0.833 || val_loss 1.7090 acc 0.399 f1 0.315\n",
            "[W3] LSTM Epoch 28 | train_loss 0.3579 acc 0.835 f1 0.835 || val_loss 1.7484 acc 0.403 f1 0.320\n",
            "[W3] LSTM Epoch 29 | train_loss 0.3431 acc 0.850 f1 0.850 || val_loss 1.8367 acc 0.401 f1 0.317\n",
            "[W3] LSTM Epoch 30 | train_loss 0.3219 acc 0.858 f1 0.858 || val_loss 1.8695 acc 0.409 f1 0.323\n",
            "[W3] LSTM Epoch 31 | train_loss 0.3055 acc 0.867 f1 0.867 || val_loss 1.9196 acc 0.424 f1 0.334\n",
            "[W3] LSTM Epoch 32 | train_loss 0.2945 acc 0.879 f1 0.879 || val_loss 1.9389 acc 0.416 f1 0.322\n",
            "[W3] LSTM Epoch 33 | train_loss 0.2800 acc 0.885 f1 0.885 || val_loss 1.9847 acc 0.420 f1 0.331\n",
            "[W3] LSTM Epoch 34 | train_loss 0.2572 acc 0.896 f1 0.896 || val_loss 2.0602 acc 0.418 f1 0.329\n",
            "[W3] LSTM Epoch 35 | train_loss 0.2415 acc 0.900 f1 0.900 || val_loss 2.1578 acc 0.418 f1 0.333\n",
            "[W3] LSTM Epoch 36 | train_loss 0.2278 acc 0.915 f1 0.915 || val_loss 2.1490 acc 0.414 f1 0.321\n",
            "[W3] LSTM Epoch 37 | train_loss 0.2129 acc 0.921 f1 0.921 || val_loss 2.2219 acc 0.412 f1 0.326\n",
            "[W3] LSTM Epoch 38 | train_loss 0.1941 acc 0.928 f1 0.928 || val_loss 2.2949 acc 0.432 f1 0.336\n",
            "[W3] LSTM Epoch 39 | train_loss 0.1789 acc 0.936 f1 0.936 || val_loss 2.3861 acc 0.414 f1 0.325\n",
            "[W3] LSTM Epoch 40 | train_loss 0.1601 acc 0.946 f1 0.946 || val_loss 2.4498 acc 0.418 f1 0.328\n",
            "[W3] LSTM Epoch 41 | train_loss 0.1440 acc 0.952 f1 0.952 || val_loss 2.5234 acc 0.407 f1 0.321\n",
            "[W3] LSTM Epoch 42 | train_loss 0.1242 acc 0.966 f1 0.966 || val_loss 2.6257 acc 0.412 f1 0.321\n",
            "[W3] LSTM Epoch 43 | train_loss 0.1160 acc 0.962 f1 0.962 || val_loss 2.6681 acc 0.416 f1 0.322\n",
            "[W3] LSTM Epoch 44 | train_loss 0.1004 acc 0.972 f1 0.972 || val_loss 2.7852 acc 0.414 f1 0.321\n",
            "[W3] LSTM Epoch 45 | train_loss 0.0923 acc 0.979 f1 0.979 || val_loss 2.8305 acc 0.420 f1 0.330\n",
            "[W3] LSTM Epoch 46 | train_loss 0.0747 acc 0.986 f1 0.986 || val_loss 2.9254 acc 0.426 f1 0.330\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  37%|      | 37/100 [18:18<30:56, 29.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=38 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=38\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 927, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1377 acc 0.404 f1 0.404 || val_loss 1.1283 acc 0.319 f1 0.290\n",
            "[W3] ANN Epoch 02 | train_loss 0.9508 acc 0.540 f1 0.534 || val_loss 1.1501 acc 0.333 f1 0.298\n",
            "[W3] ANN Epoch 03 | train_loss 0.8594 acc 0.582 f1 0.576 || val_loss 1.1146 acc 0.385 f1 0.332\n",
            "[W3] ANN Epoch 04 | train_loss 0.8103 acc 0.603 f1 0.596 || val_loss 1.1025 acc 0.385 f1 0.334\n",
            "[W3] ANN Epoch 05 | train_loss 0.7409 acc 0.653 f1 0.648 || val_loss 1.1199 acc 0.366 f1 0.317\n",
            "[W3] ANN Epoch 06 | train_loss 0.7143 acc 0.648 f1 0.645 || val_loss 1.1116 acc 0.383 f1 0.323\n",
            "[W3] ANN Epoch 07 | train_loss 0.6814 acc 0.680 f1 0.676 || val_loss 1.1327 acc 0.397 f1 0.338\n",
            "[W3] ANN Epoch 08 | train_loss 0.6460 acc 0.694 f1 0.692 || val_loss 1.1232 acc 0.422 f1 0.348\n",
            "[W3] ANN Epoch 09 | train_loss 0.6099 acc 0.715 f1 0.715 || val_loss 1.1510 acc 0.414 f1 0.329\n",
            "[W3] ANN Epoch 10 | train_loss 0.5961 acc 0.718 f1 0.715 || val_loss 1.1918 acc 0.403 f1 0.332\n",
            "[W3] ANN Epoch 11 | train_loss 0.5882 acc 0.718 f1 0.716 || val_loss 1.1764 acc 0.428 f1 0.342\n",
            "[W3] ANN Epoch 12 | train_loss 0.5446 acc 0.752 f1 0.752 || val_loss 1.2154 acc 0.412 f1 0.328\n",
            "[W3] ANN Epoch 13 | train_loss 0.5347 acc 0.759 f1 0.758 || val_loss 1.2144 acc 0.434 f1 0.348\n",
            "[W3] ANN Epoch 14 | train_loss 0.5328 acc 0.763 f1 0.762 || val_loss 1.2496 acc 0.438 f1 0.366\n",
            "[W3] ANN Epoch 15 | train_loss 0.5226 acc 0.765 f1 0.764 || val_loss 1.2841 acc 0.418 f1 0.348\n",
            "[W3] ANN Epoch 16 | train_loss 0.5096 acc 0.770 f1 0.770 || val_loss 1.2677 acc 0.412 f1 0.330\n",
            "[W3] ANN Epoch 17 | train_loss 0.5198 acc 0.768 f1 0.767 || val_loss 1.2936 acc 0.399 f1 0.339\n",
            "[W3] ANN Epoch 18 | train_loss 0.4871 acc 0.787 f1 0.786 || val_loss 1.3084 acc 0.405 f1 0.326\n",
            "[W3] ANN Epoch 19 | train_loss 0.4792 acc 0.787 f1 0.786 || val_loss 1.3270 acc 0.414 f1 0.326\n",
            "[W3] ANN Epoch 20 | train_loss 0.4438 acc 0.801 f1 0.801 || val_loss 1.3831 acc 0.414 f1 0.347\n",
            "[W3] ANN Epoch 21 | train_loss 0.4565 acc 0.795 f1 0.793 || val_loss 1.4001 acc 0.426 f1 0.355\n",
            "[W3] ANN Epoch 22 | train_loss 0.4533 acc 0.796 f1 0.796 || val_loss 1.3715 acc 0.414 f1 0.332\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=38\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 927, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0885 acc 0.381 f1 0.382 || val_loss 1.0416 acc 0.449 f1 0.336\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9737 acc 0.539 f1 0.538 || val_loss 1.0446 acc 0.391 f1 0.311\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8330 acc 0.612 f1 0.609 || val_loss 1.1014 acc 0.374 f1 0.300\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7395 acc 0.649 f1 0.647 || val_loss 1.1399 acc 0.377 f1 0.302\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6620 acc 0.691 f1 0.690 || val_loss 1.1978 acc 0.377 f1 0.311\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6078 acc 0.724 f1 0.722 || val_loss 1.2233 acc 0.399 f1 0.327\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5495 acc 0.752 f1 0.751 || val_loss 1.3170 acc 0.366 f1 0.313\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4990 acc 0.774 f1 0.774 || val_loss 1.3460 acc 0.428 f1 0.325\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4578 acc 0.805 f1 0.805 || val_loss 1.4063 acc 0.422 f1 0.327\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=38\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 927, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0997 acc 0.353 f1 0.307 || val_loss 1.0977 acc 0.395 f1 0.350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0784 acc 0.429 f1 0.429 || val_loss 1.0997 acc 0.362 f1 0.340\n",
            "[W3] RNN Epoch 03 | train_loss 1.0616 acc 0.453 f1 0.450 || val_loss 1.0956 acc 0.362 f1 0.343\n",
            "[W3] RNN Epoch 04 | train_loss 1.0428 acc 0.475 f1 0.471 || val_loss 1.0849 acc 0.352 f1 0.323\n",
            "[W3] RNN Epoch 05 | train_loss 1.0243 acc 0.480 f1 0.478 || val_loss 1.0867 acc 0.354 f1 0.331\n",
            "[W3] RNN Epoch 06 | train_loss 1.0008 acc 0.500 f1 0.492 || val_loss 1.0821 acc 0.362 f1 0.334\n",
            "[W3] RNN Epoch 07 | train_loss 0.9760 acc 0.513 f1 0.505 || val_loss 1.0838 acc 0.372 f1 0.343\n",
            "[W3] RNN Epoch 08 | train_loss 0.9482 acc 0.533 f1 0.527 || val_loss 1.0912 acc 0.372 f1 0.341\n",
            "[W3] RNN Epoch 09 | train_loss 0.9202 acc 0.555 f1 0.544 || val_loss 1.0962 acc 0.387 f1 0.355\n",
            "[W3] RNN Epoch 10 | train_loss 0.9098 acc 0.563 f1 0.554 || val_loss 1.0901 acc 0.387 f1 0.348\n",
            "[W3] RNN Epoch 11 | train_loss 0.8827 acc 0.579 f1 0.570 || val_loss 1.1102 acc 0.358 f1 0.325\n",
            "[W3] RNN Epoch 12 | train_loss 0.8621 acc 0.585 f1 0.577 || val_loss 1.1145 acc 0.381 f1 0.346\n",
            "[W3] RNN Epoch 13 | train_loss 0.8322 acc 0.607 f1 0.596 || val_loss 1.1137 acc 0.362 f1 0.329\n",
            "[W3] RNN Epoch 14 | train_loss 0.8220 acc 0.609 f1 0.600 || val_loss 1.1205 acc 0.366 f1 0.325\n",
            "[W3] RNN Epoch 15 | train_loss 0.8042 acc 0.623 f1 0.614 || val_loss 1.1440 acc 0.370 f1 0.331\n",
            "[W3] RNN Epoch 16 | train_loss 0.7879 acc 0.627 f1 0.617 || val_loss 1.1258 acc 0.374 f1 0.322\n",
            "[W3] RNN Epoch 17 | train_loss 0.7650 acc 0.637 f1 0.628 || val_loss 1.1433 acc 0.368 f1 0.326\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=38\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 927, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1001 acc 0.338 f1 0.251 || val_loss 1.0922 acc 0.372 f1 0.294\n",
            "[W3] GRU Epoch 02 | train_loss 1.0904 acc 0.385 f1 0.372 || val_loss 1.0957 acc 0.315 f1 0.300\n",
            "[W3] GRU Epoch 03 | train_loss 1.0812 acc 0.407 f1 0.407 || val_loss 1.1101 acc 0.276 f1 0.273\n",
            "[W3] GRU Epoch 04 | train_loss 1.0648 acc 0.436 f1 0.429 || val_loss 1.0935 acc 0.327 f1 0.306\n",
            "[W3] GRU Epoch 05 | train_loss 1.0371 acc 0.468 f1 0.461 || val_loss 1.0874 acc 0.362 f1 0.338\n",
            "[W3] GRU Epoch 06 | train_loss 0.9804 acc 0.525 f1 0.515 || val_loss 1.1098 acc 0.342 f1 0.318\n",
            "[W3] GRU Epoch 07 | train_loss 0.9051 acc 0.548 f1 0.539 || val_loss 1.1275 acc 0.356 f1 0.323\n",
            "[W3] GRU Epoch 08 | train_loss 0.8471 acc 0.581 f1 0.573 || val_loss 1.1039 acc 0.393 f1 0.338\n",
            "[W3] GRU Epoch 09 | train_loss 0.7985 acc 0.592 f1 0.584 || val_loss 1.1185 acc 0.383 f1 0.336\n",
            "[W3] GRU Epoch 10 | train_loss 0.7624 acc 0.617 f1 0.610 || val_loss 1.1312 acc 0.401 f1 0.344\n",
            "[W3] GRU Epoch 11 | train_loss 0.7281 acc 0.636 f1 0.630 || val_loss 1.1389 acc 0.409 f1 0.322\n",
            "[W3] GRU Epoch 12 | train_loss 0.6910 acc 0.656 f1 0.654 || val_loss 1.1740 acc 0.387 f1 0.333\n",
            "[W3] GRU Epoch 13 | train_loss 0.6700 acc 0.666 f1 0.662 || val_loss 1.2083 acc 0.385 f1 0.331\n",
            "[W3] GRU Epoch 14 | train_loss 0.6461 acc 0.674 f1 0.671 || val_loss 1.2129 acc 0.389 f1 0.316\n",
            "[W3] GRU Epoch 15 | train_loss 0.6196 acc 0.694 f1 0.692 || val_loss 1.2651 acc 0.366 f1 0.311\n",
            "[W3] GRU Epoch 16 | train_loss 0.6022 acc 0.687 f1 0.683 || val_loss 1.2679 acc 0.387 f1 0.326\n",
            "[W3] GRU Epoch 17 | train_loss 0.5877 acc 0.695 f1 0.692 || val_loss 1.2993 acc 0.379 f1 0.317\n",
            "[W3] GRU Epoch 18 | train_loss 0.5536 acc 0.725 f1 0.723 || val_loss 1.3207 acc 0.385 f1 0.321\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=38\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 927, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1009 acc 0.329 f1 0.185 || val_loss 1.0848 acc 0.432 f1 0.206\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0945 acc 0.353 f1 0.271 || val_loss 1.0846 acc 0.416 f1 0.270\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0867 acc 0.407 f1 0.403 || val_loss 1.0792 acc 0.389 f1 0.337\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0729 acc 0.421 f1 0.420 || val_loss 1.0948 acc 0.311 f1 0.302\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0417 acc 0.472 f1 0.466 || val_loss 1.0920 acc 0.348 f1 0.323\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9859 acc 0.506 f1 0.491 || val_loss 1.0898 acc 0.362 f1 0.320\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9070 acc 0.557 f1 0.546 || val_loss 1.1375 acc 0.372 f1 0.329\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8340 acc 0.586 f1 0.576 || val_loss 1.1394 acc 0.395 f1 0.333\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7892 acc 0.609 f1 0.604 || val_loss 1.1879 acc 0.370 f1 0.317\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7410 acc 0.630 f1 0.621 || val_loss 1.1764 acc 0.401 f1 0.315\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7165 acc 0.652 f1 0.646 || val_loss 1.2091 acc 0.387 f1 0.310\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  38%|      | 38/100 [18:40<28:22, 27.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=39 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=39\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 978, np.int64(1): 933, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 978, np.int64(2): 978, np.int64(0): 978})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1383 acc 0.401 f1 0.401 || val_loss 1.1317 acc 0.337 f1 0.317\n",
            "[W3] ANN Epoch 02 | train_loss 0.9675 acc 0.518 f1 0.511 || val_loss 1.1182 acc 0.348 f1 0.314\n",
            "[W3] ANN Epoch 03 | train_loss 0.8585 acc 0.584 f1 0.577 || val_loss 1.1258 acc 0.374 f1 0.330\n",
            "[W3] ANN Epoch 04 | train_loss 0.7756 acc 0.633 f1 0.628 || val_loss 1.1228 acc 0.372 f1 0.327\n",
            "[W3] ANN Epoch 05 | train_loss 0.7206 acc 0.656 f1 0.653 || val_loss 1.1254 acc 0.387 f1 0.329\n",
            "[W3] ANN Epoch 06 | train_loss 0.6717 acc 0.669 f1 0.666 || val_loss 1.1379 acc 0.391 f1 0.324\n",
            "[W3] ANN Epoch 07 | train_loss 0.6333 acc 0.708 f1 0.706 || val_loss 1.1660 acc 0.428 f1 0.352\n",
            "[W3] ANN Epoch 08 | train_loss 0.5914 acc 0.724 f1 0.723 || val_loss 1.1752 acc 0.403 f1 0.330\n",
            "[W3] ANN Epoch 09 | train_loss 0.5555 acc 0.742 f1 0.741 || val_loss 1.1521 acc 0.449 f1 0.374\n",
            "[W3] ANN Epoch 10 | train_loss 0.5350 acc 0.749 f1 0.748 || val_loss 1.2002 acc 0.414 f1 0.333\n",
            "[W3] ANN Epoch 11 | train_loss 0.5179 acc 0.771 f1 0.770 || val_loss 1.2109 acc 0.428 f1 0.348\n",
            "[W3] ANN Epoch 12 | train_loss 0.4889 acc 0.776 f1 0.774 || val_loss 1.2606 acc 0.420 f1 0.337\n",
            "[W3] ANN Epoch 13 | train_loss 0.4605 acc 0.791 f1 0.792 || val_loss 1.2979 acc 0.403 f1 0.326\n",
            "[W3] ANN Epoch 14 | train_loss 0.4538 acc 0.805 f1 0.805 || val_loss 1.3069 acc 0.436 f1 0.331\n",
            "[W3] ANN Epoch 15 | train_loss 0.4154 acc 0.822 f1 0.821 || val_loss 1.3183 acc 0.465 f1 0.388\n",
            "[W3] ANN Epoch 16 | train_loss 0.4174 acc 0.816 f1 0.816 || val_loss 1.3736 acc 0.467 f1 0.373\n",
            "[W3] ANN Epoch 17 | train_loss 0.3900 acc 0.833 f1 0.833 || val_loss 1.4158 acc 0.444 f1 0.368\n",
            "[W3] ANN Epoch 18 | train_loss 0.3800 acc 0.831 f1 0.830 || val_loss 1.4156 acc 0.459 f1 0.372\n",
            "[W3] ANN Epoch 19 | train_loss 0.3790 acc 0.843 f1 0.843 || val_loss 1.4266 acc 0.457 f1 0.365\n",
            "[W3] ANN Epoch 20 | train_loss 0.3572 acc 0.850 f1 0.850 || val_loss 1.4728 acc 0.451 f1 0.365\n",
            "[W3] ANN Epoch 21 | train_loss 0.3605 acc 0.846 f1 0.846 || val_loss 1.4814 acc 0.467 f1 0.395\n",
            "[W3] ANN Epoch 22 | train_loss 0.3327 acc 0.863 f1 0.863 || val_loss 1.4890 acc 0.459 f1 0.383\n",
            "[W3] ANN Epoch 23 | train_loss 0.3211 acc 0.866 f1 0.866 || val_loss 1.5374 acc 0.459 f1 0.385\n",
            "[W3] ANN Epoch 24 | train_loss 0.3364 acc 0.865 f1 0.864 || val_loss 1.5595 acc 0.447 f1 0.368\n",
            "[W3] ANN Epoch 25 | train_loss 0.2936 acc 0.878 f1 0.878 || val_loss 1.5671 acc 0.442 f1 0.365\n",
            "[W3] ANN Epoch 26 | train_loss 0.3015 acc 0.878 f1 0.878 || val_loss 1.6128 acc 0.414 f1 0.332\n",
            "[W3] ANN Epoch 27 | train_loss 0.2758 acc 0.885 f1 0.885 || val_loss 1.6636 acc 0.447 f1 0.367\n",
            "[W3] ANN Epoch 28 | train_loss 0.2875 acc 0.886 f1 0.886 || val_loss 1.6703 acc 0.451 f1 0.375\n",
            "[W3] ANN Epoch 29 | train_loss 0.2732 acc 0.894 f1 0.894 || val_loss 1.6907 acc 0.447 f1 0.381\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=39\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 978, np.int64(1): 933, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 978, np.int64(2): 978, np.int64(0): 978})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0740 acc 0.407 f1 0.406 || val_loss 1.0370 acc 0.395 f1 0.294\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9313 acc 0.554 f1 0.548 || val_loss 1.0321 acc 0.403 f1 0.308\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8042 acc 0.608 f1 0.604 || val_loss 1.1045 acc 0.374 f1 0.314\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7057 acc 0.666 f1 0.664 || val_loss 1.1194 acc 0.395 f1 0.319\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6346 acc 0.702 f1 0.701 || val_loss 1.2186 acc 0.393 f1 0.334\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5726 acc 0.736 f1 0.734 || val_loss 1.2242 acc 0.426 f1 0.353\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5210 acc 0.761 f1 0.762 || val_loss 1.3138 acc 0.412 f1 0.352\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4657 acc 0.794 f1 0.793 || val_loss 1.3457 acc 0.407 f1 0.329\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4199 acc 0.817 f1 0.817 || val_loss 1.4967 acc 0.412 f1 0.333\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3853 acc 0.831 f1 0.831 || val_loss 1.4947 acc 0.393 f1 0.323\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3311 acc 0.860 f1 0.860 || val_loss 1.6015 acc 0.399 f1 0.322\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3109 acc 0.876 f1 0.876 || val_loss 1.6297 acc 0.393 f1 0.313\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2494 acc 0.905 f1 0.905 || val_loss 1.7162 acc 0.416 f1 0.329\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2115 acc 0.925 f1 0.925 || val_loss 1.8560 acc 0.414 f1 0.333\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=39\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 978, np.int64(1): 933, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 978, np.int64(2): 978, np.int64(0): 978})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0983 acc 0.346 f1 0.329 || val_loss 1.0916 acc 0.372 f1 0.347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0824 acc 0.409 f1 0.403 || val_loss 1.0901 acc 0.368 f1 0.351\n",
            "[W3] RNN Epoch 03 | train_loss 1.0693 acc 0.434 f1 0.424 || val_loss 1.0773 acc 0.364 f1 0.339\n",
            "[W3] RNN Epoch 04 | train_loss 1.0520 acc 0.454 f1 0.450 || val_loss 1.0778 acc 0.340 f1 0.314\n",
            "[W3] RNN Epoch 05 | train_loss 1.0338 acc 0.464 f1 0.459 || val_loss 1.0756 acc 0.348 f1 0.315\n",
            "[W3] RNN Epoch 06 | train_loss 1.0071 acc 0.508 f1 0.503 || val_loss 1.0702 acc 0.358 f1 0.322\n",
            "[W3] RNN Epoch 07 | train_loss 0.9909 acc 0.506 f1 0.498 || val_loss 1.0623 acc 0.366 f1 0.323\n",
            "[W3] RNN Epoch 08 | train_loss 0.9565 acc 0.535 f1 0.529 || val_loss 1.0610 acc 0.364 f1 0.324\n",
            "[W3] RNN Epoch 09 | train_loss 0.9335 acc 0.552 f1 0.545 || val_loss 1.1040 acc 0.333 f1 0.311\n",
            "[W3] RNN Epoch 10 | train_loss 0.9145 acc 0.565 f1 0.557 || val_loss 1.0801 acc 0.344 f1 0.305\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=39\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 978, np.int64(1): 933, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 978, np.int64(2): 978, np.int64(0): 978})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1044 acc 0.356 f1 0.286 || val_loss 1.1089 acc 0.317 f1 0.251\n",
            "[W3] GRU Epoch 02 | train_loss 1.0923 acc 0.389 f1 0.377 || val_loss 1.0945 acc 0.362 f1 0.332\n",
            "[W3] GRU Epoch 03 | train_loss 1.0813 acc 0.430 f1 0.418 || val_loss 1.0777 acc 0.385 f1 0.351\n",
            "[W3] GRU Epoch 04 | train_loss 1.0674 acc 0.435 f1 0.435 || val_loss 1.0754 acc 0.381 f1 0.349\n",
            "[W3] GRU Epoch 05 | train_loss 1.0470 acc 0.470 f1 0.467 || val_loss 1.0669 acc 0.374 f1 0.341\n",
            "[W3] GRU Epoch 06 | train_loss 1.0155 acc 0.514 f1 0.507 || val_loss 1.0725 acc 0.370 f1 0.339\n",
            "[W3] GRU Epoch 07 | train_loss 0.9657 acc 0.539 f1 0.531 || val_loss 1.0965 acc 0.364 f1 0.331\n",
            "[W3] GRU Epoch 08 | train_loss 0.9052 acc 0.574 f1 0.567 || val_loss 1.0754 acc 0.418 f1 0.349\n",
            "[W3] GRU Epoch 09 | train_loss 0.8382 acc 0.605 f1 0.598 || val_loss 1.1362 acc 0.370 f1 0.323\n",
            "[W3] GRU Epoch 10 | train_loss 0.7819 acc 0.632 f1 0.625 || val_loss 1.1520 acc 0.397 f1 0.336\n",
            "[W3] GRU Epoch 11 | train_loss 0.7336 acc 0.655 f1 0.650 || val_loss 1.1992 acc 0.374 f1 0.322\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=39\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 978, np.int64(1): 933, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 978, np.int64(2): 978, np.int64(0): 978})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0995 acc 0.331 f1 0.190 || val_loss 1.0898 acc 0.447 f1 0.220\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0941 acc 0.373 f1 0.358 || val_loss 1.0919 acc 0.372 f1 0.337\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0885 acc 0.401 f1 0.402 || val_loss 1.0862 acc 0.368 f1 0.352\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0758 acc 0.429 f1 0.424 || val_loss 1.0801 acc 0.352 f1 0.329\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0540 acc 0.449 f1 0.442 || val_loss 1.0857 acc 0.360 f1 0.345\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9980 acc 0.500 f1 0.482 || val_loss 1.0634 acc 0.383 f1 0.339\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9045 acc 0.565 f1 0.555 || val_loss 1.1006 acc 0.350 f1 0.314\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8221 acc 0.597 f1 0.588 || val_loss 1.0965 acc 0.366 f1 0.306\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7684 acc 0.624 f1 0.620 || val_loss 1.1021 acc 0.393 f1 0.313\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7202 acc 0.660 f1 0.654 || val_loss 1.1354 acc 0.379 f1 0.312\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6841 acc 0.670 f1 0.666 || val_loss 1.1746 acc 0.377 f1 0.312\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  39%|      | 39/100 [19:04<26:40, 26.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=40 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=40\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 932, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1359 acc 0.403 f1 0.404 || val_loss 1.1818 acc 0.276 f1 0.266\n",
            "[W3] ANN Epoch 02 | train_loss 0.9837 acc 0.507 f1 0.502 || val_loss 1.1980 acc 0.305 f1 0.289\n",
            "[W3] ANN Epoch 03 | train_loss 0.8702 acc 0.581 f1 0.574 || val_loss 1.1336 acc 0.383 f1 0.335\n",
            "[W3] ANN Epoch 04 | train_loss 0.8051 acc 0.605 f1 0.596 || val_loss 1.1451 acc 0.374 f1 0.328\n",
            "[W3] ANN Epoch 05 | train_loss 0.7359 acc 0.648 f1 0.641 || val_loss 1.1387 acc 0.401 f1 0.345\n",
            "[W3] ANN Epoch 06 | train_loss 0.6698 acc 0.681 f1 0.677 || val_loss 1.1396 acc 0.438 f1 0.366\n",
            "[W3] ANN Epoch 07 | train_loss 0.6273 acc 0.695 f1 0.692 || val_loss 1.1453 acc 0.412 f1 0.345\n",
            "[W3] ANN Epoch 08 | train_loss 0.5928 acc 0.719 f1 0.716 || val_loss 1.2119 acc 0.403 f1 0.334\n",
            "[W3] ANN Epoch 09 | train_loss 0.5805 acc 0.728 f1 0.726 || val_loss 1.1951 acc 0.405 f1 0.325\n",
            "[W3] ANN Epoch 10 | train_loss 0.5418 acc 0.753 f1 0.751 || val_loss 1.1941 acc 0.422 f1 0.349\n",
            "[W3] ANN Epoch 11 | train_loss 0.5131 acc 0.761 f1 0.759 || val_loss 1.2117 acc 0.434 f1 0.349\n",
            "[W3] ANN Epoch 12 | train_loss 0.4942 acc 0.784 f1 0.782 || val_loss 1.2354 acc 0.442 f1 0.349\n",
            "[W3] ANN Epoch 13 | train_loss 0.5053 acc 0.776 f1 0.775 || val_loss 1.2356 acc 0.465 f1 0.369\n",
            "[W3] ANN Epoch 14 | train_loss 0.4849 acc 0.783 f1 0.783 || val_loss 1.2873 acc 0.412 f1 0.340\n",
            "[W3] ANN Epoch 15 | train_loss 0.4376 acc 0.805 f1 0.804 || val_loss 1.3296 acc 0.426 f1 0.363\n",
            "[W3] ANN Epoch 16 | train_loss 0.4526 acc 0.797 f1 0.797 || val_loss 1.2920 acc 0.428 f1 0.353\n",
            "[W3] ANN Epoch 17 | train_loss 0.4198 acc 0.813 f1 0.812 || val_loss 1.3771 acc 0.416 f1 0.344\n",
            "[W3] ANN Epoch 18 | train_loss 0.4148 acc 0.821 f1 0.820 || val_loss 1.4093 acc 0.434 f1 0.358\n",
            "[W3] ANN Epoch 19 | train_loss 0.4055 acc 0.821 f1 0.820 || val_loss 1.3721 acc 0.453 f1 0.377\n",
            "[W3] ANN Epoch 20 | train_loss 0.3762 acc 0.847 f1 0.846 || val_loss 1.4213 acc 0.459 f1 0.385\n",
            "[W3] ANN Epoch 21 | train_loss 0.3732 acc 0.847 f1 0.847 || val_loss 1.4323 acc 0.465 f1 0.384\n",
            "[W3] ANN Epoch 22 | train_loss 0.3665 acc 0.854 f1 0.854 || val_loss 1.4348 acc 0.430 f1 0.354\n",
            "[W3] ANN Epoch 23 | train_loss 0.3564 acc 0.852 f1 0.852 || val_loss 1.4325 acc 0.440 f1 0.352\n",
            "[W3] ANN Epoch 24 | train_loss 0.3453 acc 0.859 f1 0.859 || val_loss 1.4968 acc 0.440 f1 0.357\n",
            "[W3] ANN Epoch 25 | train_loss 0.3251 acc 0.870 f1 0.870 || val_loss 1.5238 acc 0.424 f1 0.345\n",
            "[W3] ANN Epoch 26 | train_loss 0.3141 acc 0.879 f1 0.879 || val_loss 1.5139 acc 0.453 f1 0.376\n",
            "[W3] ANN Epoch 27 | train_loss 0.3145 acc 0.870 f1 0.870 || val_loss 1.5438 acc 0.418 f1 0.346\n",
            "[W3] ANN Epoch 28 | train_loss 0.3242 acc 0.870 f1 0.870 || val_loss 1.5439 acc 0.426 f1 0.360\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=40\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 932, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0774 acc 0.398 f1 0.398 || val_loss 1.0300 acc 0.430 f1 0.345\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9581 acc 0.528 f1 0.518 || val_loss 1.0391 acc 0.409 f1 0.299\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8327 acc 0.590 f1 0.586 || val_loss 1.1163 acc 0.377 f1 0.327\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7473 acc 0.631 f1 0.628 || val_loss 1.1302 acc 0.374 f1 0.317\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6723 acc 0.677 f1 0.675 || val_loss 1.2448 acc 0.370 f1 0.323\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6151 acc 0.711 f1 0.710 || val_loss 1.2239 acc 0.360 f1 0.311\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5502 acc 0.740 f1 0.739 || val_loss 1.3599 acc 0.377 f1 0.331\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5128 acc 0.761 f1 0.759 || val_loss 1.2907 acc 0.397 f1 0.314\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4832 acc 0.776 f1 0.775 || val_loss 1.3329 acc 0.391 f1 0.317\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=40\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 932, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0936 acc 0.357 f1 0.332 || val_loss 1.0902 acc 0.302 f1 0.278\n",
            "[W3] RNN Epoch 02 | train_loss 1.0766 acc 0.409 f1 0.408 || val_loss 1.0845 acc 0.296 f1 0.279\n",
            "[W3] RNN Epoch 03 | train_loss 1.0632 acc 0.433 f1 0.428 || val_loss 1.0761 acc 0.302 f1 0.286\n",
            "[W3] RNN Epoch 04 | train_loss 1.0463 acc 0.464 f1 0.458 || val_loss 1.0874 acc 0.296 f1 0.282\n",
            "[W3] RNN Epoch 05 | train_loss 1.0350 acc 0.474 f1 0.463 || val_loss 1.0875 acc 0.321 f1 0.294\n",
            "[W3] RNN Epoch 06 | train_loss 1.0166 acc 0.495 f1 0.486 || val_loss 1.1017 acc 0.321 f1 0.300\n",
            "[W3] RNN Epoch 07 | train_loss 0.9950 acc 0.519 f1 0.512 || val_loss 1.1048 acc 0.321 f1 0.296\n",
            "[W3] RNN Epoch 08 | train_loss 0.9714 acc 0.521 f1 0.511 || val_loss 1.0918 acc 0.342 f1 0.308\n",
            "[W3] RNN Epoch 09 | train_loss 0.9433 acc 0.553 f1 0.544 || val_loss 1.0975 acc 0.342 f1 0.311\n",
            "[W3] RNN Epoch 10 | train_loss 0.9279 acc 0.558 f1 0.550 || val_loss 1.1108 acc 0.360 f1 0.331\n",
            "[W3] RNN Epoch 11 | train_loss 0.9075 acc 0.575 f1 0.567 || val_loss 1.0907 acc 0.372 f1 0.325\n",
            "[W3] RNN Epoch 12 | train_loss 0.8875 acc 0.573 f1 0.562 || val_loss 1.1078 acc 0.368 f1 0.322\n",
            "[W3] RNN Epoch 13 | train_loss 0.8680 acc 0.589 f1 0.581 || val_loss 1.0881 acc 0.374 f1 0.310\n",
            "[W3] RNN Epoch 14 | train_loss 0.8488 acc 0.607 f1 0.601 || val_loss 1.1609 acc 0.354 f1 0.324\n",
            "[W3] RNN Epoch 15 | train_loss 0.8388 acc 0.610 f1 0.602 || val_loss 1.1376 acc 0.370 f1 0.331\n",
            "[W3] RNN Epoch 16 | train_loss 0.8302 acc 0.607 f1 0.598 || val_loss 1.1169 acc 0.381 f1 0.326\n",
            "[W3] RNN Epoch 17 | train_loss 0.7862 acc 0.642 f1 0.635 || val_loss 1.1451 acc 0.362 f1 0.309\n",
            "[W3] RNN Epoch 18 | train_loss 0.7739 acc 0.646 f1 0.639 || val_loss 1.1695 acc 0.372 f1 0.330\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=40\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 932, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0969 acc 0.338 f1 0.223 || val_loss 1.0876 acc 0.412 f1 0.313\n",
            "[W3] GRU Epoch 02 | train_loss 1.0877 acc 0.389 f1 0.388 || val_loss 1.0967 acc 0.331 f1 0.320\n",
            "[W3] GRU Epoch 03 | train_loss 1.0771 acc 0.411 f1 0.403 || val_loss 1.0786 acc 0.379 f1 0.348\n",
            "[W3] GRU Epoch 04 | train_loss 1.0594 acc 0.447 f1 0.446 || val_loss 1.1000 acc 0.346 f1 0.335\n",
            "[W3] GRU Epoch 05 | train_loss 1.0353 acc 0.468 f1 0.459 || val_loss 1.0910 acc 0.325 f1 0.307\n",
            "[W3] GRU Epoch 06 | train_loss 0.9925 acc 0.520 f1 0.511 || val_loss 1.0475 acc 0.372 f1 0.333\n",
            "[W3] GRU Epoch 07 | train_loss 0.9299 acc 0.546 f1 0.530 || val_loss 1.0657 acc 0.374 f1 0.334\n",
            "[W3] GRU Epoch 08 | train_loss 0.8574 acc 0.581 f1 0.574 || val_loss 1.0721 acc 0.377 f1 0.311\n",
            "[W3] GRU Epoch 09 | train_loss 0.8098 acc 0.593 f1 0.587 || val_loss 1.1238 acc 0.366 f1 0.319\n",
            "[W3] GRU Epoch 10 | train_loss 0.7667 acc 0.614 f1 0.608 || val_loss 1.1356 acc 0.364 f1 0.312\n",
            "[W3] GRU Epoch 11 | train_loss 0.7333 acc 0.639 f1 0.634 || val_loss 1.1568 acc 0.395 f1 0.339\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=40\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 932, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0988 acc 0.327 f1 0.237 || val_loss 1.0956 acc 0.412 f1 0.304\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0944 acc 0.399 f1 0.390 || val_loss 1.0931 acc 0.327 f1 0.294\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0875 acc 0.414 f1 0.409 || val_loss 1.0934 acc 0.292 f1 0.278\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0758 acc 0.428 f1 0.422 || val_loss 1.0956 acc 0.278 f1 0.261\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0538 acc 0.450 f1 0.434 || val_loss 1.0938 acc 0.325 f1 0.307\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0151 acc 0.483 f1 0.462 || val_loss 1.1021 acc 0.313 f1 0.287\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9458 acc 0.539 f1 0.514 || val_loss 1.0981 acc 0.352 f1 0.312\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8695 acc 0.578 f1 0.562 || val_loss 1.0737 acc 0.420 f1 0.358\n",
            "[W3] LSTM Epoch 09 | train_loss 0.8040 acc 0.604 f1 0.597 || val_loss 1.0925 acc 0.407 f1 0.345\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7559 acc 0.624 f1 0.618 || val_loss 1.0955 acc 0.418 f1 0.343\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7196 acc 0.644 f1 0.636 || val_loss 1.1445 acc 0.405 f1 0.340\n",
            "[W3] LSTM Epoch 12 | train_loss 0.7124 acc 0.655 f1 0.650 || val_loss 1.1643 acc 0.393 f1 0.330\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6584 acc 0.670 f1 0.667 || val_loss 1.2117 acc 0.412 f1 0.344\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6347 acc 0.686 f1 0.682 || val_loss 1.2127 acc 0.420 f1 0.341\n",
            "[W3] LSTM Epoch 15 | train_loss 0.6133 acc 0.699 f1 0.694 || val_loss 1.2412 acc 0.416 f1 0.342\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5798 acc 0.715 f1 0.712 || val_loss 1.2688 acc 0.428 f1 0.349\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  40%|      | 40/100 [19:27<25:16, 25.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=41 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=41\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 935, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1173 acc 0.411 f1 0.410 || val_loss 1.1409 acc 0.329 f1 0.309\n",
            "[W3] ANN Epoch 02 | train_loss 0.9580 acc 0.533 f1 0.528 || val_loss 1.1453 acc 0.307 f1 0.282\n",
            "[W3] ANN Epoch 03 | train_loss 0.8501 acc 0.577 f1 0.570 || val_loss 1.1359 acc 0.348 f1 0.311\n",
            "[W3] ANN Epoch 04 | train_loss 0.7612 acc 0.638 f1 0.633 || val_loss 1.1451 acc 0.352 f1 0.295\n",
            "[W3] ANN Epoch 05 | train_loss 0.7062 acc 0.656 f1 0.652 || val_loss 1.1380 acc 0.372 f1 0.310\n",
            "[W3] ANN Epoch 06 | train_loss 0.6569 acc 0.686 f1 0.683 || val_loss 1.1626 acc 0.387 f1 0.332\n",
            "[W3] ANN Epoch 07 | train_loss 0.6082 acc 0.712 f1 0.711 || val_loss 1.2074 acc 0.407 f1 0.323\n",
            "[W3] ANN Epoch 08 | train_loss 0.5918 acc 0.718 f1 0.715 || val_loss 1.2339 acc 0.397 f1 0.321\n",
            "[W3] ANN Epoch 09 | train_loss 0.5649 acc 0.744 f1 0.743 || val_loss 1.2374 acc 0.409 f1 0.325\n",
            "[W3] ANN Epoch 10 | train_loss 0.5285 acc 0.751 f1 0.751 || val_loss 1.2655 acc 0.420 f1 0.338\n",
            "[W3] ANN Epoch 11 | train_loss 0.5107 acc 0.768 f1 0.767 || val_loss 1.2426 acc 0.434 f1 0.343\n",
            "[W3] ANN Epoch 12 | train_loss 0.4992 acc 0.775 f1 0.775 || val_loss 1.2795 acc 0.428 f1 0.336\n",
            "[W3] ANN Epoch 13 | train_loss 0.4671 acc 0.795 f1 0.794 || val_loss 1.3189 acc 0.432 f1 0.347\n",
            "[W3] ANN Epoch 14 | train_loss 0.4347 acc 0.811 f1 0.811 || val_loss 1.3321 acc 0.426 f1 0.346\n",
            "[W3] ANN Epoch 15 | train_loss 0.4267 acc 0.806 f1 0.805 || val_loss 1.3671 acc 0.426 f1 0.336\n",
            "[W3] ANN Epoch 16 | train_loss 0.4237 acc 0.812 f1 0.812 || val_loss 1.4002 acc 0.444 f1 0.355\n",
            "[W3] ANN Epoch 17 | train_loss 0.4136 acc 0.826 f1 0.826 || val_loss 1.4046 acc 0.463 f1 0.370\n",
            "[W3] ANN Epoch 18 | train_loss 0.3762 acc 0.840 f1 0.841 || val_loss 1.4429 acc 0.457 f1 0.375\n",
            "[W3] ANN Epoch 19 | train_loss 0.3462 acc 0.855 f1 0.855 || val_loss 1.4723 acc 0.455 f1 0.363\n",
            "[W3] ANN Epoch 20 | train_loss 0.3387 acc 0.848 f1 0.848 || val_loss 1.4927 acc 0.467 f1 0.388\n",
            "[W3] ANN Epoch 21 | train_loss 0.3547 acc 0.860 f1 0.860 || val_loss 1.4596 acc 0.479 f1 0.394\n",
            "[W3] ANN Epoch 22 | train_loss 0.3506 acc 0.857 f1 0.857 || val_loss 1.4992 acc 0.451 f1 0.370\n",
            "[W3] ANN Epoch 23 | train_loss 0.3338 acc 0.864 f1 0.864 || val_loss 1.5167 acc 0.453 f1 0.362\n",
            "[W3] ANN Epoch 24 | train_loss 0.3049 acc 0.874 f1 0.874 || val_loss 1.5397 acc 0.459 f1 0.372\n",
            "[W3] ANN Epoch 25 | train_loss 0.2968 acc 0.879 f1 0.879 || val_loss 1.6341 acc 0.453 f1 0.357\n",
            "[W3] ANN Epoch 26 | train_loss 0.2893 acc 0.882 f1 0.882 || val_loss 1.5877 acc 0.444 f1 0.356\n",
            "[W3] ANN Epoch 27 | train_loss 0.3121 acc 0.874 f1 0.874 || val_loss 1.5996 acc 0.451 f1 0.366\n",
            "[W3] ANN Epoch 28 | train_loss 0.2707 acc 0.897 f1 0.896 || val_loss 1.5951 acc 0.461 f1 0.373\n",
            "[W3] ANN Epoch 29 | train_loss 0.2802 acc 0.886 f1 0.886 || val_loss 1.6503 acc 0.453 f1 0.367\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=41\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 935, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0681 acc 0.424 f1 0.424 || val_loss 1.0306 acc 0.420 f1 0.318\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9312 acc 0.553 f1 0.549 || val_loss 1.0475 acc 0.409 f1 0.338\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.7952 acc 0.610 f1 0.605 || val_loss 1.1309 acc 0.383 f1 0.336\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7032 acc 0.654 f1 0.651 || val_loss 1.1743 acc 0.383 f1 0.317\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6191 acc 0.705 f1 0.702 || val_loss 1.2392 acc 0.383 f1 0.323\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5606 acc 0.732 f1 0.730 || val_loss 1.2874 acc 0.381 f1 0.306\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5071 acc 0.767 f1 0.768 || val_loss 1.3687 acc 0.370 f1 0.312\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4483 acc 0.802 f1 0.801 || val_loss 1.4109 acc 0.385 f1 0.300\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4040 acc 0.819 f1 0.819 || val_loss 1.4847 acc 0.399 f1 0.314\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3485 acc 0.856 f1 0.856 || val_loss 1.5970 acc 0.399 f1 0.311\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=41\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 935, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1005 acc 0.355 f1 0.297 || val_loss 1.1071 acc 0.302 f1 0.257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0823 acc 0.417 f1 0.389 || val_loss 1.1043 acc 0.309 f1 0.298\n",
            "[W3] RNN Epoch 03 | train_loss 1.0665 acc 0.424 f1 0.407 || val_loss 1.0905 acc 0.333 f1 0.309\n",
            "[W3] RNN Epoch 04 | train_loss 1.0502 acc 0.460 f1 0.453 || val_loss 1.1021 acc 0.342 f1 0.313\n",
            "[W3] RNN Epoch 05 | train_loss 1.0327 acc 0.478 f1 0.465 || val_loss 1.0836 acc 0.364 f1 0.334\n",
            "[W3] RNN Epoch 06 | train_loss 1.0178 acc 0.486 f1 0.474 || val_loss 1.0927 acc 0.360 f1 0.336\n",
            "[W3] RNN Epoch 07 | train_loss 0.9938 acc 0.502 f1 0.495 || val_loss 1.0888 acc 0.360 f1 0.334\n",
            "[W3] RNN Epoch 08 | train_loss 0.9653 acc 0.531 f1 0.522 || val_loss 1.0806 acc 0.387 f1 0.345\n",
            "[W3] RNN Epoch 09 | train_loss 0.9405 acc 0.541 f1 0.534 || val_loss 1.0934 acc 0.374 f1 0.340\n",
            "[W3] RNN Epoch 10 | train_loss 0.9144 acc 0.562 f1 0.554 || val_loss 1.1017 acc 0.385 f1 0.341\n",
            "[W3] RNN Epoch 11 | train_loss 0.8847 acc 0.577 f1 0.566 || val_loss 1.0871 acc 0.389 f1 0.346\n",
            "[W3] RNN Epoch 12 | train_loss 0.8579 acc 0.603 f1 0.597 || val_loss 1.1042 acc 0.395 f1 0.347\n",
            "[W3] RNN Epoch 13 | train_loss 0.8424 acc 0.605 f1 0.596 || val_loss 1.1288 acc 0.395 f1 0.362\n",
            "[W3] RNN Epoch 14 | train_loss 0.8183 acc 0.617 f1 0.610 || val_loss 1.1167 acc 0.397 f1 0.356\n",
            "[W3] RNN Epoch 15 | train_loss 0.7812 acc 0.639 f1 0.630 || val_loss 1.1089 acc 0.409 f1 0.356\n",
            "[W3] RNN Epoch 16 | train_loss 0.7654 acc 0.649 f1 0.642 || val_loss 1.1424 acc 0.385 f1 0.340\n",
            "[W3] RNN Epoch 17 | train_loss 0.7430 acc 0.657 f1 0.652 || val_loss 1.1290 acc 0.418 f1 0.355\n",
            "[W3] RNN Epoch 18 | train_loss 0.7160 acc 0.669 f1 0.662 || val_loss 1.1570 acc 0.407 f1 0.355\n",
            "[W3] RNN Epoch 19 | train_loss 0.6866 acc 0.694 f1 0.689 || val_loss 1.1740 acc 0.397 f1 0.346\n",
            "[W3] RNN Epoch 20 | train_loss 0.6607 acc 0.697 f1 0.692 || val_loss 1.1843 acc 0.395 f1 0.345\n",
            "[W3] RNN Epoch 21 | train_loss 0.6447 acc 0.698 f1 0.692 || val_loss 1.1986 acc 0.405 f1 0.340\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=41\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 935, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1006 acc 0.331 f1 0.257 || val_loss 1.0911 acc 0.395 f1 0.323\n",
            "[W3] GRU Epoch 02 | train_loss 1.0907 acc 0.388 f1 0.386 || val_loss 1.0997 acc 0.302 f1 0.276\n",
            "[W3] GRU Epoch 03 | train_loss 1.0806 acc 0.426 f1 0.423 || val_loss 1.0995 acc 0.305 f1 0.287\n",
            "[W3] GRU Epoch 04 | train_loss 1.0633 acc 0.446 f1 0.434 || val_loss 1.0819 acc 0.331 f1 0.296\n",
            "[W3] GRU Epoch 05 | train_loss 1.0291 acc 0.490 f1 0.485 || val_loss 1.0997 acc 0.327 f1 0.305\n",
            "[W3] GRU Epoch 06 | train_loss 0.9685 acc 0.530 f1 0.524 || val_loss 1.0702 acc 0.342 f1 0.299\n",
            "[W3] GRU Epoch 07 | train_loss 0.8822 acc 0.583 f1 0.575 || val_loss 1.1076 acc 0.370 f1 0.323\n",
            "[W3] GRU Epoch 08 | train_loss 0.8111 acc 0.616 f1 0.610 || val_loss 1.1417 acc 0.383 f1 0.329\n",
            "[W3] GRU Epoch 09 | train_loss 0.7543 acc 0.637 f1 0.632 || val_loss 1.1179 acc 0.405 f1 0.343\n",
            "[W3] GRU Epoch 10 | train_loss 0.7207 acc 0.653 f1 0.649 || val_loss 1.1546 acc 0.397 f1 0.332\n",
            "[W3] GRU Epoch 11 | train_loss 0.6842 acc 0.666 f1 0.662 || val_loss 1.1529 acc 0.403 f1 0.326\n",
            "[W3] GRU Epoch 12 | train_loss 0.6467 acc 0.693 f1 0.690 || val_loss 1.1711 acc 0.407 f1 0.342\n",
            "[W3] GRU Epoch 13 | train_loss 0.6132 acc 0.710 f1 0.708 || val_loss 1.2137 acc 0.405 f1 0.336\n",
            "[W3] GRU Epoch 14 | train_loss 0.5916 acc 0.715 f1 0.713 || val_loss 1.2476 acc 0.440 f1 0.361\n",
            "[W3] GRU Epoch 15 | train_loss 0.5669 acc 0.731 f1 0.729 || val_loss 1.3108 acc 0.372 f1 0.319\n",
            "[W3] GRU Epoch 16 | train_loss 0.5440 acc 0.738 f1 0.736 || val_loss 1.2923 acc 0.414 f1 0.346\n",
            "[W3] GRU Epoch 17 | train_loss 0.5203 acc 0.748 f1 0.747 || val_loss 1.3150 acc 0.409 f1 0.334\n",
            "[W3] GRU Epoch 18 | train_loss 0.4920 acc 0.768 f1 0.767 || val_loss 1.3502 acc 0.401 f1 0.319\n",
            "[W3] GRU Epoch 19 | train_loss 0.4781 acc 0.777 f1 0.776 || val_loss 1.3805 acc 0.414 f1 0.333\n",
            "[W3] GRU Epoch 20 | train_loss 0.4522 acc 0.791 f1 0.790 || val_loss 1.3960 acc 0.416 f1 0.330\n",
            "[W3] GRU Epoch 21 | train_loss 0.4360 acc 0.794 f1 0.793 || val_loss 1.4581 acc 0.405 f1 0.328\n",
            "[W3] GRU Epoch 22 | train_loss 0.4158 acc 0.806 f1 0.805 || val_loss 1.4796 acc 0.409 f1 0.331\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=41\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 935, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1041 acc 0.335 f1 0.173 || val_loss 1.1197 acc 0.132 f1 0.093\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0955 acc 0.376 f1 0.321 || val_loss 1.1039 acc 0.272 f1 0.269\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0881 acc 0.403 f1 0.399 || val_loss 1.0934 acc 0.329 f1 0.313\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0790 acc 0.424 f1 0.421 || val_loss 1.1019 acc 0.307 f1 0.303\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0575 acc 0.450 f1 0.433 || val_loss 1.0819 acc 0.335 f1 0.314\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0106 acc 0.507 f1 0.497 || val_loss 1.0818 acc 0.366 f1 0.334\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9229 acc 0.553 f1 0.545 || val_loss 1.1068 acc 0.364 f1 0.313\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8327 acc 0.599 f1 0.591 || val_loss 1.1134 acc 0.362 f1 0.296\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7808 acc 0.617 f1 0.610 || val_loss 1.1563 acc 0.383 f1 0.308\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7248 acc 0.639 f1 0.633 || val_loss 1.1953 acc 0.379 f1 0.301\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6883 acc 0.661 f1 0.657 || val_loss 1.1938 acc 0.387 f1 0.310\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6417 acc 0.691 f1 0.688 || val_loss 1.2343 acc 0.381 f1 0.298\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6162 acc 0.693 f1 0.690 || val_loss 1.2790 acc 0.381 f1 0.302\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5879 acc 0.713 f1 0.710 || val_loss 1.2831 acc 0.409 f1 0.310\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  41%|      | 41/100 [19:54<25:20, 25.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=42 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=42\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 977, np.int64(1): 938, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 977, np.int64(2): 977, np.int64(0): 977})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1385 acc 0.397 f1 0.393 || val_loss 1.1168 acc 0.346 f1 0.319\n",
            "[W3] ANN Epoch 02 | train_loss 0.9604 acc 0.507 f1 0.500 || val_loss 1.1318 acc 0.348 f1 0.311\n",
            "[W3] ANN Epoch 03 | train_loss 0.8517 acc 0.589 f1 0.581 || val_loss 1.1155 acc 0.368 f1 0.323\n",
            "[W3] ANN Epoch 04 | train_loss 0.7619 acc 0.629 f1 0.623 || val_loss 1.1377 acc 0.377 f1 0.312\n",
            "[W3] ANN Epoch 05 | train_loss 0.7163 acc 0.656 f1 0.652 || val_loss 1.1148 acc 0.391 f1 0.325\n",
            "[W3] ANN Epoch 06 | train_loss 0.6660 acc 0.689 f1 0.687 || val_loss 1.1415 acc 0.405 f1 0.327\n",
            "[W3] ANN Epoch 07 | train_loss 0.6237 acc 0.707 f1 0.706 || val_loss 1.1678 acc 0.418 f1 0.332\n",
            "[W3] ANN Epoch 08 | train_loss 0.5771 acc 0.736 f1 0.733 || val_loss 1.1880 acc 0.399 f1 0.317\n",
            "[W3] ANN Epoch 09 | train_loss 0.5520 acc 0.749 f1 0.747 || val_loss 1.2242 acc 0.401 f1 0.335\n",
            "[W3] ANN Epoch 10 | train_loss 0.5281 acc 0.761 f1 0.760 || val_loss 1.2373 acc 0.430 f1 0.355\n",
            "[W3] ANN Epoch 11 | train_loss 0.4917 acc 0.778 f1 0.779 || val_loss 1.2563 acc 0.403 f1 0.316\n",
            "[W3] ANN Epoch 12 | train_loss 0.4603 acc 0.793 f1 0.792 || val_loss 1.2823 acc 0.432 f1 0.362\n",
            "[W3] ANN Epoch 13 | train_loss 0.4342 acc 0.813 f1 0.812 || val_loss 1.3299 acc 0.422 f1 0.349\n",
            "[W3] ANN Epoch 14 | train_loss 0.4333 acc 0.816 f1 0.816 || val_loss 1.3637 acc 0.426 f1 0.355\n",
            "[W3] ANN Epoch 15 | train_loss 0.4372 acc 0.806 f1 0.805 || val_loss 1.3080 acc 0.442 f1 0.370\n",
            "[W3] ANN Epoch 16 | train_loss 0.4091 acc 0.824 f1 0.824 || val_loss 1.3277 acc 0.438 f1 0.350\n",
            "[W3] ANN Epoch 17 | train_loss 0.3906 acc 0.832 f1 0.832 || val_loss 1.3566 acc 0.434 f1 0.341\n",
            "[W3] ANN Epoch 18 | train_loss 0.3792 acc 0.840 f1 0.839 || val_loss 1.4147 acc 0.465 f1 0.385\n",
            "[W3] ANN Epoch 19 | train_loss 0.3596 acc 0.845 f1 0.846 || val_loss 1.4265 acc 0.436 f1 0.352\n",
            "[W3] ANN Epoch 20 | train_loss 0.3478 acc 0.854 f1 0.854 || val_loss 1.5082 acc 0.444 f1 0.346\n",
            "[W3] ANN Epoch 21 | train_loss 0.3448 acc 0.861 f1 0.861 || val_loss 1.5055 acc 0.430 f1 0.344\n",
            "[W3] ANN Epoch 22 | train_loss 0.3270 acc 0.869 f1 0.869 || val_loss 1.5590 acc 0.453 f1 0.352\n",
            "[W3] ANN Epoch 23 | train_loss 0.3290 acc 0.872 f1 0.873 || val_loss 1.5147 acc 0.449 f1 0.351\n",
            "[W3] ANN Epoch 24 | train_loss 0.3103 acc 0.882 f1 0.882 || val_loss 1.5609 acc 0.442 f1 0.353\n",
            "[W3] ANN Epoch 25 | train_loss 0.2942 acc 0.882 f1 0.882 || val_loss 1.5887 acc 0.436 f1 0.342\n",
            "[W3] ANN Epoch 26 | train_loss 0.2850 acc 0.889 f1 0.889 || val_loss 1.6388 acc 0.432 f1 0.347\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=42\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 977, np.int64(1): 938, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 977, np.int64(2): 977, np.int64(0): 977})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0788 acc 0.399 f1 0.401 || val_loss 1.0417 acc 0.418 f1 0.311\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9465 acc 0.549 f1 0.546 || val_loss 1.0483 acc 0.424 f1 0.337\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8017 acc 0.613 f1 0.610 || val_loss 1.1084 acc 0.428 f1 0.345\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7015 acc 0.678 f1 0.676 || val_loss 1.1759 acc 0.422 f1 0.344\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6184 acc 0.717 f1 0.715 || val_loss 1.2533 acc 0.385 f1 0.334\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5560 acc 0.747 f1 0.746 || val_loss 1.3277 acc 0.393 f1 0.320\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.4944 acc 0.785 f1 0.785 || val_loss 1.3656 acc 0.395 f1 0.325\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4385 acc 0.810 f1 0.810 || val_loss 1.5020 acc 0.387 f1 0.318\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.3901 acc 0.834 f1 0.834 || val_loss 1.5144 acc 0.385 f1 0.324\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3328 acc 0.871 f1 0.871 || val_loss 1.6518 acc 0.407 f1 0.337\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3094 acc 0.874 f1 0.874 || val_loss 1.7175 acc 0.389 f1 0.333\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=42\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 977, np.int64(1): 938, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 977, np.int64(2): 977, np.int64(0): 977})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0970 acc 0.375 f1 0.324 || val_loss 1.1005 acc 0.298 f1 0.257\n",
            "[W3] RNN Epoch 02 | train_loss 1.0802 acc 0.426 f1 0.416 || val_loss 1.0908 acc 0.327 f1 0.298\n",
            "[W3] RNN Epoch 03 | train_loss 1.0626 acc 0.446 f1 0.427 || val_loss 1.0843 acc 0.350 f1 0.308\n",
            "[W3] RNN Epoch 04 | train_loss 1.0402 acc 0.475 f1 0.463 || val_loss 1.0687 acc 0.372 f1 0.313\n",
            "[W3] RNN Epoch 05 | train_loss 1.0174 acc 0.483 f1 0.473 || val_loss 1.0680 acc 0.362 f1 0.316\n",
            "[W3] RNN Epoch 06 | train_loss 0.9923 acc 0.512 f1 0.499 || val_loss 1.0755 acc 0.358 f1 0.324\n",
            "[W3] RNN Epoch 07 | train_loss 0.9571 acc 0.531 f1 0.521 || val_loss 1.1060 acc 0.350 f1 0.316\n",
            "[W3] RNN Epoch 08 | train_loss 0.9386 acc 0.533 f1 0.522 || val_loss 1.0893 acc 0.340 f1 0.300\n",
            "[W3] RNN Epoch 09 | train_loss 0.9069 acc 0.566 f1 0.552 || val_loss 1.1026 acc 0.374 f1 0.346\n",
            "[W3] RNN Epoch 10 | train_loss 0.8832 acc 0.586 f1 0.576 || val_loss 1.0949 acc 0.366 f1 0.315\n",
            "[W3] RNN Epoch 11 | train_loss 0.8588 acc 0.591 f1 0.577 || val_loss 1.0932 acc 0.377 f1 0.322\n",
            "[W3] RNN Epoch 12 | train_loss 0.8332 acc 0.607 f1 0.597 || val_loss 1.1213 acc 0.370 f1 0.331\n",
            "[W3] RNN Epoch 13 | train_loss 0.8115 acc 0.620 f1 0.610 || val_loss 1.1401 acc 0.377 f1 0.339\n",
            "[W3] RNN Epoch 14 | train_loss 0.7943 acc 0.619 f1 0.607 || val_loss 1.1262 acc 0.387 f1 0.335\n",
            "[W3] RNN Epoch 15 | train_loss 0.7634 acc 0.635 f1 0.625 || val_loss 1.1346 acc 0.393 f1 0.334\n",
            "[W3] RNN Epoch 16 | train_loss 0.7387 acc 0.648 f1 0.639 || val_loss 1.1539 acc 0.397 f1 0.338\n",
            "[W3] RNN Epoch 17 | train_loss 0.7265 acc 0.660 f1 0.652 || val_loss 1.1705 acc 0.391 f1 0.330\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=42\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 977, np.int64(1): 938, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 977, np.int64(2): 977, np.int64(0): 977})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0989 acc 0.334 f1 0.257 || val_loss 1.0940 acc 0.389 f1 0.304\n",
            "[W3] GRU Epoch 02 | train_loss 1.0907 acc 0.384 f1 0.367 || val_loss 1.0989 acc 0.319 f1 0.300\n",
            "[W3] GRU Epoch 03 | train_loss 1.0801 acc 0.425 f1 0.421 || val_loss 1.0948 acc 0.356 f1 0.329\n",
            "[W3] GRU Epoch 04 | train_loss 1.0594 acc 0.451 f1 0.447 || val_loss 1.0880 acc 0.352 f1 0.314\n",
            "[W3] GRU Epoch 05 | train_loss 1.0226 acc 0.499 f1 0.491 || val_loss 1.1018 acc 0.352 f1 0.323\n",
            "[W3] GRU Epoch 06 | train_loss 0.9467 acc 0.553 f1 0.545 || val_loss 1.1362 acc 0.350 f1 0.323\n",
            "[W3] GRU Epoch 07 | train_loss 0.8601 acc 0.575 f1 0.568 || val_loss 1.1569 acc 0.358 f1 0.326\n",
            "[W3] GRU Epoch 08 | train_loss 0.8013 acc 0.616 f1 0.611 || val_loss 1.1115 acc 0.379 f1 0.317\n",
            "[W3] GRU Epoch 09 | train_loss 0.7658 acc 0.620 f1 0.617 || val_loss 1.1094 acc 0.405 f1 0.328\n",
            "[W3] GRU Epoch 10 | train_loss 0.7242 acc 0.645 f1 0.641 || val_loss 1.1382 acc 0.401 f1 0.324\n",
            "[W3] GRU Epoch 11 | train_loss 0.6879 acc 0.662 f1 0.659 || val_loss 1.1627 acc 0.407 f1 0.328\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=42\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 977, np.int64(1): 938, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 977, np.int64(2): 977, np.int64(0): 977})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1000 acc 0.341 f1 0.234 || val_loss 1.1010 acc 0.335 f1 0.244\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0942 acc 0.388 f1 0.371 || val_loss 1.0963 acc 0.331 f1 0.299\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0889 acc 0.402 f1 0.392 || val_loss 1.1023 acc 0.294 f1 0.274\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0751 acc 0.435 f1 0.431 || val_loss 1.0860 acc 0.391 f1 0.346\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0429 acc 0.477 f1 0.473 || val_loss 1.0964 acc 0.348 f1 0.314\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9640 acc 0.539 f1 0.527 || val_loss 1.0772 acc 0.348 f1 0.298\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8538 acc 0.582 f1 0.574 || val_loss 1.1194 acc 0.377 f1 0.312\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7905 acc 0.618 f1 0.612 || val_loss 1.1231 acc 0.391 f1 0.310\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7427 acc 0.638 f1 0.634 || val_loss 1.1968 acc 0.372 f1 0.314\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7150 acc 0.646 f1 0.641 || val_loss 1.1812 acc 0.407 f1 0.317\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6768 acc 0.673 f1 0.671 || val_loss 1.2101 acc 0.383 f1 0.302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  42%|     | 42/100 [20:16<23:54, 24.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 12 | train_loss 0.6509 acc 0.688 f1 0.686 || val_loss 1.2640 acc 0.366 f1 0.293\n",
            "Early stopping.\n",
            "LOSO: Test subject=43 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=43\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 932, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1687 acc 0.390 f1 0.387 || val_loss 1.1536 acc 0.321 f1 0.312\n",
            "[W3] ANN Epoch 02 | train_loss 0.9887 acc 0.510 f1 0.503 || val_loss 1.1399 acc 0.346 f1 0.331\n",
            "[W3] ANN Epoch 03 | train_loss 0.8743 acc 0.580 f1 0.574 || val_loss 1.1169 acc 0.364 f1 0.336\n",
            "[W3] ANN Epoch 04 | train_loss 0.7884 acc 0.629 f1 0.624 || val_loss 1.1125 acc 0.387 f1 0.342\n",
            "[W3] ANN Epoch 05 | train_loss 0.7440 acc 0.637 f1 0.633 || val_loss 1.1049 acc 0.405 f1 0.351\n",
            "[W3] ANN Epoch 06 | train_loss 0.6834 acc 0.671 f1 0.668 || val_loss 1.1179 acc 0.397 f1 0.345\n",
            "[W3] ANN Epoch 07 | train_loss 0.6401 acc 0.703 f1 0.703 || val_loss 1.1315 acc 0.414 f1 0.359\n",
            "[W3] ANN Epoch 08 | train_loss 0.5924 acc 0.729 f1 0.727 || val_loss 1.1120 acc 0.426 f1 0.365\n",
            "[W3] ANN Epoch 09 | train_loss 0.5749 acc 0.727 f1 0.725 || val_loss 1.1593 acc 0.424 f1 0.357\n",
            "[W3] ANN Epoch 10 | train_loss 0.5452 acc 0.751 f1 0.751 || val_loss 1.2011 acc 0.424 f1 0.353\n",
            "[W3] ANN Epoch 11 | train_loss 0.4972 acc 0.773 f1 0.772 || val_loss 1.2365 acc 0.432 f1 0.376\n",
            "[W3] ANN Epoch 12 | train_loss 0.5061 acc 0.778 f1 0.778 || val_loss 1.2261 acc 0.434 f1 0.359\n",
            "[W3] ANN Epoch 13 | train_loss 0.4664 acc 0.793 f1 0.792 || val_loss 1.2813 acc 0.418 f1 0.357\n",
            "[W3] ANN Epoch 14 | train_loss 0.4637 acc 0.795 f1 0.795 || val_loss 1.2671 acc 0.432 f1 0.353\n",
            "[W3] ANN Epoch 15 | train_loss 0.4447 acc 0.803 f1 0.802 || val_loss 1.2961 acc 0.434 f1 0.342\n",
            "[W3] ANN Epoch 16 | train_loss 0.4312 acc 0.812 f1 0.812 || val_loss 1.3209 acc 0.428 f1 0.357\n",
            "[W3] ANN Epoch 17 | train_loss 0.4154 acc 0.826 f1 0.826 || val_loss 1.3754 acc 0.430 f1 0.354\n",
            "[W3] ANN Epoch 18 | train_loss 0.3822 acc 0.845 f1 0.844 || val_loss 1.4079 acc 0.428 f1 0.348\n",
            "[W3] ANN Epoch 19 | train_loss 0.3695 acc 0.849 f1 0.849 || val_loss 1.4445 acc 0.418 f1 0.322\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=43\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 932, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0762 acc 0.398 f1 0.398 || val_loss 1.0538 acc 0.372 f1 0.255\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9452 acc 0.537 f1 0.531 || val_loss 1.0472 acc 0.416 f1 0.316\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8125 acc 0.600 f1 0.596 || val_loss 1.0898 acc 0.409 f1 0.337\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7140 acc 0.671 f1 0.669 || val_loss 1.1631 acc 0.412 f1 0.344\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6231 acc 0.713 f1 0.711 || val_loss 1.2468 acc 0.393 f1 0.336\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5528 acc 0.753 f1 0.751 || val_loss 1.2824 acc 0.440 f1 0.360\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.4918 acc 0.775 f1 0.774 || val_loss 1.3896 acc 0.422 f1 0.357\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4433 acc 0.804 f1 0.804 || val_loss 1.4522 acc 0.414 f1 0.347\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4013 acc 0.829 f1 0.829 || val_loss 1.5327 acc 0.455 f1 0.349\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3573 acc 0.852 f1 0.852 || val_loss 1.6146 acc 0.438 f1 0.352\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3184 acc 0.869 f1 0.868 || val_loss 1.7237 acc 0.444 f1 0.355\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2671 acc 0.903 f1 0.903 || val_loss 1.7691 acc 0.451 f1 0.371\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2490 acc 0.907 f1 0.907 || val_loss 1.8997 acc 0.442 f1 0.359\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2050 acc 0.926 f1 0.926 || val_loss 2.0222 acc 0.424 f1 0.335\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1707 acc 0.945 f1 0.945 || val_loss 2.1091 acc 0.412 f1 0.326\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1628 acc 0.942 f1 0.942 || val_loss 2.3128 acc 0.461 f1 0.356\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1436 acc 0.950 f1 0.950 || val_loss 2.2868 acc 0.455 f1 0.342\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1219 acc 0.960 f1 0.960 || val_loss 2.3384 acc 0.422 f1 0.334\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1184 acc 0.962 f1 0.962 || val_loss 2.3997 acc 0.434 f1 0.342\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.0877 acc 0.972 f1 0.972 || val_loss 2.5150 acc 0.449 f1 0.364\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=43\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 932, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0973 acc 0.342 f1 0.301 || val_loss 1.0981 acc 0.311 f1 0.286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0818 acc 0.404 f1 0.394 || val_loss 1.0870 acc 0.340 f1 0.323\n",
            "[W3] RNN Epoch 03 | train_loss 1.0639 acc 0.436 f1 0.426 || val_loss 1.0917 acc 0.313 f1 0.303\n",
            "[W3] RNN Epoch 04 | train_loss 1.0447 acc 0.463 f1 0.455 || val_loss 1.0786 acc 0.350 f1 0.328\n",
            "[W3] RNN Epoch 05 | train_loss 1.0246 acc 0.476 f1 0.467 || val_loss 1.0750 acc 0.350 f1 0.330\n",
            "[W3] RNN Epoch 06 | train_loss 0.9987 acc 0.510 f1 0.504 || val_loss 1.0908 acc 0.331 f1 0.316\n",
            "[W3] RNN Epoch 07 | train_loss 0.9723 acc 0.527 f1 0.518 || val_loss 1.0668 acc 0.379 f1 0.347\n",
            "[W3] RNN Epoch 08 | train_loss 0.9461 acc 0.535 f1 0.528 || val_loss 1.0638 acc 0.366 f1 0.336\n",
            "[W3] RNN Epoch 09 | train_loss 0.9252 acc 0.552 f1 0.543 || val_loss 1.0963 acc 0.364 f1 0.345\n",
            "[W3] RNN Epoch 10 | train_loss 0.8996 acc 0.570 f1 0.561 || val_loss 1.0745 acc 0.379 f1 0.352\n",
            "[W3] RNN Epoch 11 | train_loss 0.8737 acc 0.578 f1 0.569 || val_loss 1.0742 acc 0.387 f1 0.355\n",
            "[W3] RNN Epoch 12 | train_loss 0.8441 acc 0.601 f1 0.590 || val_loss 1.0752 acc 0.401 f1 0.369\n",
            "[W3] RNN Epoch 13 | train_loss 0.8206 acc 0.608 f1 0.598 || val_loss 1.0982 acc 0.377 f1 0.341\n",
            "[W3] RNN Epoch 14 | train_loss 0.7905 acc 0.627 f1 0.619 || val_loss 1.0829 acc 0.405 f1 0.356\n",
            "[W3] RNN Epoch 15 | train_loss 0.7673 acc 0.633 f1 0.625 || val_loss 1.0979 acc 0.381 f1 0.341\n",
            "[W3] RNN Epoch 16 | train_loss 0.7531 acc 0.636 f1 0.628 || val_loss 1.1220 acc 0.393 f1 0.346\n",
            "[W3] RNN Epoch 17 | train_loss 0.7299 acc 0.657 f1 0.649 || val_loss 1.1056 acc 0.387 f1 0.322\n",
            "[W3] RNN Epoch 18 | train_loss 0.6953 acc 0.674 f1 0.667 || val_loss 1.1329 acc 0.381 f1 0.325\n",
            "[W3] RNN Epoch 19 | train_loss 0.6768 acc 0.677 f1 0.669 || val_loss 1.1406 acc 0.383 f1 0.319\n",
            "[W3] RNN Epoch 20 | train_loss 0.6559 acc 0.690 f1 0.683 || val_loss 1.1584 acc 0.387 f1 0.326\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=43\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 932, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0982 acc 0.331 f1 0.322 || val_loss 1.0960 acc 0.333 f1 0.303\n",
            "[W3] GRU Epoch 02 | train_loss 1.0913 acc 0.388 f1 0.381 || val_loss 1.0901 acc 0.350 f1 0.313\n",
            "[W3] GRU Epoch 03 | train_loss 1.0830 acc 0.411 f1 0.403 || val_loss 1.0926 acc 0.325 f1 0.307\n",
            "[W3] GRU Epoch 04 | train_loss 1.0715 acc 0.439 f1 0.438 || val_loss 1.0827 acc 0.342 f1 0.318\n",
            "[W3] GRU Epoch 05 | train_loss 1.0515 acc 0.461 f1 0.455 || val_loss 1.0683 acc 0.368 f1 0.335\n",
            "[W3] GRU Epoch 06 | train_loss 1.0131 acc 0.504 f1 0.497 || val_loss 1.0738 acc 0.389 f1 0.351\n",
            "[W3] GRU Epoch 07 | train_loss 0.9536 acc 0.542 f1 0.535 || val_loss 1.0934 acc 0.374 f1 0.332\n",
            "[W3] GRU Epoch 08 | train_loss 0.8790 acc 0.578 f1 0.571 || val_loss 1.0806 acc 0.389 f1 0.331\n",
            "[W3] GRU Epoch 09 | train_loss 0.8235 acc 0.611 f1 0.606 || val_loss 1.1071 acc 0.418 f1 0.354\n",
            "[W3] GRU Epoch 10 | train_loss 0.7749 acc 0.629 f1 0.625 || val_loss 1.1575 acc 0.399 f1 0.340\n",
            "[W3] GRU Epoch 11 | train_loss 0.7287 acc 0.665 f1 0.661 || val_loss 1.1566 acc 0.416 f1 0.345\n",
            "[W3] GRU Epoch 12 | train_loss 0.6862 acc 0.675 f1 0.671 || val_loss 1.1923 acc 0.401 f1 0.333\n",
            "[W3] GRU Epoch 13 | train_loss 0.6542 acc 0.703 f1 0.700 || val_loss 1.2242 acc 0.409 f1 0.338\n",
            "[W3] GRU Epoch 14 | train_loss 0.6263 acc 0.706 f1 0.704 || val_loss 1.2452 acc 0.395 f1 0.315\n",
            "[W3] GRU Epoch 15 | train_loss 0.5979 acc 0.724 f1 0.722 || val_loss 1.2828 acc 0.412 f1 0.334\n",
            "[W3] GRU Epoch 16 | train_loss 0.5575 acc 0.748 f1 0.746 || val_loss 1.2940 acc 0.416 f1 0.321\n",
            "[W3] GRU Epoch 17 | train_loss 0.5388 acc 0.755 f1 0.754 || val_loss 1.3627 acc 0.422 f1 0.345\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=43\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 932, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1000 acc 0.334 f1 0.270 || val_loss 1.1047 acc 0.286 f1 0.235\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0951 acc 0.390 f1 0.367 || val_loss 1.0987 acc 0.313 f1 0.288\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0885 acc 0.407 f1 0.402 || val_loss 1.0858 acc 0.374 f1 0.346\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0762 acc 0.430 f1 0.429 || val_loss 1.0845 acc 0.321 f1 0.307\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0548 acc 0.452 f1 0.446 || val_loss 1.0874 acc 0.348 f1 0.337\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0101 acc 0.498 f1 0.486 || val_loss 1.1059 acc 0.350 f1 0.334\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9223 acc 0.555 f1 0.542 || val_loss 1.0731 acc 0.385 f1 0.324\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8314 acc 0.589 f1 0.582 || val_loss 1.0840 acc 0.418 f1 0.315\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7722 acc 0.628 f1 0.623 || val_loss 1.1054 acc 0.391 f1 0.335\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7301 acc 0.651 f1 0.646 || val_loss 1.1411 acc 0.397 f1 0.332\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6949 acc 0.667 f1 0.663 || val_loss 1.1547 acc 0.414 f1 0.349\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6541 acc 0.683 f1 0.679 || val_loss 1.2031 acc 0.383 f1 0.323\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6233 acc 0.696 f1 0.693 || val_loss 1.2159 acc 0.401 f1 0.318\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5896 acc 0.711 f1 0.707 || val_loss 1.2345 acc 0.414 f1 0.327\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5674 acc 0.717 f1 0.714 || val_loss 1.2678 acc 0.422 f1 0.353\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5330 acc 0.737 f1 0.735 || val_loss 1.3297 acc 0.403 f1 0.328\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5115 acc 0.753 f1 0.752 || val_loss 1.3439 acc 0.403 f1 0.326\n",
            "[W3] LSTM Epoch 18 | train_loss 0.4862 acc 0.759 f1 0.758 || val_loss 1.3761 acc 0.391 f1 0.325\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4648 acc 0.779 f1 0.777 || val_loss 1.4179 acc 0.407 f1 0.333\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4410 acc 0.786 f1 0.786 || val_loss 1.4757 acc 0.412 f1 0.356\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4229 acc 0.791 f1 0.790 || val_loss 1.4952 acc 0.416 f1 0.351\n",
            "[W3] LSTM Epoch 22 | train_loss 0.3992 acc 0.814 f1 0.813 || val_loss 1.5607 acc 0.418 f1 0.349\n",
            "[W3] LSTM Epoch 23 | train_loss 0.3744 acc 0.824 f1 0.823 || val_loss 1.6002 acc 0.436 f1 0.363\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3646 acc 0.838 f1 0.838 || val_loss 1.6505 acc 0.442 f1 0.369\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3408 acc 0.851 f1 0.851 || val_loss 1.7111 acc 0.442 f1 0.376\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3274 acc 0.852 f1 0.851 || val_loss 1.7524 acc 0.438 f1 0.374\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3046 acc 0.866 f1 0.866 || val_loss 1.8096 acc 0.442 f1 0.364\n",
            "[W3] LSTM Epoch 28 | train_loss 0.2872 acc 0.884 f1 0.884 || val_loss 1.8798 acc 0.430 f1 0.368\n",
            "[W3] LSTM Epoch 29 | train_loss 0.2713 acc 0.888 f1 0.888 || val_loss 1.9207 acc 0.438 f1 0.365\n",
            "[W3] LSTM Epoch 30 | train_loss 0.2575 acc 0.897 f1 0.897 || val_loss 1.9757 acc 0.440 f1 0.374\n",
            "[W3] LSTM Epoch 31 | train_loss 0.2354 acc 0.902 f1 0.902 || val_loss 2.0271 acc 0.434 f1 0.377\n",
            "[W3] LSTM Epoch 32 | train_loss 0.2150 acc 0.922 f1 0.922 || val_loss 2.0708 acc 0.438 f1 0.382\n",
            "[W3] LSTM Epoch 33 | train_loss 0.1943 acc 0.929 f1 0.929 || val_loss 2.1974 acc 0.428 f1 0.371\n",
            "[W3] LSTM Epoch 34 | train_loss 0.1808 acc 0.931 f1 0.931 || val_loss 2.2508 acc 0.422 f1 0.360\n",
            "[W3] LSTM Epoch 35 | train_loss 0.1633 acc 0.945 f1 0.945 || val_loss 2.3509 acc 0.426 f1 0.367\n",
            "[W3] LSTM Epoch 36 | train_loss 0.1387 acc 0.960 f1 0.960 || val_loss 2.4353 acc 0.438 f1 0.377\n",
            "[W3] LSTM Epoch 37 | train_loss 0.1257 acc 0.964 f1 0.964 || val_loss 2.5319 acc 0.426 f1 0.352\n",
            "[W3] LSTM Epoch 38 | train_loss 0.1049 acc 0.971 f1 0.971 || val_loss 2.6298 acc 0.420 f1 0.360\n",
            "[W3] LSTM Epoch 39 | train_loss 0.0919 acc 0.977 f1 0.977 || val_loss 2.7136 acc 0.422 f1 0.357\n",
            "[W3] LSTM Epoch 40 | train_loss 0.0817 acc 0.980 f1 0.980 || val_loss 2.8123 acc 0.424 f1 0.362\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  43%|     | 43/100 [20:49<25:43, 27.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=44 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=44\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 928, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1332 acc 0.400 f1 0.391 || val_loss 1.1711 acc 0.265 f1 0.258\n",
            "[W3] ANN Epoch 02 | train_loss 0.9724 acc 0.516 f1 0.506 || val_loss 1.1439 acc 0.290 f1 0.266\n",
            "[W3] ANN Epoch 03 | train_loss 0.8771 acc 0.557 f1 0.547 || val_loss 1.1128 acc 0.362 f1 0.317\n",
            "[W3] ANN Epoch 04 | train_loss 0.7924 acc 0.616 f1 0.610 || val_loss 1.1169 acc 0.391 f1 0.336\n",
            "[W3] ANN Epoch 05 | train_loss 0.7366 acc 0.642 f1 0.638 || val_loss 1.1375 acc 0.385 f1 0.315\n",
            "[W3] ANN Epoch 06 | train_loss 0.6821 acc 0.681 f1 0.678 || val_loss 1.1453 acc 0.432 f1 0.359\n",
            "[W3] ANN Epoch 07 | train_loss 0.6409 acc 0.692 f1 0.690 || val_loss 1.1469 acc 0.426 f1 0.338\n",
            "[W3] ANN Epoch 08 | train_loss 0.6011 acc 0.724 f1 0.722 || val_loss 1.1745 acc 0.440 f1 0.348\n",
            "[W3] ANN Epoch 09 | train_loss 0.5638 acc 0.742 f1 0.741 || val_loss 1.2216 acc 0.414 f1 0.323\n",
            "[W3] ANN Epoch 10 | train_loss 0.5494 acc 0.743 f1 0.742 || val_loss 1.2268 acc 0.420 f1 0.333\n",
            "[W3] ANN Epoch 11 | train_loss 0.5283 acc 0.759 f1 0.759 || val_loss 1.2501 acc 0.422 f1 0.339\n",
            "[W3] ANN Epoch 12 | train_loss 0.5108 acc 0.770 f1 0.768 || val_loss 1.2611 acc 0.447 f1 0.352\n",
            "[W3] ANN Epoch 13 | train_loss 0.4676 acc 0.785 f1 0.785 || val_loss 1.2675 acc 0.442 f1 0.360\n",
            "[W3] ANN Epoch 14 | train_loss 0.4586 acc 0.797 f1 0.796 || val_loss 1.3121 acc 0.461 f1 0.356\n",
            "[W3] ANN Epoch 15 | train_loss 0.4597 acc 0.798 f1 0.798 || val_loss 1.2941 acc 0.451 f1 0.355\n",
            "[W3] ANN Epoch 16 | train_loss 0.4464 acc 0.808 f1 0.806 || val_loss 1.3081 acc 0.471 f1 0.376\n",
            "[W3] ANN Epoch 17 | train_loss 0.4362 acc 0.806 f1 0.806 || val_loss 1.3219 acc 0.461 f1 0.374\n",
            "[W3] ANN Epoch 18 | train_loss 0.4385 acc 0.816 f1 0.815 || val_loss 1.3222 acc 0.442 f1 0.367\n",
            "[W3] ANN Epoch 19 | train_loss 0.3945 acc 0.822 f1 0.821 || val_loss 1.3377 acc 0.475 f1 0.366\n",
            "[W3] ANN Epoch 20 | train_loss 0.3896 acc 0.834 f1 0.834 || val_loss 1.3372 acc 0.436 f1 0.354\n",
            "[W3] ANN Epoch 21 | train_loss 0.3829 acc 0.836 f1 0.835 || val_loss 1.4130 acc 0.434 f1 0.348\n",
            "[W3] ANN Epoch 22 | train_loss 0.3560 acc 0.853 f1 0.853 || val_loss 1.4775 acc 0.434 f1 0.353\n",
            "[W3] ANN Epoch 23 | train_loss 0.3523 acc 0.859 f1 0.858 || val_loss 1.4512 acc 0.430 f1 0.348\n",
            "[W3] ANN Epoch 24 | train_loss 0.3615 acc 0.849 f1 0.849 || val_loss 1.4482 acc 0.447 f1 0.351\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=44\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 928, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0741 acc 0.397 f1 0.397 || val_loss 1.0195 acc 0.418 f1 0.289\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9258 acc 0.542 f1 0.537 || val_loss 1.0478 acc 0.416 f1 0.325\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8015 acc 0.615 f1 0.611 || val_loss 1.1680 acc 0.379 f1 0.327\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7164 acc 0.653 f1 0.652 || val_loss 1.1777 acc 0.385 f1 0.318\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6273 acc 0.713 f1 0.710 || val_loss 1.2628 acc 0.399 f1 0.339\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5668 acc 0.736 f1 0.735 || val_loss 1.2929 acc 0.405 f1 0.321\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5133 acc 0.767 f1 0.767 || val_loss 1.3531 acc 0.391 f1 0.321\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4770 acc 0.786 f1 0.785 || val_loss 1.3845 acc 0.440 f1 0.358\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4310 acc 0.807 f1 0.807 || val_loss 1.4949 acc 0.407 f1 0.333\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3895 acc 0.833 f1 0.833 || val_loss 1.5363 acc 0.407 f1 0.324\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3353 acc 0.860 f1 0.860 || val_loss 1.6694 acc 0.368 f1 0.301\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3127 acc 0.870 f1 0.870 || val_loss 1.6918 acc 0.412 f1 0.336\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3114 acc 0.876 f1 0.876 || val_loss 1.7009 acc 0.434 f1 0.346\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3261 acc 0.871 f1 0.871 || val_loss 1.7315 acc 0.428 f1 0.357\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2381 acc 0.912 f1 0.912 || val_loss 1.8422 acc 0.420 f1 0.337\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2346 acc 0.909 f1 0.909 || val_loss 1.9262 acc 0.414 f1 0.338\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=44\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 928, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1000 acc 0.353 f1 0.271 || val_loss 1.1085 acc 0.265 f1 0.264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0846 acc 0.406 f1 0.396 || val_loss 1.1045 acc 0.309 f1 0.293\n",
            "[W3] RNN Epoch 03 | train_loss 1.0699 acc 0.435 f1 0.431 || val_loss 1.1081 acc 0.309 f1 0.299\n",
            "[W3] RNN Epoch 04 | train_loss 1.0580 acc 0.432 f1 0.426 || val_loss 1.0847 acc 0.333 f1 0.299\n",
            "[W3] RNN Epoch 05 | train_loss 1.0381 acc 0.481 f1 0.478 || val_loss 1.0812 acc 0.354 f1 0.320\n",
            "[W3] RNN Epoch 06 | train_loss 1.0180 acc 0.492 f1 0.490 || val_loss 1.0766 acc 0.352 f1 0.314\n",
            "[W3] RNN Epoch 07 | train_loss 0.9971 acc 0.511 f1 0.508 || val_loss 1.0649 acc 0.374 f1 0.326\n",
            "[W3] RNN Epoch 08 | train_loss 0.9766 acc 0.523 f1 0.519 || val_loss 1.0809 acc 0.348 f1 0.315\n",
            "[W3] RNN Epoch 09 | train_loss 0.9492 acc 0.553 f1 0.548 || val_loss 1.0796 acc 0.356 f1 0.308\n",
            "[W3] RNN Epoch 10 | train_loss 0.9213 acc 0.568 f1 0.560 || val_loss 1.0674 acc 0.360 f1 0.312\n",
            "[W3] RNN Epoch 11 | train_loss 0.8989 acc 0.575 f1 0.569 || val_loss 1.0829 acc 0.362 f1 0.331\n",
            "[W3] RNN Epoch 12 | train_loss 0.8665 acc 0.591 f1 0.583 || val_loss 1.0744 acc 0.377 f1 0.319\n",
            "[W3] RNN Epoch 13 | train_loss 0.8363 acc 0.611 f1 0.606 || val_loss 1.0786 acc 0.366 f1 0.310\n",
            "[W3] RNN Epoch 14 | train_loss 0.8186 acc 0.625 f1 0.619 || val_loss 1.0925 acc 0.372 f1 0.310\n",
            "[W3] RNN Epoch 15 | train_loss 0.7882 acc 0.642 f1 0.636 || val_loss 1.1154 acc 0.372 f1 0.327\n",
            "[W3] RNN Epoch 16 | train_loss 0.7653 acc 0.646 f1 0.641 || val_loss 1.1109 acc 0.377 f1 0.311\n",
            "[W3] RNN Epoch 17 | train_loss 0.7364 acc 0.658 f1 0.649 || val_loss 1.1256 acc 0.385 f1 0.320\n",
            "[W3] RNN Epoch 18 | train_loss 0.7181 acc 0.667 f1 0.663 || val_loss 1.1480 acc 0.391 f1 0.325\n",
            "[W3] RNN Epoch 19 | train_loss 0.6845 acc 0.685 f1 0.678 || val_loss 1.1560 acc 0.395 f1 0.328\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=44\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 928, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1006 acc 0.329 f1 0.214 || val_loss 1.0931 acc 0.405 f1 0.295\n",
            "[W3] GRU Epoch 02 | train_loss 1.0917 acc 0.375 f1 0.362 || val_loss 1.0909 acc 0.358 f1 0.329\n",
            "[W3] GRU Epoch 03 | train_loss 1.0821 acc 0.404 f1 0.403 || val_loss 1.0943 acc 0.327 f1 0.314\n",
            "[W3] GRU Epoch 04 | train_loss 1.0675 acc 0.424 f1 0.419 || val_loss 1.0850 acc 0.344 f1 0.321\n",
            "[W3] GRU Epoch 05 | train_loss 1.0415 acc 0.468 f1 0.462 || val_loss 1.0963 acc 0.335 f1 0.318\n",
            "[W3] GRU Epoch 06 | train_loss 0.9912 acc 0.523 f1 0.514 || val_loss 1.0638 acc 0.395 f1 0.337\n",
            "[W3] GRU Epoch 07 | train_loss 0.9171 acc 0.556 f1 0.550 || val_loss 1.1344 acc 0.358 f1 0.325\n",
            "[W3] GRU Epoch 08 | train_loss 0.8517 acc 0.579 f1 0.572 || val_loss 1.1037 acc 0.374 f1 0.326\n",
            "[W3] GRU Epoch 09 | train_loss 0.8044 acc 0.613 f1 0.607 || val_loss 1.1381 acc 0.387 f1 0.335\n",
            "[W3] GRU Epoch 10 | train_loss 0.7654 acc 0.627 f1 0.623 || val_loss 1.1407 acc 0.401 f1 0.334\n",
            "[W3] GRU Epoch 11 | train_loss 0.7254 acc 0.652 f1 0.649 || val_loss 1.1566 acc 0.399 f1 0.336\n",
            "[W3] GRU Epoch 12 | train_loss 0.6921 acc 0.679 f1 0.676 || val_loss 1.1815 acc 0.391 f1 0.339\n",
            "[W3] GRU Epoch 13 | train_loss 0.6580 acc 0.679 f1 0.675 || val_loss 1.1657 acc 0.399 f1 0.327\n",
            "[W3] GRU Epoch 14 | train_loss 0.6286 acc 0.710 f1 0.707 || val_loss 1.1791 acc 0.414 f1 0.350\n",
            "[W3] GRU Epoch 15 | train_loss 0.6090 acc 0.709 f1 0.707 || val_loss 1.2443 acc 0.405 f1 0.336\n",
            "[W3] GRU Epoch 16 | train_loss 0.5841 acc 0.724 f1 0.723 || val_loss 1.2475 acc 0.395 f1 0.331\n",
            "[W3] GRU Epoch 17 | train_loss 0.5559 acc 0.736 f1 0.735 || val_loss 1.2744 acc 0.399 f1 0.335\n",
            "[W3] GRU Epoch 18 | train_loss 0.5267 acc 0.752 f1 0.751 || val_loss 1.2980 acc 0.416 f1 0.348\n",
            "[W3] GRU Epoch 19 | train_loss 0.5038 acc 0.755 f1 0.754 || val_loss 1.3571 acc 0.407 f1 0.345\n",
            "[W3] GRU Epoch 20 | train_loss 0.4841 acc 0.773 f1 0.772 || val_loss 1.3901 acc 0.403 f1 0.346\n",
            "[W3] GRU Epoch 21 | train_loss 0.4586 acc 0.785 f1 0.784 || val_loss 1.4264 acc 0.405 f1 0.347\n",
            "[W3] GRU Epoch 22 | train_loss 0.4427 acc 0.804 f1 0.804 || val_loss 1.4606 acc 0.414 f1 0.347\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=44\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 928, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0991 acc 0.342 f1 0.291 || val_loss 1.1046 acc 0.276 f1 0.275\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0939 acc 0.378 f1 0.370 || val_loss 1.1007 acc 0.305 f1 0.297\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0865 acc 0.401 f1 0.400 || val_loss 1.1029 acc 0.300 f1 0.294\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0765 acc 0.412 f1 0.407 || val_loss 1.0806 acc 0.350 f1 0.314\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0553 acc 0.459 f1 0.451 || val_loss 1.0949 acc 0.333 f1 0.315\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0115 acc 0.502 f1 0.492 || val_loss 1.0849 acc 0.356 f1 0.326\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9240 acc 0.555 f1 0.547 || val_loss 1.1138 acc 0.379 f1 0.321\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8422 acc 0.598 f1 0.591 || val_loss 1.1693 acc 0.346 f1 0.304\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7879 acc 0.623 f1 0.617 || val_loss 1.1661 acc 0.366 f1 0.305\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7447 acc 0.641 f1 0.636 || val_loss 1.2005 acc 0.377 f1 0.333\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7001 acc 0.663 f1 0.657 || val_loss 1.2038 acc 0.393 f1 0.312\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6642 acc 0.677 f1 0.674 || val_loss 1.2208 acc 0.393 f1 0.309\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6318 acc 0.689 f1 0.686 || val_loss 1.2601 acc 0.403 f1 0.326\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5959 acc 0.722 f1 0.720 || val_loss 1.3185 acc 0.405 f1 0.336\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5668 acc 0.730 f1 0.729 || val_loss 1.3539 acc 0.403 f1 0.322\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5418 acc 0.750 f1 0.748 || val_loss 1.4059 acc 0.395 f1 0.317\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5148 acc 0.766 f1 0.764 || val_loss 1.4306 acc 0.412 f1 0.323\n",
            "[W3] LSTM Epoch 18 | train_loss 0.4916 acc 0.772 f1 0.771 || val_loss 1.5014 acc 0.405 f1 0.322\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4679 acc 0.771 f1 0.770 || val_loss 1.5737 acc 0.399 f1 0.325\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4528 acc 0.782 f1 0.781 || val_loss 1.6150 acc 0.397 f1 0.323\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4225 acc 0.804 f1 0.804 || val_loss 1.6634 acc 0.420 f1 0.332\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4030 acc 0.812 f1 0.811 || val_loss 1.7132 acc 0.426 f1 0.338\n",
            "[W3] LSTM Epoch 23 | train_loss 0.3756 acc 0.824 f1 0.824 || val_loss 1.8096 acc 0.416 f1 0.332\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3633 acc 0.835 f1 0.834 || val_loss 1.8597 acc 0.418 f1 0.333\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3460 acc 0.841 f1 0.841 || val_loss 1.9135 acc 0.418 f1 0.328\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3307 acc 0.853 f1 0.853 || val_loss 1.9866 acc 0.432 f1 0.332\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3102 acc 0.862 f1 0.862 || val_loss 2.0415 acc 0.430 f1 0.336\n",
            "[W3] LSTM Epoch 28 | train_loss 0.2983 acc 0.880 f1 0.879 || val_loss 2.0899 acc 0.434 f1 0.348\n",
            "[W3] LSTM Epoch 29 | train_loss 0.2884 acc 0.877 f1 0.877 || val_loss 2.1679 acc 0.426 f1 0.338\n",
            "[W3] LSTM Epoch 30 | train_loss 0.2594 acc 0.901 f1 0.901 || val_loss 2.2324 acc 0.442 f1 0.349\n",
            "[W3] LSTM Epoch 31 | train_loss 0.2473 acc 0.899 f1 0.899 || val_loss 2.2977 acc 0.424 f1 0.335\n",
            "[W3] LSTM Epoch 32 | train_loss 0.2316 acc 0.915 f1 0.915 || val_loss 2.3575 acc 0.416 f1 0.325\n",
            "[W3] LSTM Epoch 33 | train_loss 0.2066 acc 0.921 f1 0.921 || val_loss 2.4489 acc 0.422 f1 0.335\n",
            "[W3] LSTM Epoch 34 | train_loss 0.1921 acc 0.927 f1 0.927 || val_loss 2.5315 acc 0.418 f1 0.336\n",
            "[W3] LSTM Epoch 35 | train_loss 0.1712 acc 0.942 f1 0.942 || val_loss 2.6092 acc 0.412 f1 0.328\n",
            "[W3] LSTM Epoch 36 | train_loss 0.1555 acc 0.949 f1 0.949 || val_loss 2.6875 acc 0.401 f1 0.327\n",
            "[W3] LSTM Epoch 37 | train_loss 0.1370 acc 0.959 f1 0.959 || val_loss 2.7866 acc 0.428 f1 0.339\n",
            "[W3] LSTM Epoch 38 | train_loss 0.1254 acc 0.960 f1 0.960 || val_loss 2.8536 acc 0.407 f1 0.329\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  44%|     | 44/100 [21:22<26:55, 28.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=45 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=45\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1043 acc 0.428 f1 0.425 || val_loss 1.1591 acc 0.263 f1 0.258\n",
            "[W3] ANN Epoch 02 | train_loss 0.9398 acc 0.528 f1 0.519 || val_loss 1.1315 acc 0.342 f1 0.316\n",
            "[W3] ANN Epoch 03 | train_loss 0.8447 acc 0.583 f1 0.573 || val_loss 1.1370 acc 0.360 f1 0.317\n",
            "[W3] ANN Epoch 04 | train_loss 0.7709 acc 0.622 f1 0.614 || val_loss 1.1642 acc 0.368 f1 0.315\n",
            "[W3] ANN Epoch 05 | train_loss 0.7221 acc 0.650 f1 0.645 || val_loss 1.1686 acc 0.393 f1 0.325\n",
            "[W3] ANN Epoch 06 | train_loss 0.6650 acc 0.680 f1 0.676 || val_loss 1.1786 acc 0.420 f1 0.355\n",
            "[W3] ANN Epoch 07 | train_loss 0.6137 acc 0.711 f1 0.708 || val_loss 1.2379 acc 0.374 f1 0.316\n",
            "[W3] ANN Epoch 08 | train_loss 0.6011 acc 0.717 f1 0.715 || val_loss 1.2438 acc 0.403 f1 0.335\n",
            "[W3] ANN Epoch 09 | train_loss 0.5873 acc 0.716 f1 0.713 || val_loss 1.2371 acc 0.414 f1 0.337\n",
            "[W3] ANN Epoch 10 | train_loss 0.5368 acc 0.747 f1 0.745 || val_loss 1.2611 acc 0.434 f1 0.340\n",
            "[W3] ANN Epoch 11 | train_loss 0.5306 acc 0.757 f1 0.756 || val_loss 1.2819 acc 0.422 f1 0.339\n",
            "[W3] ANN Epoch 12 | train_loss 0.5157 acc 0.765 f1 0.763 || val_loss 1.3178 acc 0.405 f1 0.327\n",
            "[W3] ANN Epoch 13 | train_loss 0.4931 acc 0.772 f1 0.771 || val_loss 1.3113 acc 0.434 f1 0.352\n",
            "[W3] ANN Epoch 14 | train_loss 0.4848 acc 0.779 f1 0.778 || val_loss 1.3408 acc 0.428 f1 0.338\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=45\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0739 acc 0.397 f1 0.400 || val_loss 1.0325 acc 0.416 f1 0.281\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9435 acc 0.533 f1 0.526 || val_loss 1.0241 acc 0.407 f1 0.312\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8284 acc 0.601 f1 0.596 || val_loss 1.1056 acc 0.366 f1 0.294\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7451 acc 0.647 f1 0.643 || val_loss 1.1190 acc 0.366 f1 0.315\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6823 acc 0.678 f1 0.676 || val_loss 1.1915 acc 0.370 f1 0.293\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6214 acc 0.705 f1 0.704 || val_loss 1.2122 acc 0.368 f1 0.312\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5561 acc 0.742 f1 0.739 || val_loss 1.2567 acc 0.393 f1 0.313\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5166 acc 0.777 f1 0.776 || val_loss 1.3100 acc 0.366 f1 0.299\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4748 acc 0.791 f1 0.790 || val_loss 1.3545 acc 0.387 f1 0.321\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4440 acc 0.806 f1 0.805 || val_loss 1.3677 acc 0.397 f1 0.323\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.4166 acc 0.822 f1 0.822 || val_loss 1.4898 acc 0.358 f1 0.313\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3655 acc 0.841 f1 0.841 || val_loss 1.5001 acc 0.393 f1 0.319\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3533 acc 0.849 f1 0.849 || val_loss 1.5714 acc 0.403 f1 0.322\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3230 acc 0.864 f1 0.863 || val_loss 1.5818 acc 0.418 f1 0.336\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.3265 acc 0.866 f1 0.866 || val_loss 1.6563 acc 0.393 f1 0.322\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2761 acc 0.899 f1 0.898 || val_loss 1.7864 acc 0.407 f1 0.341\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.2632 acc 0.896 f1 0.896 || val_loss 1.7945 acc 0.430 f1 0.360\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.2761 acc 0.888 f1 0.888 || val_loss 1.7957 acc 0.409 f1 0.352\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.2619 acc 0.901 f1 0.901 || val_loss 1.9058 acc 0.403 f1 0.333\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.2735 acc 0.894 f1 0.893 || val_loss 1.9777 acc 0.381 f1 0.340\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.2455 acc 0.909 f1 0.909 || val_loss 1.9116 acc 0.409 f1 0.330\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1907 acc 0.935 f1 0.935 || val_loss 1.8514 acc 0.399 f1 0.329\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.2249 acc 0.920 f1 0.920 || val_loss 1.9734 acc 0.412 f1 0.340\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.2217 acc 0.914 f1 0.914 || val_loss 2.0476 acc 0.387 f1 0.309\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.2330 acc 0.911 f1 0.911 || val_loss 2.1545 acc 0.399 f1 0.325\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=45\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0974 acc 0.354 f1 0.332 || val_loss 1.1042 acc 0.307 f1 0.293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0774 acc 0.425 f1 0.418 || val_loss 1.0938 acc 0.331 f1 0.310\n",
            "[W3] RNN Epoch 03 | train_loss 1.0634 acc 0.432 f1 0.420 || val_loss 1.1053 acc 0.302 f1 0.294\n",
            "[W3] RNN Epoch 04 | train_loss 1.0485 acc 0.450 f1 0.444 || val_loss 1.0906 acc 0.313 f1 0.296\n",
            "[W3] RNN Epoch 05 | train_loss 1.0350 acc 0.478 f1 0.474 || val_loss 1.1028 acc 0.333 f1 0.321\n",
            "[W3] RNN Epoch 06 | train_loss 1.0203 acc 0.477 f1 0.467 || val_loss 1.0758 acc 0.356 f1 0.314\n",
            "[W3] RNN Epoch 07 | train_loss 1.0056 acc 0.493 f1 0.487 || val_loss 1.0873 acc 0.352 f1 0.320\n",
            "[W3] RNN Epoch 08 | train_loss 0.9897 acc 0.506 f1 0.500 || val_loss 1.0976 acc 0.344 f1 0.316\n",
            "[W3] RNN Epoch 09 | train_loss 0.9659 acc 0.535 f1 0.529 || val_loss 1.0985 acc 0.352 f1 0.317\n",
            "[W3] RNN Epoch 10 | train_loss 0.9511 acc 0.540 f1 0.534 || val_loss 1.0927 acc 0.342 f1 0.307\n",
            "[W3] RNN Epoch 11 | train_loss 0.9276 acc 0.554 f1 0.544 || val_loss 1.1147 acc 0.344 f1 0.312\n",
            "[W3] RNN Epoch 12 | train_loss 0.9158 acc 0.569 f1 0.557 || val_loss 1.0996 acc 0.377 f1 0.334\n",
            "[W3] RNN Epoch 13 | train_loss 0.8852 acc 0.580 f1 0.573 || val_loss 1.0954 acc 0.397 f1 0.346\n",
            "[W3] RNN Epoch 14 | train_loss 0.8831 acc 0.579 f1 0.574 || val_loss 1.1120 acc 0.366 f1 0.321\n",
            "[W3] RNN Epoch 15 | train_loss 0.8566 acc 0.592 f1 0.584 || val_loss 1.1081 acc 0.387 f1 0.341\n",
            "[W3] RNN Epoch 16 | train_loss 0.8321 acc 0.621 f1 0.613 || val_loss 1.1171 acc 0.372 f1 0.335\n",
            "[W3] RNN Epoch 17 | train_loss 0.8233 acc 0.609 f1 0.601 || val_loss 1.1140 acc 0.383 f1 0.320\n",
            "[W3] RNN Epoch 18 | train_loss 0.8041 acc 0.630 f1 0.622 || val_loss 1.1274 acc 0.364 f1 0.315\n",
            "[W3] RNN Epoch 19 | train_loss 0.7760 acc 0.658 f1 0.651 || val_loss 1.1413 acc 0.377 f1 0.318\n",
            "[W3] RNN Epoch 20 | train_loss 0.7674 acc 0.635 f1 0.628 || val_loss 1.1532 acc 0.368 f1 0.312\n",
            "[W3] RNN Epoch 21 | train_loss 0.7609 acc 0.652 f1 0.647 || val_loss 1.1848 acc 0.333 f1 0.289\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=45\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1008 acc 0.339 f1 0.237 || val_loss 1.0937 acc 0.348 f1 0.267\n",
            "[W3] GRU Epoch 02 | train_loss 1.0900 acc 0.391 f1 0.387 || val_loss 1.0949 acc 0.342 f1 0.311\n",
            "[W3] GRU Epoch 03 | train_loss 1.0795 acc 0.417 f1 0.417 || val_loss 1.1082 acc 0.294 f1 0.283\n",
            "[W3] GRU Epoch 04 | train_loss 1.0619 acc 0.447 f1 0.442 || val_loss 1.0975 acc 0.335 f1 0.314\n",
            "[W3] GRU Epoch 05 | train_loss 1.0340 acc 0.467 f1 0.464 || val_loss 1.0968 acc 0.352 f1 0.331\n",
            "[W3] GRU Epoch 06 | train_loss 0.9870 acc 0.514 f1 0.505 || val_loss 1.0561 acc 0.366 f1 0.320\n",
            "[W3] GRU Epoch 07 | train_loss 0.9224 acc 0.562 f1 0.554 || val_loss 1.0696 acc 0.377 f1 0.322\n",
            "[W3] GRU Epoch 08 | train_loss 0.8580 acc 0.583 f1 0.577 || val_loss 1.0973 acc 0.348 f1 0.296\n",
            "[W3] GRU Epoch 09 | train_loss 0.8072 acc 0.605 f1 0.599 || val_loss 1.1109 acc 0.366 f1 0.319\n",
            "[W3] GRU Epoch 10 | train_loss 0.7701 acc 0.625 f1 0.619 || val_loss 1.1080 acc 0.393 f1 0.317\n",
            "[W3] GRU Epoch 11 | train_loss 0.7403 acc 0.638 f1 0.633 || val_loss 1.1140 acc 0.399 f1 0.322\n",
            "[W3] GRU Epoch 12 | train_loss 0.7100 acc 0.642 f1 0.638 || val_loss 1.1467 acc 0.393 f1 0.323\n",
            "[W3] GRU Epoch 13 | train_loss 0.6833 acc 0.658 f1 0.656 || val_loss 1.1589 acc 0.401 f1 0.344\n",
            "[W3] GRU Epoch 14 | train_loss 0.6591 acc 0.673 f1 0.670 || val_loss 1.1805 acc 0.391 f1 0.328\n",
            "[W3] GRU Epoch 15 | train_loss 0.6323 acc 0.691 f1 0.688 || val_loss 1.1917 acc 0.422 f1 0.344\n",
            "[W3] GRU Epoch 16 | train_loss 0.6543 acc 0.673 f1 0.671 || val_loss 1.2199 acc 0.405 f1 0.336\n",
            "[W3] GRU Epoch 17 | train_loss 0.5905 acc 0.714 f1 0.711 || val_loss 1.2408 acc 0.403 f1 0.334\n",
            "[W3] GRU Epoch 18 | train_loss 0.5679 acc 0.730 f1 0.728 || val_loss 1.2827 acc 0.407 f1 0.342\n",
            "[W3] GRU Epoch 19 | train_loss 0.5529 acc 0.733 f1 0.731 || val_loss 1.2913 acc 0.418 f1 0.355\n",
            "[W3] GRU Epoch 20 | train_loss 0.5363 acc 0.727 f1 0.726 || val_loss 1.3472 acc 0.391 f1 0.329\n",
            "[W3] GRU Epoch 21 | train_loss 0.5198 acc 0.736 f1 0.735 || val_loss 1.3705 acc 0.397 f1 0.334\n",
            "[W3] GRU Epoch 22 | train_loss 0.5041 acc 0.746 f1 0.744 || val_loss 1.4017 acc 0.407 f1 0.341\n",
            "[W3] GRU Epoch 23 | train_loss 0.4854 acc 0.757 f1 0.756 || val_loss 1.4300 acc 0.405 f1 0.338\n",
            "[W3] GRU Epoch 24 | train_loss 0.4715 acc 0.763 f1 0.762 || val_loss 1.4562 acc 0.414 f1 0.338\n",
            "[W3] GRU Epoch 25 | train_loss 0.4546 acc 0.782 f1 0.781 || val_loss 1.5271 acc 0.414 f1 0.345\n",
            "[W3] GRU Epoch 26 | train_loss 0.4424 acc 0.782 f1 0.782 || val_loss 1.5403 acc 0.420 f1 0.338\n",
            "[W3] GRU Epoch 27 | train_loss 0.4338 acc 0.784 f1 0.784 || val_loss 1.6051 acc 0.395 f1 0.330\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=45\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0989 acc 0.335 f1 0.233 || val_loss 1.0994 acc 0.335 f1 0.239\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0943 acc 0.373 f1 0.332 || val_loss 1.0958 acc 0.315 f1 0.291\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0888 acc 0.407 f1 0.391 || val_loss 1.0926 acc 0.311 f1 0.296\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0775 acc 0.438 f1 0.430 || val_loss 1.0771 acc 0.342 f1 0.311\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0547 acc 0.454 f1 0.451 || val_loss 1.0796 acc 0.317 f1 0.295\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0026 acc 0.514 f1 0.501 || val_loss 1.0909 acc 0.331 f1 0.304\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9092 acc 0.554 f1 0.541 || val_loss 1.0800 acc 0.370 f1 0.314\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8316 acc 0.588 f1 0.580 || val_loss 1.1480 acc 0.362 f1 0.319\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7910 acc 0.601 f1 0.594 || val_loss 1.1009 acc 0.403 f1 0.318\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7325 acc 0.626 f1 0.619 || val_loss 1.1705 acc 0.407 f1 0.338\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6973 acc 0.637 f1 0.631 || val_loss 1.2218 acc 0.387 f1 0.327\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6594 acc 0.664 f1 0.659 || val_loss 1.2192 acc 0.397 f1 0.333\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6636 acc 0.653 f1 0.649 || val_loss 1.2246 acc 0.414 f1 0.357\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6160 acc 0.679 f1 0.674 || val_loss 1.2783 acc 0.393 f1 0.327\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5857 acc 0.705 f1 0.703 || val_loss 1.3059 acc 0.401 f1 0.328\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5702 acc 0.707 f1 0.703 || val_loss 1.3562 acc 0.399 f1 0.318\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5501 acc 0.721 f1 0.718 || val_loss 1.3672 acc 0.418 f1 0.334\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5334 acc 0.720 f1 0.717 || val_loss 1.4225 acc 0.416 f1 0.348\n",
            "[W3] LSTM Epoch 19 | train_loss 0.5138 acc 0.733 f1 0.732 || val_loss 1.4694 acc 0.418 f1 0.334\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4898 acc 0.745 f1 0.744 || val_loss 1.5226 acc 0.412 f1 0.336\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4814 acc 0.751 f1 0.749 || val_loss 1.5443 acc 0.424 f1 0.350\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  45%|     | 45/100 [21:53<27:16, 29.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=46 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=46\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 932, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1554 acc 0.387 f1 0.384 || val_loss 1.1552 acc 0.321 f1 0.308\n",
            "[W3] ANN Epoch 02 | train_loss 0.9795 acc 0.512 f1 0.505 || val_loss 1.1429 acc 0.352 f1 0.325\n",
            "[W3] ANN Epoch 03 | train_loss 0.8960 acc 0.554 f1 0.547 || val_loss 1.1426 acc 0.354 f1 0.320\n",
            "[W3] ANN Epoch 04 | train_loss 0.8413 acc 0.582 f1 0.577 || val_loss 1.1488 acc 0.374 f1 0.336\n",
            "[W3] ANN Epoch 05 | train_loss 0.7836 acc 0.616 f1 0.611 || val_loss 1.1131 acc 0.372 f1 0.322\n",
            "[W3] ANN Epoch 06 | train_loss 0.7304 acc 0.649 f1 0.644 || val_loss 1.1298 acc 0.374 f1 0.316\n",
            "[W3] ANN Epoch 07 | train_loss 0.6907 acc 0.671 f1 0.667 || val_loss 1.1279 acc 0.374 f1 0.311\n",
            "[W3] ANN Epoch 08 | train_loss 0.6615 acc 0.688 f1 0.686 || val_loss 1.1566 acc 0.385 f1 0.310\n",
            "[W3] ANN Epoch 09 | train_loss 0.6376 acc 0.694 f1 0.692 || val_loss 1.1618 acc 0.405 f1 0.332\n",
            "[W3] ANN Epoch 10 | train_loss 0.6235 acc 0.703 f1 0.700 || val_loss 1.1467 acc 0.422 f1 0.350\n",
            "[W3] ANN Epoch 11 | train_loss 0.6012 acc 0.726 f1 0.724 || val_loss 1.1750 acc 0.405 f1 0.320\n",
            "[W3] ANN Epoch 12 | train_loss 0.5621 acc 0.742 f1 0.740 || val_loss 1.1811 acc 0.399 f1 0.331\n",
            "[W3] ANN Epoch 13 | train_loss 0.5622 acc 0.739 f1 0.738 || val_loss 1.2176 acc 0.401 f1 0.341\n",
            "[W3] ANN Epoch 14 | train_loss 0.5497 acc 0.744 f1 0.743 || val_loss 1.2022 acc 0.436 f1 0.361\n",
            "[W3] ANN Epoch 15 | train_loss 0.5511 acc 0.748 f1 0.748 || val_loss 1.2212 acc 0.422 f1 0.348\n",
            "[W3] ANN Epoch 16 | train_loss 0.5231 acc 0.759 f1 0.759 || val_loss 1.2125 acc 0.430 f1 0.355\n",
            "[W3] ANN Epoch 17 | train_loss 0.5291 acc 0.757 f1 0.756 || val_loss 1.2214 acc 0.455 f1 0.388\n",
            "[W3] ANN Epoch 18 | train_loss 0.5243 acc 0.768 f1 0.768 || val_loss 1.2466 acc 0.432 f1 0.364\n",
            "[W3] ANN Epoch 19 | train_loss 0.4873 acc 0.781 f1 0.780 || val_loss 1.2653 acc 0.416 f1 0.353\n",
            "[W3] ANN Epoch 20 | train_loss 0.4759 acc 0.783 f1 0.782 || val_loss 1.3025 acc 0.428 f1 0.352\n",
            "[W3] ANN Epoch 21 | train_loss 0.4440 acc 0.807 f1 0.807 || val_loss 1.3371 acc 0.426 f1 0.336\n",
            "[W3] ANN Epoch 22 | train_loss 0.4640 acc 0.799 f1 0.798 || val_loss 1.3617 acc 0.418 f1 0.350\n",
            "[W3] ANN Epoch 23 | train_loss 0.4471 acc 0.803 f1 0.802 || val_loss 1.3303 acc 0.418 f1 0.342\n",
            "[W3] ANN Epoch 24 | train_loss 0.4623 acc 0.798 f1 0.797 || val_loss 1.3528 acc 0.432 f1 0.352\n",
            "[W3] ANN Epoch 25 | train_loss 0.4394 acc 0.809 f1 0.808 || val_loss 1.3632 acc 0.416 f1 0.340\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=46\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 932, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0695 acc 0.408 f1 0.410 || val_loss 1.0240 acc 0.399 f1 0.317\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9457 acc 0.531 f1 0.527 || val_loss 1.0246 acc 0.420 f1 0.320\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8198 acc 0.593 f1 0.587 || val_loss 1.0784 acc 0.377 f1 0.301\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7269 acc 0.649 f1 0.647 || val_loss 1.1208 acc 0.401 f1 0.339\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6443 acc 0.700 f1 0.698 || val_loss 1.1714 acc 0.374 f1 0.315\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5944 acc 0.715 f1 0.712 || val_loss 1.2497 acc 0.387 f1 0.310\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5456 acc 0.740 f1 0.739 || val_loss 1.2977 acc 0.416 f1 0.362\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4854 acc 0.781 f1 0.780 || val_loss 1.3398 acc 0.399 f1 0.332\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4432 acc 0.799 f1 0.798 || val_loss 1.4237 acc 0.399 f1 0.293\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4364 acc 0.807 f1 0.807 || val_loss 1.4617 acc 0.424 f1 0.337\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3717 acc 0.838 f1 0.837 || val_loss 1.5165 acc 0.453 f1 0.362\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3321 acc 0.868 f1 0.868 || val_loss 1.5916 acc 0.444 f1 0.360\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3452 acc 0.860 f1 0.859 || val_loss 1.6643 acc 0.412 f1 0.352\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3090 acc 0.874 f1 0.873 || val_loss 1.7038 acc 0.414 f1 0.332\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2859 acc 0.878 f1 0.877 || val_loss 1.7919 acc 0.407 f1 0.336\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2776 acc 0.895 f1 0.895 || val_loss 1.7669 acc 0.420 f1 0.337\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.2105 acc 0.920 f1 0.920 || val_loss 1.9167 acc 0.436 f1 0.363\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.2567 acc 0.895 f1 0.895 || val_loss 1.9722 acc 0.416 f1 0.330\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.2612 acc 0.899 f1 0.899 || val_loss 1.9016 acc 0.409 f1 0.354\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.2285 acc 0.911 f1 0.912 || val_loss 1.8746 acc 0.424 f1 0.335\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.1920 acc 0.935 f1 0.935 || val_loss 1.9393 acc 0.430 f1 0.340\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1796 acc 0.934 f1 0.934 || val_loss 2.0651 acc 0.422 f1 0.349\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.1734 acc 0.939 f1 0.939 || val_loss 2.0590 acc 0.391 f1 0.305\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.1587 acc 0.942 f1 0.942 || val_loss 2.1420 acc 0.414 f1 0.333\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.1325 acc 0.954 f1 0.954 || val_loss 2.2757 acc 0.401 f1 0.325\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=46\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 932, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1020 acc 0.339 f1 0.279 || val_loss 1.0942 acc 0.348 f1 0.284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0832 acc 0.385 f1 0.384 || val_loss 1.0901 acc 0.335 f1 0.304\n",
            "[W3] RNN Epoch 03 | train_loss 1.0675 acc 0.416 f1 0.413 || val_loss 1.0943 acc 0.313 f1 0.296\n",
            "[W3] RNN Epoch 04 | train_loss 1.0531 acc 0.440 f1 0.429 || val_loss 1.0812 acc 0.344 f1 0.318\n",
            "[W3] RNN Epoch 05 | train_loss 1.0370 acc 0.471 f1 0.467 || val_loss 1.0938 acc 0.317 f1 0.302\n",
            "[W3] RNN Epoch 06 | train_loss 1.0173 acc 0.495 f1 0.487 || val_loss 1.0773 acc 0.360 f1 0.330\n",
            "[W3] RNN Epoch 07 | train_loss 0.9967 acc 0.512 f1 0.500 || val_loss 1.0692 acc 0.387 f1 0.349\n",
            "[W3] RNN Epoch 08 | train_loss 0.9675 acc 0.540 f1 0.533 || val_loss 1.0670 acc 0.387 f1 0.348\n",
            "[W3] RNN Epoch 09 | train_loss 0.9497 acc 0.551 f1 0.545 || val_loss 1.0729 acc 0.391 f1 0.362\n",
            "[W3] RNN Epoch 10 | train_loss 0.9204 acc 0.566 f1 0.553 || val_loss 1.0713 acc 0.403 f1 0.356\n",
            "[W3] RNN Epoch 11 | train_loss 0.8858 acc 0.587 f1 0.578 || val_loss 1.0740 acc 0.405 f1 0.366\n",
            "[W3] RNN Epoch 12 | train_loss 0.8583 acc 0.599 f1 0.590 || val_loss 1.0714 acc 0.393 f1 0.351\n",
            "[W3] RNN Epoch 13 | train_loss 0.8360 acc 0.608 f1 0.599 || val_loss 1.0775 acc 0.393 f1 0.359\n",
            "[W3] RNN Epoch 14 | train_loss 0.8093 acc 0.628 f1 0.620 || val_loss 1.0670 acc 0.407 f1 0.372\n",
            "[W3] RNN Epoch 15 | train_loss 0.7854 acc 0.628 f1 0.617 || val_loss 1.0564 acc 0.414 f1 0.365\n",
            "[W3] RNN Epoch 16 | train_loss 0.7545 acc 0.642 f1 0.634 || val_loss 1.0708 acc 0.412 f1 0.353\n",
            "[W3] RNN Epoch 17 | train_loss 0.7550 acc 0.647 f1 0.640 || val_loss 1.0700 acc 0.414 f1 0.366\n",
            "[W3] RNN Epoch 18 | train_loss 0.7278 acc 0.663 f1 0.657 || val_loss 1.0738 acc 0.426 f1 0.374\n",
            "[W3] RNN Epoch 19 | train_loss 0.7012 acc 0.669 f1 0.661 || val_loss 1.0803 acc 0.418 f1 0.372\n",
            "[W3] RNN Epoch 20 | train_loss 0.6889 acc 0.685 f1 0.679 || val_loss 1.0799 acc 0.426 f1 0.376\n",
            "[W3] RNN Epoch 21 | train_loss 0.6777 acc 0.683 f1 0.675 || val_loss 1.0985 acc 0.430 f1 0.376\n",
            "[W3] RNN Epoch 22 | train_loss 0.6487 acc 0.695 f1 0.689 || val_loss 1.0996 acc 0.418 f1 0.361\n",
            "[W3] RNN Epoch 23 | train_loss 0.6261 acc 0.709 f1 0.704 || val_loss 1.1293 acc 0.418 f1 0.362\n",
            "[W3] RNN Epoch 24 | train_loss 0.6196 acc 0.712 f1 0.708 || val_loss 1.1318 acc 0.420 f1 0.366\n",
            "[W3] RNN Epoch 25 | train_loss 0.6054 acc 0.710 f1 0.704 || val_loss 1.1384 acc 0.442 f1 0.382\n",
            "[W3] RNN Epoch 26 | train_loss 0.5892 acc 0.708 f1 0.703 || val_loss 1.1585 acc 0.430 f1 0.360\n",
            "[W3] RNN Epoch 27 | train_loss 0.5704 acc 0.719 f1 0.715 || val_loss 1.1665 acc 0.449 f1 0.385\n",
            "[W3] RNN Epoch 28 | train_loss 0.5688 acc 0.730 f1 0.726 || val_loss 1.1907 acc 0.426 f1 0.366\n",
            "[W3] RNN Epoch 29 | train_loss 0.5543 acc 0.739 f1 0.735 || val_loss 1.2095 acc 0.432 f1 0.379\n",
            "[W3] RNN Epoch 30 | train_loss 0.5330 acc 0.748 f1 0.745 || val_loss 1.2377 acc 0.418 f1 0.355\n",
            "[W3] RNN Epoch 31 | train_loss 0.5285 acc 0.746 f1 0.742 || val_loss 1.2474 acc 0.432 f1 0.362\n",
            "[W3] RNN Epoch 32 | train_loss 0.5313 acc 0.740 f1 0.737 || val_loss 1.2575 acc 0.418 f1 0.349\n",
            "[W3] RNN Epoch 33 | train_loss 0.5107 acc 0.754 f1 0.751 || val_loss 1.2646 acc 0.422 f1 0.349\n",
            "[W3] RNN Epoch 34 | train_loss 0.4914 acc 0.767 f1 0.764 || val_loss 1.2887 acc 0.442 f1 0.371\n",
            "[W3] RNN Epoch 35 | train_loss 0.4857 acc 0.761 f1 0.758 || val_loss 1.3185 acc 0.442 f1 0.365\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=46\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 932, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0978 acc 0.335 f1 0.289 || val_loss 1.0872 acc 0.418 f1 0.350\n",
            "[W3] GRU Epoch 02 | train_loss 1.0904 acc 0.387 f1 0.381 || val_loss 1.0899 acc 0.354 f1 0.317\n",
            "[W3] GRU Epoch 03 | train_loss 1.0830 acc 0.418 f1 0.417 || val_loss 1.0856 acc 0.350 f1 0.316\n",
            "[W3] GRU Epoch 04 | train_loss 1.0714 acc 0.441 f1 0.442 || val_loss 1.0951 acc 0.325 f1 0.309\n",
            "[W3] GRU Epoch 05 | train_loss 1.0523 acc 0.460 f1 0.458 || val_loss 1.0865 acc 0.354 f1 0.325\n",
            "[W3] GRU Epoch 06 | train_loss 1.0153 acc 0.504 f1 0.498 || val_loss 1.0718 acc 0.379 f1 0.342\n",
            "[W3] GRU Epoch 07 | train_loss 0.9576 acc 0.545 f1 0.539 || val_loss 1.0816 acc 0.352 f1 0.319\n",
            "[W3] GRU Epoch 08 | train_loss 0.8914 acc 0.576 f1 0.570 || val_loss 1.1025 acc 0.379 f1 0.347\n",
            "[W3] GRU Epoch 09 | train_loss 0.8374 acc 0.599 f1 0.592 || val_loss 1.1290 acc 0.372 f1 0.332\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=46\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 932, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0993 acc 0.332 f1 0.211 || val_loss 1.0951 acc 0.374 f1 0.283\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0938 acc 0.393 f1 0.387 || val_loss 1.1003 acc 0.300 f1 0.287\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0874 acc 0.413 f1 0.407 || val_loss 1.0995 acc 0.311 f1 0.294\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0744 acc 0.431 f1 0.414 || val_loss 1.0973 acc 0.321 f1 0.305\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0468 acc 0.470 f1 0.451 || val_loss 1.0781 acc 0.337 f1 0.306\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9819 acc 0.538 f1 0.529 || val_loss 1.1350 acc 0.307 f1 0.293\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8895 acc 0.566 f1 0.553 || val_loss 1.1111 acc 0.362 f1 0.327\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8168 acc 0.597 f1 0.590 || val_loss 1.0962 acc 0.395 f1 0.328\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7696 acc 0.621 f1 0.617 || val_loss 1.1635 acc 0.364 f1 0.311\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7380 acc 0.641 f1 0.634 || val_loss 1.1396 acc 0.403 f1 0.322\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6996 acc 0.660 f1 0.657 || val_loss 1.1922 acc 0.383 f1 0.323\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6631 acc 0.690 f1 0.684 || val_loss 1.2017 acc 0.391 f1 0.317\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6399 acc 0.699 f1 0.695 || val_loss 1.2156 acc 0.391 f1 0.308\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6142 acc 0.708 f1 0.705 || val_loss 1.2338 acc 0.409 f1 0.330\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5830 acc 0.732 f1 0.731 || val_loss 1.2623 acc 0.401 f1 0.320\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5636 acc 0.735 f1 0.733 || val_loss 1.3324 acc 0.391 f1 0.319\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5357 acc 0.743 f1 0.741 || val_loss 1.3140 acc 0.428 f1 0.349\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5261 acc 0.753 f1 0.753 || val_loss 1.3585 acc 0.414 f1 0.332\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4945 acc 0.769 f1 0.767 || val_loss 1.4183 acc 0.412 f1 0.335\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4742 acc 0.771 f1 0.770 || val_loss 1.4357 acc 0.440 f1 0.348\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4517 acc 0.791 f1 0.790 || val_loss 1.4988 acc 0.426 f1 0.343\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4348 acc 0.800 f1 0.800 || val_loss 1.5389 acc 0.428 f1 0.344\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4177 acc 0.811 f1 0.810 || val_loss 1.5478 acc 0.442 f1 0.354\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3923 acc 0.825 f1 0.825 || val_loss 1.6352 acc 0.434 f1 0.349\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3883 acc 0.830 f1 0.830 || val_loss 1.6519 acc 0.434 f1 0.348\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3656 acc 0.838 f1 0.837 || val_loss 1.7317 acc 0.420 f1 0.343\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3463 acc 0.855 f1 0.855 || val_loss 1.7738 acc 0.440 f1 0.357\n",
            "[W3] LSTM Epoch 28 | train_loss 0.3354 acc 0.856 f1 0.856 || val_loss 1.8289 acc 0.455 f1 0.369\n",
            "[W3] LSTM Epoch 29 | train_loss 0.3137 acc 0.866 f1 0.866 || val_loss 1.8812 acc 0.455 f1 0.364\n",
            "[W3] LSTM Epoch 30 | train_loss 0.3050 acc 0.873 f1 0.873 || val_loss 1.9288 acc 0.449 f1 0.358\n",
            "[W3] LSTM Epoch 31 | train_loss 0.2849 acc 0.885 f1 0.885 || val_loss 1.9827 acc 0.453 f1 0.362\n",
            "[W3] LSTM Epoch 32 | train_loss 0.2709 acc 0.895 f1 0.895 || val_loss 2.0714 acc 0.432 f1 0.342\n",
            "[W3] LSTM Epoch 33 | train_loss 0.2628 acc 0.899 f1 0.899 || val_loss 2.1309 acc 0.438 f1 0.356\n",
            "[W3] LSTM Epoch 34 | train_loss 0.2392 acc 0.911 f1 0.911 || val_loss 2.1747 acc 0.438 f1 0.352\n",
            "[W3] LSTM Epoch 35 | train_loss 0.2219 acc 0.920 f1 0.920 || val_loss 2.2300 acc 0.449 f1 0.364\n",
            "[W3] LSTM Epoch 36 | train_loss 0.2084 acc 0.929 f1 0.929 || val_loss 2.4095 acc 0.442 f1 0.359\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  46%|     | 46/100 [22:29<28:23, 31.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=47 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=47\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1256 acc 0.403 f1 0.398 || val_loss 1.1645 acc 0.274 f1 0.272\n",
            "[W3] ANN Epoch 02 | train_loss 0.9638 acc 0.515 f1 0.504 || val_loss 1.1478 acc 0.333 f1 0.315\n",
            "[W3] ANN Epoch 03 | train_loss 0.8812 acc 0.578 f1 0.570 || val_loss 1.1315 acc 0.350 f1 0.320\n",
            "[W3] ANN Epoch 04 | train_loss 0.7816 acc 0.621 f1 0.615 || val_loss 1.1052 acc 0.368 f1 0.319\n",
            "[W3] ANN Epoch 05 | train_loss 0.7308 acc 0.642 f1 0.637 || val_loss 1.1273 acc 0.389 f1 0.333\n",
            "[W3] ANN Epoch 06 | train_loss 0.6809 acc 0.670 f1 0.665 || val_loss 1.1327 acc 0.399 f1 0.330\n",
            "[W3] ANN Epoch 07 | train_loss 0.6323 acc 0.696 f1 0.694 || val_loss 1.1632 acc 0.397 f1 0.324\n",
            "[W3] ANN Epoch 08 | train_loss 0.6094 acc 0.707 f1 0.705 || val_loss 1.1946 acc 0.397 f1 0.332\n",
            "[W3] ANN Epoch 09 | train_loss 0.5901 acc 0.716 f1 0.714 || val_loss 1.1744 acc 0.412 f1 0.337\n",
            "[W3] ANN Epoch 10 | train_loss 0.5695 acc 0.737 f1 0.736 || val_loss 1.1970 acc 0.393 f1 0.326\n",
            "[W3] ANN Epoch 11 | train_loss 0.5523 acc 0.750 f1 0.749 || val_loss 1.2108 acc 0.409 f1 0.332\n",
            "[W3] ANN Epoch 12 | train_loss 0.5323 acc 0.756 f1 0.755 || val_loss 1.2289 acc 0.401 f1 0.327\n",
            "[W3] ANN Epoch 13 | train_loss 0.4982 acc 0.786 f1 0.785 || val_loss 1.2460 acc 0.430 f1 0.360\n",
            "[W3] ANN Epoch 14 | train_loss 0.4963 acc 0.771 f1 0.771 || val_loss 1.2574 acc 0.440 f1 0.355\n",
            "[W3] ANN Epoch 15 | train_loss 0.4668 acc 0.783 f1 0.782 || val_loss 1.3015 acc 0.424 f1 0.335\n",
            "[W3] ANN Epoch 16 | train_loss 0.4699 acc 0.795 f1 0.794 || val_loss 1.3087 acc 0.418 f1 0.329\n",
            "[W3] ANN Epoch 17 | train_loss 0.4334 acc 0.820 f1 0.820 || val_loss 1.3179 acc 0.422 f1 0.337\n",
            "[W3] ANN Epoch 18 | train_loss 0.4021 acc 0.825 f1 0.824 || val_loss 1.3885 acc 0.442 f1 0.352\n",
            "[W3] ANN Epoch 19 | train_loss 0.4148 acc 0.827 f1 0.825 || val_loss 1.3951 acc 0.426 f1 0.327\n",
            "[W3] ANN Epoch 20 | train_loss 0.3971 acc 0.831 f1 0.831 || val_loss 1.4234 acc 0.434 f1 0.342\n",
            "[W3] ANN Epoch 21 | train_loss 0.4044 acc 0.829 f1 0.829 || val_loss 1.4263 acc 0.442 f1 0.354\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=47\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0657 acc 0.420 f1 0.419 || val_loss 1.0262 acc 0.440 f1 0.362\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9191 acc 0.549 f1 0.543 || val_loss 1.0415 acc 0.414 f1 0.333\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8060 acc 0.602 f1 0.596 || val_loss 1.1076 acc 0.387 f1 0.336\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7157 acc 0.657 f1 0.656 || val_loss 1.1971 acc 0.360 f1 0.306\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6552 acc 0.689 f1 0.685 || val_loss 1.1991 acc 0.389 f1 0.320\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5936 acc 0.717 f1 0.715 || val_loss 1.2634 acc 0.377 f1 0.316\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5439 acc 0.745 f1 0.743 || val_loss 1.2938 acc 0.426 f1 0.355\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4766 acc 0.791 f1 0.790 || val_loss 1.3801 acc 0.426 f1 0.356\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4472 acc 0.796 f1 0.796 || val_loss 1.4769 acc 0.397 f1 0.334\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=47\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0989 acc 0.355 f1 0.289 || val_loss 1.1096 acc 0.302 f1 0.289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0828 acc 0.404 f1 0.381 || val_loss 1.1005 acc 0.292 f1 0.278\n",
            "[W3] RNN Epoch 03 | train_loss 1.0676 acc 0.429 f1 0.408 || val_loss 1.1118 acc 0.292 f1 0.283\n",
            "[W3] RNN Epoch 04 | train_loss 1.0555 acc 0.436 f1 0.428 || val_loss 1.1077 acc 0.317 f1 0.304\n",
            "[W3] RNN Epoch 05 | train_loss 1.0372 acc 0.462 f1 0.452 || val_loss 1.0961 acc 0.337 f1 0.318\n",
            "[W3] RNN Epoch 06 | train_loss 1.0169 acc 0.481 f1 0.471 || val_loss 1.1143 acc 0.329 f1 0.313\n",
            "[W3] RNN Epoch 07 | train_loss 0.9932 acc 0.510 f1 0.502 || val_loss 1.0745 acc 0.337 f1 0.304\n",
            "[W3] RNN Epoch 08 | train_loss 0.9644 acc 0.535 f1 0.529 || val_loss 1.0958 acc 0.329 f1 0.304\n",
            "[W3] RNN Epoch 09 | train_loss 0.9281 acc 0.549 f1 0.539 || val_loss 1.0775 acc 0.344 f1 0.315\n",
            "[W3] RNN Epoch 10 | train_loss 0.9063 acc 0.567 f1 0.558 || val_loss 1.0624 acc 0.368 f1 0.335\n",
            "[W3] RNN Epoch 11 | train_loss 0.8785 acc 0.585 f1 0.577 || val_loss 1.0530 acc 0.401 f1 0.350\n",
            "[W3] RNN Epoch 12 | train_loss 0.8466 acc 0.591 f1 0.583 || val_loss 1.0809 acc 0.389 f1 0.342\n",
            "[W3] RNN Epoch 13 | train_loss 0.8297 acc 0.601 f1 0.592 || val_loss 1.0991 acc 0.397 f1 0.355\n",
            "[W3] RNN Epoch 14 | train_loss 0.8032 acc 0.620 f1 0.612 || val_loss 1.0956 acc 0.401 f1 0.350\n",
            "[W3] RNN Epoch 15 | train_loss 0.7819 acc 0.620 f1 0.611 || val_loss 1.1027 acc 0.401 f1 0.357\n",
            "[W3] RNN Epoch 16 | train_loss 0.7526 acc 0.644 f1 0.637 || val_loss 1.1148 acc 0.395 f1 0.348\n",
            "[W3] RNN Epoch 17 | train_loss 0.7405 acc 0.639 f1 0.631 || val_loss 1.1196 acc 0.401 f1 0.352\n",
            "[W3] RNN Epoch 18 | train_loss 0.7082 acc 0.652 f1 0.645 || val_loss 1.1485 acc 0.399 f1 0.351\n",
            "[W3] RNN Epoch 19 | train_loss 0.6909 acc 0.665 f1 0.659 || val_loss 1.1581 acc 0.387 f1 0.342\n",
            "[W3] RNN Epoch 20 | train_loss 0.6710 acc 0.678 f1 0.673 || val_loss 1.1601 acc 0.383 f1 0.341\n",
            "[W3] RNN Epoch 21 | train_loss 0.6466 acc 0.692 f1 0.688 || val_loss 1.1955 acc 0.383 f1 0.335\n",
            "[W3] RNN Epoch 22 | train_loss 0.6363 acc 0.693 f1 0.688 || val_loss 1.1767 acc 0.395 f1 0.336\n",
            "[W3] RNN Epoch 23 | train_loss 0.6047 acc 0.716 f1 0.711 || val_loss 1.2161 acc 0.387 f1 0.335\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=47\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0999 acc 0.361 f1 0.316 || val_loss 1.1046 acc 0.265 f1 0.233\n",
            "[W3] GRU Epoch 02 | train_loss 1.0909 acc 0.376 f1 0.362 || val_loss 1.1028 acc 0.300 f1 0.285\n",
            "[W3] GRU Epoch 03 | train_loss 1.0816 acc 0.411 f1 0.398 || val_loss 1.1050 acc 0.290 f1 0.286\n",
            "[W3] GRU Epoch 04 | train_loss 1.0685 acc 0.424 f1 0.418 || val_loss 1.0964 acc 0.311 f1 0.300\n",
            "[W3] GRU Epoch 05 | train_loss 1.0484 acc 0.456 f1 0.452 || val_loss 1.1111 acc 0.298 f1 0.292\n",
            "[W3] GRU Epoch 06 | train_loss 1.0111 acc 0.501 f1 0.493 || val_loss 1.0894 acc 0.319 f1 0.300\n",
            "[W3] GRU Epoch 07 | train_loss 0.9481 acc 0.535 f1 0.524 || val_loss 1.0588 acc 0.377 f1 0.335\n",
            "[W3] GRU Epoch 08 | train_loss 0.8837 acc 0.570 f1 0.563 || val_loss 1.0623 acc 0.381 f1 0.334\n",
            "[W3] GRU Epoch 09 | train_loss 0.8200 acc 0.591 f1 0.584 || val_loss 1.0839 acc 0.374 f1 0.322\n",
            "[W3] GRU Epoch 10 | train_loss 0.7789 acc 0.626 f1 0.622 || val_loss 1.1133 acc 0.379 f1 0.329\n",
            "[W3] GRU Epoch 11 | train_loss 0.7346 acc 0.641 f1 0.636 || val_loss 1.1333 acc 0.377 f1 0.306\n",
            "[W3] GRU Epoch 12 | train_loss 0.7128 acc 0.645 f1 0.641 || val_loss 1.1524 acc 0.395 f1 0.331\n",
            "[W3] GRU Epoch 13 | train_loss 0.6655 acc 0.682 f1 0.679 || val_loss 1.2260 acc 0.374 f1 0.332\n",
            "[W3] GRU Epoch 14 | train_loss 0.6478 acc 0.675 f1 0.673 || val_loss 1.2125 acc 0.405 f1 0.354\n",
            "[W3] GRU Epoch 15 | train_loss 0.6165 acc 0.700 f1 0.697 || val_loss 1.2268 acc 0.379 f1 0.319\n",
            "[W3] GRU Epoch 16 | train_loss 0.5856 acc 0.715 f1 0.713 || val_loss 1.2538 acc 0.393 f1 0.327\n",
            "[W3] GRU Epoch 17 | train_loss 0.5594 acc 0.726 f1 0.723 || val_loss 1.2955 acc 0.381 f1 0.321\n",
            "[W3] GRU Epoch 18 | train_loss 0.5387 acc 0.739 f1 0.738 || val_loss 1.3404 acc 0.395 f1 0.343\n",
            "[W3] GRU Epoch 19 | train_loss 0.5146 acc 0.747 f1 0.746 || val_loss 1.3432 acc 0.397 f1 0.323\n",
            "[W3] GRU Epoch 20 | train_loss 0.4839 acc 0.769 f1 0.768 || val_loss 1.4253 acc 0.381 f1 0.329\n",
            "[W3] GRU Epoch 21 | train_loss 0.4677 acc 0.775 f1 0.774 || val_loss 1.4600 acc 0.385 f1 0.320\n",
            "[W3] GRU Epoch 22 | train_loss 0.4523 acc 0.778 f1 0.777 || val_loss 1.4856 acc 0.420 f1 0.347\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=47\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0998 acc 0.333 f1 0.212 || val_loss 1.0896 acc 0.412 f1 0.272\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0944 acc 0.365 f1 0.355 || val_loss 1.0957 acc 0.305 f1 0.285\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0865 acc 0.408 f1 0.407 || val_loss 1.1006 acc 0.313 f1 0.308\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0722 acc 0.431 f1 0.420 || val_loss 1.1089 acc 0.300 f1 0.298\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0420 acc 0.467 f1 0.457 || val_loss 1.0864 acc 0.335 f1 0.314\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9636 acc 0.537 f1 0.528 || val_loss 1.1491 acc 0.313 f1 0.293\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8800 acc 0.563 f1 0.554 || val_loss 1.1478 acc 0.340 f1 0.293\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8094 acc 0.602 f1 0.596 || val_loss 1.1522 acc 0.370 f1 0.304\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7687 acc 0.616 f1 0.610 || val_loss 1.1585 acc 0.374 f1 0.310\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7243 acc 0.643 f1 0.636 || val_loss 1.2016 acc 0.372 f1 0.316\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6901 acc 0.663 f1 0.659 || val_loss 1.2037 acc 0.391 f1 0.303\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6551 acc 0.670 f1 0.666 || val_loss 1.2543 acc 0.385 f1 0.309\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6240 acc 0.686 f1 0.682 || val_loss 1.2901 acc 0.403 f1 0.324\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6003 acc 0.703 f1 0.699 || val_loss 1.3185 acc 0.426 f1 0.335\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5837 acc 0.711 f1 0.708 || val_loss 1.3497 acc 0.397 f1 0.333\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5552 acc 0.720 f1 0.717 || val_loss 1.3879 acc 0.422 f1 0.335\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5265 acc 0.736 f1 0.735 || val_loss 1.4376 acc 0.426 f1 0.343\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5029 acc 0.756 f1 0.756 || val_loss 1.4495 acc 0.432 f1 0.347\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4820 acc 0.768 f1 0.767 || val_loss 1.4994 acc 0.422 f1 0.331\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4677 acc 0.774 f1 0.773 || val_loss 1.5466 acc 0.395 f1 0.319\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4394 acc 0.791 f1 0.791 || val_loss 1.6145 acc 0.420 f1 0.343\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4262 acc 0.790 f1 0.789 || val_loss 1.6340 acc 0.434 f1 0.347\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4065 acc 0.801 f1 0.800 || val_loss 1.7261 acc 0.418 f1 0.341\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3901 acc 0.813 f1 0.813 || val_loss 1.7524 acc 0.426 f1 0.343\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3754 acc 0.826 f1 0.826 || val_loss 1.8089 acc 0.416 f1 0.329\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3760 acc 0.830 f1 0.830 || val_loss 1.8592 acc 0.434 f1 0.344\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  47%|     | 47/100 [22:58<27:05, 30.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=48 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=48\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 927, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1509 acc 0.397 f1 0.396 || val_loss 1.1707 acc 0.280 f1 0.278\n",
            "[W3] ANN Epoch 02 | train_loss 0.9744 acc 0.521 f1 0.513 || val_loss 1.1560 acc 0.325 f1 0.309\n",
            "[W3] ANN Epoch 03 | train_loss 0.8998 acc 0.561 f1 0.555 || val_loss 1.1576 acc 0.333 f1 0.305\n",
            "[W3] ANN Epoch 04 | train_loss 0.8178 acc 0.598 f1 0.592 || val_loss 1.1518 acc 0.337 f1 0.309\n",
            "[W3] ANN Epoch 05 | train_loss 0.7441 acc 0.634 f1 0.629 || val_loss 1.1322 acc 0.377 f1 0.323\n",
            "[W3] ANN Epoch 06 | train_loss 0.6925 acc 0.677 f1 0.674 || val_loss 1.1293 acc 0.391 f1 0.327\n",
            "[W3] ANN Epoch 07 | train_loss 0.6469 acc 0.692 f1 0.690 || val_loss 1.1586 acc 0.405 f1 0.343\n",
            "[W3] ANN Epoch 08 | train_loss 0.6156 acc 0.702 f1 0.700 || val_loss 1.1476 acc 0.409 f1 0.342\n",
            "[W3] ANN Epoch 09 | train_loss 0.6141 acc 0.714 f1 0.712 || val_loss 1.1682 acc 0.389 f1 0.323\n",
            "[W3] ANN Epoch 10 | train_loss 0.5649 acc 0.742 f1 0.741 || val_loss 1.1631 acc 0.442 f1 0.373\n",
            "[W3] ANN Epoch 11 | train_loss 0.5092 acc 0.772 f1 0.771 || val_loss 1.2234 acc 0.416 f1 0.349\n",
            "[W3] ANN Epoch 12 | train_loss 0.5432 acc 0.754 f1 0.753 || val_loss 1.2393 acc 0.409 f1 0.338\n",
            "[W3] ANN Epoch 13 | train_loss 0.5016 acc 0.775 f1 0.775 || val_loss 1.2516 acc 0.434 f1 0.348\n",
            "[W3] ANN Epoch 14 | train_loss 0.4835 acc 0.786 f1 0.784 || val_loss 1.2617 acc 0.438 f1 0.363\n",
            "[W3] ANN Epoch 15 | train_loss 0.4571 acc 0.798 f1 0.797 || val_loss 1.2847 acc 0.414 f1 0.347\n",
            "[W3] ANN Epoch 16 | train_loss 0.4563 acc 0.798 f1 0.797 || val_loss 1.3248 acc 0.422 f1 0.345\n",
            "[W3] ANN Epoch 17 | train_loss 0.4315 acc 0.814 f1 0.814 || val_loss 1.3192 acc 0.436 f1 0.356\n",
            "[W3] ANN Epoch 18 | train_loss 0.4288 acc 0.820 f1 0.820 || val_loss 1.3526 acc 0.447 f1 0.364\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=48\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 927, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0857 acc 0.380 f1 0.375 || val_loss 1.0486 acc 0.409 f1 0.330\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9678 acc 0.522 f1 0.519 || val_loss 1.0315 acc 0.391 f1 0.310\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8358 acc 0.598 f1 0.593 || val_loss 1.1288 acc 0.366 f1 0.305\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7295 acc 0.649 f1 0.646 || val_loss 1.1817 acc 0.348 f1 0.289\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6540 acc 0.689 f1 0.687 || val_loss 1.2623 acc 0.362 f1 0.303\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5923 acc 0.733 f1 0.732 || val_loss 1.4205 acc 0.346 f1 0.290\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5460 acc 0.748 f1 0.745 || val_loss 1.3791 acc 0.377 f1 0.312\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4789 acc 0.792 f1 0.792 || val_loss 1.4760 acc 0.362 f1 0.293\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4402 acc 0.805 f1 0.804 || val_loss 1.6070 acc 0.344 f1 0.270\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=48\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 927, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1011 acc 0.355 f1 0.298 || val_loss 1.0932 acc 0.372 f1 0.295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0828 acc 0.416 f1 0.400 || val_loss 1.0901 acc 0.348 f1 0.319\n",
            "[W3] RNN Epoch 03 | train_loss 1.0649 acc 0.443 f1 0.440 || val_loss 1.0867 acc 0.323 f1 0.304\n",
            "[W3] RNN Epoch 04 | train_loss 1.0443 acc 0.472 f1 0.461 || val_loss 1.0915 acc 0.319 f1 0.305\n",
            "[W3] RNN Epoch 05 | train_loss 1.0227 acc 0.497 f1 0.488 || val_loss 1.0712 acc 0.358 f1 0.326\n",
            "[W3] RNN Epoch 06 | train_loss 1.0003 acc 0.512 f1 0.504 || val_loss 1.0667 acc 0.370 f1 0.336\n",
            "[W3] RNN Epoch 07 | train_loss 0.9729 acc 0.529 f1 0.522 || val_loss 1.0951 acc 0.327 f1 0.304\n",
            "[W3] RNN Epoch 08 | train_loss 0.9439 acc 0.547 f1 0.540 || val_loss 1.0973 acc 0.333 f1 0.311\n",
            "[W3] RNN Epoch 09 | train_loss 0.9153 acc 0.566 f1 0.556 || val_loss 1.0812 acc 0.354 f1 0.315\n",
            "[W3] RNN Epoch 10 | train_loss 0.8893 acc 0.575 f1 0.566 || val_loss 1.0726 acc 0.364 f1 0.314\n",
            "[W3] RNN Epoch 11 | train_loss 0.8664 acc 0.591 f1 0.582 || val_loss 1.0838 acc 0.360 f1 0.315\n",
            "[W3] RNN Epoch 12 | train_loss 0.8390 acc 0.595 f1 0.585 || val_loss 1.0756 acc 0.387 f1 0.322\n",
            "[W3] RNN Epoch 13 | train_loss 0.8185 acc 0.615 f1 0.605 || val_loss 1.0767 acc 0.383 f1 0.315\n",
            "[W3] RNN Epoch 14 | train_loss 0.7832 acc 0.623 f1 0.615 || val_loss 1.1053 acc 0.368 f1 0.316\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=48\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 927, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1007 acc 0.346 f1 0.274 || val_loss 1.1092 acc 0.228 f1 0.228\n",
            "[W3] GRU Epoch 02 | train_loss 1.0918 acc 0.388 f1 0.379 || val_loss 1.1020 acc 0.296 f1 0.273\n",
            "[W3] GRU Epoch 03 | train_loss 1.0859 acc 0.411 f1 0.406 || val_loss 1.1050 acc 0.300 f1 0.289\n",
            "[W3] GRU Epoch 04 | train_loss 1.0769 acc 0.426 f1 0.423 || val_loss 1.1019 acc 0.317 f1 0.300\n",
            "[W3] GRU Epoch 05 | train_loss 1.0655 acc 0.433 f1 0.430 || val_loss 1.0908 acc 0.348 f1 0.320\n",
            "[W3] GRU Epoch 06 | train_loss 1.0451 acc 0.471 f1 0.470 || val_loss 1.1006 acc 0.348 f1 0.329\n",
            "[W3] GRU Epoch 07 | train_loss 1.0096 acc 0.508 f1 0.501 || val_loss 1.1094 acc 0.319 f1 0.292\n",
            "[W3] GRU Epoch 08 | train_loss 0.9389 acc 0.561 f1 0.554 || val_loss 1.1038 acc 0.358 f1 0.307\n",
            "[W3] GRU Epoch 09 | train_loss 0.8638 acc 0.596 f1 0.590 || val_loss 1.0825 acc 0.356 f1 0.295\n",
            "[W3] GRU Epoch 10 | train_loss 0.8075 acc 0.619 f1 0.614 || val_loss 1.1124 acc 0.366 f1 0.312\n",
            "[W3] GRU Epoch 11 | train_loss 0.7520 acc 0.646 f1 0.642 || val_loss 1.1381 acc 0.364 f1 0.310\n",
            "[W3] GRU Epoch 12 | train_loss 0.7122 acc 0.673 f1 0.671 || val_loss 1.1784 acc 0.397 f1 0.338\n",
            "[W3] GRU Epoch 13 | train_loss 0.6721 acc 0.698 f1 0.696 || val_loss 1.1795 acc 0.381 f1 0.314\n",
            "[W3] GRU Epoch 14 | train_loss 0.6449 acc 0.702 f1 0.701 || val_loss 1.2034 acc 0.397 f1 0.322\n",
            "[W3] GRU Epoch 15 | train_loss 0.6110 acc 0.721 f1 0.720 || val_loss 1.2581 acc 0.379 f1 0.320\n",
            "[W3] GRU Epoch 16 | train_loss 0.5846 acc 0.736 f1 0.735 || val_loss 1.2515 acc 0.381 f1 0.305\n",
            "[W3] GRU Epoch 17 | train_loss 0.5574 acc 0.744 f1 0.743 || val_loss 1.3188 acc 0.362 f1 0.300\n",
            "[W3] GRU Epoch 18 | train_loss 0.5305 acc 0.752 f1 0.751 || val_loss 1.3358 acc 0.387 f1 0.318\n",
            "[W3] GRU Epoch 19 | train_loss 0.4998 acc 0.780 f1 0.779 || val_loss 1.3635 acc 0.405 f1 0.340\n",
            "[W3] GRU Epoch 20 | train_loss 0.4815 acc 0.781 f1 0.781 || val_loss 1.3856 acc 0.393 f1 0.317\n",
            "[W3] GRU Epoch 21 | train_loss 0.4615 acc 0.795 f1 0.794 || val_loss 1.4323 acc 0.409 f1 0.339\n",
            "[W3] GRU Epoch 22 | train_loss 0.4314 acc 0.805 f1 0.805 || val_loss 1.5085 acc 0.391 f1 0.329\n",
            "[W3] GRU Epoch 23 | train_loss 0.4051 acc 0.826 f1 0.825 || val_loss 1.5188 acc 0.399 f1 0.331\n",
            "[W3] GRU Epoch 24 | train_loss 0.3890 acc 0.832 f1 0.832 || val_loss 1.5738 acc 0.409 f1 0.337\n",
            "[W3] GRU Epoch 25 | train_loss 0.3684 acc 0.850 f1 0.850 || val_loss 1.6556 acc 0.393 f1 0.331\n",
            "[W3] GRU Epoch 26 | train_loss 0.3437 acc 0.854 f1 0.854 || val_loss 1.7091 acc 0.395 f1 0.332\n",
            "[W3] GRU Epoch 27 | train_loss 0.3321 acc 0.864 f1 0.864 || val_loss 1.7435 acc 0.405 f1 0.336\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=48\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 927, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1011 acc 0.341 f1 0.247 || val_loss 1.1127 acc 0.156 f1 0.128\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0957 acc 0.371 f1 0.315 || val_loss 1.1039 acc 0.259 f1 0.241\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0897 acc 0.396 f1 0.385 || val_loss 1.0975 acc 0.292 f1 0.282\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0773 acc 0.428 f1 0.427 || val_loss 1.0914 acc 0.315 f1 0.298\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0478 acc 0.463 f1 0.458 || val_loss 1.0650 acc 0.358 f1 0.315\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9728 acc 0.533 f1 0.519 || val_loss 1.1108 acc 0.348 f1 0.318\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8695 acc 0.576 f1 0.568 || val_loss 1.0745 acc 0.405 f1 0.319\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8117 acc 0.608 f1 0.604 || val_loss 1.0887 acc 0.385 f1 0.313\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7614 acc 0.632 f1 0.627 || val_loss 1.1102 acc 0.391 f1 0.317\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7174 acc 0.653 f1 0.649 || val_loss 1.1411 acc 0.385 f1 0.312\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6952 acc 0.662 f1 0.658 || val_loss 1.2029 acc 0.364 f1 0.300\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6668 acc 0.670 f1 0.665 || val_loss 1.2325 acc 0.370 f1 0.308\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6301 acc 0.688 f1 0.685 || val_loss 1.2233 acc 0.399 f1 0.327\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6160 acc 0.686 f1 0.683 || val_loss 1.2689 acc 0.393 f1 0.318\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5879 acc 0.708 f1 0.707 || val_loss 1.3113 acc 0.409 f1 0.332\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5554 acc 0.730 f1 0.729 || val_loss 1.3373 acc 0.418 f1 0.350\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5311 acc 0.736 f1 0.735 || val_loss 1.3603 acc 0.405 f1 0.324\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5093 acc 0.751 f1 0.750 || val_loss 1.4370 acc 0.401 f1 0.319\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4892 acc 0.758 f1 0.757 || val_loss 1.4549 acc 0.428 f1 0.340\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4639 acc 0.770 f1 0.769 || val_loss 1.5046 acc 0.405 f1 0.321\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4452 acc 0.787 f1 0.787 || val_loss 1.5552 acc 0.409 f1 0.321\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4308 acc 0.797 f1 0.797 || val_loss 1.6016 acc 0.422 f1 0.344\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4144 acc 0.803 f1 0.803 || val_loss 1.6339 acc 0.409 f1 0.335\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3989 acc 0.813 f1 0.812 || val_loss 1.6771 acc 0.395 f1 0.311\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  48%|     | 48/100 [23:25<25:48, 29.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=49 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=49\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1200 acc 0.408 f1 0.407 || val_loss 1.1709 acc 0.286 f1 0.277\n",
            "[W3] ANN Epoch 02 | train_loss 0.9658 acc 0.514 f1 0.507 || val_loss 1.1678 acc 0.329 f1 0.299\n",
            "[W3] ANN Epoch 03 | train_loss 0.8709 acc 0.567 f1 0.560 || val_loss 1.1570 acc 0.342 f1 0.301\n",
            "[W3] ANN Epoch 04 | train_loss 0.8178 acc 0.592 f1 0.585 || val_loss 1.1711 acc 0.342 f1 0.294\n",
            "[W3] ANN Epoch 05 | train_loss 0.7621 acc 0.623 f1 0.616 || val_loss 1.1462 acc 0.401 f1 0.336\n",
            "[W3] ANN Epoch 06 | train_loss 0.7200 acc 0.642 f1 0.637 || val_loss 1.1581 acc 0.401 f1 0.346\n",
            "[W3] ANN Epoch 07 | train_loss 0.6754 acc 0.670 f1 0.666 || val_loss 1.1607 acc 0.403 f1 0.333\n",
            "[W3] ANN Epoch 08 | train_loss 0.6403 acc 0.700 f1 0.698 || val_loss 1.1936 acc 0.403 f1 0.340\n",
            "[W3] ANN Epoch 09 | train_loss 0.6096 acc 0.714 f1 0.712 || val_loss 1.2188 acc 0.399 f1 0.338\n",
            "[W3] ANN Epoch 10 | train_loss 0.6080 acc 0.711 f1 0.708 || val_loss 1.2041 acc 0.407 f1 0.335\n",
            "[W3] ANN Epoch 11 | train_loss 0.5794 acc 0.723 f1 0.723 || val_loss 1.2218 acc 0.424 f1 0.354\n",
            "[W3] ANN Epoch 12 | train_loss 0.5619 acc 0.735 f1 0.733 || val_loss 1.2170 acc 0.424 f1 0.353\n",
            "[W3] ANN Epoch 13 | train_loss 0.5388 acc 0.739 f1 0.738 || val_loss 1.2338 acc 0.436 f1 0.364\n",
            "[W3] ANN Epoch 14 | train_loss 0.5231 acc 0.760 f1 0.759 || val_loss 1.2690 acc 0.405 f1 0.335\n",
            "[W3] ANN Epoch 15 | train_loss 0.5164 acc 0.763 f1 0.762 || val_loss 1.2713 acc 0.453 f1 0.371\n",
            "[W3] ANN Epoch 16 | train_loss 0.5233 acc 0.762 f1 0.761 || val_loss 1.2882 acc 0.434 f1 0.360\n",
            "[W3] ANN Epoch 17 | train_loss 0.5140 acc 0.779 f1 0.778 || val_loss 1.2944 acc 0.422 f1 0.356\n",
            "[W3] ANN Epoch 18 | train_loss 0.4934 acc 0.784 f1 0.784 || val_loss 1.2877 acc 0.438 f1 0.360\n",
            "[W3] ANN Epoch 19 | train_loss 0.4704 acc 0.783 f1 0.783 || val_loss 1.2971 acc 0.461 f1 0.377\n",
            "[W3] ANN Epoch 20 | train_loss 0.4565 acc 0.802 f1 0.801 || val_loss 1.3214 acc 0.457 f1 0.364\n",
            "[W3] ANN Epoch 21 | train_loss 0.4490 acc 0.806 f1 0.805 || val_loss 1.3677 acc 0.434 f1 0.342\n",
            "[W3] ANN Epoch 22 | train_loss 0.4193 acc 0.818 f1 0.818 || val_loss 1.4284 acc 0.418 f1 0.341\n",
            "[W3] ANN Epoch 23 | train_loss 0.4325 acc 0.816 f1 0.815 || val_loss 1.3981 acc 0.444 f1 0.355\n",
            "[W3] ANN Epoch 24 | train_loss 0.4065 acc 0.816 f1 0.815 || val_loss 1.3967 acc 0.447 f1 0.377\n",
            "[W3] ANN Epoch 25 | train_loss 0.4061 acc 0.830 f1 0.830 || val_loss 1.4026 acc 0.481 f1 0.397\n",
            "[W3] ANN Epoch 26 | train_loss 0.4193 acc 0.822 f1 0.821 || val_loss 1.4083 acc 0.444 f1 0.365\n",
            "[W3] ANN Epoch 27 | train_loss 0.4089 acc 0.822 f1 0.821 || val_loss 1.4337 acc 0.449 f1 0.358\n",
            "[W3] ANN Epoch 28 | train_loss 0.4283 acc 0.817 f1 0.816 || val_loss 1.3642 acc 0.461 f1 0.382\n",
            "[W3] ANN Epoch 29 | train_loss 0.4100 acc 0.821 f1 0.820 || val_loss 1.3744 acc 0.459 f1 0.367\n",
            "[W3] ANN Epoch 30 | train_loss 0.3589 acc 0.849 f1 0.849 || val_loss 1.3957 acc 0.451 f1 0.369\n",
            "[W3] ANN Epoch 31 | train_loss 0.3392 acc 0.862 f1 0.862 || val_loss 1.4826 acc 0.457 f1 0.371\n",
            "[W3] ANN Epoch 32 | train_loss 0.3490 acc 0.855 f1 0.855 || val_loss 1.4507 acc 0.463 f1 0.379\n",
            "[W3] ANN Epoch 33 | train_loss 0.3357 acc 0.863 f1 0.862 || val_loss 1.5030 acc 0.459 f1 0.375\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=49\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0654 acc 0.426 f1 0.427 || val_loss 1.0424 acc 0.422 f1 0.355\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9332 acc 0.565 f1 0.558 || val_loss 1.0472 acc 0.391 f1 0.305\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8019 acc 0.628 f1 0.624 || val_loss 1.0972 acc 0.379 f1 0.309\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7149 acc 0.668 f1 0.665 || val_loss 1.1112 acc 0.430 f1 0.358\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6427 acc 0.709 f1 0.707 || val_loss 1.1435 acc 0.418 f1 0.341\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5728 acc 0.743 f1 0.743 || val_loss 1.2663 acc 0.399 f1 0.348\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5207 acc 0.763 f1 0.762 || val_loss 1.2644 acc 0.409 f1 0.338\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4817 acc 0.799 f1 0.798 || val_loss 1.3119 acc 0.434 f1 0.345\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4516 acc 0.803 f1 0.802 || val_loss 1.3640 acc 0.426 f1 0.352\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4125 acc 0.833 f1 0.833 || val_loss 1.3882 acc 0.430 f1 0.354\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3574 acc 0.857 f1 0.857 || val_loss 1.4872 acc 0.424 f1 0.342\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3245 acc 0.864 f1 0.864 || val_loss 1.5392 acc 0.434 f1 0.340\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=49\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0972 acc 0.360 f1 0.328 || val_loss 1.0986 acc 0.344 f1 0.333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0767 acc 0.430 f1 0.426 || val_loss 1.1129 acc 0.311 f1 0.307\n",
            "[W3] RNN Epoch 03 | train_loss 1.0585 acc 0.437 f1 0.429 || val_loss 1.0907 acc 0.344 f1 0.319\n",
            "[W3] RNN Epoch 04 | train_loss 1.0386 acc 0.459 f1 0.448 || val_loss 1.0731 acc 0.374 f1 0.341\n",
            "[W3] RNN Epoch 05 | train_loss 1.0159 acc 0.497 f1 0.493 || val_loss 1.0887 acc 0.340 f1 0.314\n",
            "[W3] RNN Epoch 06 | train_loss 0.9946 acc 0.508 f1 0.501 || val_loss 1.0797 acc 0.385 f1 0.347\n",
            "[W3] RNN Epoch 07 | train_loss 0.9690 acc 0.535 f1 0.530 || val_loss 1.0949 acc 0.350 f1 0.321\n",
            "[W3] RNN Epoch 08 | train_loss 0.9432 acc 0.545 f1 0.537 || val_loss 1.0802 acc 0.354 f1 0.312\n",
            "[W3] RNN Epoch 09 | train_loss 0.9214 acc 0.560 f1 0.551 || val_loss 1.0920 acc 0.352 f1 0.320\n",
            "[W3] RNN Epoch 10 | train_loss 0.8922 acc 0.575 f1 0.567 || val_loss 1.1021 acc 0.346 f1 0.313\n",
            "[W3] RNN Epoch 11 | train_loss 0.8700 acc 0.583 f1 0.575 || val_loss 1.0970 acc 0.370 f1 0.321\n",
            "[W3] RNN Epoch 12 | train_loss 0.8477 acc 0.601 f1 0.595 || val_loss 1.1328 acc 0.325 f1 0.297\n",
            "[W3] RNN Epoch 13 | train_loss 0.8305 acc 0.601 f1 0.591 || val_loss 1.0877 acc 0.391 f1 0.329\n",
            "[W3] RNN Epoch 14 | train_loss 0.8074 acc 0.615 f1 0.607 || val_loss 1.1110 acc 0.370 f1 0.318\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=49\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0987 acc 0.338 f1 0.337 || val_loss 1.0975 acc 0.335 f1 0.322\n",
            "[W3] GRU Epoch 02 | train_loss 1.0895 acc 0.396 f1 0.393 || val_loss 1.0939 acc 0.340 f1 0.318\n",
            "[W3] GRU Epoch 03 | train_loss 1.0812 acc 0.405 f1 0.405 || val_loss 1.0897 acc 0.342 f1 0.313\n",
            "[W3] GRU Epoch 04 | train_loss 1.0668 acc 0.440 f1 0.437 || val_loss 1.0973 acc 0.329 f1 0.313\n",
            "[W3] GRU Epoch 05 | train_loss 1.0461 acc 0.454 f1 0.451 || val_loss 1.0738 acc 0.366 f1 0.327\n",
            "[W3] GRU Epoch 06 | train_loss 1.0076 acc 0.501 f1 0.494 || val_loss 1.0782 acc 0.364 f1 0.337\n",
            "[W3] GRU Epoch 07 | train_loss 0.9434 acc 0.548 f1 0.539 || val_loss 1.0820 acc 0.354 f1 0.317\n",
            "[W3] GRU Epoch 08 | train_loss 0.8775 acc 0.577 f1 0.569 || val_loss 1.0930 acc 0.356 f1 0.300\n",
            "[W3] GRU Epoch 09 | train_loss 0.8239 acc 0.599 f1 0.591 || val_loss 1.1161 acc 0.379 f1 0.335\n",
            "[W3] GRU Epoch 10 | train_loss 0.7781 acc 0.617 f1 0.611 || val_loss 1.0851 acc 0.383 f1 0.318\n",
            "[W3] GRU Epoch 11 | train_loss 0.7406 acc 0.650 f1 0.646 || val_loss 1.1297 acc 0.395 f1 0.353\n",
            "[W3] GRU Epoch 12 | train_loss 0.7045 acc 0.672 f1 0.668 || val_loss 1.1240 acc 0.389 f1 0.327\n",
            "[W3] GRU Epoch 13 | train_loss 0.6771 acc 0.687 f1 0.684 || val_loss 1.1473 acc 0.403 f1 0.338\n",
            "[W3] GRU Epoch 14 | train_loss 0.6479 acc 0.699 f1 0.696 || val_loss 1.1716 acc 0.393 f1 0.338\n",
            "[W3] GRU Epoch 15 | train_loss 0.6197 acc 0.693 f1 0.690 || val_loss 1.1994 acc 0.397 f1 0.345\n",
            "[W3] GRU Epoch 16 | train_loss 0.5836 acc 0.724 f1 0.721 || val_loss 1.2206 acc 0.409 f1 0.339\n",
            "[W3] GRU Epoch 17 | train_loss 0.5588 acc 0.741 f1 0.740 || val_loss 1.2664 acc 0.399 f1 0.334\n",
            "[W3] GRU Epoch 18 | train_loss 0.5387 acc 0.755 f1 0.754 || val_loss 1.2779 acc 0.409 f1 0.338\n",
            "[W3] GRU Epoch 19 | train_loss 0.5119 acc 0.755 f1 0.753 || val_loss 1.3198 acc 0.416 f1 0.348\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=49\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0992 acc 0.340 f1 0.205 || val_loss 1.1097 acc 0.181 f1 0.162\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0945 acc 0.371 f1 0.327 || val_loss 1.0968 acc 0.309 f1 0.297\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0879 acc 0.403 f1 0.402 || val_loss 1.0894 acc 0.313 f1 0.294\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0743 acc 0.419 f1 0.410 || val_loss 1.0820 acc 0.325 f1 0.307\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0516 acc 0.446 f1 0.438 || val_loss 1.1081 acc 0.317 f1 0.311\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0042 acc 0.492 f1 0.480 || val_loss 1.0573 acc 0.385 f1 0.339\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9241 acc 0.562 f1 0.554 || val_loss 1.0916 acc 0.379 f1 0.332\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8466 acc 0.589 f1 0.580 || val_loss 1.1122 acc 0.374 f1 0.308\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7919 acc 0.615 f1 0.607 || val_loss 1.1535 acc 0.387 f1 0.315\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7463 acc 0.656 f1 0.652 || val_loss 1.1652 acc 0.368 f1 0.297\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7148 acc 0.654 f1 0.650 || val_loss 1.1708 acc 0.397 f1 0.306\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6716 acc 0.690 f1 0.688 || val_loss 1.2484 acc 0.331 f1 0.279\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6482 acc 0.694 f1 0.691 || val_loss 1.2509 acc 0.391 f1 0.315\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6064 acc 0.713 f1 0.710 || val_loss 1.3146 acc 0.360 f1 0.291\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  49%|     | 49/100 [23:54<24:51, 29.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=50 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=50\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1134 acc 0.405 f1 0.392 || val_loss 1.1687 acc 0.280 f1 0.276\n",
            "[W3] ANN Epoch 02 | train_loss 0.9563 acc 0.524 f1 0.512 || val_loss 1.1432 acc 0.319 f1 0.297\n",
            "[W3] ANN Epoch 03 | train_loss 0.8591 acc 0.577 f1 0.568 || val_loss 1.1447 acc 0.358 f1 0.314\n",
            "[W3] ANN Epoch 04 | train_loss 0.7854 acc 0.616 f1 0.610 || val_loss 1.1108 acc 0.385 f1 0.330\n",
            "[W3] ANN Epoch 05 | train_loss 0.6993 acc 0.659 f1 0.654 || val_loss 1.1327 acc 0.370 f1 0.314\n",
            "[W3] ANN Epoch 06 | train_loss 0.6583 acc 0.679 f1 0.674 || val_loss 1.1413 acc 0.395 f1 0.324\n",
            "[W3] ANN Epoch 07 | train_loss 0.6144 acc 0.716 f1 0.714 || val_loss 1.1722 acc 0.381 f1 0.309\n",
            "[W3] ANN Epoch 08 | train_loss 0.5954 acc 0.716 f1 0.714 || val_loss 1.1980 acc 0.393 f1 0.318\n",
            "[W3] ANN Epoch 09 | train_loss 0.5638 acc 0.723 f1 0.721 || val_loss 1.2214 acc 0.414 f1 0.333\n",
            "[W3] ANN Epoch 10 | train_loss 0.5346 acc 0.759 f1 0.758 || val_loss 1.2701 acc 0.395 f1 0.320\n",
            "[W3] ANN Epoch 11 | train_loss 0.5251 acc 0.758 f1 0.758 || val_loss 1.2769 acc 0.416 f1 0.335\n",
            "[W3] ANN Epoch 12 | train_loss 0.4905 acc 0.767 f1 0.766 || val_loss 1.3041 acc 0.459 f1 0.366\n",
            "[W3] ANN Epoch 13 | train_loss 0.4633 acc 0.802 f1 0.801 || val_loss 1.3570 acc 0.426 f1 0.347\n",
            "[W3] ANN Epoch 14 | train_loss 0.4589 acc 0.793 f1 0.793 || val_loss 1.3373 acc 0.438 f1 0.357\n",
            "[W3] ANN Epoch 15 | train_loss 0.4519 acc 0.805 f1 0.805 || val_loss 1.3877 acc 0.403 f1 0.325\n",
            "[W3] ANN Epoch 16 | train_loss 0.4277 acc 0.805 f1 0.805 || val_loss 1.3706 acc 0.395 f1 0.324\n",
            "[W3] ANN Epoch 17 | train_loss 0.3951 acc 0.827 f1 0.827 || val_loss 1.4653 acc 0.381 f1 0.309\n",
            "[W3] ANN Epoch 18 | train_loss 0.3771 acc 0.835 f1 0.835 || val_loss 1.4653 acc 0.424 f1 0.350\n",
            "[W3] ANN Epoch 19 | train_loss 0.3913 acc 0.834 f1 0.833 || val_loss 1.5199 acc 0.409 f1 0.318\n",
            "[W3] ANN Epoch 20 | train_loss 0.3812 acc 0.834 f1 0.833 || val_loss 1.5305 acc 0.416 f1 0.329\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=50\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0800 acc 0.395 f1 0.395 || val_loss 1.0324 acc 0.447 f1 0.326\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9333 acc 0.551 f1 0.547 || val_loss 1.0297 acc 0.401 f1 0.330\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.7972 acc 0.615 f1 0.610 || val_loss 1.0894 acc 0.370 f1 0.313\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7048 acc 0.653 f1 0.651 || val_loss 1.1376 acc 0.401 f1 0.333\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6200 acc 0.712 f1 0.710 || val_loss 1.1560 acc 0.414 f1 0.350\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5705 acc 0.740 f1 0.739 || val_loss 1.2143 acc 0.434 f1 0.349\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5118 acc 0.765 f1 0.764 || val_loss 1.2570 acc 0.401 f1 0.320\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4464 acc 0.803 f1 0.803 || val_loss 1.3241 acc 0.405 f1 0.320\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4053 acc 0.824 f1 0.823 || val_loss 1.4073 acc 0.418 f1 0.332\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3573 acc 0.846 f1 0.846 || val_loss 1.4609 acc 0.399 f1 0.332\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3316 acc 0.863 f1 0.863 || val_loss 1.5209 acc 0.412 f1 0.337\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2798 acc 0.894 f1 0.894 || val_loss 1.5837 acc 0.412 f1 0.319\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2384 acc 0.911 f1 0.911 || val_loss 1.6861 acc 0.391 f1 0.310\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=50\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0986 acc 0.356 f1 0.330 || val_loss 1.0975 acc 0.352 f1 0.323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0807 acc 0.426 f1 0.423 || val_loss 1.0989 acc 0.346 f1 0.326\n",
            "[W3] RNN Epoch 03 | train_loss 1.0649 acc 0.436 f1 0.430 || val_loss 1.0852 acc 0.352 f1 0.324\n",
            "[W3] RNN Epoch 04 | train_loss 1.0483 acc 0.459 f1 0.456 || val_loss 1.0825 acc 0.346 f1 0.311\n",
            "[W3] RNN Epoch 05 | train_loss 1.0279 acc 0.478 f1 0.473 || val_loss 1.0782 acc 0.381 f1 0.343\n",
            "[W3] RNN Epoch 06 | train_loss 1.0067 acc 0.495 f1 0.490 || val_loss 1.1006 acc 0.348 f1 0.326\n",
            "[W3] RNN Epoch 07 | train_loss 0.9750 acc 0.532 f1 0.527 || val_loss 1.0766 acc 0.360 f1 0.331\n",
            "[W3] RNN Epoch 08 | train_loss 0.9461 acc 0.550 f1 0.544 || val_loss 1.0898 acc 0.362 f1 0.337\n",
            "[W3] RNN Epoch 09 | train_loss 0.9189 acc 0.558 f1 0.551 || val_loss 1.0943 acc 0.362 f1 0.341\n",
            "[W3] RNN Epoch 10 | train_loss 0.8922 acc 0.581 f1 0.573 || val_loss 1.0820 acc 0.374 f1 0.345\n",
            "[W3] RNN Epoch 11 | train_loss 0.8654 acc 0.597 f1 0.588 || val_loss 1.0634 acc 0.377 f1 0.334\n",
            "[W3] RNN Epoch 12 | train_loss 0.8417 acc 0.590 f1 0.580 || val_loss 1.0853 acc 0.403 f1 0.365\n",
            "[W3] RNN Epoch 13 | train_loss 0.8187 acc 0.624 f1 0.618 || val_loss 1.0839 acc 0.397 f1 0.360\n",
            "[W3] RNN Epoch 14 | train_loss 0.7945 acc 0.621 f1 0.610 || val_loss 1.0956 acc 0.395 f1 0.342\n",
            "[W3] RNN Epoch 15 | train_loss 0.7683 acc 0.640 f1 0.632 || val_loss 1.0992 acc 0.399 f1 0.349\n",
            "[W3] RNN Epoch 16 | train_loss 0.7440 acc 0.648 f1 0.639 || val_loss 1.1210 acc 0.403 f1 0.344\n",
            "[W3] RNN Epoch 17 | train_loss 0.7235 acc 0.655 f1 0.648 || val_loss 1.1420 acc 0.397 f1 0.350\n",
            "[W3] RNN Epoch 18 | train_loss 0.7029 acc 0.672 f1 0.666 || val_loss 1.1717 acc 0.383 f1 0.328\n",
            "[W3] RNN Epoch 19 | train_loss 0.6834 acc 0.675 f1 0.668 || val_loss 1.1654 acc 0.409 f1 0.350\n",
            "[W3] RNN Epoch 20 | train_loss 0.6593 acc 0.691 f1 0.686 || val_loss 1.1883 acc 0.409 f1 0.348\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=50\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1027 acc 0.328 f1 0.257 || val_loss 1.0755 acc 0.397 f1 0.289\n",
            "[W3] GRU Epoch 02 | train_loss 1.0904 acc 0.366 f1 0.347 || val_loss 1.0808 acc 0.397 f1 0.359\n",
            "[W3] GRU Epoch 03 | train_loss 1.0765 acc 0.418 f1 0.418 || val_loss 1.0828 acc 0.356 f1 0.333\n",
            "[W3] GRU Epoch 04 | train_loss 1.0568 acc 0.436 f1 0.431 || val_loss 1.0768 acc 0.352 f1 0.330\n",
            "[W3] GRU Epoch 05 | train_loss 1.0217 acc 0.484 f1 0.476 || val_loss 1.0734 acc 0.364 f1 0.332\n",
            "[W3] GRU Epoch 06 | train_loss 0.9649 acc 0.536 f1 0.527 || val_loss 1.0854 acc 0.383 f1 0.337\n",
            "[W3] GRU Epoch 07 | train_loss 0.9009 acc 0.552 f1 0.543 || val_loss 1.1054 acc 0.379 f1 0.318\n",
            "[W3] GRU Epoch 08 | train_loss 0.8350 acc 0.594 f1 0.586 || val_loss 1.1368 acc 0.389 f1 0.332\n",
            "[W3] GRU Epoch 09 | train_loss 0.7863 acc 0.620 f1 0.615 || val_loss 1.1457 acc 0.393 f1 0.328\n",
            "[W3] GRU Epoch 10 | train_loss 0.7597 acc 0.634 f1 0.629 || val_loss 1.1358 acc 0.440 f1 0.358\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=50\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0992 acc 0.334 f1 0.238 || val_loss 1.0948 acc 0.360 f1 0.279\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0941 acc 0.380 f1 0.374 || val_loss 1.0957 acc 0.333 f1 0.316\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0868 acc 0.412 f1 0.409 || val_loss 1.0997 acc 0.321 f1 0.315\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0743 acc 0.434 f1 0.423 || val_loss 1.1081 acc 0.319 f1 0.316\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0455 acc 0.458 f1 0.444 || val_loss 1.0948 acc 0.342 f1 0.322\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9658 acc 0.517 f1 0.504 || val_loss 1.0684 acc 0.395 f1 0.331\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8640 acc 0.565 f1 0.558 || val_loss 1.0984 acc 0.403 f1 0.313\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8069 acc 0.601 f1 0.596 || val_loss 1.1057 acc 0.420 f1 0.336\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7546 acc 0.627 f1 0.623 || val_loss 1.1750 acc 0.393 f1 0.331\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7102 acc 0.654 f1 0.649 || val_loss 1.1385 acc 0.447 f1 0.345\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6750 acc 0.667 f1 0.662 || val_loss 1.1896 acc 0.416 f1 0.319\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6529 acc 0.677 f1 0.675 || val_loss 1.2429 acc 0.405 f1 0.338\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6259 acc 0.692 f1 0.690 || val_loss 1.2405 acc 0.414 f1 0.341\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5954 acc 0.707 f1 0.705 || val_loss 1.3087 acc 0.422 f1 0.353\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5709 acc 0.724 f1 0.723 || val_loss 1.3104 acc 0.418 f1 0.347\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5407 acc 0.747 f1 0.745 || val_loss 1.3311 acc 0.422 f1 0.333\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5140 acc 0.765 f1 0.764 || val_loss 1.3884 acc 0.420 f1 0.330\n",
            "[W3] LSTM Epoch 18 | train_loss 0.4972 acc 0.760 f1 0.759 || val_loss 1.4309 acc 0.432 f1 0.357\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4796 acc 0.773 f1 0.772 || val_loss 1.4813 acc 0.428 f1 0.350\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4533 acc 0.785 f1 0.785 || val_loss 1.5287 acc 0.409 f1 0.330\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4338 acc 0.796 f1 0.796 || val_loss 1.5664 acc 0.428 f1 0.342\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4237 acc 0.801 f1 0.801 || val_loss 1.6079 acc 0.409 f1 0.328\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4017 acc 0.819 f1 0.818 || val_loss 1.6967 acc 0.412 f1 0.331\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3870 acc 0.818 f1 0.818 || val_loss 1.7503 acc 0.420 f1 0.338\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3705 acc 0.830 f1 0.830 || val_loss 1.8113 acc 0.401 f1 0.328\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3560 acc 0.835 f1 0.835 || val_loss 1.8308 acc 0.412 f1 0.326\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  50%|     | 50/100 [24:20<23:40, 28.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=51 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=51\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1617 acc 0.386 f1 0.387 || val_loss 1.0966 acc 0.385 f1 0.344\n",
            "[W3] ANN Epoch 02 | train_loss 0.9802 acc 0.516 f1 0.511 || val_loss 1.0890 acc 0.364 f1 0.321\n",
            "[W3] ANN Epoch 03 | train_loss 0.8548 acc 0.587 f1 0.580 || val_loss 1.1095 acc 0.370 f1 0.325\n",
            "[W3] ANN Epoch 04 | train_loss 0.7943 acc 0.616 f1 0.611 || val_loss 1.0819 acc 0.399 f1 0.335\n",
            "[W3] ANN Epoch 05 | train_loss 0.7411 acc 0.645 f1 0.640 || val_loss 1.1203 acc 0.397 f1 0.330\n",
            "[W3] ANN Epoch 06 | train_loss 0.6709 acc 0.688 f1 0.684 || val_loss 1.1073 acc 0.407 f1 0.328\n",
            "[W3] ANN Epoch 07 | train_loss 0.6527 acc 0.697 f1 0.696 || val_loss 1.1363 acc 0.426 f1 0.337\n",
            "[W3] ANN Epoch 08 | train_loss 0.6134 acc 0.719 f1 0.717 || val_loss 1.1528 acc 0.432 f1 0.347\n",
            "[W3] ANN Epoch 09 | train_loss 0.5704 acc 0.745 f1 0.744 || val_loss 1.1698 acc 0.442 f1 0.354\n",
            "[W3] ANN Epoch 10 | train_loss 0.5699 acc 0.741 f1 0.741 || val_loss 1.2042 acc 0.426 f1 0.336\n",
            "[W3] ANN Epoch 11 | train_loss 0.5425 acc 0.758 f1 0.757 || val_loss 1.2100 acc 0.405 f1 0.325\n",
            "[W3] ANN Epoch 12 | train_loss 0.4984 acc 0.771 f1 0.770 || val_loss 1.2282 acc 0.414 f1 0.328\n",
            "[W3] ANN Epoch 13 | train_loss 0.4922 acc 0.778 f1 0.778 || val_loss 1.2278 acc 0.430 f1 0.352\n",
            "[W3] ANN Epoch 14 | train_loss 0.4831 acc 0.794 f1 0.794 || val_loss 1.2681 acc 0.434 f1 0.348\n",
            "[W3] ANN Epoch 15 | train_loss 0.4610 acc 0.797 f1 0.796 || val_loss 1.2837 acc 0.428 f1 0.354\n",
            "[W3] ANN Epoch 16 | train_loss 0.4528 acc 0.804 f1 0.803 || val_loss 1.2899 acc 0.416 f1 0.330\n",
            "[W3] ANN Epoch 17 | train_loss 0.4469 acc 0.803 f1 0.803 || val_loss 1.3317 acc 0.399 f1 0.328\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=51\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0746 acc 0.412 f1 0.414 || val_loss 1.0191 acc 0.430 f1 0.331\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9597 acc 0.532 f1 0.526 || val_loss 1.0173 acc 0.434 f1 0.333\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8334 acc 0.608 f1 0.605 || val_loss 1.0782 acc 0.416 f1 0.342\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7544 acc 0.646 f1 0.640 || val_loss 1.1339 acc 0.368 f1 0.324\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6715 acc 0.692 f1 0.692 || val_loss 1.1791 acc 0.385 f1 0.311\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6248 acc 0.708 f1 0.706 || val_loss 1.1534 acc 0.395 f1 0.313\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5538 acc 0.748 f1 0.746 || val_loss 1.2191 acc 0.354 f1 0.300\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4934 acc 0.785 f1 0.785 || val_loss 1.3463 acc 0.385 f1 0.333\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4601 acc 0.803 f1 0.802 || val_loss 1.3097 acc 0.403 f1 0.336\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4276 acc 0.821 f1 0.821 || val_loss 1.4115 acc 0.379 f1 0.299\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3710 acc 0.842 f1 0.842 || val_loss 1.4193 acc 0.395 f1 0.319\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=51\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1003 acc 0.341 f1 0.338 || val_loss 1.0956 acc 0.358 f1 0.333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0840 acc 0.421 f1 0.413 || val_loss 1.0930 acc 0.340 f1 0.312\n",
            "[W3] RNN Epoch 03 | train_loss 1.0733 acc 0.436 f1 0.435 || val_loss 1.0929 acc 0.342 f1 0.328\n",
            "[W3] RNN Epoch 04 | train_loss 1.0535 acc 0.450 f1 0.446 || val_loss 1.0764 acc 0.383 f1 0.345\n",
            "[W3] RNN Epoch 05 | train_loss 1.0392 acc 0.468 f1 0.466 || val_loss 1.0768 acc 0.391 f1 0.356\n",
            "[W3] RNN Epoch 06 | train_loss 1.0186 acc 0.486 f1 0.480 || val_loss 1.0817 acc 0.368 f1 0.334\n",
            "[W3] RNN Epoch 07 | train_loss 1.0008 acc 0.498 f1 0.490 || val_loss 1.0845 acc 0.370 f1 0.336\n",
            "[W3] RNN Epoch 08 | train_loss 0.9863 acc 0.517 f1 0.507 || val_loss 1.0878 acc 0.383 f1 0.351\n",
            "[W3] RNN Epoch 09 | train_loss 0.9726 acc 0.533 f1 0.519 || val_loss 1.0898 acc 0.374 f1 0.332\n",
            "[W3] RNN Epoch 10 | train_loss 0.9540 acc 0.540 f1 0.532 || val_loss 1.0755 acc 0.383 f1 0.330\n",
            "[W3] RNN Epoch 11 | train_loss 0.9270 acc 0.557 f1 0.550 || val_loss 1.0817 acc 0.397 f1 0.351\n",
            "[W3] RNN Epoch 12 | train_loss 0.9094 acc 0.573 f1 0.565 || val_loss 1.0983 acc 0.379 f1 0.343\n",
            "[W3] RNN Epoch 13 | train_loss 0.9013 acc 0.570 f1 0.561 || val_loss 1.0907 acc 0.389 f1 0.346\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=51\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0989 acc 0.334 f1 0.239 || val_loss 1.0867 acc 0.418 f1 0.329\n",
            "[W3] GRU Epoch 02 | train_loss 1.0917 acc 0.385 f1 0.380 || val_loss 1.0899 acc 0.370 f1 0.339\n",
            "[W3] GRU Epoch 03 | train_loss 1.0824 acc 0.408 f1 0.406 || val_loss 1.0906 acc 0.340 f1 0.312\n",
            "[W3] GRU Epoch 04 | train_loss 1.0703 acc 0.427 f1 0.422 || val_loss 1.0901 acc 0.340 f1 0.319\n",
            "[W3] GRU Epoch 05 | train_loss 1.0523 acc 0.454 f1 0.445 || val_loss 1.0901 acc 0.354 f1 0.333\n",
            "[W3] GRU Epoch 06 | train_loss 1.0198 acc 0.506 f1 0.500 || val_loss 1.0751 acc 0.374 f1 0.338\n",
            "[W3] GRU Epoch 07 | train_loss 0.9706 acc 0.538 f1 0.534 || val_loss 1.0555 acc 0.416 f1 0.346\n",
            "[W3] GRU Epoch 08 | train_loss 0.9017 acc 0.576 f1 0.570 || val_loss 1.0992 acc 0.374 f1 0.333\n",
            "[W3] GRU Epoch 09 | train_loss 0.8419 acc 0.593 f1 0.584 || val_loss 1.1055 acc 0.383 f1 0.340\n",
            "[W3] GRU Epoch 10 | train_loss 0.7943 acc 0.621 f1 0.614 || val_loss 1.0994 acc 0.405 f1 0.332\n",
            "[W3] GRU Epoch 11 | train_loss 0.7516 acc 0.639 f1 0.633 || val_loss 1.1392 acc 0.407 f1 0.334\n",
            "[W3] GRU Epoch 12 | train_loss 0.7163 acc 0.652 f1 0.649 || val_loss 1.1522 acc 0.407 f1 0.329\n",
            "[W3] GRU Epoch 13 | train_loss 0.6938 acc 0.657 f1 0.653 || val_loss 1.1764 acc 0.422 f1 0.357\n",
            "[W3] GRU Epoch 14 | train_loss 0.6612 acc 0.690 f1 0.687 || val_loss 1.1880 acc 0.438 f1 0.356\n",
            "[W3] GRU Epoch 15 | train_loss 0.6311 acc 0.695 f1 0.691 || val_loss 1.2175 acc 0.416 f1 0.331\n",
            "[W3] GRU Epoch 16 | train_loss 0.6066 acc 0.706 f1 0.704 || val_loss 1.2511 acc 0.426 f1 0.332\n",
            "[W3] GRU Epoch 17 | train_loss 0.5789 acc 0.717 f1 0.715 || val_loss 1.2842 acc 0.432 f1 0.337\n",
            "[W3] GRU Epoch 18 | train_loss 0.5596 acc 0.729 f1 0.727 || val_loss 1.3124 acc 0.422 f1 0.345\n",
            "[W3] GRU Epoch 19 | train_loss 0.5464 acc 0.735 f1 0.734 || val_loss 1.3466 acc 0.420 f1 0.327\n",
            "[W3] GRU Epoch 20 | train_loss 0.5197 acc 0.749 f1 0.749 || val_loss 1.3898 acc 0.436 f1 0.355\n",
            "[W3] GRU Epoch 21 | train_loss 0.4991 acc 0.758 f1 0.756 || val_loss 1.4266 acc 0.444 f1 0.356\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=51\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 931, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1000 acc 0.342 f1 0.259 || val_loss 1.0893 acc 0.424 f1 0.305\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0932 acc 0.386 f1 0.385 || val_loss 1.0970 acc 0.344 f1 0.313\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0879 acc 0.394 f1 0.387 || val_loss 1.1004 acc 0.302 f1 0.291\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0735 acc 0.424 f1 0.413 || val_loss 1.0935 acc 0.305 f1 0.291\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0392 acc 0.467 f1 0.450 || val_loss 1.0850 acc 0.346 f1 0.314\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9675 acc 0.521 f1 0.503 || val_loss 1.0976 acc 0.360 f1 0.325\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8905 acc 0.553 f1 0.541 || val_loss 1.0934 acc 0.385 f1 0.326\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8255 acc 0.590 f1 0.583 || val_loss 1.0665 acc 0.393 f1 0.317\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7882 acc 0.600 f1 0.595 || val_loss 1.1306 acc 0.385 f1 0.331\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7553 acc 0.617 f1 0.608 || val_loss 1.0999 acc 0.397 f1 0.326\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7218 acc 0.638 f1 0.635 || val_loss 1.1115 acc 0.399 f1 0.323\n",
            "[W3] LSTM Epoch 12 | train_loss 0.7097 acc 0.648 f1 0.645 || val_loss 1.1323 acc 0.381 f1 0.299\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6735 acc 0.658 f1 0.653 || val_loss 1.1674 acc 0.395 f1 0.315\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6452 acc 0.678 f1 0.676 || val_loss 1.1710 acc 0.399 f1 0.322\n",
            "[W3] LSTM Epoch 15 | train_loss 0.6286 acc 0.679 f1 0.676 || val_loss 1.1840 acc 0.395 f1 0.319\n",
            "[W3] LSTM Epoch 16 | train_loss 0.6074 acc 0.700 f1 0.698 || val_loss 1.2519 acc 0.387 f1 0.319\n",
            "[W3] LSTM Epoch 17 | train_loss 0.6123 acc 0.690 f1 0.686 || val_loss 1.2463 acc 0.405 f1 0.332\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5712 acc 0.719 f1 0.718 || val_loss 1.2542 acc 0.409 f1 0.331\n",
            "[W3] LSTM Epoch 19 | train_loss 0.5534 acc 0.732 f1 0.729 || val_loss 1.2653 acc 0.409 f1 0.329\n",
            "[W3] LSTM Epoch 20 | train_loss 0.5452 acc 0.719 f1 0.719 || val_loss 1.3389 acc 0.387 f1 0.319\n",
            "[W3] LSTM Epoch 21 | train_loss 0.5239 acc 0.737 f1 0.735 || val_loss 1.3182 acc 0.409 f1 0.335\n",
            "[W3] LSTM Epoch 22 | train_loss 0.5079 acc 0.751 f1 0.751 || val_loss 1.3716 acc 0.399 f1 0.323\n",
            "[W3] LSTM Epoch 23 | train_loss 0.5140 acc 0.746 f1 0.746 || val_loss 1.4091 acc 0.399 f1 0.324\n",
            "[W3] LSTM Epoch 24 | train_loss 0.4820 acc 0.773 f1 0.773 || val_loss 1.4212 acc 0.399 f1 0.328\n",
            "[W3] LSTM Epoch 25 | train_loss 0.4649 acc 0.777 f1 0.777 || val_loss 1.4681 acc 0.403 f1 0.327\n",
            "[W3] LSTM Epoch 26 | train_loss 0.4565 acc 0.778 f1 0.778 || val_loss 1.5041 acc 0.389 f1 0.316\n",
            "[W3] LSTM Epoch 27 | train_loss 0.4403 acc 0.794 f1 0.794 || val_loss 1.5111 acc 0.428 f1 0.339\n",
            "[W3] LSTM Epoch 28 | train_loss 0.4318 acc 0.801 f1 0.801 || val_loss 1.5591 acc 0.412 f1 0.332\n",
            "[W3] LSTM Epoch 29 | train_loss 0.4281 acc 0.796 f1 0.797 || val_loss 1.6182 acc 0.414 f1 0.329\n",
            "[W3] LSTM Epoch 30 | train_loss 0.4092 acc 0.799 f1 0.799 || val_loss 1.6283 acc 0.401 f1 0.320\n",
            "[W3] LSTM Epoch 31 | train_loss 0.3977 acc 0.816 f1 0.815 || val_loss 1.6650 acc 0.397 f1 0.315\n",
            "[W3] LSTM Epoch 32 | train_loss 0.3809 acc 0.820 f1 0.820 || val_loss 1.6814 acc 0.409 f1 0.326\n",
            "[W3] LSTM Epoch 33 | train_loss 0.3680 acc 0.834 f1 0.834 || val_loss 1.7540 acc 0.418 f1 0.333\n",
            "[W3] LSTM Epoch 34 | train_loss 0.3523 acc 0.837 f1 0.837 || val_loss 1.7915 acc 0.403 f1 0.331\n",
            "[W3] LSTM Epoch 35 | train_loss 0.3405 acc 0.843 f1 0.843 || val_loss 1.7865 acc 0.418 f1 0.337\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  51%|     | 51/100 [24:48<23:01, 28.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=52 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=52\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 933, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1256 acc 0.394 f1 0.393 || val_loss 1.1027 acc 0.364 f1 0.339\n",
            "[W3] ANN Epoch 02 | train_loss 0.9578 acc 0.529 f1 0.523 || val_loss 1.1020 acc 0.370 f1 0.341\n",
            "[W3] ANN Epoch 03 | train_loss 0.8557 acc 0.577 f1 0.570 || val_loss 1.1159 acc 0.368 f1 0.326\n",
            "[W3] ANN Epoch 04 | train_loss 0.7781 acc 0.621 f1 0.616 || val_loss 1.1099 acc 0.385 f1 0.335\n",
            "[W3] ANN Epoch 05 | train_loss 0.7208 acc 0.652 f1 0.648 || val_loss 1.1354 acc 0.399 f1 0.332\n",
            "[W3] ANN Epoch 06 | train_loss 0.6678 acc 0.685 f1 0.684 || val_loss 1.1300 acc 0.381 f1 0.313\n",
            "[W3] ANN Epoch 07 | train_loss 0.6344 acc 0.695 f1 0.692 || val_loss 1.1213 acc 0.440 f1 0.367\n",
            "[W3] ANN Epoch 08 | train_loss 0.5864 acc 0.722 f1 0.722 || val_loss 1.1447 acc 0.440 f1 0.358\n",
            "[W3] ANN Epoch 09 | train_loss 0.5712 acc 0.739 f1 0.739 || val_loss 1.1491 acc 0.457 f1 0.376\n",
            "[W3] ANN Epoch 10 | train_loss 0.5371 acc 0.758 f1 0.757 || val_loss 1.2055 acc 0.444 f1 0.366\n",
            "[W3] ANN Epoch 11 | train_loss 0.5089 acc 0.773 f1 0.772 || val_loss 1.2157 acc 0.426 f1 0.342\n",
            "[W3] ANN Epoch 12 | train_loss 0.5113 acc 0.774 f1 0.774 || val_loss 1.2552 acc 0.449 f1 0.370\n",
            "[W3] ANN Epoch 13 | train_loss 0.4798 acc 0.784 f1 0.784 || val_loss 1.2629 acc 0.440 f1 0.358\n",
            "[W3] ANN Epoch 14 | train_loss 0.4602 acc 0.794 f1 0.794 || val_loss 1.3140 acc 0.444 f1 0.371\n",
            "[W3] ANN Epoch 15 | train_loss 0.4405 acc 0.815 f1 0.814 || val_loss 1.3132 acc 0.457 f1 0.377\n",
            "[W3] ANN Epoch 16 | train_loss 0.4051 acc 0.831 f1 0.831 || val_loss 1.3457 acc 0.467 f1 0.378\n",
            "[W3] ANN Epoch 17 | train_loss 0.4109 acc 0.821 f1 0.820 || val_loss 1.3801 acc 0.449 f1 0.364\n",
            "[W3] ANN Epoch 18 | train_loss 0.3866 acc 0.840 f1 0.840 || val_loss 1.3804 acc 0.451 f1 0.360\n",
            "[W3] ANN Epoch 19 | train_loss 0.3720 acc 0.846 f1 0.847 || val_loss 1.4093 acc 0.447 f1 0.361\n",
            "[W3] ANN Epoch 20 | train_loss 0.3572 acc 0.849 f1 0.849 || val_loss 1.4332 acc 0.471 f1 0.384\n",
            "[W3] ANN Epoch 21 | train_loss 0.3346 acc 0.861 f1 0.861 || val_loss 1.4262 acc 0.481 f1 0.383\n",
            "[W3] ANN Epoch 22 | train_loss 0.3404 acc 0.861 f1 0.860 || val_loss 1.4453 acc 0.490 f1 0.383\n",
            "[W3] ANN Epoch 23 | train_loss 0.3186 acc 0.867 f1 0.867 || val_loss 1.4850 acc 0.473 f1 0.371\n",
            "[W3] ANN Epoch 24 | train_loss 0.3261 acc 0.871 f1 0.871 || val_loss 1.4626 acc 0.455 f1 0.363\n",
            "[W3] ANN Epoch 25 | train_loss 0.3074 acc 0.877 f1 0.877 || val_loss 1.5500 acc 0.453 f1 0.356\n",
            "[W3] ANN Epoch 26 | train_loss 0.3052 acc 0.878 f1 0.877 || val_loss 1.5405 acc 0.449 f1 0.359\n",
            "[W3] ANN Epoch 27 | train_loss 0.2808 acc 0.887 f1 0.887 || val_loss 1.5649 acc 0.457 f1 0.365\n",
            "[W3] ANN Epoch 28 | train_loss 0.2799 acc 0.888 f1 0.888 || val_loss 1.6294 acc 0.447 f1 0.363\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=52\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 933, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0744 acc 0.404 f1 0.406 || val_loss 1.0304 acc 0.426 f1 0.342\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9393 acc 0.535 f1 0.528 || val_loss 1.0368 acc 0.397 f1 0.308\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8004 acc 0.611 f1 0.607 || val_loss 1.1288 acc 0.368 f1 0.298\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7065 acc 0.661 f1 0.657 || val_loss 1.1641 acc 0.381 f1 0.321\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6253 acc 0.705 f1 0.703 || val_loss 1.2068 acc 0.393 f1 0.327\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5550 acc 0.747 f1 0.746 || val_loss 1.2661 acc 0.374 f1 0.312\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5012 acc 0.776 f1 0.776 || val_loss 1.3406 acc 0.389 f1 0.313\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4454 acc 0.793 f1 0.792 || val_loss 1.4424 acc 0.368 f1 0.318\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4034 acc 0.821 f1 0.821 || val_loss 1.4979 acc 0.395 f1 0.324\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=52\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 933, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0973 acc 0.359 f1 0.333 || val_loss 1.0963 acc 0.305 f1 0.291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0762 acc 0.426 f1 0.422 || val_loss 1.0841 acc 0.348 f1 0.324\n",
            "[W3] RNN Epoch 03 | train_loss 1.0553 acc 0.460 f1 0.454 || val_loss 1.0808 acc 0.354 f1 0.327\n",
            "[W3] RNN Epoch 04 | train_loss 1.0282 acc 0.485 f1 0.479 || val_loss 1.0732 acc 0.364 f1 0.335\n",
            "[W3] RNN Epoch 05 | train_loss 1.0024 acc 0.498 f1 0.492 || val_loss 1.0939 acc 0.346 f1 0.318\n",
            "[W3] RNN Epoch 06 | train_loss 0.9727 acc 0.525 f1 0.515 || val_loss 1.0920 acc 0.356 f1 0.329\n",
            "[W3] RNN Epoch 07 | train_loss 0.9502 acc 0.532 f1 0.523 || val_loss 1.1028 acc 0.366 f1 0.333\n",
            "[W3] RNN Epoch 08 | train_loss 0.9290 acc 0.545 f1 0.538 || val_loss 1.0973 acc 0.379 f1 0.344\n",
            "[W3] RNN Epoch 09 | train_loss 0.9126 acc 0.560 f1 0.549 || val_loss 1.1023 acc 0.362 f1 0.322\n",
            "[W3] RNN Epoch 10 | train_loss 0.8939 acc 0.562 f1 0.551 || val_loss 1.1234 acc 0.360 f1 0.328\n",
            "[W3] RNN Epoch 11 | train_loss 0.8655 acc 0.590 f1 0.581 || val_loss 1.1164 acc 0.366 f1 0.325\n",
            "[W3] RNN Epoch 12 | train_loss 0.8503 acc 0.596 f1 0.587 || val_loss 1.1292 acc 0.370 f1 0.324\n",
            "[W3] RNN Epoch 13 | train_loss 0.8289 acc 0.604 f1 0.595 || val_loss 1.1138 acc 0.381 f1 0.328\n",
            "[W3] RNN Epoch 14 | train_loss 0.8028 acc 0.627 f1 0.617 || val_loss 1.1352 acc 0.374 f1 0.330\n",
            "[W3] RNN Epoch 15 | train_loss 0.7923 acc 0.618 f1 0.610 || val_loss 1.1415 acc 0.379 f1 0.322\n",
            "[W3] RNN Epoch 16 | train_loss 0.7678 acc 0.628 f1 0.618 || val_loss 1.1586 acc 0.383 f1 0.329\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=52\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 933, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1014 acc 0.340 f1 0.222 || val_loss 1.1103 acc 0.241 f1 0.232\n",
            "[W3] GRU Epoch 02 | train_loss 1.0898 acc 0.402 f1 0.387 || val_loss 1.0921 acc 0.323 f1 0.291\n",
            "[W3] GRU Epoch 03 | train_loss 1.0793 acc 0.419 f1 0.419 || val_loss 1.0890 acc 0.313 f1 0.292\n",
            "[W3] GRU Epoch 04 | train_loss 1.0670 acc 0.441 f1 0.439 || val_loss 1.0761 acc 0.342 f1 0.298\n",
            "[W3] GRU Epoch 05 | train_loss 1.0478 acc 0.471 f1 0.462 || val_loss 1.0705 acc 0.344 f1 0.308\n",
            "[W3] GRU Epoch 06 | train_loss 1.0099 acc 0.499 f1 0.494 || val_loss 1.0621 acc 0.352 f1 0.307\n",
            "[W3] GRU Epoch 07 | train_loss 0.9525 acc 0.545 f1 0.538 || val_loss 1.0813 acc 0.362 f1 0.311\n",
            "[W3] GRU Epoch 08 | train_loss 0.8670 acc 0.592 f1 0.584 || val_loss 1.0828 acc 0.389 f1 0.325\n",
            "[W3] GRU Epoch 09 | train_loss 0.8090 acc 0.613 f1 0.606 || val_loss 1.1257 acc 0.379 f1 0.320\n",
            "[W3] GRU Epoch 10 | train_loss 0.7543 acc 0.650 f1 0.645 || val_loss 1.1576 acc 0.350 f1 0.300\n",
            "[W3] GRU Epoch 11 | train_loss 0.7128 acc 0.655 f1 0.650 || val_loss 1.1678 acc 0.403 f1 0.335\n",
            "[W3] GRU Epoch 12 | train_loss 0.6807 acc 0.673 f1 0.669 || val_loss 1.1838 acc 0.412 f1 0.341\n",
            "[W3] GRU Epoch 13 | train_loss 0.6440 acc 0.695 f1 0.691 || val_loss 1.2434 acc 0.399 f1 0.332\n",
            "[W3] GRU Epoch 14 | train_loss 0.6098 acc 0.708 f1 0.705 || val_loss 1.2706 acc 0.405 f1 0.336\n",
            "[W3] GRU Epoch 15 | train_loss 0.5822 acc 0.726 f1 0.723 || val_loss 1.3000 acc 0.407 f1 0.328\n",
            "[W3] GRU Epoch 16 | train_loss 0.5523 acc 0.734 f1 0.732 || val_loss 1.3511 acc 0.422 f1 0.348\n",
            "[W3] GRU Epoch 17 | train_loss 0.5233 acc 0.750 f1 0.748 || val_loss 1.3863 acc 0.416 f1 0.338\n",
            "[W3] GRU Epoch 18 | train_loss 0.4958 acc 0.770 f1 0.768 || val_loss 1.4316 acc 0.420 f1 0.347\n",
            "[W3] GRU Epoch 19 | train_loss 0.4864 acc 0.767 f1 0.764 || val_loss 1.4720 acc 0.432 f1 0.347\n",
            "[W3] GRU Epoch 20 | train_loss 0.4557 acc 0.783 f1 0.783 || val_loss 1.5299 acc 0.420 f1 0.347\n",
            "[W3] GRU Epoch 21 | train_loss 0.4428 acc 0.791 f1 0.790 || val_loss 1.5597 acc 0.434 f1 0.353\n",
            "[W3] GRU Epoch 22 | train_loss 0.4180 acc 0.800 f1 0.799 || val_loss 1.6121 acc 0.434 f1 0.353\n",
            "[W3] GRU Epoch 23 | train_loss 0.3930 acc 0.815 f1 0.814 || val_loss 1.6733 acc 0.432 f1 0.347\n",
            "[W3] GRU Epoch 24 | train_loss 0.3808 acc 0.827 f1 0.827 || val_loss 1.7213 acc 0.426 f1 0.342\n",
            "[W3] GRU Epoch 25 | train_loss 0.3668 acc 0.828 f1 0.828 || val_loss 1.7746 acc 0.432 f1 0.352\n",
            "[W3] GRU Epoch 26 | train_loss 0.3470 acc 0.842 f1 0.842 || val_loss 1.8155 acc 0.434 f1 0.353\n",
            "[W3] GRU Epoch 27 | train_loss 0.3324 acc 0.847 f1 0.847 || val_loss 1.8748 acc 0.426 f1 0.342\n",
            "[W3] GRU Epoch 28 | train_loss 0.3230 acc 0.855 f1 0.855 || val_loss 1.9328 acc 0.430 f1 0.344\n",
            "[W3] GRU Epoch 29 | train_loss 0.3040 acc 0.867 f1 0.867 || val_loss 1.9957 acc 0.434 f1 0.353\n",
            "[W3] GRU Epoch 30 | train_loss 0.2841 acc 0.872 f1 0.872 || val_loss 2.0621 acc 0.422 f1 0.340\n",
            "[W3] GRU Epoch 31 | train_loss 0.2645 acc 0.894 f1 0.893 || val_loss 2.1052 acc 0.447 f1 0.357\n",
            "[W3] GRU Epoch 32 | train_loss 0.2501 acc 0.894 f1 0.894 || val_loss 2.1508 acc 0.436 f1 0.348\n",
            "[W3] GRU Epoch 33 | train_loss 0.2327 acc 0.909 f1 0.909 || val_loss 2.2284 acc 0.455 f1 0.363\n",
            "[W3] GRU Epoch 34 | train_loss 0.2168 acc 0.920 f1 0.920 || val_loss 2.2986 acc 0.455 f1 0.360\n",
            "[W3] GRU Epoch 35 | train_loss 0.1958 acc 0.930 f1 0.930 || val_loss 2.3626 acc 0.432 f1 0.347\n",
            "[W3] GRU Epoch 36 | train_loss 0.1817 acc 0.933 f1 0.933 || val_loss 2.4141 acc 0.451 f1 0.360\n",
            "[W3] GRU Epoch 37 | train_loss 0.1612 acc 0.945 f1 0.945 || val_loss 2.5330 acc 0.438 f1 0.350\n",
            "[W3] GRU Epoch 38 | train_loss 0.1499 acc 0.951 f1 0.951 || val_loss 2.5959 acc 0.453 f1 0.362\n",
            "[W3] GRU Epoch 39 | train_loss 0.1280 acc 0.957 f1 0.957 || val_loss 2.6990 acc 0.459 f1 0.367\n",
            "[W3] GRU Epoch 40 | train_loss 0.1176 acc 0.967 f1 0.967 || val_loss 2.7687 acc 0.461 f1 0.368\n",
            "[W3] GRU Epoch 41 | train_loss 0.1049 acc 0.972 f1 0.972 || val_loss 2.8289 acc 0.449 f1 0.357\n",
            "[W3] GRU Epoch 42 | train_loss 0.0927 acc 0.977 f1 0.977 || val_loss 2.9381 acc 0.459 f1 0.368\n",
            "[W3] GRU Epoch 43 | train_loss 0.0836 acc 0.978 f1 0.978 || val_loss 3.0047 acc 0.457 f1 0.374\n",
            "[W3] GRU Epoch 44 | train_loss 0.0767 acc 0.980 f1 0.980 || val_loss 3.0994 acc 0.453 f1 0.363\n",
            "[W3] GRU Epoch 45 | train_loss 0.0640 acc 0.984 f1 0.984 || val_loss 3.1545 acc 0.447 f1 0.362\n",
            "[W3] GRU Epoch 46 | train_loss 0.0568 acc 0.988 f1 0.988 || val_loss 3.2496 acc 0.451 f1 0.359\n",
            "[W3] GRU Epoch 47 | train_loss 0.0575 acc 0.986 f1 0.986 || val_loss 3.3094 acc 0.463 f1 0.370\n",
            "[W3] GRU Epoch 48 | train_loss 0.0517 acc 0.984 f1 0.984 || val_loss 3.4140 acc 0.444 f1 0.355\n",
            "[W3] GRU Epoch 49 | train_loss 0.0409 acc 0.990 f1 0.990 || val_loss 3.4714 acc 0.457 f1 0.366\n",
            "[W3] GRU Epoch 50 | train_loss 0.0373 acc 0.993 f1 0.993 || val_loss 3.5605 acc 0.461 f1 0.367\n",
            "[W3] GRU Epoch 51 | train_loss 0.0297 acc 0.996 f1 0.996 || val_loss 3.6690 acc 0.467 f1 0.377\n",
            "[W3] GRU Epoch 52 | train_loss 0.0301 acc 0.995 f1 0.995 || val_loss 3.7659 acc 0.449 f1 0.368\n",
            "[W3] GRU Epoch 53 | train_loss 0.0253 acc 0.996 f1 0.996 || val_loss 3.8260 acc 0.457 f1 0.365\n",
            "[W3] GRU Epoch 54 | train_loss 0.0252 acc 0.995 f1 0.995 || val_loss 3.8617 acc 0.451 f1 0.360\n",
            "[W3] GRU Epoch 55 | train_loss 0.0222 acc 0.996 f1 0.996 || val_loss 3.9399 acc 0.453 f1 0.362\n",
            "[W3] GRU Epoch 56 | train_loss 0.0214 acc 0.996 f1 0.996 || val_loss 4.0582 acc 0.449 f1 0.363\n",
            "[W3] GRU Epoch 57 | train_loss 0.0202 acc 0.996 f1 0.996 || val_loss 4.1721 acc 0.455 f1 0.368\n",
            "[W3] GRU Epoch 58 | train_loss 0.0177 acc 0.997 f1 0.997 || val_loss 4.1485 acc 0.447 f1 0.362\n",
            "[W3] GRU Epoch 59 | train_loss 0.0166 acc 0.997 f1 0.997 || val_loss 4.2221 acc 0.453 f1 0.362\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=52\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 980, np.int64(1): 933, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 980, np.int64(2): 980, np.int64(0): 980})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1027 acc 0.335 f1 0.204 || val_loss 1.1049 acc 0.405 f1 0.211\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0952 acc 0.357 f1 0.334 || val_loss 1.0911 acc 0.399 f1 0.331\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0892 acc 0.390 f1 0.390 || val_loss 1.0928 acc 0.325 f1 0.299\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0767 acc 0.435 f1 0.426 || val_loss 1.0920 acc 0.331 f1 0.319\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0486 acc 0.472 f1 0.466 || val_loss 1.1059 acc 0.309 f1 0.300\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9748 acc 0.531 f1 0.518 || val_loss 1.0551 acc 0.389 f1 0.342\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8873 acc 0.562 f1 0.552 || val_loss 1.1336 acc 0.342 f1 0.320\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8222 acc 0.596 f1 0.590 || val_loss 1.0667 acc 0.397 f1 0.345\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7841 acc 0.619 f1 0.614 || val_loss 1.0872 acc 0.366 f1 0.326\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7316 acc 0.649 f1 0.646 || val_loss 1.1518 acc 0.356 f1 0.309\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7021 acc 0.658 f1 0.654 || val_loss 1.1666 acc 0.377 f1 0.325\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6707 acc 0.660 f1 0.657 || val_loss 1.1643 acc 0.364 f1 0.297\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6386 acc 0.694 f1 0.691 || val_loss 1.2032 acc 0.381 f1 0.309\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6111 acc 0.698 f1 0.696 || val_loss 1.2457 acc 0.368 f1 0.301\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5852 acc 0.724 f1 0.722 || val_loss 1.2727 acc 0.383 f1 0.310\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5604 acc 0.721 f1 0.719 || val_loss 1.2912 acc 0.383 f1 0.308\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  52%|    | 52/100 [25:23<24:22, 30.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=53 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=53\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 929, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1202 acc 0.420 f1 0.415 || val_loss 1.1744 acc 0.294 f1 0.283\n",
            "[W3] ANN Epoch 02 | train_loss 0.9735 acc 0.524 f1 0.516 || val_loss 1.1208 acc 0.358 f1 0.325\n",
            "[W3] ANN Epoch 03 | train_loss 0.8867 acc 0.572 f1 0.565 || val_loss 1.1253 acc 0.368 f1 0.329\n",
            "[W3] ANN Epoch 04 | train_loss 0.8016 acc 0.602 f1 0.596 || val_loss 1.0841 acc 0.393 f1 0.340\n",
            "[W3] ANN Epoch 05 | train_loss 0.7495 acc 0.645 f1 0.642 || val_loss 1.0933 acc 0.407 f1 0.341\n",
            "[W3] ANN Epoch 06 | train_loss 0.7108 acc 0.658 f1 0.655 || val_loss 1.1197 acc 0.399 f1 0.327\n",
            "[W3] ANN Epoch 07 | train_loss 0.6751 acc 0.678 f1 0.675 || val_loss 1.1253 acc 0.399 f1 0.318\n",
            "[W3] ANN Epoch 08 | train_loss 0.6593 acc 0.685 f1 0.684 || val_loss 1.1579 acc 0.405 f1 0.323\n",
            "[W3] ANN Epoch 09 | train_loss 0.6228 acc 0.704 f1 0.701 || val_loss 1.1688 acc 0.420 f1 0.343\n",
            "[W3] ANN Epoch 10 | train_loss 0.5991 acc 0.721 f1 0.720 || val_loss 1.1933 acc 0.430 f1 0.340\n",
            "[W3] ANN Epoch 11 | train_loss 0.6121 acc 0.715 f1 0.714 || val_loss 1.1834 acc 0.430 f1 0.345\n",
            "[W3] ANN Epoch 12 | train_loss 0.5707 acc 0.731 f1 0.730 || val_loss 1.1974 acc 0.420 f1 0.333\n",
            "[W3] ANN Epoch 13 | train_loss 0.5600 acc 0.745 f1 0.744 || val_loss 1.1971 acc 0.436 f1 0.350\n",
            "[W3] ANN Epoch 14 | train_loss 0.5608 acc 0.739 f1 0.739 || val_loss 1.2020 acc 0.436 f1 0.333\n",
            "[W3] ANN Epoch 15 | train_loss 0.5476 acc 0.750 f1 0.750 || val_loss 1.2573 acc 0.414 f1 0.322\n",
            "[W3] ANN Epoch 16 | train_loss 0.5149 acc 0.768 f1 0.767 || val_loss 1.2682 acc 0.407 f1 0.325\n",
            "[W3] ANN Epoch 17 | train_loss 0.4970 acc 0.772 f1 0.771 || val_loss 1.2972 acc 0.420 f1 0.338\n",
            "[W3] ANN Epoch 18 | train_loss 0.5172 acc 0.776 f1 0.775 || val_loss 1.2947 acc 0.420 f1 0.342\n",
            "[W3] ANN Epoch 19 | train_loss 0.4954 acc 0.779 f1 0.777 || val_loss 1.3483 acc 0.418 f1 0.345\n",
            "[W3] ANN Epoch 20 | train_loss 0.4768 acc 0.788 f1 0.788 || val_loss 1.3517 acc 0.409 f1 0.334\n",
            "[W3] ANN Epoch 21 | train_loss 0.4994 acc 0.774 f1 0.774 || val_loss 1.3408 acc 0.418 f1 0.342\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=53\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 929, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0715 acc 0.406 f1 0.406 || val_loss 1.0269 acc 0.434 f1 0.322\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9339 acc 0.543 f1 0.540 || val_loss 1.0543 acc 0.407 f1 0.333\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8182 acc 0.593 f1 0.588 || val_loss 1.1382 acc 0.356 f1 0.301\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7325 acc 0.651 f1 0.650 || val_loss 1.1833 acc 0.356 f1 0.309\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6575 acc 0.687 f1 0.686 || val_loss 1.2866 acc 0.358 f1 0.315\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5914 acc 0.719 f1 0.717 || val_loss 1.3261 acc 0.348 f1 0.307\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5356 acc 0.749 f1 0.748 || val_loss 1.3392 acc 0.393 f1 0.317\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5149 acc 0.763 f1 0.763 || val_loss 1.4057 acc 0.391 f1 0.311\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4578 acc 0.797 f1 0.796 || val_loss 1.4518 acc 0.385 f1 0.314\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4268 acc 0.817 f1 0.816 || val_loss 1.5062 acc 0.418 f1 0.334\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.4006 acc 0.826 f1 0.825 || val_loss 1.5834 acc 0.381 f1 0.304\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3536 acc 0.852 f1 0.852 || val_loss 1.6456 acc 0.397 f1 0.311\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3445 acc 0.846 f1 0.846 || val_loss 1.7333 acc 0.385 f1 0.303\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3175 acc 0.875 f1 0.875 || val_loss 1.7598 acc 0.397 f1 0.307\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.3256 acc 0.868 f1 0.868 || val_loss 1.7912 acc 0.420 f1 0.316\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2432 acc 0.914 f1 0.914 || val_loss 1.8795 acc 0.391 f1 0.302\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.2412 acc 0.906 f1 0.906 || val_loss 2.0483 acc 0.377 f1 0.291\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1938 acc 0.926 f1 0.926 || val_loss 2.0531 acc 0.389 f1 0.299\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=53\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 929, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1000 acc 0.356 f1 0.330 || val_loss 1.1008 acc 0.327 f1 0.297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0859 acc 0.393 f1 0.385 || val_loss 1.0950 acc 0.327 f1 0.297\n",
            "[W3] RNN Epoch 03 | train_loss 1.0725 acc 0.430 f1 0.429 || val_loss 1.0966 acc 0.313 f1 0.294\n",
            "[W3] RNN Epoch 04 | train_loss 1.0575 acc 0.443 f1 0.436 || val_loss 1.0895 acc 0.333 f1 0.309\n",
            "[W3] RNN Epoch 05 | train_loss 1.0400 acc 0.468 f1 0.461 || val_loss 1.0876 acc 0.346 f1 0.324\n",
            "[W3] RNN Epoch 06 | train_loss 1.0173 acc 0.482 f1 0.478 || val_loss 1.0915 acc 0.335 f1 0.319\n",
            "[W3] RNN Epoch 07 | train_loss 0.9900 acc 0.516 f1 0.510 || val_loss 1.1032 acc 0.337 f1 0.324\n",
            "[W3] RNN Epoch 08 | train_loss 0.9638 acc 0.530 f1 0.521 || val_loss 1.1057 acc 0.323 f1 0.303\n",
            "[W3] RNN Epoch 09 | train_loss 0.9440 acc 0.545 f1 0.536 || val_loss 1.1087 acc 0.313 f1 0.295\n",
            "[W3] RNN Epoch 10 | train_loss 0.9248 acc 0.552 f1 0.542 || val_loss 1.1040 acc 0.325 f1 0.302\n",
            "[W3] RNN Epoch 11 | train_loss 0.8924 acc 0.590 f1 0.582 || val_loss 1.1161 acc 0.333 f1 0.315\n",
            "[W3] RNN Epoch 12 | train_loss 0.8859 acc 0.575 f1 0.565 || val_loss 1.1275 acc 0.329 f1 0.307\n",
            "[W3] RNN Epoch 13 | train_loss 0.8543 acc 0.598 f1 0.588 || val_loss 1.1056 acc 0.348 f1 0.310\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=53\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 929, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0991 acc 0.329 f1 0.268 || val_loss 1.0909 acc 0.432 f1 0.357\n",
            "[W3] GRU Epoch 02 | train_loss 1.0923 acc 0.393 f1 0.387 || val_loss 1.0890 acc 0.405 f1 0.347\n",
            "[W3] GRU Epoch 03 | train_loss 1.0844 acc 0.406 f1 0.403 || val_loss 1.0896 acc 0.362 f1 0.324\n",
            "[W3] GRU Epoch 04 | train_loss 1.0740 acc 0.441 f1 0.440 || val_loss 1.0902 acc 0.340 f1 0.315\n",
            "[W3] GRU Epoch 05 | train_loss 1.0555 acc 0.464 f1 0.460 || val_loss 1.0937 acc 0.327 f1 0.302\n",
            "[W3] GRU Epoch 06 | train_loss 1.0245 acc 0.500 f1 0.493 || val_loss 1.0895 acc 0.352 f1 0.317\n",
            "[W3] GRU Epoch 07 | train_loss 0.9707 acc 0.541 f1 0.534 || val_loss 1.1282 acc 0.335 f1 0.317\n",
            "[W3] GRU Epoch 08 | train_loss 0.8995 acc 0.583 f1 0.573 || val_loss 1.1061 acc 0.370 f1 0.309\n",
            "[W3] GRU Epoch 09 | train_loss 0.8280 acc 0.604 f1 0.600 || val_loss 1.1764 acc 0.366 f1 0.328\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=53\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 929, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0995 acc 0.340 f1 0.272 || val_loss 1.1094 acc 0.189 f1 0.163\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0947 acc 0.378 f1 0.320 || val_loss 1.1021 acc 0.255 f1 0.248\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0879 acc 0.420 f1 0.405 || val_loss 1.0989 acc 0.257 f1 0.251\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0738 acc 0.443 f1 0.435 || val_loss 1.0957 acc 0.307 f1 0.280\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0445 acc 0.462 f1 0.448 || val_loss 1.0864 acc 0.335 f1 0.304\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9771 acc 0.514 f1 0.502 || val_loss 1.0982 acc 0.333 f1 0.293\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8800 acc 0.566 f1 0.556 || val_loss 1.0878 acc 0.387 f1 0.309\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8118 acc 0.603 f1 0.598 || val_loss 1.1296 acc 0.379 f1 0.298\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7713 acc 0.619 f1 0.613 || val_loss 1.1583 acc 0.370 f1 0.294\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7381 acc 0.639 f1 0.635 || val_loss 1.1411 acc 0.377 f1 0.298\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7049 acc 0.653 f1 0.648 || val_loss 1.1987 acc 0.362 f1 0.293\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6751 acc 0.670 f1 0.666 || val_loss 1.2236 acc 0.374 f1 0.294\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6536 acc 0.674 f1 0.672 || val_loss 1.2630 acc 0.374 f1 0.317\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6338 acc 0.681 f1 0.678 || val_loss 1.2965 acc 0.364 f1 0.294\n",
            "[W3] LSTM Epoch 15 | train_loss 0.6042 acc 0.702 f1 0.698 || val_loss 1.3227 acc 0.385 f1 0.303\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5774 acc 0.710 f1 0.709 || val_loss 1.3289 acc 0.405 f1 0.328\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5556 acc 0.727 f1 0.726 || val_loss 1.3510 acc 0.383 f1 0.301\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5295 acc 0.747 f1 0.746 || val_loss 1.4152 acc 0.379 f1 0.300\n",
            "[W3] LSTM Epoch 19 | train_loss 0.5054 acc 0.748 f1 0.747 || val_loss 1.4271 acc 0.395 f1 0.305\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4836 acc 0.766 f1 0.765 || val_loss 1.4955 acc 0.397 f1 0.321\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4627 acc 0.776 f1 0.775 || val_loss 1.5891 acc 0.397 f1 0.317\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4430 acc 0.786 f1 0.786 || val_loss 1.6038 acc 0.401 f1 0.315\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4264 acc 0.785 f1 0.785 || val_loss 1.6775 acc 0.412 f1 0.332\n",
            "[W3] LSTM Epoch 24 | train_loss 0.4176 acc 0.804 f1 0.804 || val_loss 1.7549 acc 0.412 f1 0.341\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3967 acc 0.819 f1 0.819 || val_loss 1.7714 acc 0.416 f1 0.331\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3814 acc 0.822 f1 0.821 || val_loss 1.8201 acc 0.422 f1 0.336\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3641 acc 0.829 f1 0.829 || val_loss 1.8617 acc 0.420 f1 0.333\n",
            "[W3] LSTM Epoch 28 | train_loss 0.3466 acc 0.843 f1 0.843 || val_loss 1.9031 acc 0.414 f1 0.324\n",
            "[W3] LSTM Epoch 29 | train_loss 0.3328 acc 0.851 f1 0.851 || val_loss 1.9490 acc 0.416 f1 0.331\n",
            "[W3] LSTM Epoch 30 | train_loss 0.3221 acc 0.856 f1 0.856 || val_loss 2.0190 acc 0.414 f1 0.329\n",
            "[W3] LSTM Epoch 31 | train_loss 0.3030 acc 0.875 f1 0.875 || val_loss 2.0893 acc 0.407 f1 0.325\n",
            "[W3] LSTM Epoch 32 | train_loss 0.2851 acc 0.882 f1 0.882 || val_loss 2.1893 acc 0.418 f1 0.331\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  53%|    | 53/100 [25:52<23:28, 29.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=54 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=54\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1297 acc 0.401 f1 0.401 || val_loss 1.1365 acc 0.329 f1 0.306\n",
            "[W3] ANN Epoch 02 | train_loss 0.9833 acc 0.489 f1 0.484 || val_loss 1.1240 acc 0.329 f1 0.297\n",
            "[W3] ANN Epoch 03 | train_loss 0.8851 acc 0.558 f1 0.551 || val_loss 1.1144 acc 0.387 f1 0.346\n",
            "[W3] ANN Epoch 04 | train_loss 0.8040 acc 0.620 f1 0.615 || val_loss 1.1131 acc 0.364 f1 0.312\n",
            "[W3] ANN Epoch 05 | train_loss 0.7430 acc 0.643 f1 0.638 || val_loss 1.1146 acc 0.383 f1 0.319\n",
            "[W3] ANN Epoch 06 | train_loss 0.7045 acc 0.660 f1 0.658 || val_loss 1.1045 acc 0.418 f1 0.345\n",
            "[W3] ANN Epoch 07 | train_loss 0.6874 acc 0.678 f1 0.675 || val_loss 1.1500 acc 0.395 f1 0.320\n",
            "[W3] ANN Epoch 08 | train_loss 0.6434 acc 0.692 f1 0.691 || val_loss 1.1304 acc 0.401 f1 0.329\n",
            "[W3] ANN Epoch 09 | train_loss 0.6241 acc 0.701 f1 0.699 || val_loss 1.1591 acc 0.407 f1 0.334\n",
            "[W3] ANN Epoch 10 | train_loss 0.6143 acc 0.717 f1 0.715 || val_loss 1.1567 acc 0.424 f1 0.366\n",
            "[W3] ANN Epoch 11 | train_loss 0.5785 acc 0.726 f1 0.725 || val_loss 1.1817 acc 0.426 f1 0.352\n",
            "[W3] ANN Epoch 12 | train_loss 0.5976 acc 0.728 f1 0.727 || val_loss 1.1959 acc 0.432 f1 0.356\n",
            "[W3] ANN Epoch 13 | train_loss 0.5630 acc 0.744 f1 0.743 || val_loss 1.1798 acc 0.416 f1 0.345\n",
            "[W3] ANN Epoch 14 | train_loss 0.5453 acc 0.753 f1 0.752 || val_loss 1.2153 acc 0.422 f1 0.344\n",
            "[W3] ANN Epoch 15 | train_loss 0.5369 acc 0.747 f1 0.747 || val_loss 1.2321 acc 0.403 f1 0.330\n",
            "[W3] ANN Epoch 16 | train_loss 0.5241 acc 0.763 f1 0.762 || val_loss 1.3220 acc 0.395 f1 0.328\n",
            "[W3] ANN Epoch 17 | train_loss 0.5021 acc 0.771 f1 0.770 || val_loss 1.2988 acc 0.422 f1 0.339\n",
            "[W3] ANN Epoch 18 | train_loss 0.5002 acc 0.771 f1 0.770 || val_loss 1.2991 acc 0.420 f1 0.342\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=54\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0682 acc 0.412 f1 0.413 || val_loss 1.0286 acc 0.432 f1 0.352\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9395 acc 0.545 f1 0.539 || val_loss 1.0609 acc 0.397 f1 0.317\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8183 acc 0.615 f1 0.611 || val_loss 1.1198 acc 0.385 f1 0.321\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7288 acc 0.666 f1 0.664 || val_loss 1.1527 acc 0.397 f1 0.344\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6446 acc 0.709 f1 0.707 || val_loss 1.2038 acc 0.403 f1 0.331\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5981 acc 0.727 f1 0.724 || val_loss 1.2361 acc 0.424 f1 0.337\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5467 acc 0.750 f1 0.749 || val_loss 1.2524 acc 0.416 f1 0.347\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4965 acc 0.786 f1 0.785 || val_loss 1.3137 acc 0.418 f1 0.337\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4610 acc 0.798 f1 0.796 || val_loss 1.3471 acc 0.422 f1 0.346\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=54\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0984 acc 0.342 f1 0.327 || val_loss 1.0961 acc 0.319 f1 0.294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0827 acc 0.409 f1 0.396 || val_loss 1.0945 acc 0.335 f1 0.320\n",
            "[W3] RNN Epoch 03 | train_loss 1.0669 acc 0.426 f1 0.413 || val_loss 1.0906 acc 0.346 f1 0.331\n",
            "[W3] RNN Epoch 04 | train_loss 1.0504 acc 0.456 f1 0.448 || val_loss 1.0876 acc 0.352 f1 0.331\n",
            "[W3] RNN Epoch 05 | train_loss 1.0343 acc 0.469 f1 0.463 || val_loss 1.0773 acc 0.366 f1 0.340\n",
            "[W3] RNN Epoch 06 | train_loss 1.0132 acc 0.491 f1 0.483 || val_loss 1.0810 acc 0.344 f1 0.317\n",
            "[W3] RNN Epoch 07 | train_loss 0.9916 acc 0.514 f1 0.506 || val_loss 1.0707 acc 0.379 f1 0.341\n",
            "[W3] RNN Epoch 08 | train_loss 0.9613 acc 0.534 f1 0.528 || val_loss 1.0815 acc 0.362 f1 0.330\n",
            "[W3] RNN Epoch 09 | train_loss 0.9383 acc 0.556 f1 0.546 || val_loss 1.0649 acc 0.379 f1 0.334\n",
            "[W3] RNN Epoch 10 | train_loss 0.9116 acc 0.566 f1 0.557 || val_loss 1.1034 acc 0.366 f1 0.338\n",
            "[W3] RNN Epoch 11 | train_loss 0.8872 acc 0.575 f1 0.565 || val_loss 1.0816 acc 0.379 f1 0.337\n",
            "[W3] RNN Epoch 12 | train_loss 0.8634 acc 0.591 f1 0.582 || val_loss 1.1100 acc 0.366 f1 0.337\n",
            "[W3] RNN Epoch 13 | train_loss 0.8399 acc 0.603 f1 0.593 || val_loss 1.0823 acc 0.407 f1 0.357\n",
            "[W3] RNN Epoch 14 | train_loss 0.8133 acc 0.608 f1 0.599 || val_loss 1.0959 acc 0.381 f1 0.336\n",
            "[W3] RNN Epoch 15 | train_loss 0.7878 acc 0.641 f1 0.634 || val_loss 1.1384 acc 0.364 f1 0.331\n",
            "[W3] RNN Epoch 16 | train_loss 0.7690 acc 0.635 f1 0.626 || val_loss 1.0923 acc 0.405 f1 0.339\n",
            "[W3] RNN Epoch 17 | train_loss 0.7442 acc 0.649 f1 0.641 || val_loss 1.1246 acc 0.401 f1 0.354\n",
            "[W3] RNN Epoch 18 | train_loss 0.7122 acc 0.669 f1 0.661 || val_loss 1.1380 acc 0.385 f1 0.335\n",
            "[W3] RNN Epoch 19 | train_loss 0.6955 acc 0.669 f1 0.662 || val_loss 1.1381 acc 0.393 f1 0.327\n",
            "[W3] RNN Epoch 20 | train_loss 0.6755 acc 0.688 f1 0.681 || val_loss 1.1534 acc 0.391 f1 0.329\n",
            "[W3] RNN Epoch 21 | train_loss 0.6512 acc 0.692 f1 0.686 || val_loss 1.1729 acc 0.414 f1 0.353\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=54\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0999 acc 0.338 f1 0.216 || val_loss 1.1110 acc 0.220 f1 0.208\n",
            "[W3] GRU Epoch 02 | train_loss 1.0909 acc 0.405 f1 0.385 || val_loss 1.0921 acc 0.346 f1 0.319\n",
            "[W3] GRU Epoch 03 | train_loss 1.0800 acc 0.424 f1 0.424 || val_loss 1.0766 acc 0.379 f1 0.328\n",
            "[W3] GRU Epoch 04 | train_loss 1.0668 acc 0.452 f1 0.453 || val_loss 1.0641 acc 0.385 f1 0.326\n",
            "[W3] GRU Epoch 05 | train_loss 1.0499 acc 0.482 f1 0.479 || val_loss 1.0673 acc 0.381 f1 0.325\n",
            "[W3] GRU Epoch 06 | train_loss 1.0205 acc 0.516 f1 0.511 || val_loss 1.0654 acc 0.360 f1 0.320\n",
            "[W3] GRU Epoch 07 | train_loss 0.9778 acc 0.530 f1 0.526 || val_loss 1.0669 acc 0.379 f1 0.332\n",
            "[W3] GRU Epoch 08 | train_loss 0.9187 acc 0.568 f1 0.558 || val_loss 1.0810 acc 0.389 f1 0.332\n",
            "[W3] GRU Epoch 09 | train_loss 0.8449 acc 0.600 f1 0.592 || val_loss 1.0967 acc 0.399 f1 0.318\n",
            "[W3] GRU Epoch 10 | train_loss 0.7920 acc 0.627 f1 0.620 || val_loss 1.1419 acc 0.387 f1 0.325\n",
            "[W3] GRU Epoch 11 | train_loss 0.7553 acc 0.642 f1 0.637 || val_loss 1.1808 acc 0.372 f1 0.313\n",
            "[W3] GRU Epoch 12 | train_loss 0.7151 acc 0.663 f1 0.657 || val_loss 1.1885 acc 0.391 f1 0.325\n",
            "[W3] GRU Epoch 13 | train_loss 0.6859 acc 0.674 f1 0.669 || val_loss 1.1974 acc 0.420 f1 0.342\n",
            "[W3] GRU Epoch 14 | train_loss 0.6541 acc 0.693 f1 0.689 || val_loss 1.2414 acc 0.391 f1 0.309\n",
            "[W3] GRU Epoch 15 | train_loss 0.6270 acc 0.706 f1 0.703 || val_loss 1.2865 acc 0.387 f1 0.315\n",
            "[W3] GRU Epoch 16 | train_loss 0.5921 acc 0.721 f1 0.718 || val_loss 1.3210 acc 0.399 f1 0.314\n",
            "[W3] GRU Epoch 17 | train_loss 0.5754 acc 0.731 f1 0.729 || val_loss 1.3383 acc 0.407 f1 0.314\n",
            "[W3] GRU Epoch 18 | train_loss 0.5542 acc 0.748 f1 0.746 || val_loss 1.3952 acc 0.407 f1 0.334\n",
            "[W3] GRU Epoch 19 | train_loss 0.5297 acc 0.745 f1 0.744 || val_loss 1.4149 acc 0.409 f1 0.327\n",
            "[W3] GRU Epoch 20 | train_loss 0.4978 acc 0.772 f1 0.771 || val_loss 1.4637 acc 0.403 f1 0.323\n",
            "[W3] GRU Epoch 21 | train_loss 0.4873 acc 0.767 f1 0.766 || val_loss 1.4762 acc 0.405 f1 0.318\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=54\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0994 acc 0.333 f1 0.210 || val_loss 1.0983 acc 0.389 f1 0.277\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0960 acc 0.371 f1 0.368 || val_loss 1.0941 acc 0.385 f1 0.337\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0906 acc 0.392 f1 0.374 || val_loss 1.0939 acc 0.350 f1 0.314\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0806 acc 0.414 f1 0.415 || val_loss 1.1016 acc 0.284 f1 0.275\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0587 acc 0.466 f1 0.458 || val_loss 1.0984 acc 0.296 f1 0.280\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0052 acc 0.519 f1 0.510 || val_loss 1.1688 acc 0.241 f1 0.240\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9213 acc 0.551 f1 0.539 || val_loss 1.1548 acc 0.305 f1 0.284\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8327 acc 0.601 f1 0.594 || val_loss 1.1721 acc 0.317 f1 0.272\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7776 acc 0.614 f1 0.607 || val_loss 1.1733 acc 0.344 f1 0.296\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7219 acc 0.653 f1 0.649 || val_loss 1.1750 acc 0.387 f1 0.304\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  54%|    | 54/100 [26:15<21:16, 27.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=55 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=55\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 990, np.int64(1): 925, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 990, np.int64(2): 990, np.int64(0): 990})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1494 acc 0.384 f1 0.378 || val_loss 1.1872 acc 0.243 f1 0.241\n",
            "[W3] ANN Epoch 02 | train_loss 0.9779 acc 0.523 f1 0.513 || val_loss 1.1660 acc 0.342 f1 0.325\n",
            "[W3] ANN Epoch 03 | train_loss 0.8971 acc 0.555 f1 0.545 || val_loss 1.1727 acc 0.329 f1 0.292\n",
            "[W3] ANN Epoch 04 | train_loss 0.7922 acc 0.618 f1 0.610 || val_loss 1.1507 acc 0.344 f1 0.288\n",
            "[W3] ANN Epoch 05 | train_loss 0.7409 acc 0.643 f1 0.637 || val_loss 1.1235 acc 0.403 f1 0.338\n",
            "[W3] ANN Epoch 06 | train_loss 0.6803 acc 0.678 f1 0.675 || val_loss 1.1508 acc 0.393 f1 0.334\n",
            "[W3] ANN Epoch 07 | train_loss 0.6387 acc 0.684 f1 0.681 || val_loss 1.1885 acc 0.389 f1 0.322\n",
            "[W3] ANN Epoch 08 | train_loss 0.5987 acc 0.721 f1 0.719 || val_loss 1.2010 acc 0.416 f1 0.335\n",
            "[W3] ANN Epoch 09 | train_loss 0.5735 acc 0.735 f1 0.733 || val_loss 1.1905 acc 0.432 f1 0.336\n",
            "[W3] ANN Epoch 10 | train_loss 0.5489 acc 0.751 f1 0.750 || val_loss 1.2380 acc 0.418 f1 0.341\n",
            "[W3] ANN Epoch 11 | train_loss 0.5135 acc 0.762 f1 0.761 || val_loss 1.2864 acc 0.405 f1 0.322\n",
            "[W3] ANN Epoch 12 | train_loss 0.5036 acc 0.775 f1 0.774 || val_loss 1.3127 acc 0.409 f1 0.322\n",
            "[W3] ANN Epoch 13 | train_loss 0.4708 acc 0.787 f1 0.786 || val_loss 1.3006 acc 0.442 f1 0.359\n",
            "[W3] ANN Epoch 14 | train_loss 0.4443 acc 0.809 f1 0.809 || val_loss 1.3843 acc 0.447 f1 0.358\n",
            "[W3] ANN Epoch 15 | train_loss 0.4380 acc 0.804 f1 0.804 || val_loss 1.3946 acc 0.434 f1 0.342\n",
            "[W3] ANN Epoch 16 | train_loss 0.4076 acc 0.825 f1 0.824 || val_loss 1.4088 acc 0.434 f1 0.348\n",
            "[W3] ANN Epoch 17 | train_loss 0.3886 acc 0.832 f1 0.832 || val_loss 1.4405 acc 0.457 f1 0.348\n",
            "[W3] ANN Epoch 18 | train_loss 0.3725 acc 0.837 f1 0.837 || val_loss 1.4739 acc 0.463 f1 0.358\n",
            "[W3] ANN Epoch 19 | train_loss 0.3803 acc 0.840 f1 0.840 || val_loss 1.5302 acc 0.434 f1 0.348\n",
            "[W3] ANN Epoch 20 | train_loss 0.3753 acc 0.844 f1 0.844 || val_loss 1.5339 acc 0.416 f1 0.324\n",
            "[W3] ANN Epoch 21 | train_loss 0.3634 acc 0.841 f1 0.841 || val_loss 1.5485 acc 0.416 f1 0.329\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=55\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 990, np.int64(1): 925, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 990, np.int64(2): 990, np.int64(0): 990})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0659 acc 0.413 f1 0.414 || val_loss 1.0350 acc 0.416 f1 0.324\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9259 acc 0.541 f1 0.536 || val_loss 1.0258 acc 0.432 f1 0.344\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.7972 acc 0.609 f1 0.605 || val_loss 1.0860 acc 0.409 f1 0.359\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.6998 acc 0.673 f1 0.672 || val_loss 1.1560 acc 0.403 f1 0.343\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6222 acc 0.717 f1 0.716 || val_loss 1.1784 acc 0.414 f1 0.354\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5600 acc 0.752 f1 0.751 || val_loss 1.2406 acc 0.412 f1 0.344\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5142 acc 0.764 f1 0.763 || val_loss 1.2859 acc 0.401 f1 0.322\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4642 acc 0.795 f1 0.794 || val_loss 1.3440 acc 0.426 f1 0.348\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4036 acc 0.825 f1 0.824 || val_loss 1.3921 acc 0.430 f1 0.345\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3565 acc 0.851 f1 0.850 || val_loss 1.4912 acc 0.438 f1 0.360\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3038 acc 0.880 f1 0.881 || val_loss 1.6405 acc 0.418 f1 0.337\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2776 acc 0.886 f1 0.886 || val_loss 1.6380 acc 0.434 f1 0.362\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2343 acc 0.919 f1 0.919 || val_loss 1.8013 acc 0.409 f1 0.326\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.1960 acc 0.931 f1 0.931 || val_loss 1.9091 acc 0.442 f1 0.359\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1757 acc 0.937 f1 0.937 || val_loss 2.0668 acc 0.430 f1 0.369\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1687 acc 0.943 f1 0.943 || val_loss 2.1738 acc 0.422 f1 0.353\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1455 acc 0.947 f1 0.947 || val_loss 2.1574 acc 0.444 f1 0.358\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1231 acc 0.961 f1 0.961 || val_loss 2.2836 acc 0.420 f1 0.363\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1059 acc 0.965 f1 0.965 || val_loss 2.3276 acc 0.414 f1 0.346\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.0871 acc 0.975 f1 0.975 || val_loss 2.4475 acc 0.405 f1 0.349\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.0856 acc 0.973 f1 0.973 || val_loss 2.5339 acc 0.401 f1 0.337\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.0826 acc 0.971 f1 0.971 || val_loss 2.6103 acc 0.428 f1 0.372\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.0704 acc 0.977 f1 0.977 || val_loss 2.6979 acc 0.422 f1 0.348\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.0799 acc 0.971 f1 0.971 || val_loss 2.8293 acc 0.407 f1 0.353\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.0642 acc 0.980 f1 0.980 || val_loss 2.8488 acc 0.420 f1 0.352\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.0673 acc 0.979 f1 0.979 || val_loss 2.9402 acc 0.424 f1 0.369\n",
            "[W3] CNN1D Epoch 27 | train_loss 0.0614 acc 0.980 f1 0.980 || val_loss 2.8799 acc 0.412 f1 0.344\n",
            "[W3] CNN1D Epoch 28 | train_loss 0.0620 acc 0.979 f1 0.979 || val_loss 2.9709 acc 0.420 f1 0.349\n",
            "[W3] CNN1D Epoch 29 | train_loss 0.0484 acc 0.986 f1 0.986 || val_loss 3.0468 acc 0.428 f1 0.362\n",
            "[W3] CNN1D Epoch 30 | train_loss 0.0474 acc 0.985 f1 0.985 || val_loss 3.1456 acc 0.426 f1 0.364\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=55\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 990, np.int64(1): 925, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 990, np.int64(2): 990, np.int64(0): 990})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0998 acc 0.339 f1 0.265 || val_loss 1.0896 acc 0.409 f1 0.313\n",
            "[W3] RNN Epoch 02 | train_loss 1.0871 acc 0.385 f1 0.369 || val_loss 1.0921 acc 0.366 f1 0.309\n",
            "[W3] RNN Epoch 03 | train_loss 1.0738 acc 0.431 f1 0.431 || val_loss 1.0824 acc 0.372 f1 0.322\n",
            "[W3] RNN Epoch 04 | train_loss 1.0510 acc 0.458 f1 0.456 || val_loss 1.0991 acc 0.368 f1 0.339\n",
            "[W3] RNN Epoch 05 | train_loss 1.0275 acc 0.493 f1 0.487 || val_loss 1.0843 acc 0.379 f1 0.346\n",
            "[W3] RNN Epoch 06 | train_loss 1.0004 acc 0.518 f1 0.514 || val_loss 1.1071 acc 0.333 f1 0.315\n",
            "[W3] RNN Epoch 07 | train_loss 0.9752 acc 0.530 f1 0.519 || val_loss 1.0667 acc 0.381 f1 0.347\n",
            "[W3] RNN Epoch 08 | train_loss 0.9411 acc 0.559 f1 0.550 || val_loss 1.0822 acc 0.374 f1 0.343\n",
            "[W3] RNN Epoch 09 | train_loss 0.9082 acc 0.579 f1 0.571 || val_loss 1.0832 acc 0.397 f1 0.361\n",
            "[W3] RNN Epoch 10 | train_loss 0.8804 acc 0.584 f1 0.575 || val_loss 1.0863 acc 0.395 f1 0.360\n",
            "[W3] RNN Epoch 11 | train_loss 0.8569 acc 0.605 f1 0.596 || val_loss 1.0914 acc 0.397 f1 0.364\n",
            "[W3] RNN Epoch 12 | train_loss 0.8315 acc 0.612 f1 0.602 || val_loss 1.0970 acc 0.405 f1 0.364\n",
            "[W3] RNN Epoch 13 | train_loss 0.8023 acc 0.630 f1 0.623 || val_loss 1.1113 acc 0.385 f1 0.339\n",
            "[W3] RNN Epoch 14 | train_loss 0.7759 acc 0.642 f1 0.633 || val_loss 1.0938 acc 0.412 f1 0.345\n",
            "[W3] RNN Epoch 15 | train_loss 0.7531 acc 0.647 f1 0.640 || val_loss 1.1256 acc 0.379 f1 0.325\n",
            "[W3] RNN Epoch 16 | train_loss 0.7282 acc 0.664 f1 0.658 || val_loss 1.1311 acc 0.399 f1 0.342\n",
            "[W3] RNN Epoch 17 | train_loss 0.6998 acc 0.681 f1 0.674 || val_loss 1.1428 acc 0.405 f1 0.344\n",
            "[W3] RNN Epoch 18 | train_loss 0.6862 acc 0.681 f1 0.675 || val_loss 1.1644 acc 0.393 f1 0.340\n",
            "[W3] RNN Epoch 19 | train_loss 0.6709 acc 0.691 f1 0.683 || val_loss 1.1625 acc 0.407 f1 0.349\n",
            "[W3] RNN Epoch 20 | train_loss 0.6550 acc 0.695 f1 0.689 || val_loss 1.1630 acc 0.409 f1 0.354\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=55\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 990, np.int64(1): 925, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 990, np.int64(2): 990, np.int64(0): 990})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0997 acc 0.342 f1 0.236 || val_loss 1.1067 acc 0.261 f1 0.246\n",
            "[W3] GRU Epoch 02 | train_loss 1.0907 acc 0.393 f1 0.389 || val_loss 1.1017 acc 0.302 f1 0.283\n",
            "[W3] GRU Epoch 03 | train_loss 1.0827 acc 0.416 f1 0.415 || val_loss 1.0978 acc 0.317 f1 0.291\n",
            "[W3] GRU Epoch 04 | train_loss 1.0692 acc 0.434 f1 0.433 || val_loss 1.1028 acc 0.305 f1 0.283\n",
            "[W3] GRU Epoch 05 | train_loss 1.0480 acc 0.472 f1 0.465 || val_loss 1.0845 acc 0.374 f1 0.341\n",
            "[W3] GRU Epoch 06 | train_loss 1.0139 acc 0.504 f1 0.502 || val_loss 1.1127 acc 0.321 f1 0.300\n",
            "[W3] GRU Epoch 07 | train_loss 0.9488 acc 0.561 f1 0.551 || val_loss 1.0774 acc 0.358 f1 0.304\n",
            "[W3] GRU Epoch 08 | train_loss 0.8766 acc 0.590 f1 0.582 || val_loss 1.1249 acc 0.344 f1 0.297\n",
            "[W3] GRU Epoch 09 | train_loss 0.8059 acc 0.619 f1 0.612 || val_loss 1.1458 acc 0.350 f1 0.309\n",
            "[W3] GRU Epoch 10 | train_loss 0.7552 acc 0.641 f1 0.636 || val_loss 1.1557 acc 0.403 f1 0.338\n",
            "[W3] GRU Epoch 11 | train_loss 0.7084 acc 0.669 f1 0.664 || val_loss 1.1636 acc 0.364 f1 0.292\n",
            "[W3] GRU Epoch 12 | train_loss 0.6768 acc 0.684 f1 0.680 || val_loss 1.2336 acc 0.348 f1 0.293\n",
            "[W3] GRU Epoch 13 | train_loss 0.6406 acc 0.698 f1 0.694 || val_loss 1.2654 acc 0.364 f1 0.302\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=55\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 990, np.int64(1): 925, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 990, np.int64(2): 990, np.int64(0): 990})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1002 acc 0.329 f1 0.208 || val_loss 1.1027 acc 0.422 f1 0.249\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0955 acc 0.377 f1 0.312 || val_loss 1.1002 acc 0.325 f1 0.251\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0913 acc 0.403 f1 0.389 || val_loss 1.0968 acc 0.309 f1 0.273\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0799 acc 0.433 f1 0.433 || val_loss 1.0983 acc 0.313 f1 0.301\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0490 acc 0.487 f1 0.478 || val_loss 1.1069 acc 0.331 f1 0.319\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9723 acc 0.530 f1 0.521 || val_loss 1.1568 acc 0.315 f1 0.300\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8818 acc 0.565 f1 0.557 || val_loss 1.1156 acc 0.346 f1 0.305\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8081 acc 0.618 f1 0.612 || val_loss 1.0857 acc 0.383 f1 0.319\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7582 acc 0.626 f1 0.621 || val_loss 1.1267 acc 0.395 f1 0.336\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7137 acc 0.649 f1 0.645 || val_loss 1.1642 acc 0.387 f1 0.312\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6885 acc 0.665 f1 0.662 || val_loss 1.1856 acc 0.387 f1 0.323\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6562 acc 0.681 f1 0.677 || val_loss 1.1986 acc 0.399 f1 0.329\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6272 acc 0.692 f1 0.689 || val_loss 1.2165 acc 0.412 f1 0.343\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5965 acc 0.713 f1 0.710 || val_loss 1.2451 acc 0.420 f1 0.333\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5695 acc 0.725 f1 0.724 || val_loss 1.3085 acc 0.424 f1 0.350\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5454 acc 0.732 f1 0.730 || val_loss 1.3352 acc 0.403 f1 0.327\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5237 acc 0.737 f1 0.736 || val_loss 1.3521 acc 0.409 f1 0.336\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5028 acc 0.765 f1 0.764 || val_loss 1.4450 acc 0.414 f1 0.351\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4842 acc 0.767 f1 0.765 || val_loss 1.4051 acc 0.418 f1 0.343\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4685 acc 0.772 f1 0.772 || val_loss 1.4726 acc 0.420 f1 0.358\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4462 acc 0.788 f1 0.788 || val_loss 1.4939 acc 0.428 f1 0.353\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4221 acc 0.794 f1 0.793 || val_loss 1.5250 acc 0.409 f1 0.332\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4064 acc 0.810 f1 0.810 || val_loss 1.5755 acc 0.407 f1 0.333\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3911 acc 0.820 f1 0.820 || val_loss 1.6410 acc 0.409 f1 0.343\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3718 acc 0.828 f1 0.828 || val_loss 1.7190 acc 0.416 f1 0.348\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3620 acc 0.836 f1 0.836 || val_loss 1.8061 acc 0.397 f1 0.332\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3407 acc 0.851 f1 0.850 || val_loss 1.7999 acc 0.422 f1 0.344\n",
            "[W3] LSTM Epoch 28 | train_loss 0.3308 acc 0.852 f1 0.852 || val_loss 1.8677 acc 0.436 f1 0.356\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  55%|    | 55/100 [26:48<22:05, 29.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=56 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=56\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 931, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1127 acc 0.422 f1 0.421 || val_loss 1.1402 acc 0.329 f1 0.312\n",
            "[W3] ANN Epoch 02 | train_loss 0.9466 acc 0.538 f1 0.529 || val_loss 1.1392 acc 0.329 f1 0.299\n",
            "[W3] ANN Epoch 03 | train_loss 0.8722 acc 0.573 f1 0.566 || val_loss 1.1288 acc 0.360 f1 0.323\n",
            "[W3] ANN Epoch 04 | train_loss 0.7883 acc 0.623 f1 0.616 || val_loss 1.1419 acc 0.354 f1 0.306\n",
            "[W3] ANN Epoch 05 | train_loss 0.7226 acc 0.659 f1 0.655 || val_loss 1.1597 acc 0.360 f1 0.314\n",
            "[W3] ANN Epoch 06 | train_loss 0.6797 acc 0.672 f1 0.669 || val_loss 1.1482 acc 0.381 f1 0.321\n",
            "[W3] ANN Epoch 07 | train_loss 0.6607 acc 0.691 f1 0.688 || val_loss 1.1424 acc 0.418 f1 0.344\n",
            "[W3] ANN Epoch 08 | train_loss 0.6230 acc 0.702 f1 0.701 || val_loss 1.1677 acc 0.422 f1 0.350\n",
            "[W3] ANN Epoch 09 | train_loss 0.5996 acc 0.720 f1 0.718 || val_loss 1.1876 acc 0.393 f1 0.339\n",
            "[W3] ANN Epoch 10 | train_loss 0.5670 acc 0.734 f1 0.733 || val_loss 1.1710 acc 0.442 f1 0.369\n",
            "[W3] ANN Epoch 11 | train_loss 0.5529 acc 0.737 f1 0.737 || val_loss 1.1890 acc 0.438 f1 0.377\n",
            "[W3] ANN Epoch 12 | train_loss 0.5269 acc 0.764 f1 0.763 || val_loss 1.2204 acc 0.401 f1 0.337\n",
            "[W3] ANN Epoch 13 | train_loss 0.5195 acc 0.764 f1 0.763 || val_loss 1.2413 acc 0.416 f1 0.343\n",
            "[W3] ANN Epoch 14 | train_loss 0.4996 acc 0.781 f1 0.780 || val_loss 1.2705 acc 0.430 f1 0.368\n",
            "[W3] ANN Epoch 15 | train_loss 0.5053 acc 0.778 f1 0.777 || val_loss 1.2789 acc 0.418 f1 0.341\n",
            "[W3] ANN Epoch 16 | train_loss 0.4713 acc 0.795 f1 0.794 || val_loss 1.2833 acc 0.405 f1 0.334\n",
            "[W3] ANN Epoch 17 | train_loss 0.4524 acc 0.803 f1 0.802 || val_loss 1.2958 acc 0.422 f1 0.357\n",
            "[W3] ANN Epoch 18 | train_loss 0.4587 acc 0.798 f1 0.798 || val_loss 1.3188 acc 0.420 f1 0.345\n",
            "[W3] ANN Epoch 19 | train_loss 0.4172 acc 0.823 f1 0.822 || val_loss 1.3272 acc 0.424 f1 0.354\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=56\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 931, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0678 acc 0.415 f1 0.417 || val_loss 1.0320 acc 0.434 f1 0.317\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9161 acc 0.548 f1 0.541 || val_loss 1.0607 acc 0.397 f1 0.318\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.7840 acc 0.622 f1 0.620 || val_loss 1.1417 acc 0.393 f1 0.313\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7127 acc 0.657 f1 0.654 || val_loss 1.1316 acc 0.407 f1 0.336\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6469 acc 0.689 f1 0.687 || val_loss 1.1701 acc 0.389 f1 0.322\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5883 acc 0.724 f1 0.723 || val_loss 1.1998 acc 0.418 f1 0.341\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5320 acc 0.757 f1 0.756 || val_loss 1.2755 acc 0.399 f1 0.333\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4831 acc 0.773 f1 0.773 || val_loss 1.3432 acc 0.393 f1 0.328\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4352 acc 0.800 f1 0.799 || val_loss 1.3906 acc 0.438 f1 0.355\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4400 acc 0.808 f1 0.808 || val_loss 1.3763 acc 0.438 f1 0.350\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3932 acc 0.831 f1 0.831 || val_loss 1.5248 acc 0.416 f1 0.337\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3521 acc 0.853 f1 0.853 || val_loss 1.5292 acc 0.422 f1 0.353\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3125 acc 0.871 f1 0.871 || val_loss 1.6121 acc 0.420 f1 0.347\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2938 acc 0.876 f1 0.876 || val_loss 1.6693 acc 0.444 f1 0.350\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2500 acc 0.901 f1 0.901 || val_loss 1.8278 acc 0.407 f1 0.326\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2595 acc 0.900 f1 0.900 || val_loss 1.7853 acc 0.428 f1 0.353\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.2482 acc 0.905 f1 0.905 || val_loss 1.8560 acc 0.407 f1 0.331\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=56\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 931, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1002 acc 0.349 f1 0.262 || val_loss 1.0823 acc 0.416 f1 0.359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0800 acc 0.409 f1 0.403 || val_loss 1.0783 acc 0.395 f1 0.347\n",
            "[W3] RNN Epoch 03 | train_loss 1.0636 acc 0.443 f1 0.441 || val_loss 1.0830 acc 0.350 f1 0.320\n",
            "[W3] RNN Epoch 04 | train_loss 1.0452 acc 0.463 f1 0.459 || val_loss 1.0842 acc 0.366 f1 0.337\n",
            "[W3] RNN Epoch 05 | train_loss 1.0250 acc 0.496 f1 0.491 || val_loss 1.0785 acc 0.358 f1 0.315\n",
            "[W3] RNN Epoch 06 | train_loss 1.0074 acc 0.499 f1 0.490 || val_loss 1.0797 acc 0.356 f1 0.318\n",
            "[W3] RNN Epoch 07 | train_loss 0.9816 acc 0.518 f1 0.511 || val_loss 1.0756 acc 0.366 f1 0.322\n",
            "[W3] RNN Epoch 08 | train_loss 0.9551 acc 0.545 f1 0.540 || val_loss 1.0940 acc 0.331 f1 0.299\n",
            "[W3] RNN Epoch 09 | train_loss 0.9291 acc 0.563 f1 0.553 || val_loss 1.0859 acc 0.356 f1 0.316\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=56\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 931, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0976 acc 0.355 f1 0.275 || val_loss 1.1024 acc 0.290 f1 0.281\n",
            "[W3] GRU Epoch 02 | train_loss 1.0883 acc 0.412 f1 0.407 || val_loss 1.0917 acc 0.356 f1 0.328\n",
            "[W3] GRU Epoch 03 | train_loss 1.0768 acc 0.447 f1 0.445 || val_loss 1.0862 acc 0.374 f1 0.346\n",
            "[W3] GRU Epoch 04 | train_loss 1.0604 acc 0.460 f1 0.453 || val_loss 1.0717 acc 0.395 f1 0.356\n",
            "[W3] GRU Epoch 05 | train_loss 1.0338 acc 0.491 f1 0.486 || val_loss 1.0951 acc 0.356 f1 0.330\n",
            "[W3] GRU Epoch 06 | train_loss 0.9804 acc 0.529 f1 0.522 || val_loss 1.0614 acc 0.377 f1 0.326\n",
            "[W3] GRU Epoch 07 | train_loss 0.9082 acc 0.564 f1 0.556 || val_loss 1.0909 acc 0.379 f1 0.321\n",
            "[W3] GRU Epoch 08 | train_loss 0.8422 acc 0.585 f1 0.578 || val_loss 1.1156 acc 0.387 f1 0.336\n",
            "[W3] GRU Epoch 09 | train_loss 0.7985 acc 0.618 f1 0.612 || val_loss 1.1816 acc 0.366 f1 0.324\n",
            "[W3] GRU Epoch 10 | train_loss 0.7634 acc 0.622 f1 0.616 || val_loss 1.1965 acc 0.395 f1 0.346\n",
            "[W3] GRU Epoch 11 | train_loss 0.7204 acc 0.647 f1 0.643 || val_loss 1.1711 acc 0.409 f1 0.337\n",
            "[W3] GRU Epoch 12 | train_loss 0.6867 acc 0.658 f1 0.653 || val_loss 1.1832 acc 0.418 f1 0.331\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=56\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 931, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0993 acc 0.340 f1 0.231 || val_loss 1.1006 acc 0.327 f1 0.237\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0937 acc 0.382 f1 0.336 || val_loss 1.0979 acc 0.323 f1 0.287\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0867 acc 0.414 f1 0.399 || val_loss 1.0985 acc 0.315 f1 0.300\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0742 acc 0.443 f1 0.431 || val_loss 1.0948 acc 0.327 f1 0.310\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0476 acc 0.459 f1 0.447 || val_loss 1.1034 acc 0.329 f1 0.302\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9786 acc 0.537 f1 0.521 || val_loss 1.1174 acc 0.354 f1 0.319\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8826 acc 0.564 f1 0.553 || val_loss 1.1452 acc 0.366 f1 0.299\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8217 acc 0.595 f1 0.583 || val_loss 1.1739 acc 0.360 f1 0.291\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7685 acc 0.624 f1 0.617 || val_loss 1.2094 acc 0.356 f1 0.304\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7289 acc 0.645 f1 0.638 || val_loss 1.1688 acc 0.397 f1 0.307\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6965 acc 0.663 f1 0.659 || val_loss 1.1856 acc 0.403 f1 0.311\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6557 acc 0.672 f1 0.670 || val_loss 1.2524 acc 0.385 f1 0.310\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6351 acc 0.695 f1 0.691 || val_loss 1.2599 acc 0.364 f1 0.294\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6026 acc 0.716 f1 0.714 || val_loss 1.3183 acc 0.385 f1 0.306\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  56%|    | 56/100 [27:10<19:56, 27.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=57 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=57\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1196 acc 0.423 f1 0.416 || val_loss 1.1515 acc 0.280 f1 0.265\n",
            "[W3] ANN Epoch 02 | train_loss 0.9574 acc 0.526 f1 0.516 || val_loss 1.1358 acc 0.298 f1 0.275\n",
            "[W3] ANN Epoch 03 | train_loss 0.8827 acc 0.567 f1 0.557 || val_loss 1.1335 acc 0.346 f1 0.311\n",
            "[W3] ANN Epoch 04 | train_loss 0.7946 acc 0.607 f1 0.599 || val_loss 1.1336 acc 0.337 f1 0.286\n",
            "[W3] ANN Epoch 05 | train_loss 0.7365 acc 0.653 f1 0.648 || val_loss 1.1464 acc 0.391 f1 0.327\n",
            "[W3] ANN Epoch 06 | train_loss 0.6778 acc 0.681 f1 0.677 || val_loss 1.1416 acc 0.391 f1 0.325\n",
            "[W3] ANN Epoch 07 | train_loss 0.6479 acc 0.695 f1 0.692 || val_loss 1.1747 acc 0.405 f1 0.332\n",
            "[W3] ANN Epoch 08 | train_loss 0.6146 acc 0.709 f1 0.707 || val_loss 1.1648 acc 0.414 f1 0.338\n",
            "[W3] ANN Epoch 09 | train_loss 0.5953 acc 0.720 f1 0.718 || val_loss 1.1900 acc 0.407 f1 0.337\n",
            "[W3] ANN Epoch 10 | train_loss 0.5503 acc 0.743 f1 0.741 || val_loss 1.2475 acc 0.426 f1 0.352\n",
            "[W3] ANN Epoch 11 | train_loss 0.5439 acc 0.738 f1 0.737 || val_loss 1.2228 acc 0.438 f1 0.351\n",
            "[W3] ANN Epoch 12 | train_loss 0.5074 acc 0.766 f1 0.764 || val_loss 1.2779 acc 0.426 f1 0.343\n",
            "[W3] ANN Epoch 13 | train_loss 0.5055 acc 0.774 f1 0.773 || val_loss 1.2519 acc 0.461 f1 0.383\n",
            "[W3] ANN Epoch 14 | train_loss 0.4930 acc 0.774 f1 0.773 || val_loss 1.2887 acc 0.457 f1 0.365\n",
            "[W3] ANN Epoch 15 | train_loss 0.4668 acc 0.783 f1 0.782 || val_loss 1.3174 acc 0.444 f1 0.350\n",
            "[W3] ANN Epoch 16 | train_loss 0.4548 acc 0.802 f1 0.801 || val_loss 1.3110 acc 0.459 f1 0.368\n",
            "[W3] ANN Epoch 17 | train_loss 0.4308 acc 0.810 f1 0.809 || val_loss 1.3875 acc 0.409 f1 0.330\n",
            "[W3] ANN Epoch 18 | train_loss 0.4195 acc 0.816 f1 0.815 || val_loss 1.3591 acc 0.436 f1 0.355\n",
            "[W3] ANN Epoch 19 | train_loss 0.3921 acc 0.835 f1 0.835 || val_loss 1.4212 acc 0.449 f1 0.365\n",
            "[W3] ANN Epoch 20 | train_loss 0.3755 acc 0.841 f1 0.842 || val_loss 1.4829 acc 0.426 f1 0.352\n",
            "[W3] ANN Epoch 21 | train_loss 0.3645 acc 0.850 f1 0.850 || val_loss 1.5134 acc 0.424 f1 0.335\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=57\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0755 acc 0.406 f1 0.405 || val_loss 1.0172 acc 0.465 f1 0.330\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9432 acc 0.548 f1 0.545 || val_loss 1.0687 acc 0.409 f1 0.336\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8123 acc 0.605 f1 0.597 || val_loss 1.1331 acc 0.377 f1 0.306\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7257 acc 0.660 f1 0.656 || val_loss 1.1570 acc 0.391 f1 0.333\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6483 acc 0.705 f1 0.701 || val_loss 1.1966 acc 0.430 f1 0.370\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5968 acc 0.740 f1 0.737 || val_loss 1.2840 acc 0.393 f1 0.343\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5515 acc 0.747 f1 0.744 || val_loss 1.2777 acc 0.416 f1 0.357\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4952 acc 0.789 f1 0.788 || val_loss 1.3502 acc 0.420 f1 0.343\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4527 acc 0.804 f1 0.803 || val_loss 1.4459 acc 0.389 f1 0.346\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4464 acc 0.810 f1 0.810 || val_loss 1.4081 acc 0.414 f1 0.345\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3985 acc 0.836 f1 0.835 || val_loss 1.4416 acc 0.438 f1 0.369\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3828 acc 0.849 f1 0.848 || val_loss 1.4822 acc 0.438 f1 0.376\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3434 acc 0.862 f1 0.861 || val_loss 1.5070 acc 0.432 f1 0.368\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3141 acc 0.877 f1 0.877 || val_loss 1.5999 acc 0.432 f1 0.364\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2731 acc 0.900 f1 0.900 || val_loss 1.7190 acc 0.428 f1 0.364\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2845 acc 0.881 f1 0.881 || val_loss 1.6916 acc 0.438 f1 0.374\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.2839 acc 0.889 f1 0.889 || val_loss 1.7225 acc 0.422 f1 0.357\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.2220 acc 0.921 f1 0.921 || val_loss 1.7900 acc 0.453 f1 0.377\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.2115 acc 0.925 f1 0.925 || val_loss 1.9132 acc 0.407 f1 0.360\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1849 acc 0.940 f1 0.940 || val_loss 2.0616 acc 0.405 f1 0.338\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.1670 acc 0.943 f1 0.943 || val_loss 2.0371 acc 0.426 f1 0.348\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1630 acc 0.941 f1 0.941 || val_loss 2.0469 acc 0.436 f1 0.358\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.1670 acc 0.939 f1 0.938 || val_loss 2.1170 acc 0.447 f1 0.371\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.1701 acc 0.941 f1 0.941 || val_loss 2.1608 acc 0.407 f1 0.349\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.1853 acc 0.925 f1 0.925 || val_loss 2.2313 acc 0.418 f1 0.355\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.2337 acc 0.916 f1 0.915 || val_loss 2.0625 acc 0.432 f1 0.362\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=57\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0961 acc 0.349 f1 0.316 || val_loss 1.0974 acc 0.352 f1 0.309\n",
            "[W3] RNN Epoch 02 | train_loss 1.0776 acc 0.424 f1 0.411 || val_loss 1.0910 acc 0.352 f1 0.328\n",
            "[W3] RNN Epoch 03 | train_loss 1.0614 acc 0.449 f1 0.443 || val_loss 1.0797 acc 0.362 f1 0.342\n",
            "[W3] RNN Epoch 04 | train_loss 1.0403 acc 0.464 f1 0.457 || val_loss 1.0797 acc 0.368 f1 0.344\n",
            "[W3] RNN Epoch 05 | train_loss 1.0210 acc 0.492 f1 0.487 || val_loss 1.0824 acc 0.340 f1 0.315\n",
            "[W3] RNN Epoch 06 | train_loss 1.0004 acc 0.498 f1 0.489 || val_loss 1.0708 acc 0.348 f1 0.319\n",
            "[W3] RNN Epoch 07 | train_loss 0.9814 acc 0.520 f1 0.512 || val_loss 1.0465 acc 0.372 f1 0.339\n",
            "[W3] RNN Epoch 08 | train_loss 0.9612 acc 0.529 f1 0.520 || val_loss 1.0947 acc 0.356 f1 0.333\n",
            "[W3] RNN Epoch 09 | train_loss 0.9373 acc 0.540 f1 0.529 || val_loss 1.0701 acc 0.364 f1 0.336\n",
            "[W3] RNN Epoch 10 | train_loss 0.9231 acc 0.550 f1 0.539 || val_loss 1.0948 acc 0.352 f1 0.329\n",
            "[W3] RNN Epoch 11 | train_loss 0.9080 acc 0.558 f1 0.546 || val_loss 1.0792 acc 0.350 f1 0.310\n",
            "[W3] RNN Epoch 12 | train_loss 0.8908 acc 0.569 f1 0.559 || val_loss 1.0856 acc 0.360 f1 0.321\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=57\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0991 acc 0.329 f1 0.320 || val_loss 1.1020 acc 0.282 f1 0.264\n",
            "[W3] GRU Epoch 02 | train_loss 1.0923 acc 0.388 f1 0.387 || val_loss 1.0998 acc 0.300 f1 0.280\n",
            "[W3] GRU Epoch 03 | train_loss 1.0850 acc 0.400 f1 0.397 || val_loss 1.0951 acc 0.340 f1 0.314\n",
            "[W3] GRU Epoch 04 | train_loss 1.0742 acc 0.427 f1 0.423 || val_loss 1.0928 acc 0.346 f1 0.314\n",
            "[W3] GRU Epoch 05 | train_loss 1.0579 acc 0.456 f1 0.450 || val_loss 1.0968 acc 0.325 f1 0.301\n",
            "[W3] GRU Epoch 06 | train_loss 1.0289 acc 0.490 f1 0.488 || val_loss 1.0974 acc 0.342 f1 0.313\n",
            "[W3] GRU Epoch 07 | train_loss 0.9776 acc 0.531 f1 0.518 || val_loss 1.0992 acc 0.337 f1 0.306\n",
            "[W3] GRU Epoch 08 | train_loss 0.9123 acc 0.571 f1 0.563 || val_loss 1.0817 acc 0.395 f1 0.345\n",
            "[W3] GRU Epoch 09 | train_loss 0.8643 acc 0.573 f1 0.565 || val_loss 1.1174 acc 0.381 f1 0.340\n",
            "[W3] GRU Epoch 10 | train_loss 0.8074 acc 0.606 f1 0.602 || val_loss 1.1499 acc 0.368 f1 0.323\n",
            "[W3] GRU Epoch 11 | train_loss 0.7713 acc 0.625 f1 0.621 || val_loss 1.1403 acc 0.356 f1 0.308\n",
            "[W3] GRU Epoch 12 | train_loss 0.7348 acc 0.647 f1 0.642 || val_loss 1.1856 acc 0.337 f1 0.287\n",
            "[W3] GRU Epoch 13 | train_loss 0.7062 acc 0.660 f1 0.656 || val_loss 1.2493 acc 0.337 f1 0.308\n",
            "[W3] GRU Epoch 14 | train_loss 0.6793 acc 0.673 f1 0.670 || val_loss 1.2071 acc 0.374 f1 0.324\n",
            "[W3] GRU Epoch 15 | train_loss 0.6650 acc 0.675 f1 0.671 || val_loss 1.1740 acc 0.401 f1 0.337\n",
            "[W3] GRU Epoch 16 | train_loss 0.6444 acc 0.682 f1 0.679 || val_loss 1.2641 acc 0.362 f1 0.314\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=57\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1022 acc 0.333 f1 0.167 || val_loss 1.0846 acc 0.453 f1 0.214\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0939 acc 0.356 f1 0.345 || val_loss 1.0895 acc 0.329 f1 0.291\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0876 acc 0.374 f1 0.371 || val_loss 1.1010 acc 0.249 f1 0.247\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0747 acc 0.426 f1 0.407 || val_loss 1.0977 acc 0.290 f1 0.280\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0537 acc 0.446 f1 0.424 || val_loss 1.1091 acc 0.276 f1 0.268\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0087 acc 0.508 f1 0.493 || val_loss 1.0887 acc 0.327 f1 0.294\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9291 acc 0.544 f1 0.523 || val_loss 1.0915 acc 0.352 f1 0.311\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8564 acc 0.575 f1 0.568 || val_loss 1.1322 acc 0.368 f1 0.314\n",
            "[W3] LSTM Epoch 09 | train_loss 0.8039 acc 0.613 f1 0.607 || val_loss 1.1102 acc 0.387 f1 0.315\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7702 acc 0.624 f1 0.619 || val_loss 1.1136 acc 0.414 f1 0.331\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7337 acc 0.640 f1 0.632 || val_loss 1.1616 acc 0.385 f1 0.313\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6957 acc 0.666 f1 0.663 || val_loss 1.1839 acc 0.393 f1 0.324\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6777 acc 0.675 f1 0.673 || val_loss 1.2108 acc 0.395 f1 0.327\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6506 acc 0.689 f1 0.687 || val_loss 1.2379 acc 0.405 f1 0.344\n",
            "[W3] LSTM Epoch 15 | train_loss 0.6262 acc 0.694 f1 0.691 || val_loss 1.2422 acc 0.391 f1 0.319\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5905 acc 0.717 f1 0.716 || val_loss 1.2624 acc 0.407 f1 0.335\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5801 acc 0.723 f1 0.721 || val_loss 1.2712 acc 0.416 f1 0.335\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5526 acc 0.732 f1 0.731 || val_loss 1.3208 acc 0.414 f1 0.334\n",
            "[W3] LSTM Epoch 19 | train_loss 0.5435 acc 0.734 f1 0.732 || val_loss 1.3494 acc 0.399 f1 0.320\n",
            "[W3] LSTM Epoch 20 | train_loss 0.5068 acc 0.759 f1 0.758 || val_loss 1.4299 acc 0.387 f1 0.320\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4918 acc 0.764 f1 0.764 || val_loss 1.4434 acc 0.377 f1 0.307\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4774 acc 0.763 f1 0.763 || val_loss 1.4670 acc 0.379 f1 0.312\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  57%|    | 57/100 [27:48<21:49, 30.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=58 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=58\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 928, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1210 acc 0.399 f1 0.398 || val_loss 1.1530 acc 0.321 f1 0.305\n",
            "[W3] ANN Epoch 02 | train_loss 0.9830 acc 0.515 f1 0.507 || val_loss 1.1549 acc 0.323 f1 0.300\n",
            "[W3] ANN Epoch 03 | train_loss 0.8842 acc 0.555 f1 0.548 || val_loss 1.1548 acc 0.337 f1 0.298\n",
            "[W3] ANN Epoch 04 | train_loss 0.8070 acc 0.613 f1 0.606 || val_loss 1.1558 acc 0.346 f1 0.305\n",
            "[W3] ANN Epoch 05 | train_loss 0.7328 acc 0.648 f1 0.643 || val_loss 1.1504 acc 0.391 f1 0.322\n",
            "[W3] ANN Epoch 06 | train_loss 0.7191 acc 0.645 f1 0.641 || val_loss 1.1826 acc 0.389 f1 0.331\n",
            "[W3] ANN Epoch 07 | train_loss 0.6750 acc 0.687 f1 0.685 || val_loss 1.1974 acc 0.364 f1 0.308\n",
            "[W3] ANN Epoch 08 | train_loss 0.6381 acc 0.693 f1 0.690 || val_loss 1.1821 acc 0.379 f1 0.312\n",
            "[W3] ANN Epoch 09 | train_loss 0.5871 acc 0.720 f1 0.718 || val_loss 1.2216 acc 0.374 f1 0.298\n",
            "[W3] ANN Epoch 10 | train_loss 0.5890 acc 0.721 f1 0.719 || val_loss 1.2333 acc 0.393 f1 0.316\n",
            "[W3] ANN Epoch 11 | train_loss 0.5642 acc 0.731 f1 0.730 || val_loss 1.2863 acc 0.383 f1 0.323\n",
            "[W3] ANN Epoch 12 | train_loss 0.5350 acc 0.752 f1 0.750 || val_loss 1.2477 acc 0.377 f1 0.311\n",
            "[W3] ANN Epoch 13 | train_loss 0.5317 acc 0.750 f1 0.750 || val_loss 1.2691 acc 0.383 f1 0.305\n",
            "[W3] ANN Epoch 14 | train_loss 0.5456 acc 0.756 f1 0.755 || val_loss 1.2620 acc 0.399 f1 0.322\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=58\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 928, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0674 acc 0.415 f1 0.415 || val_loss 1.0210 acc 0.403 f1 0.300\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9313 acc 0.531 f1 0.525 || val_loss 1.0282 acc 0.393 f1 0.295\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8043 acc 0.607 f1 0.603 || val_loss 1.0855 acc 0.383 f1 0.335\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7215 acc 0.649 f1 0.644 || val_loss 1.1521 acc 0.368 f1 0.316\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6422 acc 0.706 f1 0.703 || val_loss 1.1846 acc 0.368 f1 0.300\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5877 acc 0.735 f1 0.733 || val_loss 1.2402 acc 0.346 f1 0.288\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5508 acc 0.742 f1 0.740 || val_loss 1.2810 acc 0.385 f1 0.308\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5051 acc 0.767 f1 0.766 || val_loss 1.3182 acc 0.383 f1 0.326\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4715 acc 0.791 f1 0.791 || val_loss 1.4082 acc 0.362 f1 0.311\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4156 acc 0.816 f1 0.815 || val_loss 1.4458 acc 0.418 f1 0.335\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3787 acc 0.834 f1 0.833 || val_loss 1.5012 acc 0.405 f1 0.323\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=58\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 928, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1008 acc 0.341 f1 0.294 || val_loss 1.1060 acc 0.255 f1 0.244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0860 acc 0.405 f1 0.387 || val_loss 1.0956 acc 0.315 f1 0.299\n",
            "[W3] RNN Epoch 03 | train_loss 1.0725 acc 0.418 f1 0.408 || val_loss 1.0849 acc 0.342 f1 0.319\n",
            "[W3] RNN Epoch 04 | train_loss 1.0568 acc 0.434 f1 0.427 || val_loss 1.1012 acc 0.294 f1 0.285\n",
            "[W3] RNN Epoch 05 | train_loss 1.0413 acc 0.455 f1 0.442 || val_loss 1.0898 acc 0.340 f1 0.321\n",
            "[W3] RNN Epoch 06 | train_loss 1.0280 acc 0.472 f1 0.465 || val_loss 1.0808 acc 0.325 f1 0.305\n",
            "[W3] RNN Epoch 07 | train_loss 1.0066 acc 0.489 f1 0.481 || val_loss 1.0838 acc 0.335 f1 0.308\n",
            "[W3] RNN Epoch 08 | train_loss 0.9851 acc 0.517 f1 0.510 || val_loss 1.0794 acc 0.331 f1 0.301\n",
            "[W3] RNN Epoch 09 | train_loss 0.9672 acc 0.529 f1 0.520 || val_loss 1.0736 acc 0.340 f1 0.310\n",
            "[W3] RNN Epoch 10 | train_loss 0.9425 acc 0.557 f1 0.548 || val_loss 1.0956 acc 0.333 f1 0.306\n",
            "[W3] RNN Epoch 11 | train_loss 0.9204 acc 0.567 f1 0.559 || val_loss 1.0869 acc 0.340 f1 0.304\n",
            "[W3] RNN Epoch 12 | train_loss 0.8978 acc 0.579 f1 0.569 || val_loss 1.0884 acc 0.352 f1 0.318\n",
            "[W3] RNN Epoch 13 | train_loss 0.8758 acc 0.587 f1 0.576 || val_loss 1.1022 acc 0.352 f1 0.314\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=58\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 928, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0991 acc 0.345 f1 0.339 || val_loss 1.1029 acc 0.309 f1 0.294\n",
            "[W3] GRU Epoch 02 | train_loss 1.0927 acc 0.379 f1 0.355 || val_loss 1.0931 acc 0.354 f1 0.315\n",
            "[W3] GRU Epoch 03 | train_loss 1.0842 acc 0.412 f1 0.408 || val_loss 1.0877 acc 0.337 f1 0.315\n",
            "[W3] GRU Epoch 04 | train_loss 1.0708 acc 0.444 f1 0.439 || val_loss 1.0809 acc 0.340 f1 0.310\n",
            "[W3] GRU Epoch 05 | train_loss 1.0555 acc 0.461 f1 0.456 || val_loss 1.0845 acc 0.346 f1 0.324\n",
            "[W3] GRU Epoch 06 | train_loss 1.0249 acc 0.504 f1 0.497 || val_loss 1.0777 acc 0.364 f1 0.329\n",
            "[W3] GRU Epoch 07 | train_loss 0.9741 acc 0.540 f1 0.533 || val_loss 1.0982 acc 0.340 f1 0.307\n",
            "[W3] GRU Epoch 08 | train_loss 0.9086 acc 0.557 f1 0.546 || val_loss 1.1046 acc 0.397 f1 0.335\n",
            "[W3] GRU Epoch 09 | train_loss 0.8476 acc 0.587 f1 0.580 || val_loss 1.1258 acc 0.387 f1 0.330\n",
            "[W3] GRU Epoch 10 | train_loss 0.7907 acc 0.623 f1 0.616 || val_loss 1.1662 acc 0.387 f1 0.329\n",
            "[W3] GRU Epoch 11 | train_loss 0.7539 acc 0.631 f1 0.625 || val_loss 1.1900 acc 0.401 f1 0.334\n",
            "[W3] GRU Epoch 12 | train_loss 0.7259 acc 0.658 f1 0.652 || val_loss 1.1858 acc 0.395 f1 0.317\n",
            "[W3] GRU Epoch 13 | train_loss 0.6960 acc 0.662 f1 0.656 || val_loss 1.1943 acc 0.422 f1 0.333\n",
            "[W3] GRU Epoch 14 | train_loss 0.6566 acc 0.678 f1 0.673 || val_loss 1.2349 acc 0.416 f1 0.332\n",
            "[W3] GRU Epoch 15 | train_loss 0.6289 acc 0.694 f1 0.690 || val_loss 1.2786 acc 0.414 f1 0.324\n",
            "[W3] GRU Epoch 16 | train_loss 0.6144 acc 0.700 f1 0.696 || val_loss 1.2958 acc 0.399 f1 0.311\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=58\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 928, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0977 acc 0.336 f1 0.271 || val_loss 1.0929 acc 0.387 f1 0.309\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0934 acc 0.393 f1 0.389 || val_loss 1.0916 acc 0.358 f1 0.311\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0866 acc 0.411 f1 0.408 || val_loss 1.0924 acc 0.342 f1 0.321\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0699 acc 0.447 f1 0.445 || val_loss 1.0870 acc 0.337 f1 0.318\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0317 acc 0.492 f1 0.481 || val_loss 1.0717 acc 0.374 f1 0.334\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9645 acc 0.515 f1 0.502 || val_loss 1.0753 acc 0.372 f1 0.325\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8786 acc 0.573 f1 0.564 || val_loss 1.1368 acc 0.364 f1 0.326\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8204 acc 0.607 f1 0.599 || val_loss 1.1454 acc 0.383 f1 0.333\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7642 acc 0.616 f1 0.609 || val_loss 1.1645 acc 0.409 f1 0.347\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7239 acc 0.658 f1 0.652 || val_loss 1.1755 acc 0.401 f1 0.340\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6892 acc 0.658 f1 0.652 || val_loss 1.2219 acc 0.389 f1 0.328\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6758 acc 0.670 f1 0.664 || val_loss 1.2477 acc 0.393 f1 0.332\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6307 acc 0.693 f1 0.690 || val_loss 1.2405 acc 0.403 f1 0.333\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6143 acc 0.707 f1 0.704 || val_loss 1.2978 acc 0.403 f1 0.342\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5912 acc 0.713 f1 0.710 || val_loss 1.2947 acc 0.399 f1 0.327\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5609 acc 0.731 f1 0.728 || val_loss 1.3243 acc 0.401 f1 0.329\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5405 acc 0.746 f1 0.744 || val_loss 1.3645 acc 0.403 f1 0.330\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  58%|    | 58/100 [28:10<19:28, 27.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=59 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=59\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1182 acc 0.415 f1 0.408 || val_loss 1.2117 acc 0.267 f1 0.268\n",
            "[W3] ANN Epoch 02 | train_loss 0.9466 acc 0.538 f1 0.526 || val_loss 1.1867 acc 0.323 f1 0.304\n",
            "[W3] ANN Epoch 03 | train_loss 0.8745 acc 0.565 f1 0.554 || val_loss 1.1719 acc 0.333 f1 0.305\n",
            "[W3] ANN Epoch 04 | train_loss 0.7998 acc 0.613 f1 0.605 || val_loss 1.1824 acc 0.348 f1 0.299\n",
            "[W3] ANN Epoch 05 | train_loss 0.7314 acc 0.655 f1 0.651 || val_loss 1.1817 acc 0.360 f1 0.304\n",
            "[W3] ANN Epoch 06 | train_loss 0.7030 acc 0.665 f1 0.661 || val_loss 1.1784 acc 0.377 f1 0.322\n",
            "[W3] ANN Epoch 07 | train_loss 0.6610 acc 0.678 f1 0.674 || val_loss 1.1599 acc 0.407 f1 0.345\n",
            "[W3] ANN Epoch 08 | train_loss 0.6196 acc 0.711 f1 0.709 || val_loss 1.1936 acc 0.393 f1 0.323\n",
            "[W3] ANN Epoch 09 | train_loss 0.5888 acc 0.732 f1 0.730 || val_loss 1.2041 acc 0.399 f1 0.331\n",
            "[W3] ANN Epoch 10 | train_loss 0.5905 acc 0.721 f1 0.719 || val_loss 1.2272 acc 0.381 f1 0.320\n",
            "[W3] ANN Epoch 11 | train_loss 0.5555 acc 0.749 f1 0.748 || val_loss 1.2578 acc 0.403 f1 0.326\n",
            "[W3] ANN Epoch 12 | train_loss 0.5189 acc 0.773 f1 0.772 || val_loss 1.2654 acc 0.412 f1 0.331\n",
            "[W3] ANN Epoch 13 | train_loss 0.5163 acc 0.763 f1 0.762 || val_loss 1.3046 acc 0.409 f1 0.335\n",
            "[W3] ANN Epoch 14 | train_loss 0.4823 acc 0.783 f1 0.782 || val_loss 1.2807 acc 0.412 f1 0.336\n",
            "[W3] ANN Epoch 15 | train_loss 0.4702 acc 0.787 f1 0.786 || val_loss 1.3665 acc 0.387 f1 0.308\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=59\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0754 acc 0.396 f1 0.397 || val_loss 1.0292 acc 0.434 f1 0.337\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9319 acc 0.550 f1 0.548 || val_loss 1.0697 acc 0.399 f1 0.338\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8159 acc 0.611 f1 0.608 || val_loss 1.0999 acc 0.401 f1 0.344\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7309 acc 0.652 f1 0.651 || val_loss 1.1643 acc 0.385 f1 0.329\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6680 acc 0.692 f1 0.689 || val_loss 1.2025 acc 0.395 f1 0.345\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6121 acc 0.720 f1 0.719 || val_loss 1.1936 acc 0.397 f1 0.344\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5382 acc 0.753 f1 0.752 || val_loss 1.2677 acc 0.389 f1 0.330\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4787 acc 0.792 f1 0.792 || val_loss 1.3671 acc 0.377 f1 0.309\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4618 acc 0.791 f1 0.790 || val_loss 1.4200 acc 0.412 f1 0.347\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4256 acc 0.815 f1 0.814 || val_loss 1.4176 acc 0.401 f1 0.335\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3749 acc 0.840 f1 0.839 || val_loss 1.4971 acc 0.422 f1 0.328\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3475 acc 0.854 f1 0.854 || val_loss 1.5630 acc 0.391 f1 0.325\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3318 acc 0.865 f1 0.865 || val_loss 1.6062 acc 0.395 f1 0.340\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2802 acc 0.891 f1 0.891 || val_loss 1.7244 acc 0.381 f1 0.308\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2615 acc 0.898 f1 0.898 || val_loss 1.7878 acc 0.430 f1 0.345\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2521 acc 0.901 f1 0.901 || val_loss 1.8320 acc 0.424 f1 0.362\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.2057 acc 0.924 f1 0.924 || val_loss 1.9099 acc 0.409 f1 0.346\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1751 acc 0.938 f1 0.938 || val_loss 1.9917 acc 0.430 f1 0.354\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1454 acc 0.954 f1 0.954 || val_loss 2.1473 acc 0.432 f1 0.350\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1601 acc 0.947 f1 0.947 || val_loss 2.1644 acc 0.414 f1 0.328\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.1454 acc 0.945 f1 0.945 || val_loss 2.1724 acc 0.420 f1 0.343\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1412 acc 0.951 f1 0.951 || val_loss 2.3499 acc 0.414 f1 0.343\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.1321 acc 0.954 f1 0.954 || val_loss 2.4086 acc 0.430 f1 0.342\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.1057 acc 0.968 f1 0.968 || val_loss 2.4617 acc 0.391 f1 0.329\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=59\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1001 acc 0.350 f1 0.318 || val_loss 1.1027 acc 0.323 f1 0.308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0825 acc 0.413 f1 0.412 || val_loss 1.1083 acc 0.327 f1 0.313\n",
            "[W3] RNN Epoch 03 | train_loss 1.0699 acc 0.431 f1 0.427 || val_loss 1.1154 acc 0.317 f1 0.302\n",
            "[W3] RNN Epoch 04 | train_loss 1.0535 acc 0.447 f1 0.443 || val_loss 1.1013 acc 0.311 f1 0.288\n",
            "[W3] RNN Epoch 05 | train_loss 1.0389 acc 0.459 f1 0.454 || val_loss 1.1028 acc 0.331 f1 0.306\n",
            "[W3] RNN Epoch 06 | train_loss 1.0195 acc 0.489 f1 0.485 || val_loss 1.0856 acc 0.366 f1 0.324\n",
            "[W3] RNN Epoch 07 | train_loss 0.9998 acc 0.519 f1 0.515 || val_loss 1.0915 acc 0.370 f1 0.333\n",
            "[W3] RNN Epoch 08 | train_loss 0.9740 acc 0.514 f1 0.508 || val_loss 1.0806 acc 0.370 f1 0.323\n",
            "[W3] RNN Epoch 09 | train_loss 0.9495 acc 0.544 f1 0.540 || val_loss 1.0705 acc 0.370 f1 0.324\n",
            "[W3] RNN Epoch 10 | train_loss 0.9220 acc 0.565 f1 0.560 || val_loss 1.0835 acc 0.379 f1 0.335\n",
            "[W3] RNN Epoch 11 | train_loss 0.9032 acc 0.571 f1 0.564 || val_loss 1.0816 acc 0.358 f1 0.304\n",
            "[W3] RNN Epoch 12 | train_loss 0.8822 acc 0.582 f1 0.574 || val_loss 1.1015 acc 0.370 f1 0.326\n",
            "[W3] RNN Epoch 13 | train_loss 0.8530 acc 0.603 f1 0.596 || val_loss 1.0736 acc 0.403 f1 0.335\n",
            "[W3] RNN Epoch 14 | train_loss 0.8339 acc 0.605 f1 0.597 || val_loss 1.1043 acc 0.395 f1 0.332\n",
            "[W3] RNN Epoch 15 | train_loss 0.8185 acc 0.610 f1 0.603 || val_loss 1.1213 acc 0.403 f1 0.343\n",
            "[W3] RNN Epoch 16 | train_loss 0.7955 acc 0.630 f1 0.624 || val_loss 1.1617 acc 0.352 f1 0.310\n",
            "[W3] RNN Epoch 17 | train_loss 0.7803 acc 0.629 f1 0.620 || val_loss 1.1439 acc 0.368 f1 0.315\n",
            "[W3] RNN Epoch 18 | train_loss 0.7609 acc 0.647 f1 0.640 || val_loss 1.1567 acc 0.379 f1 0.315\n",
            "[W3] RNN Epoch 19 | train_loss 0.7401 acc 0.656 f1 0.649 || val_loss 1.1576 acc 0.379 f1 0.313\n",
            "[W3] RNN Epoch 20 | train_loss 0.7175 acc 0.669 f1 0.662 || val_loss 1.1597 acc 0.362 f1 0.308\n",
            "[W3] RNN Epoch 21 | train_loss 0.6995 acc 0.666 f1 0.659 || val_loss 1.1799 acc 0.366 f1 0.314\n",
            "[W3] RNN Epoch 22 | train_loss 0.6744 acc 0.683 f1 0.677 || val_loss 1.1895 acc 0.385 f1 0.324\n",
            "[W3] RNN Epoch 23 | train_loss 0.6519 acc 0.699 f1 0.693 || val_loss 1.2181 acc 0.397 f1 0.326\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=59\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0971 acc 0.333 f1 0.296 || val_loss 1.0893 acc 0.379 f1 0.325\n",
            "[W3] GRU Epoch 02 | train_loss 1.0879 acc 0.407 f1 0.407 || val_loss 1.0934 acc 0.335 f1 0.316\n",
            "[W3] GRU Epoch 03 | train_loss 1.0743 acc 0.444 f1 0.441 || val_loss 1.0925 acc 0.331 f1 0.310\n",
            "[W3] GRU Epoch 04 | train_loss 1.0525 acc 0.466 f1 0.460 || val_loss 1.0984 acc 0.313 f1 0.287\n",
            "[W3] GRU Epoch 05 | train_loss 1.0134 acc 0.491 f1 0.477 || val_loss 1.0548 acc 0.387 f1 0.337\n",
            "[W3] GRU Epoch 06 | train_loss 0.9530 acc 0.537 f1 0.528 || val_loss 1.1074 acc 0.333 f1 0.304\n",
            "[W3] GRU Epoch 07 | train_loss 0.8764 acc 0.575 f1 0.563 || val_loss 1.1000 acc 0.346 f1 0.292\n",
            "[W3] GRU Epoch 08 | train_loss 0.8217 acc 0.602 f1 0.594 || val_loss 1.1056 acc 0.354 f1 0.294\n",
            "[W3] GRU Epoch 09 | train_loss 0.7839 acc 0.612 f1 0.609 || val_loss 1.1458 acc 0.358 f1 0.302\n",
            "[W3] GRU Epoch 10 | train_loss 0.7429 acc 0.634 f1 0.630 || val_loss 1.1615 acc 0.362 f1 0.299\n",
            "[W3] GRU Epoch 11 | train_loss 0.7165 acc 0.649 f1 0.643 || val_loss 1.1664 acc 0.372 f1 0.318\n",
            "[W3] GRU Epoch 12 | train_loss 0.6897 acc 0.658 f1 0.654 || val_loss 1.1888 acc 0.383 f1 0.326\n",
            "[W3] GRU Epoch 13 | train_loss 0.6687 acc 0.667 f1 0.664 || val_loss 1.1855 acc 0.387 f1 0.318\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=59\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0988 acc 0.343 f1 0.211 || val_loss 1.1062 acc 0.224 f1 0.222\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0938 acc 0.391 f1 0.370 || val_loss 1.0970 acc 0.315 f1 0.285\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0865 acc 0.417 f1 0.412 || val_loss 1.0978 acc 0.292 f1 0.287\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0731 acc 0.431 f1 0.423 || val_loss 1.0898 acc 0.333 f1 0.320\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0425 acc 0.471 f1 0.465 || val_loss 1.0973 acc 0.340 f1 0.323\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9612 acc 0.527 f1 0.516 || val_loss 1.0841 acc 0.346 f1 0.301\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8722 acc 0.567 f1 0.561 || val_loss 1.1359 acc 0.360 f1 0.310\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8032 acc 0.594 f1 0.591 || val_loss 1.1653 acc 0.360 f1 0.313\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7596 acc 0.619 f1 0.614 || val_loss 1.1461 acc 0.358 f1 0.303\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7198 acc 0.642 f1 0.639 || val_loss 1.1884 acc 0.385 f1 0.324\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6844 acc 0.672 f1 0.669 || val_loss 1.2235 acc 0.387 f1 0.329\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6524 acc 0.684 f1 0.682 || val_loss 1.2247 acc 0.385 f1 0.323\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6269 acc 0.697 f1 0.694 || val_loss 1.2532 acc 0.389 f1 0.335\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6001 acc 0.709 f1 0.707 || val_loss 1.3273 acc 0.379 f1 0.326\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5725 acc 0.727 f1 0.725 || val_loss 1.3603 acc 0.381 f1 0.328\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5500 acc 0.734 f1 0.732 || val_loss 1.3810 acc 0.381 f1 0.321\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5194 acc 0.755 f1 0.754 || val_loss 1.4135 acc 0.395 f1 0.328\n",
            "[W3] LSTM Epoch 18 | train_loss 0.4933 acc 0.769 f1 0.768 || val_loss 1.5078 acc 0.379 f1 0.320\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4751 acc 0.778 f1 0.777 || val_loss 1.5441 acc 0.387 f1 0.329\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4564 acc 0.775 f1 0.774 || val_loss 1.5633 acc 0.409 f1 0.351\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4378 acc 0.789 f1 0.788 || val_loss 1.6022 acc 0.412 f1 0.352\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4129 acc 0.814 f1 0.814 || val_loss 1.6448 acc 0.409 f1 0.343\n",
            "[W3] LSTM Epoch 23 | train_loss 0.3980 acc 0.807 f1 0.807 || val_loss 1.7789 acc 0.401 f1 0.343\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3824 acc 0.821 f1 0.821 || val_loss 1.8044 acc 0.409 f1 0.353\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3613 acc 0.831 f1 0.831 || val_loss 1.8291 acc 0.422 f1 0.357\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3482 acc 0.851 f1 0.851 || val_loss 1.9135 acc 0.414 f1 0.347\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3275 acc 0.859 f1 0.859 || val_loss 1.9656 acc 0.412 f1 0.344\n",
            "[W3] LSTM Epoch 28 | train_loss 0.3129 acc 0.863 f1 0.863 || val_loss 2.0128 acc 0.434 f1 0.361\n",
            "[W3] LSTM Epoch 29 | train_loss 0.2972 acc 0.870 f1 0.869 || val_loss 2.1062 acc 0.412 f1 0.351\n",
            "[W3] LSTM Epoch 30 | train_loss 0.2835 acc 0.878 f1 0.878 || val_loss 2.1542 acc 0.428 f1 0.365\n",
            "[W3] LSTM Epoch 31 | train_loss 0.2625 acc 0.895 f1 0.895 || val_loss 2.1905 acc 0.434 f1 0.366\n",
            "[W3] LSTM Epoch 32 | train_loss 0.2386 acc 0.906 f1 0.906 || val_loss 2.3340 acc 0.422 f1 0.358\n",
            "[W3] LSTM Epoch 33 | train_loss 0.2194 acc 0.919 f1 0.919 || val_loss 2.4049 acc 0.418 f1 0.356\n",
            "[W3] LSTM Epoch 34 | train_loss 0.2071 acc 0.922 f1 0.922 || val_loss 2.4808 acc 0.409 f1 0.349\n",
            "[W3] LSTM Epoch 35 | train_loss 0.1837 acc 0.930 f1 0.930 || val_loss 2.5771 acc 0.407 f1 0.348\n",
            "[W3] LSTM Epoch 36 | train_loss 0.1682 acc 0.940 f1 0.940 || val_loss 2.6266 acc 0.401 f1 0.344\n",
            "[W3] LSTM Epoch 37 | train_loss 0.1583 acc 0.945 f1 0.945 || val_loss 2.7614 acc 0.418 f1 0.363\n",
            "[W3] LSTM Epoch 38 | train_loss 0.1375 acc 0.957 f1 0.957 || val_loss 2.8009 acc 0.416 f1 0.355\n",
            "[W3] LSTM Epoch 39 | train_loss 0.1255 acc 0.964 f1 0.964 || val_loss 2.9193 acc 0.389 f1 0.338\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  59%|    | 59/100 [28:43<20:05, 29.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=60 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=60\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 927, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1274 acc 0.414 f1 0.412 || val_loss 1.1335 acc 0.352 f1 0.307\n",
            "[W3] ANN Epoch 02 | train_loss 0.9621 acc 0.516 f1 0.509 || val_loss 1.1302 acc 0.372 f1 0.325\n",
            "[W3] ANN Epoch 03 | train_loss 0.8773 acc 0.565 f1 0.559 || val_loss 1.1001 acc 0.407 f1 0.342\n",
            "[W3] ANN Epoch 04 | train_loss 0.8049 acc 0.604 f1 0.599 || val_loss 1.0952 acc 0.414 f1 0.356\n",
            "[W3] ANN Epoch 05 | train_loss 0.7365 acc 0.651 f1 0.648 || val_loss 1.1122 acc 0.420 f1 0.349\n",
            "[W3] ANN Epoch 06 | train_loss 0.6916 acc 0.664 f1 0.659 || val_loss 1.1225 acc 0.409 f1 0.338\n",
            "[W3] ANN Epoch 07 | train_loss 0.6573 acc 0.686 f1 0.683 || val_loss 1.1453 acc 0.414 f1 0.335\n",
            "[W3] ANN Epoch 08 | train_loss 0.6181 acc 0.710 f1 0.708 || val_loss 1.1671 acc 0.407 f1 0.330\n",
            "[W3] ANN Epoch 09 | train_loss 0.6033 acc 0.722 f1 0.719 || val_loss 1.1797 acc 0.430 f1 0.349\n",
            "[W3] ANN Epoch 10 | train_loss 0.5595 acc 0.743 f1 0.742 || val_loss 1.2119 acc 0.407 f1 0.331\n",
            "[W3] ANN Epoch 11 | train_loss 0.5542 acc 0.750 f1 0.749 || val_loss 1.1947 acc 0.418 f1 0.341\n",
            "[W3] ANN Epoch 12 | train_loss 0.5450 acc 0.750 f1 0.749 || val_loss 1.2415 acc 0.407 f1 0.333\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=60\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 927, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0789 acc 0.390 f1 0.392 || val_loss 1.0203 acc 0.430 f1 0.310\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9425 acc 0.537 f1 0.533 || val_loss 1.0849 acc 0.397 f1 0.309\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8037 acc 0.603 f1 0.600 || val_loss 1.1380 acc 0.409 f1 0.325\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7089 acc 0.660 f1 0.657 || val_loss 1.1893 acc 0.407 f1 0.319\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6283 acc 0.705 f1 0.703 || val_loss 1.2661 acc 0.379 f1 0.305\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5887 acc 0.715 f1 0.713 || val_loss 1.3118 acc 0.391 f1 0.309\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5263 acc 0.754 f1 0.753 || val_loss 1.3236 acc 0.401 f1 0.312\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4800 acc 0.779 f1 0.777 || val_loss 1.4194 acc 0.422 f1 0.326\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4452 acc 0.799 f1 0.800 || val_loss 1.5305 acc 0.385 f1 0.317\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4114 acc 0.816 f1 0.815 || val_loss 1.5822 acc 0.374 f1 0.309\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3733 acc 0.846 f1 0.845 || val_loss 1.6107 acc 0.405 f1 0.319\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3195 acc 0.871 f1 0.871 || val_loss 1.6785 acc 0.395 f1 0.310\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3128 acc 0.873 f1 0.873 || val_loss 1.7200 acc 0.393 f1 0.304\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3203 acc 0.872 f1 0.871 || val_loss 1.7467 acc 0.379 f1 0.304\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2518 acc 0.904 f1 0.904 || val_loss 1.7651 acc 0.414 f1 0.317\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2335 acc 0.911 f1 0.911 || val_loss 1.9590 acc 0.409 f1 0.309\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=60\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 927, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0974 acc 0.350 f1 0.343 || val_loss 1.0946 acc 0.360 f1 0.337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0796 acc 0.426 f1 0.425 || val_loss 1.0886 acc 0.356 f1 0.325\n",
            "[W3] RNN Epoch 03 | train_loss 1.0676 acc 0.444 f1 0.442 || val_loss 1.0916 acc 0.329 f1 0.300\n",
            "[W3] RNN Epoch 04 | train_loss 1.0512 acc 0.456 f1 0.447 || val_loss 1.0935 acc 0.346 f1 0.326\n",
            "[W3] RNN Epoch 05 | train_loss 1.0352 acc 0.473 f1 0.463 || val_loss 1.0765 acc 0.356 f1 0.315\n",
            "[W3] RNN Epoch 06 | train_loss 1.0088 acc 0.512 f1 0.506 || val_loss 1.0844 acc 0.370 f1 0.334\n",
            "[W3] RNN Epoch 07 | train_loss 0.9897 acc 0.516 f1 0.507 || val_loss 1.0785 acc 0.350 f1 0.309\n",
            "[W3] RNN Epoch 08 | train_loss 0.9610 acc 0.535 f1 0.526 || val_loss 1.1156 acc 0.317 f1 0.293\n",
            "[W3] RNN Epoch 09 | train_loss 0.9360 acc 0.551 f1 0.541 || val_loss 1.0969 acc 0.344 f1 0.303\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=60\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 927, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1004 acc 0.344 f1 0.276 || val_loss 1.0981 acc 0.374 f1 0.293\n",
            "[W3] GRU Epoch 02 | train_loss 1.0911 acc 0.386 f1 0.362 || val_loss 1.0931 acc 0.319 f1 0.308\n",
            "[W3] GRU Epoch 03 | train_loss 1.0831 acc 0.421 f1 0.403 || val_loss 1.0865 acc 0.325 f1 0.302\n",
            "[W3] GRU Epoch 04 | train_loss 1.0716 acc 0.451 f1 0.451 || val_loss 1.0794 acc 0.342 f1 0.310\n",
            "[W3] GRU Epoch 05 | train_loss 1.0480 acc 0.469 f1 0.465 || val_loss 1.0941 acc 0.352 f1 0.330\n",
            "[W3] GRU Epoch 06 | train_loss 1.0055 acc 0.515 f1 0.507 || val_loss 1.0770 acc 0.364 f1 0.320\n",
            "[W3] GRU Epoch 07 | train_loss 0.9337 acc 0.561 f1 0.555 || val_loss 1.1312 acc 0.313 f1 0.283\n",
            "[W3] GRU Epoch 08 | train_loss 0.8598 acc 0.580 f1 0.569 || val_loss 1.1177 acc 0.362 f1 0.300\n",
            "[W3] GRU Epoch 09 | train_loss 0.8052 acc 0.604 f1 0.597 || val_loss 1.1206 acc 0.381 f1 0.321\n",
            "[W3] GRU Epoch 10 | train_loss 0.7591 acc 0.623 f1 0.617 || val_loss 1.1428 acc 0.377 f1 0.305\n",
            "[W3] GRU Epoch 11 | train_loss 0.7243 acc 0.655 f1 0.648 || val_loss 1.1560 acc 0.397 f1 0.326\n",
            "[W3] GRU Epoch 12 | train_loss 0.6834 acc 0.664 f1 0.661 || val_loss 1.1801 acc 0.397 f1 0.322\n",
            "[W3] GRU Epoch 13 | train_loss 0.6614 acc 0.673 f1 0.670 || val_loss 1.2289 acc 0.387 f1 0.326\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=60\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 927, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1030 acc 0.332 f1 0.168 || val_loss 1.0735 acc 0.430 f1 0.203\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0951 acc 0.359 f1 0.291 || val_loss 1.0871 acc 0.391 f1 0.335\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0887 acc 0.397 f1 0.397 || val_loss 1.0963 acc 0.329 f1 0.314\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0717 acc 0.436 f1 0.428 || val_loss 1.0883 acc 0.356 f1 0.338\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0366 acc 0.479 f1 0.470 || val_loss 1.0782 acc 0.350 f1 0.322\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9497 acc 0.532 f1 0.522 || val_loss 1.0658 acc 0.348 f1 0.282\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8623 acc 0.575 f1 0.568 || val_loss 1.1504 acc 0.346 f1 0.302\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8093 acc 0.604 f1 0.598 || val_loss 1.1173 acc 0.368 f1 0.309\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7692 acc 0.615 f1 0.609 || val_loss 1.1356 acc 0.370 f1 0.300\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7425 acc 0.638 f1 0.631 || val_loss 1.1413 acc 0.379 f1 0.316\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7066 acc 0.667 f1 0.664 || val_loss 1.1884 acc 0.368 f1 0.299\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6832 acc 0.665 f1 0.658 || val_loss 1.1894 acc 0.387 f1 0.308\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  60%|    | 60/100 [29:03<17:45, 26.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=61 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=61\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 933, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1152 acc 0.408 f1 0.404 || val_loss 1.1495 acc 0.319 f1 0.300\n",
            "[W3] ANN Epoch 02 | train_loss 0.9477 acc 0.518 f1 0.509 || val_loss 1.1285 acc 0.350 f1 0.312\n",
            "[W3] ANN Epoch 03 | train_loss 0.8491 acc 0.584 f1 0.577 || val_loss 1.1094 acc 0.377 f1 0.333\n",
            "[W3] ANN Epoch 04 | train_loss 0.7781 acc 0.611 f1 0.606 || val_loss 1.1115 acc 0.389 f1 0.344\n",
            "[W3] ANN Epoch 05 | train_loss 0.7169 acc 0.649 f1 0.645 || val_loss 1.1112 acc 0.377 f1 0.324\n",
            "[W3] ANN Epoch 06 | train_loss 0.6715 acc 0.671 f1 0.669 || val_loss 1.1113 acc 0.374 f1 0.313\n",
            "[W3] ANN Epoch 07 | train_loss 0.6312 acc 0.690 f1 0.688 || val_loss 1.1527 acc 0.364 f1 0.309\n",
            "[W3] ANN Epoch 08 | train_loss 0.6235 acc 0.699 f1 0.697 || val_loss 1.1501 acc 0.387 f1 0.322\n",
            "[W3] ANN Epoch 09 | train_loss 0.5849 acc 0.715 f1 0.714 || val_loss 1.1684 acc 0.405 f1 0.339\n",
            "[W3] ANN Epoch 10 | train_loss 0.5913 acc 0.710 f1 0.709 || val_loss 1.1511 acc 0.422 f1 0.353\n",
            "[W3] ANN Epoch 11 | train_loss 0.5479 acc 0.743 f1 0.741 || val_loss 1.1584 acc 0.453 f1 0.372\n",
            "[W3] ANN Epoch 12 | train_loss 0.5368 acc 0.743 f1 0.742 || val_loss 1.2266 acc 0.412 f1 0.332\n",
            "[W3] ANN Epoch 13 | train_loss 0.5209 acc 0.763 f1 0.762 || val_loss 1.2285 acc 0.405 f1 0.336\n",
            "[W3] ANN Epoch 14 | train_loss 0.4992 acc 0.779 f1 0.778 || val_loss 1.2440 acc 0.401 f1 0.324\n",
            "[W3] ANN Epoch 15 | train_loss 0.4865 acc 0.780 f1 0.780 || val_loss 1.2601 acc 0.426 f1 0.346\n",
            "[W3] ANN Epoch 16 | train_loss 0.4582 acc 0.795 f1 0.795 || val_loss 1.2573 acc 0.432 f1 0.361\n",
            "[W3] ANN Epoch 17 | train_loss 0.4491 acc 0.795 f1 0.795 || val_loss 1.3097 acc 0.422 f1 0.345\n",
            "[W3] ANN Epoch 18 | train_loss 0.4720 acc 0.796 f1 0.795 || val_loss 1.2914 acc 0.426 f1 0.341\n",
            "[W3] ANN Epoch 19 | train_loss 0.4288 acc 0.813 f1 0.812 || val_loss 1.3405 acc 0.432 f1 0.357\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=61\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 933, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0761 acc 0.399 f1 0.401 || val_loss 1.0233 acc 0.414 f1 0.327\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9459 acc 0.538 f1 0.531 || val_loss 1.0368 acc 0.393 f1 0.317\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8200 acc 0.613 f1 0.609 || val_loss 1.0950 acc 0.377 f1 0.320\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7319 acc 0.653 f1 0.651 || val_loss 1.1256 acc 0.403 f1 0.342\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6647 acc 0.684 f1 0.681 || val_loss 1.1534 acc 0.391 f1 0.320\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5981 acc 0.729 f1 0.728 || val_loss 1.2440 acc 0.391 f1 0.308\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5296 acc 0.756 f1 0.755 || val_loss 1.2949 acc 0.379 f1 0.326\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5030 acc 0.766 f1 0.765 || val_loss 1.3250 acc 0.389 f1 0.323\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4476 acc 0.793 f1 0.793 || val_loss 1.4192 acc 0.377 f1 0.322\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4000 acc 0.825 f1 0.825 || val_loss 1.4628 acc 0.389 f1 0.323\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3546 acc 0.853 f1 0.853 || val_loss 1.5344 acc 0.366 f1 0.302\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3478 acc 0.859 f1 0.859 || val_loss 1.5997 acc 0.385 f1 0.297\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=61\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 933, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0972 acc 0.356 f1 0.310 || val_loss 1.1003 acc 0.325 f1 0.301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0820 acc 0.417 f1 0.409 || val_loss 1.0937 acc 0.317 f1 0.287\n",
            "[W3] RNN Epoch 03 | train_loss 1.0681 acc 0.437 f1 0.421 || val_loss 1.0890 acc 0.321 f1 0.298\n",
            "[W3] RNN Epoch 04 | train_loss 1.0451 acc 0.470 f1 0.462 || val_loss 1.0885 acc 0.346 f1 0.319\n",
            "[W3] RNN Epoch 05 | train_loss 1.0185 acc 0.496 f1 0.490 || val_loss 1.0639 acc 0.391 f1 0.338\n",
            "[W3] RNN Epoch 06 | train_loss 0.9946 acc 0.514 f1 0.510 || val_loss 1.1081 acc 0.327 f1 0.308\n",
            "[W3] RNN Epoch 07 | train_loss 0.9712 acc 0.532 f1 0.523 || val_loss 1.0791 acc 0.374 f1 0.341\n",
            "[W3] RNN Epoch 08 | train_loss 0.9501 acc 0.538 f1 0.529 || val_loss 1.0851 acc 0.362 f1 0.326\n",
            "[W3] RNN Epoch 09 | train_loss 0.9167 acc 0.558 f1 0.552 || val_loss 1.1139 acc 0.352 f1 0.331\n",
            "[W3] RNN Epoch 10 | train_loss 0.8980 acc 0.567 f1 0.558 || val_loss 1.0894 acc 0.379 f1 0.344\n",
            "[W3] RNN Epoch 11 | train_loss 0.8705 acc 0.593 f1 0.585 || val_loss 1.1004 acc 0.385 f1 0.349\n",
            "[W3] RNN Epoch 12 | train_loss 0.8549 acc 0.598 f1 0.589 || val_loss 1.1326 acc 0.358 f1 0.332\n",
            "[W3] RNN Epoch 13 | train_loss 0.8214 acc 0.618 f1 0.611 || val_loss 1.1427 acc 0.368 f1 0.343\n",
            "[W3] RNN Epoch 14 | train_loss 0.7930 acc 0.635 f1 0.626 || val_loss 1.1142 acc 0.393 f1 0.349\n",
            "[W3] RNN Epoch 15 | train_loss 0.7750 acc 0.637 f1 0.629 || val_loss 1.1405 acc 0.399 f1 0.366\n",
            "[W3] RNN Epoch 16 | train_loss 0.7536 acc 0.644 f1 0.637 || val_loss 1.1338 acc 0.379 f1 0.315\n",
            "[W3] RNN Epoch 17 | train_loss 0.7342 acc 0.649 f1 0.642 || val_loss 1.1517 acc 0.395 f1 0.354\n",
            "[W3] RNN Epoch 18 | train_loss 0.7174 acc 0.658 f1 0.651 || val_loss 1.1713 acc 0.385 f1 0.350\n",
            "[W3] RNN Epoch 19 | train_loss 0.6956 acc 0.671 f1 0.664 || val_loss 1.1793 acc 0.389 f1 0.344\n",
            "[W3] RNN Epoch 20 | train_loss 0.6708 acc 0.684 f1 0.678 || val_loss 1.1882 acc 0.393 f1 0.344\n",
            "[W3] RNN Epoch 21 | train_loss 0.6514 acc 0.700 f1 0.695 || val_loss 1.1876 acc 0.426 f1 0.357\n",
            "[W3] RNN Epoch 22 | train_loss 0.6319 acc 0.702 f1 0.697 || val_loss 1.2189 acc 0.395 f1 0.344\n",
            "[W3] RNN Epoch 23 | train_loss 0.6120 acc 0.709 f1 0.704 || val_loss 1.2353 acc 0.405 f1 0.357\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=61\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 933, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0989 acc 0.342 f1 0.297 || val_loss 1.1029 acc 0.294 f1 0.283\n",
            "[W3] GRU Epoch 02 | train_loss 1.0906 acc 0.402 f1 0.402 || val_loss 1.0999 acc 0.317 f1 0.294\n",
            "[W3] GRU Epoch 03 | train_loss 1.0810 acc 0.437 f1 0.435 || val_loss 1.0947 acc 0.333 f1 0.294\n",
            "[W3] GRU Epoch 04 | train_loss 1.0658 acc 0.449 f1 0.447 || val_loss 1.0799 acc 0.356 f1 0.310\n",
            "[W3] GRU Epoch 05 | train_loss 1.0411 acc 0.486 f1 0.485 || val_loss 1.0847 acc 0.360 f1 0.315\n",
            "[W3] GRU Epoch 06 | train_loss 0.9901 acc 0.526 f1 0.523 || val_loss 1.1149 acc 0.340 f1 0.316\n",
            "[W3] GRU Epoch 07 | train_loss 0.9166 acc 0.559 f1 0.551 || val_loss 1.1138 acc 0.329 f1 0.285\n",
            "[W3] GRU Epoch 08 | train_loss 0.8465 acc 0.593 f1 0.587 || val_loss 1.1223 acc 0.348 f1 0.289\n",
            "[W3] GRU Epoch 09 | train_loss 0.7954 acc 0.620 f1 0.614 || val_loss 1.1501 acc 0.352 f1 0.303\n",
            "[W3] GRU Epoch 10 | train_loss 0.7569 acc 0.636 f1 0.632 || val_loss 1.1471 acc 0.344 f1 0.279\n",
            "[W3] GRU Epoch 11 | train_loss 0.7230 acc 0.665 f1 0.661 || val_loss 1.1985 acc 0.348 f1 0.294\n",
            "[W3] GRU Epoch 12 | train_loss 0.6859 acc 0.665 f1 0.661 || val_loss 1.2021 acc 0.329 f1 0.265\n",
            "[W3] GRU Epoch 13 | train_loss 0.6568 acc 0.684 f1 0.680 || val_loss 1.2352 acc 0.354 f1 0.291\n",
            "[W3] GRU Epoch 14 | train_loss 0.6327 acc 0.697 f1 0.694 || val_loss 1.2285 acc 0.356 f1 0.280\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=61\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 933, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1039 acc 0.337 f1 0.272 || val_loss 1.0863 acc 0.434 f1 0.347\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0934 acc 0.388 f1 0.374 || val_loss 1.0968 acc 0.296 f1 0.286\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0853 acc 0.409 f1 0.406 || val_loss 1.1100 acc 0.286 f1 0.282\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0709 acc 0.441 f1 0.429 || val_loss 1.0939 acc 0.333 f1 0.316\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0282 acc 0.478 f1 0.464 || val_loss 1.1082 acc 0.335 f1 0.319\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9367 acc 0.544 f1 0.535 || val_loss 1.1009 acc 0.356 f1 0.289\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8583 acc 0.575 f1 0.567 || val_loss 1.0987 acc 0.360 f1 0.283\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8168 acc 0.602 f1 0.598 || val_loss 1.1217 acc 0.358 f1 0.291\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7696 acc 0.631 f1 0.626 || val_loss 1.1928 acc 0.372 f1 0.322\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  61%|    | 61/100 [29:27<16:50, 25.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=62 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=62\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 932, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1203 acc 0.394 f1 0.389 || val_loss 1.1060 acc 0.364 f1 0.335\n",
            "[W3] ANN Epoch 02 | train_loss 0.9642 acc 0.513 f1 0.503 || val_loss 1.0971 acc 0.354 f1 0.315\n",
            "[W3] ANN Epoch 03 | train_loss 0.8675 acc 0.577 f1 0.567 || val_loss 1.0961 acc 0.362 f1 0.314\n",
            "[W3] ANN Epoch 04 | train_loss 0.7756 acc 0.620 f1 0.612 || val_loss 1.0994 acc 0.405 f1 0.361\n",
            "[W3] ANN Epoch 05 | train_loss 0.7298 acc 0.646 f1 0.641 || val_loss 1.0804 acc 0.418 f1 0.329\n",
            "[W3] ANN Epoch 06 | train_loss 0.6760 acc 0.675 f1 0.671 || val_loss 1.1073 acc 0.399 f1 0.326\n",
            "[W3] ANN Epoch 07 | train_loss 0.6327 acc 0.697 f1 0.693 || val_loss 1.1551 acc 0.403 f1 0.342\n",
            "[W3] ANN Epoch 08 | train_loss 0.6161 acc 0.705 f1 0.704 || val_loss 1.1404 acc 0.434 f1 0.339\n",
            "[W3] ANN Epoch 09 | train_loss 0.5731 acc 0.728 f1 0.726 || val_loss 1.1752 acc 0.397 f1 0.329\n",
            "[W3] ANN Epoch 10 | train_loss 0.5534 acc 0.754 f1 0.752 || val_loss 1.1876 acc 0.416 f1 0.335\n",
            "[W3] ANN Epoch 11 | train_loss 0.5444 acc 0.749 f1 0.749 || val_loss 1.1801 acc 0.447 f1 0.372\n",
            "[W3] ANN Epoch 12 | train_loss 0.5172 acc 0.758 f1 0.757 || val_loss 1.1893 acc 0.469 f1 0.361\n",
            "[W3] ANN Epoch 13 | train_loss 0.4979 acc 0.783 f1 0.782 || val_loss 1.2040 acc 0.449 f1 0.364\n",
            "[W3] ANN Epoch 14 | train_loss 0.4856 acc 0.783 f1 0.782 || val_loss 1.2480 acc 0.459 f1 0.377\n",
            "[W3] ANN Epoch 15 | train_loss 0.4780 acc 0.792 f1 0.791 || val_loss 1.2616 acc 0.453 f1 0.361\n",
            "[W3] ANN Epoch 16 | train_loss 0.4699 acc 0.792 f1 0.792 || val_loss 1.2598 acc 0.442 f1 0.343\n",
            "[W3] ANN Epoch 17 | train_loss 0.4391 acc 0.812 f1 0.811 || val_loss 1.3490 acc 0.440 f1 0.362\n",
            "[W3] ANN Epoch 18 | train_loss 0.4192 acc 0.817 f1 0.816 || val_loss 1.3530 acc 0.436 f1 0.338\n",
            "[W3] ANN Epoch 19 | train_loss 0.4166 acc 0.816 f1 0.816 || val_loss 1.3189 acc 0.449 f1 0.339\n",
            "[W3] ANN Epoch 20 | train_loss 0.4095 acc 0.830 f1 0.829 || val_loss 1.3968 acc 0.432 f1 0.326\n",
            "[W3] ANN Epoch 21 | train_loss 0.3992 acc 0.830 f1 0.830 || val_loss 1.3888 acc 0.457 f1 0.358\n",
            "[W3] ANN Epoch 22 | train_loss 0.3855 acc 0.830 f1 0.830 || val_loss 1.3999 acc 0.463 f1 0.370\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=62\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 932, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0725 acc 0.407 f1 0.407 || val_loss 1.0301 acc 0.409 f1 0.326\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9366 acc 0.557 f1 0.553 || val_loss 1.0436 acc 0.409 f1 0.350\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8247 acc 0.610 f1 0.606 || val_loss 1.0931 acc 0.405 f1 0.342\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7382 acc 0.656 f1 0.655 || val_loss 1.1383 acc 0.403 f1 0.341\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6751 acc 0.687 f1 0.683 || val_loss 1.2203 acc 0.387 f1 0.320\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6232 acc 0.712 f1 0.711 || val_loss 1.2326 acc 0.385 f1 0.330\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5404 acc 0.757 f1 0.756 || val_loss 1.2953 acc 0.424 f1 0.345\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5264 acc 0.768 f1 0.766 || val_loss 1.3190 acc 0.385 f1 0.307\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4910 acc 0.787 f1 0.785 || val_loss 1.3548 acc 0.407 f1 0.345\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4375 acc 0.813 f1 0.813 || val_loss 1.3989 acc 0.399 f1 0.327\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=62\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 932, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0972 acc 0.368 f1 0.357 || val_loss 1.0980 acc 0.360 f1 0.339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0839 acc 0.409 f1 0.409 || val_loss 1.0952 acc 0.319 f1 0.298\n",
            "[W3] RNN Epoch 03 | train_loss 1.0704 acc 0.431 f1 0.429 || val_loss 1.0933 acc 0.325 f1 0.303\n",
            "[W3] RNN Epoch 04 | train_loss 1.0579 acc 0.444 f1 0.443 || val_loss 1.1019 acc 0.337 f1 0.319\n",
            "[W3] RNN Epoch 05 | train_loss 1.0450 acc 0.463 f1 0.458 || val_loss 1.0974 acc 0.348 f1 0.327\n",
            "[W3] RNN Epoch 06 | train_loss 1.0345 acc 0.487 f1 0.486 || val_loss 1.0977 acc 0.358 f1 0.334\n",
            "[W3] RNN Epoch 07 | train_loss 1.0203 acc 0.487 f1 0.480 || val_loss 1.0868 acc 0.374 f1 0.344\n",
            "[W3] RNN Epoch 08 | train_loss 1.0090 acc 0.496 f1 0.493 || val_loss 1.0832 acc 0.358 f1 0.325\n",
            "[W3] RNN Epoch 09 | train_loss 0.9922 acc 0.518 f1 0.512 || val_loss 1.0835 acc 0.362 f1 0.335\n",
            "[W3] RNN Epoch 10 | train_loss 0.9763 acc 0.528 f1 0.525 || val_loss 1.0978 acc 0.352 f1 0.327\n",
            "[W3] RNN Epoch 11 | train_loss 0.9634 acc 0.538 f1 0.533 || val_loss 1.1066 acc 0.335 f1 0.314\n",
            "[W3] RNN Epoch 12 | train_loss 0.9453 acc 0.556 f1 0.550 || val_loss 1.0976 acc 0.354 f1 0.325\n",
            "[W3] RNN Epoch 13 | train_loss 0.9229 acc 0.559 f1 0.554 || val_loss 1.1077 acc 0.370 f1 0.336\n",
            "[W3] RNN Epoch 14 | train_loss 0.9082 acc 0.567 f1 0.556 || val_loss 1.0782 acc 0.374 f1 0.329\n",
            "[W3] RNN Epoch 15 | train_loss 0.8908 acc 0.581 f1 0.575 || val_loss 1.0945 acc 0.362 f1 0.322\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=62\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 932, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0999 acc 0.336 f1 0.307 || val_loss 1.0939 acc 0.340 f1 0.310\n",
            "[W3] GRU Epoch 02 | train_loss 1.0901 acc 0.387 f1 0.387 || val_loss 1.0977 acc 0.311 f1 0.286\n",
            "[W3] GRU Epoch 03 | train_loss 1.0805 acc 0.411 f1 0.407 || val_loss 1.1072 acc 0.307 f1 0.296\n",
            "[W3] GRU Epoch 04 | train_loss 1.0678 acc 0.451 f1 0.441 || val_loss 1.0936 acc 0.323 f1 0.296\n",
            "[W3] GRU Epoch 05 | train_loss 1.0462 acc 0.484 f1 0.481 || val_loss 1.0934 acc 0.354 f1 0.329\n",
            "[W3] GRU Epoch 06 | train_loss 1.0148 acc 0.500 f1 0.494 || val_loss 1.1078 acc 0.335 f1 0.317\n",
            "[W3] GRU Epoch 07 | train_loss 0.9585 acc 0.538 f1 0.530 || val_loss 1.0707 acc 0.387 f1 0.338\n",
            "[W3] GRU Epoch 08 | train_loss 0.8954 acc 0.569 f1 0.559 || val_loss 1.0757 acc 0.395 f1 0.332\n",
            "[W3] GRU Epoch 09 | train_loss 0.8478 acc 0.585 f1 0.580 || val_loss 1.1044 acc 0.399 f1 0.350\n",
            "[W3] GRU Epoch 10 | train_loss 0.7940 acc 0.615 f1 0.608 || val_loss 1.0952 acc 0.424 f1 0.365\n",
            "[W3] GRU Epoch 11 | train_loss 0.7589 acc 0.636 f1 0.629 || val_loss 1.0968 acc 0.399 f1 0.330\n",
            "[W3] GRU Epoch 12 | train_loss 0.7445 acc 0.636 f1 0.633 || val_loss 1.1594 acc 0.391 f1 0.333\n",
            "[W3] GRU Epoch 13 | train_loss 0.7065 acc 0.660 f1 0.657 || val_loss 1.1430 acc 0.420 f1 0.354\n",
            "[W3] GRU Epoch 14 | train_loss 0.6757 acc 0.669 f1 0.663 || val_loss 1.2185 acc 0.385 f1 0.331\n",
            "[W3] GRU Epoch 15 | train_loss 0.6614 acc 0.682 f1 0.679 || val_loss 1.2059 acc 0.401 f1 0.340\n",
            "[W3] GRU Epoch 16 | train_loss 0.6349 acc 0.700 f1 0.696 || val_loss 1.2117 acc 0.401 f1 0.337\n",
            "[W3] GRU Epoch 17 | train_loss 0.6282 acc 0.701 f1 0.699 || val_loss 1.2147 acc 0.395 f1 0.332\n",
            "[W3] GRU Epoch 18 | train_loss 0.5875 acc 0.718 f1 0.715 || val_loss 1.2514 acc 0.395 f1 0.332\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=62\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 932, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0990 acc 0.342 f1 0.271 || val_loss 1.0971 acc 0.315 f1 0.258\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0941 acc 0.377 f1 0.354 || val_loss 1.0988 acc 0.282 f1 0.247\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0887 acc 0.407 f1 0.405 || val_loss 1.0997 acc 0.307 f1 0.293\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0752 acc 0.441 f1 0.434 || val_loss 1.1091 acc 0.337 f1 0.331\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0493 acc 0.462 f1 0.440 || val_loss 1.0975 acc 0.331 f1 0.307\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9831 acc 0.520 f1 0.490 || val_loss 1.1626 acc 0.329 f1 0.316\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9088 acc 0.541 f1 0.531 || val_loss 1.0962 acc 0.379 f1 0.324\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8372 acc 0.577 f1 0.568 || val_loss 1.1160 acc 0.399 f1 0.337\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7951 acc 0.600 f1 0.595 || val_loss 1.1564 acc 0.383 f1 0.325\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7734 acc 0.611 f1 0.605 || val_loss 1.1394 acc 0.389 f1 0.317\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7356 acc 0.632 f1 0.629 || val_loss 1.1852 acc 0.374 f1 0.313\n",
            "[W3] LSTM Epoch 12 | train_loss 0.7106 acc 0.646 f1 0.641 || val_loss 1.1660 acc 0.403 f1 0.316\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6906 acc 0.643 f1 0.641 || val_loss 1.1920 acc 0.393 f1 0.319\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6587 acc 0.673 f1 0.669 || val_loss 1.2397 acc 0.387 f1 0.315\n",
            "[W3] LSTM Epoch 15 | train_loss 0.6288 acc 0.681 f1 0.678 || val_loss 1.2826 acc 0.393 f1 0.315\n",
            "[W3] LSTM Epoch 16 | train_loss 0.6074 acc 0.696 f1 0.692 || val_loss 1.3116 acc 0.364 f1 0.297\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  62%|   | 62/100 [29:51<16:02, 25.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=63 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=63\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 930, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1427 acc 0.390 f1 0.390 || val_loss 1.0927 acc 0.385 f1 0.352\n",
            "[W3] ANN Epoch 02 | train_loss 0.9697 acc 0.517 f1 0.510 || val_loss 1.0966 acc 0.364 f1 0.332\n",
            "[W3] ANN Epoch 03 | train_loss 0.8659 acc 0.572 f1 0.564 || val_loss 1.1065 acc 0.385 f1 0.341\n",
            "[W3] ANN Epoch 04 | train_loss 0.7823 acc 0.615 f1 0.609 || val_loss 1.0914 acc 0.414 f1 0.344\n",
            "[W3] ANN Epoch 05 | train_loss 0.7129 acc 0.661 f1 0.656 || val_loss 1.1051 acc 0.397 f1 0.322\n",
            "[W3] ANN Epoch 06 | train_loss 0.6558 acc 0.689 f1 0.685 || val_loss 1.1191 acc 0.403 f1 0.328\n",
            "[W3] ANN Epoch 07 | train_loss 0.6274 acc 0.708 f1 0.707 || val_loss 1.1359 acc 0.434 f1 0.365\n",
            "[W3] ANN Epoch 08 | train_loss 0.5916 acc 0.728 f1 0.726 || val_loss 1.1722 acc 0.418 f1 0.342\n",
            "[W3] ANN Epoch 09 | train_loss 0.5619 acc 0.738 f1 0.737 || val_loss 1.1857 acc 0.414 f1 0.328\n",
            "[W3] ANN Epoch 10 | train_loss 0.5261 acc 0.754 f1 0.753 || val_loss 1.2218 acc 0.432 f1 0.339\n",
            "[W3] ANN Epoch 11 | train_loss 0.4980 acc 0.775 f1 0.774 || val_loss 1.2571 acc 0.430 f1 0.351\n",
            "[W3] ANN Epoch 12 | train_loss 0.4751 acc 0.790 f1 0.789 || val_loss 1.2940 acc 0.432 f1 0.338\n",
            "[W3] ANN Epoch 13 | train_loss 0.4660 acc 0.787 f1 0.786 || val_loss 1.2732 acc 0.438 f1 0.356\n",
            "[W3] ANN Epoch 14 | train_loss 0.4498 acc 0.808 f1 0.807 || val_loss 1.3207 acc 0.455 f1 0.373\n",
            "[W3] ANN Epoch 15 | train_loss 0.4195 acc 0.822 f1 0.822 || val_loss 1.3865 acc 0.430 f1 0.355\n",
            "[W3] ANN Epoch 16 | train_loss 0.4055 acc 0.829 f1 0.829 || val_loss 1.4115 acc 0.440 f1 0.345\n",
            "[W3] ANN Epoch 17 | train_loss 0.3953 acc 0.836 f1 0.836 || val_loss 1.4316 acc 0.442 f1 0.352\n",
            "[W3] ANN Epoch 18 | train_loss 0.3556 acc 0.850 f1 0.850 || val_loss 1.4648 acc 0.436 f1 0.355\n",
            "[W3] ANN Epoch 19 | train_loss 0.3527 acc 0.854 f1 0.854 || val_loss 1.4462 acc 0.471 f1 0.370\n",
            "[W3] ANN Epoch 20 | train_loss 0.3739 acc 0.845 f1 0.844 || val_loss 1.4690 acc 0.434 f1 0.348\n",
            "[W3] ANN Epoch 21 | train_loss 0.3371 acc 0.860 f1 0.859 || val_loss 1.5071 acc 0.436 f1 0.344\n",
            "[W3] ANN Epoch 22 | train_loss 0.3449 acc 0.860 f1 0.860 || val_loss 1.5284 acc 0.440 f1 0.340\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=63\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 930, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0680 acc 0.407 f1 0.408 || val_loss 1.0328 acc 0.399 f1 0.300\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9232 acc 0.554 f1 0.548 || val_loss 1.0758 acc 0.379 f1 0.300\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8046 acc 0.602 f1 0.598 || val_loss 1.1778 acc 0.333 f1 0.270\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7123 acc 0.659 f1 0.655 || val_loss 1.1907 acc 0.372 f1 0.303\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6212 acc 0.704 f1 0.701 || val_loss 1.2520 acc 0.385 f1 0.312\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5602 acc 0.741 f1 0.739 || val_loss 1.3923 acc 0.379 f1 0.331\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.4980 acc 0.778 f1 0.777 || val_loss 1.4036 acc 0.389 f1 0.299\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4455 acc 0.800 f1 0.799 || val_loss 1.5021 acc 0.389 f1 0.320\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4040 acc 0.822 f1 0.822 || val_loss 1.5767 acc 0.366 f1 0.290\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3547 acc 0.854 f1 0.854 || val_loss 1.6861 acc 0.372 f1 0.291\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3231 acc 0.868 f1 0.868 || val_loss 1.7574 acc 0.391 f1 0.317\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2872 acc 0.879 f1 0.880 || val_loss 1.8498 acc 0.395 f1 0.304\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2553 acc 0.902 f1 0.902 || val_loss 1.9731 acc 0.374 f1 0.284\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2085 acc 0.926 f1 0.926 || val_loss 2.0970 acc 0.383 f1 0.301\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=63\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 930, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1002 acc 0.333 f1 0.314 || val_loss 1.0955 acc 0.377 f1 0.351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0860 acc 0.413 f1 0.409 || val_loss 1.0961 acc 0.335 f1 0.310\n",
            "[W3] RNN Epoch 03 | train_loss 1.0716 acc 0.432 f1 0.421 || val_loss 1.0832 acc 0.350 f1 0.326\n",
            "[W3] RNN Epoch 04 | train_loss 1.0551 acc 0.452 f1 0.447 || val_loss 1.0903 acc 0.329 f1 0.315\n",
            "[W3] RNN Epoch 05 | train_loss 1.0362 acc 0.466 f1 0.460 || val_loss 1.1013 acc 0.309 f1 0.295\n",
            "[W3] RNN Epoch 06 | train_loss 1.0140 acc 0.491 f1 0.482 || val_loss 1.0901 acc 0.340 f1 0.319\n",
            "[W3] RNN Epoch 07 | train_loss 0.9939 acc 0.510 f1 0.500 || val_loss 1.0752 acc 0.348 f1 0.310\n",
            "[W3] RNN Epoch 08 | train_loss 0.9675 acc 0.530 f1 0.522 || val_loss 1.0907 acc 0.348 f1 0.319\n",
            "[W3] RNN Epoch 09 | train_loss 0.9393 acc 0.541 f1 0.530 || val_loss 1.1061 acc 0.331 f1 0.304\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=63\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 930, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0983 acc 0.359 f1 0.291 || val_loss 1.1029 acc 0.300 f1 0.283\n",
            "[W3] GRU Epoch 02 | train_loss 1.0901 acc 0.389 f1 0.379 || val_loss 1.0907 acc 0.352 f1 0.332\n",
            "[W3] GRU Epoch 03 | train_loss 1.0809 acc 0.423 f1 0.422 || val_loss 1.0823 acc 0.360 f1 0.335\n",
            "[W3] GRU Epoch 04 | train_loss 1.0666 acc 0.437 f1 0.435 || val_loss 1.0864 acc 0.327 f1 0.313\n",
            "[W3] GRU Epoch 05 | train_loss 1.0438 acc 0.470 f1 0.464 || val_loss 1.1009 acc 0.319 f1 0.307\n",
            "[W3] GRU Epoch 06 | train_loss 1.0063 acc 0.519 f1 0.508 || val_loss 1.0850 acc 0.350 f1 0.326\n",
            "[W3] GRU Epoch 07 | train_loss 0.9392 acc 0.563 f1 0.556 || val_loss 1.0914 acc 0.366 f1 0.331\n",
            "[W3] GRU Epoch 08 | train_loss 0.8677 acc 0.586 f1 0.575 || val_loss 1.0926 acc 0.401 f1 0.350\n",
            "[W3] GRU Epoch 09 | train_loss 0.8151 acc 0.607 f1 0.602 || val_loss 1.1149 acc 0.381 f1 0.321\n",
            "[W3] GRU Epoch 10 | train_loss 0.7683 acc 0.637 f1 0.631 || val_loss 1.1200 acc 0.395 f1 0.334\n",
            "[W3] GRU Epoch 11 | train_loss 0.7264 acc 0.648 f1 0.643 || val_loss 1.1755 acc 0.395 f1 0.328\n",
            "[W3] GRU Epoch 12 | train_loss 0.6948 acc 0.665 f1 0.660 || val_loss 1.1851 acc 0.397 f1 0.324\n",
            "[W3] GRU Epoch 13 | train_loss 0.6622 acc 0.671 f1 0.667 || val_loss 1.2388 acc 0.379 f1 0.320\n",
            "[W3] GRU Epoch 14 | train_loss 0.6388 acc 0.689 f1 0.685 || val_loss 1.2260 acc 0.405 f1 0.334\n",
            "[W3] GRU Epoch 15 | train_loss 0.6051 acc 0.710 f1 0.708 || val_loss 1.2703 acc 0.383 f1 0.315\n",
            "[W3] GRU Epoch 16 | train_loss 0.5741 acc 0.712 f1 0.710 || val_loss 1.3088 acc 0.405 f1 0.335\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=63\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 930, np.int64(0): 276})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0987 acc 0.329 f1 0.301 || val_loss 1.0948 acc 0.395 f1 0.339\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0942 acc 0.379 f1 0.371 || val_loss 1.0961 acc 0.333 f1 0.300\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0881 acc 0.407 f1 0.406 || val_loss 1.0974 acc 0.321 f1 0.304\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0744 acc 0.438 f1 0.435 || val_loss 1.0963 acc 0.329 f1 0.307\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0417 acc 0.488 f1 0.480 || val_loss 1.0806 acc 0.360 f1 0.319\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9697 acc 0.534 f1 0.526 || val_loss 1.0829 acc 0.370 f1 0.308\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8786 acc 0.568 f1 0.560 || val_loss 1.1103 acc 0.393 f1 0.329\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8151 acc 0.609 f1 0.603 || val_loss 1.1113 acc 0.399 f1 0.327\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7543 acc 0.635 f1 0.631 || val_loss 1.1552 acc 0.407 f1 0.338\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  63%|   | 63/100 [30:12<14:47, 23.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=64 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=64\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 978, np.int64(1): 932, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 978, np.int64(2): 978, np.int64(0): 978})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1195 acc 0.397 f1 0.397 || val_loss 1.1192 acc 0.300 f1 0.280\n",
            "[W3] ANN Epoch 02 | train_loss 0.9615 acc 0.515 f1 0.510 || val_loss 1.1263 acc 0.352 f1 0.329\n",
            "[W3] ANN Epoch 03 | train_loss 0.8596 acc 0.571 f1 0.564 || val_loss 1.1097 acc 0.344 f1 0.299\n",
            "[W3] ANN Epoch 04 | train_loss 0.7864 acc 0.618 f1 0.612 || val_loss 1.1037 acc 0.348 f1 0.303\n",
            "[W3] ANN Epoch 05 | train_loss 0.7306 acc 0.639 f1 0.635 || val_loss 1.1202 acc 0.354 f1 0.301\n",
            "[W3] ANN Epoch 06 | train_loss 0.6768 acc 0.670 f1 0.667 || val_loss 1.1281 acc 0.395 f1 0.332\n",
            "[W3] ANN Epoch 07 | train_loss 0.6373 acc 0.690 f1 0.687 || val_loss 1.1567 acc 0.391 f1 0.311\n",
            "[W3] ANN Epoch 08 | train_loss 0.6032 acc 0.707 f1 0.705 || val_loss 1.1746 acc 0.405 f1 0.332\n",
            "[W3] ANN Epoch 09 | train_loss 0.5729 acc 0.725 f1 0.724 || val_loss 1.2357 acc 0.389 f1 0.305\n",
            "[W3] ANN Epoch 10 | train_loss 0.5426 acc 0.742 f1 0.741 || val_loss 1.2385 acc 0.395 f1 0.324\n",
            "[W3] ANN Epoch 11 | train_loss 0.5238 acc 0.759 f1 0.758 || val_loss 1.2459 acc 0.414 f1 0.343\n",
            "[W3] ANN Epoch 12 | train_loss 0.4863 acc 0.779 f1 0.779 || val_loss 1.2729 acc 0.426 f1 0.348\n",
            "[W3] ANN Epoch 13 | train_loss 0.4794 acc 0.785 f1 0.785 || val_loss 1.3348 acc 0.424 f1 0.336\n",
            "[W3] ANN Epoch 14 | train_loss 0.4745 acc 0.774 f1 0.774 || val_loss 1.3776 acc 0.399 f1 0.328\n",
            "[W3] ANN Epoch 15 | train_loss 0.4465 acc 0.801 f1 0.801 || val_loss 1.3629 acc 0.434 f1 0.368\n",
            "[W3] ANN Epoch 16 | train_loss 0.4468 acc 0.802 f1 0.802 || val_loss 1.4037 acc 0.438 f1 0.357\n",
            "[W3] ANN Epoch 17 | train_loss 0.4247 acc 0.823 f1 0.823 || val_loss 1.3933 acc 0.418 f1 0.346\n",
            "[W3] ANN Epoch 18 | train_loss 0.4020 acc 0.829 f1 0.829 || val_loss 1.4557 acc 0.409 f1 0.334\n",
            "[W3] ANN Epoch 19 | train_loss 0.3971 acc 0.830 f1 0.830 || val_loss 1.4704 acc 0.438 f1 0.361\n",
            "[W3] ANN Epoch 20 | train_loss 0.3842 acc 0.837 f1 0.837 || val_loss 1.4155 acc 0.422 f1 0.355\n",
            "[W3] ANN Epoch 21 | train_loss 0.3525 acc 0.852 f1 0.852 || val_loss 1.4719 acc 0.414 f1 0.339\n",
            "[W3] ANN Epoch 22 | train_loss 0.3566 acc 0.856 f1 0.856 || val_loss 1.5543 acc 0.401 f1 0.333\n",
            "[W3] ANN Epoch 23 | train_loss 0.3407 acc 0.853 f1 0.852 || val_loss 1.5160 acc 0.438 f1 0.352\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=64\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 978, np.int64(1): 932, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 978, np.int64(2): 978, np.int64(0): 978})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0638 acc 0.416 f1 0.414 || val_loss 1.0213 acc 0.430 f1 0.331\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9404 acc 0.545 f1 0.538 || val_loss 1.0421 acc 0.403 f1 0.332\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8176 acc 0.600 f1 0.596 || val_loss 1.1292 acc 0.385 f1 0.330\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7169 acc 0.663 f1 0.659 || val_loss 1.2031 acc 0.379 f1 0.318\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6483 acc 0.691 f1 0.688 || val_loss 1.2314 acc 0.391 f1 0.318\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5690 acc 0.736 f1 0.733 || val_loss 1.3294 acc 0.393 f1 0.340\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5199 acc 0.760 f1 0.760 || val_loss 1.3840 acc 0.385 f1 0.327\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4585 acc 0.793 f1 0.792 || val_loss 1.4464 acc 0.403 f1 0.335\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4135 acc 0.821 f1 0.821 || val_loss 1.5466 acc 0.391 f1 0.336\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3659 acc 0.841 f1 0.841 || val_loss 1.6528 acc 0.412 f1 0.345\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3262 acc 0.864 f1 0.864 || val_loss 1.7441 acc 0.409 f1 0.346\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2943 acc 0.879 f1 0.879 || val_loss 1.8307 acc 0.381 f1 0.316\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2594 acc 0.899 f1 0.899 || val_loss 1.8857 acc 0.414 f1 0.342\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2298 acc 0.913 f1 0.913 || val_loss 1.9827 acc 0.393 f1 0.326\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2025 acc 0.925 f1 0.925 || val_loss 2.1475 acc 0.381 f1 0.305\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1846 acc 0.934 f1 0.934 || val_loss 2.2466 acc 0.374 f1 0.312\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1579 acc 0.946 f1 0.946 || val_loss 2.3086 acc 0.405 f1 0.326\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1406 acc 0.952 f1 0.952 || val_loss 2.3897 acc 0.397 f1 0.330\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1226 acc 0.963 f1 0.962 || val_loss 2.5532 acc 0.374 f1 0.303\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=64\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 978, np.int64(1): 932, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 978, np.int64(2): 978, np.int64(0): 978})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0976 acc 0.351 f1 0.334 || val_loss 1.1000 acc 0.335 f1 0.324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0828 acc 0.411 f1 0.405 || val_loss 1.0931 acc 0.327 f1 0.301\n",
            "[W3] RNN Epoch 03 | train_loss 1.0700 acc 0.443 f1 0.441 || val_loss 1.0914 acc 0.323 f1 0.294\n",
            "[W3] RNN Epoch 04 | train_loss 1.0552 acc 0.454 f1 0.447 || val_loss 1.1003 acc 0.311 f1 0.295\n",
            "[W3] RNN Epoch 05 | train_loss 1.0371 acc 0.479 f1 0.474 || val_loss 1.0942 acc 0.342 f1 0.322\n",
            "[W3] RNN Epoch 06 | train_loss 1.0164 acc 0.489 f1 0.485 || val_loss 1.0880 acc 0.342 f1 0.318\n",
            "[W3] RNN Epoch 07 | train_loss 0.9899 acc 0.527 f1 0.520 || val_loss 1.0684 acc 0.366 f1 0.334\n",
            "[W3] RNN Epoch 08 | train_loss 0.9699 acc 0.536 f1 0.530 || val_loss 1.0975 acc 0.385 f1 0.362\n",
            "[W3] RNN Epoch 09 | train_loss 0.9437 acc 0.542 f1 0.536 || val_loss 1.0763 acc 0.403 f1 0.374\n",
            "[W3] RNN Epoch 10 | train_loss 0.9121 acc 0.580 f1 0.570 || val_loss 1.0678 acc 0.385 f1 0.342\n",
            "[W3] RNN Epoch 11 | train_loss 0.8871 acc 0.586 f1 0.578 || val_loss 1.0969 acc 0.385 f1 0.352\n",
            "[W3] RNN Epoch 12 | train_loss 0.8567 acc 0.610 f1 0.601 || val_loss 1.0884 acc 0.379 f1 0.342\n",
            "[W3] RNN Epoch 13 | train_loss 0.8426 acc 0.613 f1 0.604 || val_loss 1.1097 acc 0.395 f1 0.355\n",
            "[W3] RNN Epoch 14 | train_loss 0.8116 acc 0.629 f1 0.620 || val_loss 1.1305 acc 0.397 f1 0.355\n",
            "[W3] RNN Epoch 15 | train_loss 0.7900 acc 0.639 f1 0.631 || val_loss 1.1197 acc 0.403 f1 0.348\n",
            "[W3] RNN Epoch 16 | train_loss 0.7651 acc 0.640 f1 0.632 || val_loss 1.1464 acc 0.393 f1 0.346\n",
            "[W3] RNN Epoch 17 | train_loss 0.7350 acc 0.668 f1 0.661 || val_loss 1.1565 acc 0.391 f1 0.349\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=64\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 978, np.int64(1): 932, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 978, np.int64(2): 978, np.int64(0): 978})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0997 acc 0.338 f1 0.273 || val_loss 1.0862 acc 0.428 f1 0.328\n",
            "[W3] GRU Epoch 02 | train_loss 1.0912 acc 0.388 f1 0.379 || val_loss 1.0903 acc 0.323 f1 0.298\n",
            "[W3] GRU Epoch 03 | train_loss 1.0821 acc 0.429 f1 0.429 || val_loss 1.0927 acc 0.319 f1 0.306\n",
            "[W3] GRU Epoch 04 | train_loss 1.0667 acc 0.449 f1 0.447 || val_loss 1.0819 acc 0.333 f1 0.313\n",
            "[W3] GRU Epoch 05 | train_loss 1.0450 acc 0.475 f1 0.469 || val_loss 1.0875 acc 0.331 f1 0.309\n",
            "[W3] GRU Epoch 06 | train_loss 0.9933 acc 0.520 f1 0.510 || val_loss 1.0830 acc 0.348 f1 0.315\n",
            "[W3] GRU Epoch 07 | train_loss 0.9142 acc 0.555 f1 0.547 || val_loss 1.0965 acc 0.350 f1 0.303\n",
            "[W3] GRU Epoch 08 | train_loss 0.8370 acc 0.584 f1 0.575 || val_loss 1.1291 acc 0.356 f1 0.317\n",
            "[W3] GRU Epoch 09 | train_loss 0.8026 acc 0.599 f1 0.593 || val_loss 1.1264 acc 0.377 f1 0.328\n",
            "[W3] GRU Epoch 10 | train_loss 0.7609 acc 0.620 f1 0.614 || val_loss 1.1381 acc 0.395 f1 0.331\n",
            "[W3] GRU Epoch 11 | train_loss 0.7289 acc 0.655 f1 0.651 || val_loss 1.1449 acc 0.412 f1 0.344\n",
            "[W3] GRU Epoch 12 | train_loss 0.6977 acc 0.661 f1 0.656 || val_loss 1.1759 acc 0.412 f1 0.348\n",
            "[W3] GRU Epoch 13 | train_loss 0.6659 acc 0.681 f1 0.677 || val_loss 1.2288 acc 0.409 f1 0.361\n",
            "[W3] GRU Epoch 14 | train_loss 0.6400 acc 0.701 f1 0.698 || val_loss 1.2131 acc 0.416 f1 0.352\n",
            "[W3] GRU Epoch 15 | train_loss 0.6115 acc 0.714 f1 0.711 || val_loss 1.2571 acc 0.403 f1 0.346\n",
            "[W3] GRU Epoch 16 | train_loss 0.5801 acc 0.731 f1 0.729 || val_loss 1.2779 acc 0.399 f1 0.333\n",
            "[W3] GRU Epoch 17 | train_loss 0.5659 acc 0.739 f1 0.738 || val_loss 1.3168 acc 0.403 f1 0.339\n",
            "[W3] GRU Epoch 18 | train_loss 0.5352 acc 0.753 f1 0.751 || val_loss 1.3497 acc 0.403 f1 0.342\n",
            "[W3] GRU Epoch 19 | train_loss 0.5072 acc 0.758 f1 0.756 || val_loss 1.3955 acc 0.409 f1 0.347\n",
            "[W3] GRU Epoch 20 | train_loss 0.4830 acc 0.776 f1 0.775 || val_loss 1.4439 acc 0.393 f1 0.329\n",
            "[W3] GRU Epoch 21 | train_loss 0.4653 acc 0.787 f1 0.786 || val_loss 1.5032 acc 0.393 f1 0.334\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=64\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 978, np.int64(1): 932, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 978, np.int64(2): 978, np.int64(0): 978})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0986 acc 0.326 f1 0.276 || val_loss 1.0928 acc 0.395 f1 0.314\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0945 acc 0.380 f1 0.371 || val_loss 1.0994 acc 0.280 f1 0.274\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0882 acc 0.405 f1 0.390 || val_loss 1.0970 acc 0.286 f1 0.264\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0751 acc 0.440 f1 0.432 || val_loss 1.0854 acc 0.331 f1 0.314\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0381 acc 0.477 f1 0.469 || val_loss 1.0815 acc 0.333 f1 0.306\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9548 acc 0.524 f1 0.514 || val_loss 1.1376 acc 0.311 f1 0.285\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8699 acc 0.571 f1 0.562 || val_loss 1.1068 acc 0.342 f1 0.292\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8013 acc 0.613 f1 0.607 || val_loss 1.1312 acc 0.358 f1 0.290\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7558 acc 0.633 f1 0.629 || val_loss 1.1576 acc 0.362 f1 0.290\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7186 acc 0.641 f1 0.637 || val_loss 1.1884 acc 0.387 f1 0.315\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6888 acc 0.667 f1 0.664 || val_loss 1.2136 acc 0.379 f1 0.299\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6567 acc 0.683 f1 0.680 || val_loss 1.2619 acc 0.381 f1 0.303\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6314 acc 0.691 f1 0.689 || val_loss 1.2815 acc 0.397 f1 0.314\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5980 acc 0.709 f1 0.707 || val_loss 1.3418 acc 0.412 f1 0.326\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5710 acc 0.715 f1 0.714 || val_loss 1.3773 acc 0.397 f1 0.315\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5459 acc 0.732 f1 0.730 || val_loss 1.4030 acc 0.414 f1 0.330\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5235 acc 0.754 f1 0.753 || val_loss 1.4456 acc 0.381 f1 0.303\n",
            "[W3] LSTM Epoch 18 | train_loss 0.4935 acc 0.767 f1 0.765 || val_loss 1.5305 acc 0.401 f1 0.321\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4768 acc 0.771 f1 0.771 || val_loss 1.5839 acc 0.389 f1 0.317\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4533 acc 0.786 f1 0.786 || val_loss 1.6611 acc 0.391 f1 0.312\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4416 acc 0.788 f1 0.788 || val_loss 1.6979 acc 0.399 f1 0.316\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4181 acc 0.796 f1 0.796 || val_loss 1.7282 acc 0.395 f1 0.318\n",
            "[W3] LSTM Epoch 23 | train_loss 0.3989 acc 0.812 f1 0.812 || val_loss 1.8004 acc 0.414 f1 0.331\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3833 acc 0.813 f1 0.813 || val_loss 1.8780 acc 0.389 f1 0.315\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3771 acc 0.818 f1 0.818 || val_loss 1.9262 acc 0.387 f1 0.308\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3515 acc 0.839 f1 0.839 || val_loss 1.9560 acc 0.420 f1 0.335\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3353 acc 0.849 f1 0.849 || val_loss 2.0314 acc 0.403 f1 0.319\n",
            "[W3] LSTM Epoch 28 | train_loss 0.3213 acc 0.858 f1 0.858 || val_loss 2.1380 acc 0.395 f1 0.316\n",
            "[W3] LSTM Epoch 29 | train_loss 0.3087 acc 0.865 f1 0.865 || val_loss 2.0906 acc 0.395 f1 0.307\n",
            "[W3] LSTM Epoch 30 | train_loss 0.2867 acc 0.883 f1 0.883 || val_loss 2.2237 acc 0.389 f1 0.308\n",
            "[W3] LSTM Epoch 31 | train_loss 0.2723 acc 0.890 f1 0.890 || val_loss 2.2953 acc 0.391 f1 0.313\n",
            "[W3] LSTM Epoch 32 | train_loss 0.2586 acc 0.897 f1 0.897 || val_loss 2.3582 acc 0.403 f1 0.322\n",
            "[W3] LSTM Epoch 33 | train_loss 0.2377 acc 0.901 f1 0.901 || val_loss 2.4945 acc 0.395 f1 0.319\n",
            "[W3] LSTM Epoch 34 | train_loss 0.2223 acc 0.911 f1 0.911 || val_loss 2.4774 acc 0.397 f1 0.313\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  64%|   | 64/100 [30:46<16:05, 26.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=65 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=65\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 933, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1225 acc 0.405 f1 0.405 || val_loss 1.1808 acc 0.296 f1 0.283\n",
            "[W3] ANN Epoch 02 | train_loss 0.9656 acc 0.519 f1 0.512 || val_loss 1.1226 acc 0.348 f1 0.317\n",
            "[W3] ANN Epoch 03 | train_loss 0.8746 acc 0.577 f1 0.570 || val_loss 1.1603 acc 0.323 f1 0.291\n",
            "[W3] ANN Epoch 04 | train_loss 0.8019 acc 0.605 f1 0.599 || val_loss 1.1340 acc 0.342 f1 0.307\n",
            "[W3] ANN Epoch 05 | train_loss 0.7239 acc 0.654 f1 0.650 || val_loss 1.1250 acc 0.377 f1 0.325\n",
            "[W3] ANN Epoch 06 | train_loss 0.6925 acc 0.649 f1 0.646 || val_loss 1.1731 acc 0.350 f1 0.319\n",
            "[W3] ANN Epoch 07 | train_loss 0.6513 acc 0.684 f1 0.680 || val_loss 1.1454 acc 0.368 f1 0.318\n",
            "[W3] ANN Epoch 08 | train_loss 0.6251 acc 0.702 f1 0.700 || val_loss 1.1814 acc 0.374 f1 0.317\n",
            "[W3] ANN Epoch 09 | train_loss 0.6197 acc 0.707 f1 0.706 || val_loss 1.1898 acc 0.377 f1 0.314\n",
            "[W3] ANN Epoch 10 | train_loss 0.5630 acc 0.740 f1 0.739 || val_loss 1.2222 acc 0.403 f1 0.326\n",
            "[W3] ANN Epoch 11 | train_loss 0.5412 acc 0.738 f1 0.737 || val_loss 1.2320 acc 0.397 f1 0.342\n",
            "[W3] ANN Epoch 12 | train_loss 0.5057 acc 0.761 f1 0.760 || val_loss 1.2533 acc 0.409 f1 0.351\n",
            "[W3] ANN Epoch 13 | train_loss 0.4960 acc 0.769 f1 0.768 || val_loss 1.2899 acc 0.397 f1 0.341\n",
            "[W3] ANN Epoch 14 | train_loss 0.4848 acc 0.779 f1 0.779 || val_loss 1.2851 acc 0.430 f1 0.360\n",
            "[W3] ANN Epoch 15 | train_loss 0.4625 acc 0.799 f1 0.798 || val_loss 1.3077 acc 0.412 f1 0.336\n",
            "[W3] ANN Epoch 16 | train_loss 0.4784 acc 0.788 f1 0.787 || val_loss 1.3407 acc 0.438 f1 0.376\n",
            "[W3] ANN Epoch 17 | train_loss 0.4575 acc 0.798 f1 0.798 || val_loss 1.3507 acc 0.436 f1 0.360\n",
            "[W3] ANN Epoch 18 | train_loss 0.4437 acc 0.812 f1 0.812 || val_loss 1.3440 acc 0.436 f1 0.341\n",
            "[W3] ANN Epoch 19 | train_loss 0.4299 acc 0.814 f1 0.814 || val_loss 1.3852 acc 0.405 f1 0.330\n",
            "[W3] ANN Epoch 20 | train_loss 0.4336 acc 0.812 f1 0.811 || val_loss 1.3475 acc 0.418 f1 0.325\n",
            "[W3] ANN Epoch 21 | train_loss 0.4266 acc 0.821 f1 0.821 || val_loss 1.3719 acc 0.426 f1 0.345\n",
            "[W3] ANN Epoch 22 | train_loss 0.3935 acc 0.831 f1 0.831 || val_loss 1.4266 acc 0.424 f1 0.346\n",
            "[W3] ANN Epoch 23 | train_loss 0.4098 acc 0.831 f1 0.830 || val_loss 1.3777 acc 0.438 f1 0.356\n",
            "[W3] ANN Epoch 24 | train_loss 0.3721 acc 0.837 f1 0.837 || val_loss 1.4509 acc 0.436 f1 0.350\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=65\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 933, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0673 acc 0.408 f1 0.408 || val_loss 1.0303 acc 0.403 f1 0.320\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9304 acc 0.547 f1 0.541 || val_loss 1.0499 acc 0.395 f1 0.314\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8101 acc 0.605 f1 0.602 || val_loss 1.1396 acc 0.346 f1 0.292\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7160 acc 0.656 f1 0.653 || val_loss 1.1986 acc 0.362 f1 0.312\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6376 acc 0.701 f1 0.699 || val_loss 1.2947 acc 0.362 f1 0.319\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5759 acc 0.730 f1 0.729 || val_loss 1.3423 acc 0.364 f1 0.305\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5265 acc 0.755 f1 0.754 || val_loss 1.4254 acc 0.342 f1 0.287\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4732 acc 0.779 f1 0.778 || val_loss 1.4693 acc 0.372 f1 0.293\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4268 acc 0.817 f1 0.816 || val_loss 1.5595 acc 0.387 f1 0.306\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=65\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 933, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0967 acc 0.369 f1 0.360 || val_loss 1.1017 acc 0.329 f1 0.319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0813 acc 0.420 f1 0.415 || val_loss 1.1091 acc 0.300 f1 0.294\n",
            "[W3] RNN Epoch 03 | train_loss 1.0669 acc 0.433 f1 0.427 || val_loss 1.1048 acc 0.317 f1 0.305\n",
            "[W3] RNN Epoch 04 | train_loss 1.0526 acc 0.454 f1 0.444 || val_loss 1.0806 acc 0.356 f1 0.323\n",
            "[W3] RNN Epoch 05 | train_loss 1.0332 acc 0.484 f1 0.478 || val_loss 1.0806 acc 0.352 f1 0.321\n",
            "[W3] RNN Epoch 06 | train_loss 1.0136 acc 0.497 f1 0.493 || val_loss 1.0887 acc 0.348 f1 0.323\n",
            "[W3] RNN Epoch 07 | train_loss 0.9920 acc 0.512 f1 0.506 || val_loss 1.1027 acc 0.352 f1 0.331\n",
            "[W3] RNN Epoch 08 | train_loss 0.9660 acc 0.526 f1 0.517 || val_loss 1.0867 acc 0.379 f1 0.345\n",
            "[W3] RNN Epoch 09 | train_loss 0.9388 acc 0.550 f1 0.542 || val_loss 1.1010 acc 0.372 f1 0.345\n",
            "[W3] RNN Epoch 10 | train_loss 0.9153 acc 0.567 f1 0.559 || val_loss 1.0867 acc 0.377 f1 0.342\n",
            "[W3] RNN Epoch 11 | train_loss 0.8889 acc 0.590 f1 0.581 || val_loss 1.0968 acc 0.399 f1 0.361\n",
            "[W3] RNN Epoch 12 | train_loss 0.8559 acc 0.598 f1 0.589 || val_loss 1.1306 acc 0.377 f1 0.347\n",
            "[W3] RNN Epoch 13 | train_loss 0.8374 acc 0.607 f1 0.599 || val_loss 1.1121 acc 0.399 f1 0.355\n",
            "[W3] RNN Epoch 14 | train_loss 0.8096 acc 0.627 f1 0.619 || val_loss 1.1328 acc 0.385 f1 0.348\n",
            "[W3] RNN Epoch 15 | train_loss 0.7938 acc 0.622 f1 0.613 || val_loss 1.1278 acc 0.420 f1 0.370\n",
            "[W3] RNN Epoch 16 | train_loss 0.7639 acc 0.648 f1 0.640 || val_loss 1.1513 acc 0.409 f1 0.359\n",
            "[W3] RNN Epoch 17 | train_loss 0.7435 acc 0.635 f1 0.627 || val_loss 1.1649 acc 0.397 f1 0.357\n",
            "[W3] RNN Epoch 18 | train_loss 0.7179 acc 0.659 f1 0.652 || val_loss 1.1524 acc 0.434 f1 0.373\n",
            "[W3] RNN Epoch 19 | train_loss 0.6999 acc 0.670 f1 0.665 || val_loss 1.1718 acc 0.434 f1 0.379\n",
            "[W3] RNN Epoch 20 | train_loss 0.6810 acc 0.674 f1 0.668 || val_loss 1.2029 acc 0.416 f1 0.363\n",
            "[W3] RNN Epoch 21 | train_loss 0.6465 acc 0.697 f1 0.691 || val_loss 1.2057 acc 0.424 f1 0.364\n",
            "[W3] RNN Epoch 22 | train_loss 0.6420 acc 0.691 f1 0.686 || val_loss 1.2344 acc 0.420 f1 0.362\n",
            "[W3] RNN Epoch 23 | train_loss 0.6279 acc 0.701 f1 0.695 || val_loss 1.2407 acc 0.422 f1 0.356\n",
            "[W3] RNN Epoch 24 | train_loss 0.6072 acc 0.699 f1 0.694 || val_loss 1.2601 acc 0.420 f1 0.351\n",
            "[W3] RNN Epoch 25 | train_loss 0.5862 acc 0.720 f1 0.715 || val_loss 1.2829 acc 0.428 f1 0.365\n",
            "[W3] RNN Epoch 26 | train_loss 0.5681 acc 0.726 f1 0.722 || val_loss 1.2781 acc 0.434 f1 0.366\n",
            "[W3] RNN Epoch 27 | train_loss 0.5618 acc 0.734 f1 0.730 || val_loss 1.3168 acc 0.434 f1 0.379\n",
            "[W3] RNN Epoch 28 | train_loss 0.5459 acc 0.738 f1 0.735 || val_loss 1.3387 acc 0.428 f1 0.365\n",
            "[W3] RNN Epoch 29 | train_loss 0.5280 acc 0.755 f1 0.751 || val_loss 1.3278 acc 0.449 f1 0.373\n",
            "[W3] RNN Epoch 30 | train_loss 0.5167 acc 0.755 f1 0.751 || val_loss 1.3883 acc 0.440 f1 0.371\n",
            "[W3] RNN Epoch 31 | train_loss 0.5090 acc 0.753 f1 0.750 || val_loss 1.3865 acc 0.432 f1 0.356\n",
            "[W3] RNN Epoch 32 | train_loss 0.4958 acc 0.763 f1 0.761 || val_loss 1.3988 acc 0.430 f1 0.369\n",
            "[W3] RNN Epoch 33 | train_loss 0.4842 acc 0.773 f1 0.771 || val_loss 1.4149 acc 0.416 f1 0.354\n",
            "[W3] RNN Epoch 34 | train_loss 0.4775 acc 0.777 f1 0.775 || val_loss 1.4353 acc 0.438 f1 0.365\n",
            "[W3] RNN Epoch 35 | train_loss 0.4542 acc 0.784 f1 0.782 || val_loss 1.4793 acc 0.430 f1 0.355\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=65\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 933, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1002 acc 0.342 f1 0.282 || val_loss 1.1062 acc 0.282 f1 0.235\n",
            "[W3] GRU Epoch 02 | train_loss 1.0919 acc 0.392 f1 0.376 || val_loss 1.0966 acc 0.307 f1 0.287\n",
            "[W3] GRU Epoch 03 | train_loss 1.0826 acc 0.424 f1 0.415 || val_loss 1.0843 acc 0.311 f1 0.291\n",
            "[W3] GRU Epoch 04 | train_loss 1.0682 acc 0.444 f1 0.434 || val_loss 1.0957 acc 0.288 f1 0.282\n",
            "[W3] GRU Epoch 05 | train_loss 1.0427 acc 0.478 f1 0.472 || val_loss 1.0901 acc 0.296 f1 0.283\n",
            "[W3] GRU Epoch 06 | train_loss 0.9962 acc 0.512 f1 0.503 || val_loss 1.0773 acc 0.335 f1 0.309\n",
            "[W3] GRU Epoch 07 | train_loss 0.9260 acc 0.560 f1 0.552 || val_loss 1.1018 acc 0.331 f1 0.297\n",
            "[W3] GRU Epoch 08 | train_loss 0.8618 acc 0.580 f1 0.572 || val_loss 1.0947 acc 0.374 f1 0.322\n",
            "[W3] GRU Epoch 09 | train_loss 0.8067 acc 0.616 f1 0.610 || val_loss 1.1299 acc 0.379 f1 0.323\n",
            "[W3] GRU Epoch 10 | train_loss 0.7614 acc 0.634 f1 0.629 || val_loss 1.1504 acc 0.385 f1 0.320\n",
            "[W3] GRU Epoch 11 | train_loss 0.7289 acc 0.651 f1 0.646 || val_loss 1.1726 acc 0.383 f1 0.314\n",
            "[W3] GRU Epoch 12 | train_loss 0.6961 acc 0.663 f1 0.659 || val_loss 1.1713 acc 0.395 f1 0.319\n",
            "[W3] GRU Epoch 13 | train_loss 0.6626 acc 0.670 f1 0.666 || val_loss 1.2350 acc 0.393 f1 0.323\n",
            "[W3] GRU Epoch 14 | train_loss 0.6309 acc 0.683 f1 0.679 || val_loss 1.2282 acc 0.391 f1 0.311\n",
            "[W3] GRU Epoch 15 | train_loss 0.5979 acc 0.716 f1 0.714 || val_loss 1.2479 acc 0.393 f1 0.308\n",
            "[W3] GRU Epoch 16 | train_loss 0.5699 acc 0.720 f1 0.717 || val_loss 1.3063 acc 0.395 f1 0.327\n",
            "[W3] GRU Epoch 17 | train_loss 0.5470 acc 0.739 f1 0.737 || val_loss 1.3314 acc 0.385 f1 0.311\n",
            "[W3] GRU Epoch 18 | train_loss 0.5268 acc 0.744 f1 0.742 || val_loss 1.3600 acc 0.393 f1 0.313\n",
            "[W3] GRU Epoch 19 | train_loss 0.4980 acc 0.760 f1 0.759 || val_loss 1.4033 acc 0.372 f1 0.304\n",
            "[W3] GRU Epoch 20 | train_loss 0.4805 acc 0.767 f1 0.766 || val_loss 1.4500 acc 0.381 f1 0.300\n",
            "[W3] GRU Epoch 21 | train_loss 0.4591 acc 0.780 f1 0.780 || val_loss 1.4889 acc 0.372 f1 0.292\n",
            "[W3] GRU Epoch 22 | train_loss 0.4351 acc 0.798 f1 0.797 || val_loss 1.5401 acc 0.358 f1 0.292\n",
            "[W3] GRU Epoch 23 | train_loss 0.4225 acc 0.799 f1 0.798 || val_loss 1.5843 acc 0.362 f1 0.301\n",
            "[W3] GRU Epoch 24 | train_loss 0.4031 acc 0.813 f1 0.813 || val_loss 1.6196 acc 0.385 f1 0.306\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=65\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 933, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1000 acc 0.331 f1 0.317 || val_loss 1.1017 acc 0.311 f1 0.299\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0952 acc 0.373 f1 0.370 || val_loss 1.0953 acc 0.364 f1 0.326\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0901 acc 0.411 f1 0.409 || val_loss 1.0928 acc 0.337 f1 0.317\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0806 acc 0.425 f1 0.422 || val_loss 1.0853 acc 0.342 f1 0.325\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0581 acc 0.453 f1 0.446 || val_loss 1.0805 acc 0.325 f1 0.305\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0044 acc 0.508 f1 0.498 || val_loss 1.0714 acc 0.350 f1 0.304\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9014 acc 0.566 f1 0.559 || val_loss 1.1134 acc 0.368 f1 0.318\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8252 acc 0.605 f1 0.600 || val_loss 1.1083 acc 0.366 f1 0.290\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7736 acc 0.620 f1 0.614 || val_loss 1.1248 acc 0.385 f1 0.326\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7335 acc 0.639 f1 0.634 || val_loss 1.1655 acc 0.381 f1 0.318\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  65%|   | 65/100 [31:13<15:42, 26.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=66 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=66\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 935, np.int64(0): 269})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1165 acc 0.418 f1 0.412 || val_loss 1.1819 acc 0.288 f1 0.286\n",
            "[W3] ANN Epoch 02 | train_loss 0.9770 acc 0.507 f1 0.495 || val_loss 1.1438 acc 0.325 f1 0.311\n",
            "[W3] ANN Epoch 03 | train_loss 0.8900 acc 0.558 f1 0.549 || val_loss 1.1139 acc 0.381 f1 0.341\n",
            "[W3] ANN Epoch 04 | train_loss 0.8124 acc 0.594 f1 0.587 || val_loss 1.1087 acc 0.385 f1 0.340\n",
            "[W3] ANN Epoch 05 | train_loss 0.7571 acc 0.625 f1 0.620 || val_loss 1.1062 acc 0.401 f1 0.353\n",
            "[W3] ANN Epoch 06 | train_loss 0.6934 acc 0.673 f1 0.669 || val_loss 1.1099 acc 0.403 f1 0.346\n",
            "[W3] ANN Epoch 07 | train_loss 0.6977 acc 0.660 f1 0.658 || val_loss 1.1379 acc 0.401 f1 0.345\n",
            "[W3] ANN Epoch 08 | train_loss 0.6904 acc 0.674 f1 0.671 || val_loss 1.1115 acc 0.436 f1 0.364\n",
            "[W3] ANN Epoch 09 | train_loss 0.6560 acc 0.689 f1 0.687 || val_loss 1.1425 acc 0.407 f1 0.348\n",
            "[W3] ANN Epoch 10 | train_loss 0.6085 acc 0.705 f1 0.702 || val_loss 1.1546 acc 0.405 f1 0.351\n",
            "[W3] ANN Epoch 11 | train_loss 0.6123 acc 0.706 f1 0.705 || val_loss 1.1400 acc 0.403 f1 0.342\n",
            "[W3] ANN Epoch 12 | train_loss 0.6043 acc 0.713 f1 0.712 || val_loss 1.1406 acc 0.405 f1 0.350\n",
            "[W3] ANN Epoch 13 | train_loss 0.5935 acc 0.712 f1 0.711 || val_loss 1.1640 acc 0.412 f1 0.353\n",
            "[W3] ANN Epoch 14 | train_loss 0.5570 acc 0.738 f1 0.736 || val_loss 1.1903 acc 0.409 f1 0.348\n",
            "[W3] ANN Epoch 15 | train_loss 0.5409 acc 0.744 f1 0.744 || val_loss 1.1744 acc 0.444 f1 0.356\n",
            "[W3] ANN Epoch 16 | train_loss 0.5314 acc 0.747 f1 0.745 || val_loss 1.1989 acc 0.422 f1 0.329\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=66\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 935, np.int64(0): 269})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0816 acc 0.398 f1 0.400 || val_loss 1.0408 acc 0.393 f1 0.296\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9498 acc 0.547 f1 0.541 || val_loss 1.0438 acc 0.412 f1 0.300\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8180 acc 0.612 f1 0.606 || val_loss 1.1324 acc 0.399 f1 0.335\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7277 acc 0.655 f1 0.654 || val_loss 1.1619 acc 0.395 f1 0.348\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6559 acc 0.697 f1 0.695 || val_loss 1.1949 acc 0.370 f1 0.323\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5947 acc 0.732 f1 0.730 || val_loss 1.2174 acc 0.377 f1 0.304\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5346 acc 0.767 f1 0.766 || val_loss 1.3224 acc 0.368 f1 0.317\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4912 acc 0.773 f1 0.772 || val_loss 1.3124 acc 0.381 f1 0.329\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4521 acc 0.800 f1 0.799 || val_loss 1.3585 acc 0.391 f1 0.339\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4108 acc 0.821 f1 0.820 || val_loss 1.4066 acc 0.389 f1 0.316\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.4074 acc 0.823 f1 0.822 || val_loss 1.4476 acc 0.399 f1 0.319\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3543 acc 0.853 f1 0.853 || val_loss 1.5615 acc 0.389 f1 0.315\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=66\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 935, np.int64(0): 269})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0996 acc 0.342 f1 0.265 || val_loss 1.0945 acc 0.360 f1 0.312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0819 acc 0.399 f1 0.380 || val_loss 1.0872 acc 0.366 f1 0.328\n",
            "[W3] RNN Epoch 03 | train_loss 1.0647 acc 0.462 f1 0.458 || val_loss 1.0724 acc 0.370 f1 0.336\n",
            "[W3] RNN Epoch 04 | train_loss 1.0363 acc 0.471 f1 0.466 || val_loss 1.0689 acc 0.356 f1 0.326\n",
            "[W3] RNN Epoch 05 | train_loss 1.0112 acc 0.491 f1 0.486 || val_loss 1.0747 acc 0.362 f1 0.341\n",
            "[W3] RNN Epoch 06 | train_loss 0.9833 acc 0.515 f1 0.507 || val_loss 1.0909 acc 0.346 f1 0.334\n",
            "[W3] RNN Epoch 07 | train_loss 0.9566 acc 0.527 f1 0.515 || val_loss 1.0759 acc 0.354 f1 0.332\n",
            "[W3] RNN Epoch 08 | train_loss 0.9350 acc 0.548 f1 0.537 || val_loss 1.0718 acc 0.372 f1 0.348\n",
            "[W3] RNN Epoch 09 | train_loss 0.9167 acc 0.557 f1 0.548 || val_loss 1.0859 acc 0.360 f1 0.338\n",
            "[W3] RNN Epoch 10 | train_loss 0.8892 acc 0.574 f1 0.565 || val_loss 1.0759 acc 0.377 f1 0.336\n",
            "[W3] RNN Epoch 11 | train_loss 0.8736 acc 0.577 f1 0.568 || val_loss 1.0828 acc 0.366 f1 0.322\n",
            "[W3] RNN Epoch 12 | train_loss 0.8503 acc 0.596 f1 0.587 || val_loss 1.1160 acc 0.356 f1 0.332\n",
            "[W3] RNN Epoch 13 | train_loss 0.8232 acc 0.619 f1 0.610 || val_loss 1.1108 acc 0.374 f1 0.341\n",
            "[W3] RNN Epoch 14 | train_loss 0.8073 acc 0.616 f1 0.608 || val_loss 1.1175 acc 0.366 f1 0.332\n",
            "[W3] RNN Epoch 15 | train_loss 0.7911 acc 0.626 f1 0.618 || val_loss 1.1306 acc 0.358 f1 0.312\n",
            "[W3] RNN Epoch 16 | train_loss 0.7757 acc 0.622 f1 0.613 || val_loss 1.1377 acc 0.385 f1 0.337\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=66\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 935, np.int64(0): 269})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0994 acc 0.329 f1 0.184 || val_loss 1.0854 acc 0.416 f1 0.258\n",
            "[W3] GRU Epoch 02 | train_loss 1.0894 acc 0.381 f1 0.363 || val_loss 1.0823 acc 0.358 f1 0.323\n",
            "[W3] GRU Epoch 03 | train_loss 1.0788 acc 0.420 f1 0.420 || val_loss 1.0864 acc 0.292 f1 0.276\n",
            "[W3] GRU Epoch 04 | train_loss 1.0654 acc 0.431 f1 0.423 || val_loss 1.0981 acc 0.276 f1 0.269\n",
            "[W3] GRU Epoch 05 | train_loss 1.0383 acc 0.479 f1 0.465 || val_loss 1.0604 acc 0.348 f1 0.317\n",
            "[W3] GRU Epoch 06 | train_loss 0.9979 acc 0.517 f1 0.507 || val_loss 1.0751 acc 0.346 f1 0.315\n",
            "[W3] GRU Epoch 07 | train_loss 0.9265 acc 0.568 f1 0.559 || val_loss 1.0860 acc 0.356 f1 0.318\n",
            "[W3] GRU Epoch 08 | train_loss 0.8653 acc 0.574 f1 0.566 || val_loss 1.0993 acc 0.340 f1 0.293\n",
            "[W3] GRU Epoch 09 | train_loss 0.8054 acc 0.611 f1 0.604 || val_loss 1.1039 acc 0.374 f1 0.320\n",
            "[W3] GRU Epoch 10 | train_loss 0.7761 acc 0.631 f1 0.625 || val_loss 1.1311 acc 0.350 f1 0.298\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=66\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 935, np.int64(0): 269})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0990 acc 0.346 f1 0.259 || val_loss 1.0950 acc 0.364 f1 0.295\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0938 acc 0.403 f1 0.389 || val_loss 1.0877 acc 0.368 f1 0.310\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0864 acc 0.412 f1 0.410 || val_loss 1.0783 acc 0.350 f1 0.303\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0706 acc 0.433 f1 0.429 || val_loss 1.0603 acc 0.356 f1 0.318\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0374 acc 0.479 f1 0.471 || val_loss 1.0638 acc 0.366 f1 0.328\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9634 acc 0.532 f1 0.523 || val_loss 1.0623 acc 0.372 f1 0.309\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8854 acc 0.560 f1 0.550 || val_loss 1.0975 acc 0.356 f1 0.299\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8153 acc 0.592 f1 0.585 || val_loss 1.1454 acc 0.381 f1 0.298\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7919 acc 0.608 f1 0.602 || val_loss 1.1645 acc 0.352 f1 0.299\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7467 acc 0.641 f1 0.635 || val_loss 1.1803 acc 0.403 f1 0.316\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7430 acc 0.633 f1 0.627 || val_loss 1.2153 acc 0.358 f1 0.294\n",
            "[W3] LSTM Epoch 12 | train_loss 0.7149 acc 0.644 f1 0.638 || val_loss 1.2074 acc 0.377 f1 0.309\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6721 acc 0.669 f1 0.664 || val_loss 1.2512 acc 0.368 f1 0.307\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  66%|   | 66/100 [31:33<14:10, 25.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=67 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=67\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 928, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1152 acc 0.431 f1 0.427 || val_loss 1.1640 acc 0.284 f1 0.271\n",
            "[W3] ANN Epoch 02 | train_loss 0.9638 acc 0.523 f1 0.513 || val_loss 1.1319 acc 0.317 f1 0.283\n",
            "[W3] ANN Epoch 03 | train_loss 0.8745 acc 0.564 f1 0.556 || val_loss 1.1093 acc 0.366 f1 0.312\n",
            "[W3] ANN Epoch 04 | train_loss 0.7856 acc 0.622 f1 0.616 || val_loss 1.1044 acc 0.379 f1 0.328\n",
            "[W3] ANN Epoch 05 | train_loss 0.7227 acc 0.658 f1 0.653 || val_loss 1.1249 acc 0.387 f1 0.323\n",
            "[W3] ANN Epoch 06 | train_loss 0.6856 acc 0.669 f1 0.667 || val_loss 1.1232 acc 0.407 f1 0.341\n",
            "[W3] ANN Epoch 07 | train_loss 0.6476 acc 0.692 f1 0.690 || val_loss 1.1393 acc 0.407 f1 0.320\n",
            "[W3] ANN Epoch 08 | train_loss 0.6288 acc 0.700 f1 0.697 || val_loss 1.1712 acc 0.399 f1 0.326\n",
            "[W3] ANN Epoch 09 | train_loss 0.5828 acc 0.727 f1 0.726 || val_loss 1.1750 acc 0.422 f1 0.354\n",
            "[W3] ANN Epoch 10 | train_loss 0.5354 acc 0.753 f1 0.751 || val_loss 1.1852 acc 0.438 f1 0.362\n",
            "[W3] ANN Epoch 11 | train_loss 0.5487 acc 0.750 f1 0.750 || val_loss 1.2512 acc 0.405 f1 0.327\n",
            "[W3] ANN Epoch 12 | train_loss 0.5190 acc 0.764 f1 0.763 || val_loss 1.2387 acc 0.418 f1 0.342\n",
            "[W3] ANN Epoch 13 | train_loss 0.5110 acc 0.770 f1 0.769 || val_loss 1.2543 acc 0.420 f1 0.337\n",
            "[W3] ANN Epoch 14 | train_loss 0.4619 acc 0.797 f1 0.796 || val_loss 1.2944 acc 0.430 f1 0.346\n",
            "[W3] ANN Epoch 15 | train_loss 0.4498 acc 0.803 f1 0.803 || val_loss 1.3021 acc 0.436 f1 0.361\n",
            "[W3] ANN Epoch 16 | train_loss 0.4524 acc 0.798 f1 0.798 || val_loss 1.3419 acc 0.449 f1 0.365\n",
            "[W3] ANN Epoch 17 | train_loss 0.4526 acc 0.806 f1 0.805 || val_loss 1.3548 acc 0.440 f1 0.355\n",
            "[W3] ANN Epoch 18 | train_loss 0.4430 acc 0.805 f1 0.805 || val_loss 1.3628 acc 0.461 f1 0.372\n",
            "[W3] ANN Epoch 19 | train_loss 0.4312 acc 0.813 f1 0.812 || val_loss 1.3765 acc 0.434 f1 0.347\n",
            "[W3] ANN Epoch 20 | train_loss 0.4025 acc 0.831 f1 0.831 || val_loss 1.3849 acc 0.438 f1 0.350\n",
            "[W3] ANN Epoch 21 | train_loss 0.3807 acc 0.837 f1 0.836 || val_loss 1.4258 acc 0.444 f1 0.361\n",
            "[W3] ANN Epoch 22 | train_loss 0.3487 acc 0.859 f1 0.858 || val_loss 1.4571 acc 0.424 f1 0.344\n",
            "[W3] ANN Epoch 23 | train_loss 0.3668 acc 0.846 f1 0.846 || val_loss 1.4933 acc 0.438 f1 0.356\n",
            "[W3] ANN Epoch 24 | train_loss 0.3487 acc 0.853 f1 0.853 || val_loss 1.5175 acc 0.432 f1 0.345\n",
            "[W3] ANN Epoch 25 | train_loss 0.3396 acc 0.854 f1 0.854 || val_loss 1.5397 acc 0.414 f1 0.331\n",
            "[W3] ANN Epoch 26 | train_loss 0.3355 acc 0.861 f1 0.861 || val_loss 1.4974 acc 0.451 f1 0.376\n",
            "[W3] ANN Epoch 27 | train_loss 0.3309 acc 0.864 f1 0.864 || val_loss 1.5422 acc 0.453 f1 0.370\n",
            "[W3] ANN Epoch 28 | train_loss 0.3252 acc 0.860 f1 0.860 || val_loss 1.5481 acc 0.440 f1 0.357\n",
            "[W3] ANN Epoch 29 | train_loss 0.3108 acc 0.876 f1 0.876 || val_loss 1.6092 acc 0.451 f1 0.364\n",
            "[W3] ANN Epoch 30 | train_loss 0.3064 acc 0.877 f1 0.877 || val_loss 1.5697 acc 0.449 f1 0.363\n",
            "[W3] ANN Epoch 31 | train_loss 0.3019 acc 0.878 f1 0.878 || val_loss 1.6067 acc 0.432 f1 0.357\n",
            "[W3] ANN Epoch 32 | train_loss 0.2833 acc 0.888 f1 0.888 || val_loss 1.6304 acc 0.473 f1 0.399\n",
            "[W3] ANN Epoch 33 | train_loss 0.2862 acc 0.882 f1 0.882 || val_loss 1.6570 acc 0.453 f1 0.362\n",
            "[W3] ANN Epoch 34 | train_loss 0.2644 acc 0.890 f1 0.890 || val_loss 1.6553 acc 0.422 f1 0.338\n",
            "[W3] ANN Epoch 35 | train_loss 0.2702 acc 0.892 f1 0.892 || val_loss 1.6735 acc 0.414 f1 0.341\n",
            "[W3] ANN Epoch 36 | train_loss 0.2652 acc 0.893 f1 0.893 || val_loss 1.6700 acc 0.461 f1 0.373\n",
            "[W3] ANN Epoch 37 | train_loss 0.2676 acc 0.895 f1 0.895 || val_loss 1.6873 acc 0.432 f1 0.352\n",
            "[W3] ANN Epoch 38 | train_loss 0.2671 acc 0.891 f1 0.891 || val_loss 1.7451 acc 0.428 f1 0.336\n",
            "[W3] ANN Epoch 39 | train_loss 0.2705 acc 0.890 f1 0.890 || val_loss 1.6976 acc 0.447 f1 0.357\n",
            "[W3] ANN Epoch 40 | train_loss 0.2236 acc 0.910 f1 0.910 || val_loss 1.7049 acc 0.432 f1 0.357\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=67\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 928, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0849 acc 0.397 f1 0.397 || val_loss 1.0337 acc 0.453 f1 0.357\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9726 acc 0.514 f1 0.507 || val_loss 1.0279 acc 0.422 f1 0.344\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8452 acc 0.598 f1 0.594 || val_loss 1.0643 acc 0.395 f1 0.321\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7397 acc 0.642 f1 0.638 || val_loss 1.1098 acc 0.412 f1 0.351\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6646 acc 0.670 f1 0.667 || val_loss 1.1649 acc 0.385 f1 0.332\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5952 acc 0.718 f1 0.716 || val_loss 1.2056 acc 0.397 f1 0.336\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5295 acc 0.755 f1 0.755 || val_loss 1.2124 acc 0.391 f1 0.314\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4885 acc 0.781 f1 0.781 || val_loss 1.2687 acc 0.422 f1 0.364\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4236 acc 0.819 f1 0.818 || val_loss 1.3326 acc 0.399 f1 0.336\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3825 acc 0.836 f1 0.836 || val_loss 1.4395 acc 0.401 f1 0.333\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3645 acc 0.843 f1 0.843 || val_loss 1.4587 acc 0.430 f1 0.352\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3079 acc 0.883 f1 0.883 || val_loss 1.5564 acc 0.418 f1 0.330\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2802 acc 0.893 f1 0.892 || val_loss 1.6765 acc 0.428 f1 0.338\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2485 acc 0.907 f1 0.907 || val_loss 1.7644 acc 0.432 f1 0.338\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2169 acc 0.918 f1 0.918 || val_loss 1.7732 acc 0.434 f1 0.339\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1932 acc 0.930 f1 0.930 || val_loss 1.9054 acc 0.432 f1 0.347\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=67\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 928, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1008 acc 0.338 f1 0.300 || val_loss 1.0928 acc 0.335 f1 0.290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0848 acc 0.415 f1 0.413 || val_loss 1.1064 acc 0.305 f1 0.293\n",
            "[W3] RNN Epoch 03 | train_loss 1.0722 acc 0.421 f1 0.415 || val_loss 1.1071 acc 0.290 f1 0.275\n",
            "[W3] RNN Epoch 04 | train_loss 1.0589 acc 0.442 f1 0.435 || val_loss 1.0891 acc 0.340 f1 0.311\n",
            "[W3] RNN Epoch 05 | train_loss 1.0462 acc 0.462 f1 0.459 || val_loss 1.1068 acc 0.307 f1 0.285\n",
            "[W3] RNN Epoch 06 | train_loss 1.0305 acc 0.476 f1 0.471 || val_loss 1.1013 acc 0.327 f1 0.297\n",
            "[W3] RNN Epoch 07 | train_loss 1.0176 acc 0.476 f1 0.471 || val_loss 1.1072 acc 0.305 f1 0.283\n",
            "[W3] RNN Epoch 08 | train_loss 0.9972 acc 0.502 f1 0.498 || val_loss 1.1276 acc 0.311 f1 0.292\n",
            "[W3] RNN Epoch 09 | train_loss 0.9782 acc 0.523 f1 0.516 || val_loss 1.1010 acc 0.313 f1 0.280\n",
            "[W3] RNN Epoch 10 | train_loss 0.9587 acc 0.540 f1 0.533 || val_loss 1.1379 acc 0.284 f1 0.268\n",
            "[W3] RNN Epoch 11 | train_loss 0.9341 acc 0.555 f1 0.548 || val_loss 1.0908 acc 0.340 f1 0.296\n",
            "[W3] RNN Epoch 12 | train_loss 0.9147 acc 0.568 f1 0.562 || val_loss 1.1020 acc 0.346 f1 0.305\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=67\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 928, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0984 acc 0.343 f1 0.324 || val_loss 1.0973 acc 0.335 f1 0.306\n",
            "[W3] GRU Epoch 02 | train_loss 1.0889 acc 0.418 f1 0.404 || val_loss 1.0936 acc 0.344 f1 0.323\n",
            "[W3] GRU Epoch 03 | train_loss 1.0796 acc 0.426 f1 0.423 || val_loss 1.0980 acc 0.317 f1 0.306\n",
            "[W3] GRU Epoch 04 | train_loss 1.0638 acc 0.454 f1 0.447 || val_loss 1.0792 acc 0.352 f1 0.324\n",
            "[W3] GRU Epoch 05 | train_loss 1.0419 acc 0.466 f1 0.462 || val_loss 1.0990 acc 0.313 f1 0.303\n",
            "[W3] GRU Epoch 06 | train_loss 0.9986 acc 0.513 f1 0.506 || val_loss 1.0862 acc 0.342 f1 0.317\n",
            "[W3] GRU Epoch 07 | train_loss 0.9339 acc 0.554 f1 0.545 || val_loss 1.0908 acc 0.368 f1 0.341\n",
            "[W3] GRU Epoch 08 | train_loss 0.8598 acc 0.586 f1 0.580 || val_loss 1.1071 acc 0.358 f1 0.315\n",
            "[W3] GRU Epoch 09 | train_loss 0.8037 acc 0.621 f1 0.613 || val_loss 1.1088 acc 0.389 f1 0.334\n",
            "[W3] GRU Epoch 10 | train_loss 0.7693 acc 0.626 f1 0.622 || val_loss 1.1370 acc 0.397 f1 0.337\n",
            "[W3] GRU Epoch 11 | train_loss 0.7294 acc 0.644 f1 0.640 || val_loss 1.1423 acc 0.389 f1 0.331\n",
            "[W3] GRU Epoch 12 | train_loss 0.6944 acc 0.669 f1 0.665 || val_loss 1.1716 acc 0.397 f1 0.334\n",
            "[W3] GRU Epoch 13 | train_loss 0.6639 acc 0.684 f1 0.680 || val_loss 1.1975 acc 0.387 f1 0.326\n",
            "[W3] GRU Epoch 14 | train_loss 0.6342 acc 0.695 f1 0.692 || val_loss 1.2239 acc 0.401 f1 0.351\n",
            "[W3] GRU Epoch 15 | train_loss 0.6022 acc 0.703 f1 0.701 || val_loss 1.2672 acc 0.397 f1 0.347\n",
            "[W3] GRU Epoch 16 | train_loss 0.5770 acc 0.725 f1 0.723 || val_loss 1.2544 acc 0.409 f1 0.343\n",
            "[W3] GRU Epoch 17 | train_loss 0.5533 acc 0.732 f1 0.731 || val_loss 1.3331 acc 0.403 f1 0.341\n",
            "[W3] GRU Epoch 18 | train_loss 0.5340 acc 0.741 f1 0.739 || val_loss 1.3202 acc 0.407 f1 0.342\n",
            "[W3] GRU Epoch 19 | train_loss 0.5112 acc 0.760 f1 0.759 || val_loss 1.3707 acc 0.414 f1 0.346\n",
            "[W3] GRU Epoch 20 | train_loss 0.4945 acc 0.754 f1 0.752 || val_loss 1.4007 acc 0.401 f1 0.322\n",
            "[W3] GRU Epoch 21 | train_loss 0.4657 acc 0.774 f1 0.773 || val_loss 1.4459 acc 0.407 f1 0.346\n",
            "[W3] GRU Epoch 22 | train_loss 0.4457 acc 0.781 f1 0.780 || val_loss 1.4859 acc 0.401 f1 0.341\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=67\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 928, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0985 acc 0.358 f1 0.275 || val_loss 1.1055 acc 0.237 f1 0.201\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0937 acc 0.386 f1 0.367 || val_loss 1.0990 acc 0.292 f1 0.271\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0859 acc 0.397 f1 0.381 || val_loss 1.0959 acc 0.294 f1 0.284\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0714 acc 0.426 f1 0.419 || val_loss 1.0855 acc 0.307 f1 0.290\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0451 acc 0.461 f1 0.456 || val_loss 1.0888 acc 0.337 f1 0.311\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9825 acc 0.523 f1 0.511 || val_loss 1.1027 acc 0.346 f1 0.316\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8947 acc 0.557 f1 0.548 || val_loss 1.1450 acc 0.346 f1 0.304\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8307 acc 0.584 f1 0.577 || val_loss 1.1349 acc 0.383 f1 0.338\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7800 acc 0.615 f1 0.610 || val_loss 1.1486 acc 0.348 f1 0.281\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7384 acc 0.640 f1 0.636 || val_loss 1.1955 acc 0.377 f1 0.318\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7114 acc 0.661 f1 0.656 || val_loss 1.2214 acc 0.356 f1 0.297\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6759 acc 0.667 f1 0.662 || val_loss 1.2528 acc 0.366 f1 0.304\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6437 acc 0.699 f1 0.696 || val_loss 1.2750 acc 0.360 f1 0.300\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6160 acc 0.701 f1 0.697 || val_loss 1.3219 acc 0.374 f1 0.302\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5960 acc 0.713 f1 0.711 || val_loss 1.3626 acc 0.374 f1 0.304\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5727 acc 0.720 f1 0.717 || val_loss 1.4020 acc 0.372 f1 0.301\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  67%|   | 67/100 [32:04<14:41, 26.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=68 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=68\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 989, np.int64(1): 924, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 989, np.int64(2): 989, np.int64(0): 989})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1368 acc 0.397 f1 0.395 || val_loss 1.1258 acc 0.348 f1 0.317\n",
            "[W3] ANN Epoch 02 | train_loss 0.9564 acc 0.531 f1 0.526 || val_loss 1.1182 acc 0.379 f1 0.326\n",
            "[W3] ANN Epoch 03 | train_loss 0.8493 acc 0.585 f1 0.581 || val_loss 1.1133 acc 0.372 f1 0.313\n",
            "[W3] ANN Epoch 04 | train_loss 0.7759 acc 0.625 f1 0.621 || val_loss 1.1221 acc 0.393 f1 0.331\n",
            "[W3] ANN Epoch 05 | train_loss 0.7313 acc 0.638 f1 0.636 || val_loss 1.0958 acc 0.395 f1 0.328\n",
            "[W3] ANN Epoch 06 | train_loss 0.6650 acc 0.690 f1 0.688 || val_loss 1.1509 acc 0.377 f1 0.313\n",
            "[W3] ANN Epoch 07 | train_loss 0.6207 acc 0.711 f1 0.710 || val_loss 1.1838 acc 0.407 f1 0.338\n",
            "[W3] ANN Epoch 08 | train_loss 0.6101 acc 0.717 f1 0.716 || val_loss 1.1643 acc 0.397 f1 0.334\n",
            "[W3] ANN Epoch 09 | train_loss 0.5574 acc 0.752 f1 0.752 || val_loss 1.1764 acc 0.451 f1 0.386\n",
            "[W3] ANN Epoch 10 | train_loss 0.5435 acc 0.751 f1 0.751 || val_loss 1.2675 acc 0.383 f1 0.319\n",
            "[W3] ANN Epoch 11 | train_loss 0.5219 acc 0.764 f1 0.763 || val_loss 1.2778 acc 0.403 f1 0.320\n",
            "[W3] ANN Epoch 12 | train_loss 0.5053 acc 0.783 f1 0.783 || val_loss 1.2768 acc 0.426 f1 0.342\n",
            "[W3] ANN Epoch 13 | train_loss 0.4690 acc 0.792 f1 0.792 || val_loss 1.3330 acc 0.414 f1 0.343\n",
            "[W3] ANN Epoch 14 | train_loss 0.4732 acc 0.792 f1 0.792 || val_loss 1.3043 acc 0.436 f1 0.365\n",
            "[W3] ANN Epoch 15 | train_loss 0.4417 acc 0.805 f1 0.805 || val_loss 1.3859 acc 0.424 f1 0.350\n",
            "[W3] ANN Epoch 16 | train_loss 0.4120 acc 0.818 f1 0.818 || val_loss 1.3580 acc 0.451 f1 0.376\n",
            "[W3] ANN Epoch 17 | train_loss 0.3941 acc 0.828 f1 0.828 || val_loss 1.3981 acc 0.440 f1 0.368\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=68\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 989, np.int64(1): 924, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 989, np.int64(2): 989, np.int64(0): 989})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0793 acc 0.388 f1 0.389 || val_loss 1.0490 acc 0.412 f1 0.338\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9351 acc 0.563 f1 0.558 || val_loss 1.0558 acc 0.395 f1 0.336\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.7982 acc 0.618 f1 0.614 || val_loss 1.1208 acc 0.401 f1 0.341\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7056 acc 0.667 f1 0.663 || val_loss 1.1555 acc 0.399 f1 0.330\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6219 acc 0.703 f1 0.701 || val_loss 1.2145 acc 0.405 f1 0.344\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5572 acc 0.748 f1 0.746 || val_loss 1.2989 acc 0.395 f1 0.336\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5079 acc 0.777 f1 0.776 || val_loss 1.3495 acc 0.401 f1 0.333\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4512 acc 0.806 f1 0.806 || val_loss 1.4137 acc 0.399 f1 0.335\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.3998 acc 0.834 f1 0.834 || val_loss 1.5330 acc 0.403 f1 0.340\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3574 acc 0.853 f1 0.853 || val_loss 1.5942 acc 0.377 f1 0.305\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3152 acc 0.868 f1 0.868 || val_loss 1.6911 acc 0.416 f1 0.337\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2772 acc 0.905 f1 0.904 || val_loss 1.8040 acc 0.379 f1 0.307\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2449 acc 0.908 f1 0.908 || val_loss 1.8660 acc 0.418 f1 0.330\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=68\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 989, np.int64(1): 924, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 989, np.int64(2): 989, np.int64(0): 989})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0980 acc 0.356 f1 0.304 || val_loss 1.0973 acc 0.340 f1 0.317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0821 acc 0.420 f1 0.416 || val_loss 1.0974 acc 0.337 f1 0.320\n",
            "[W3] RNN Epoch 03 | train_loss 1.0663 acc 0.437 f1 0.434 || val_loss 1.1058 acc 0.335 f1 0.318\n",
            "[W3] RNN Epoch 04 | train_loss 1.0505 acc 0.458 f1 0.454 || val_loss 1.1110 acc 0.331 f1 0.313\n",
            "[W3] RNN Epoch 05 | train_loss 1.0338 acc 0.479 f1 0.474 || val_loss 1.0940 acc 0.360 f1 0.332\n",
            "[W3] RNN Epoch 06 | train_loss 1.0131 acc 0.491 f1 0.486 || val_loss 1.1127 acc 0.344 f1 0.327\n",
            "[W3] RNN Epoch 07 | train_loss 0.9894 acc 0.516 f1 0.507 || val_loss 1.0714 acc 0.370 f1 0.323\n",
            "[W3] RNN Epoch 08 | train_loss 0.9647 acc 0.534 f1 0.527 || val_loss 1.0910 acc 0.350 f1 0.319\n",
            "[W3] RNN Epoch 09 | train_loss 0.9354 acc 0.559 f1 0.552 || val_loss 1.0821 acc 0.372 f1 0.337\n",
            "[W3] RNN Epoch 10 | train_loss 0.9056 acc 0.571 f1 0.561 || val_loss 1.1167 acc 0.379 f1 0.356\n",
            "[W3] RNN Epoch 11 | train_loss 0.8883 acc 0.572 f1 0.562 || val_loss 1.0947 acc 0.374 f1 0.338\n",
            "[W3] RNN Epoch 12 | train_loss 0.8528 acc 0.597 f1 0.588 || val_loss 1.1274 acc 0.354 f1 0.327\n",
            "[W3] RNN Epoch 13 | train_loss 0.8302 acc 0.605 f1 0.595 || val_loss 1.1193 acc 0.370 f1 0.346\n",
            "[W3] RNN Epoch 14 | train_loss 0.8073 acc 0.625 f1 0.616 || val_loss 1.1011 acc 0.389 f1 0.349\n",
            "[W3] RNN Epoch 15 | train_loss 0.7765 acc 0.642 f1 0.633 || val_loss 1.1059 acc 0.381 f1 0.341\n",
            "[W3] RNN Epoch 16 | train_loss 0.7526 acc 0.647 f1 0.639 || val_loss 1.1120 acc 0.389 f1 0.346\n",
            "[W3] RNN Epoch 17 | train_loss 0.7271 acc 0.667 f1 0.661 || val_loss 1.1127 acc 0.405 f1 0.361\n",
            "[W3] RNN Epoch 18 | train_loss 0.7067 acc 0.658 f1 0.651 || val_loss 1.1238 acc 0.399 f1 0.360\n",
            "[W3] RNN Epoch 19 | train_loss 0.6756 acc 0.692 f1 0.685 || val_loss 1.1068 acc 0.416 f1 0.359\n",
            "[W3] RNN Epoch 20 | train_loss 0.6617 acc 0.687 f1 0.681 || val_loss 1.1400 acc 0.409 f1 0.361\n",
            "[W3] RNN Epoch 21 | train_loss 0.6426 acc 0.689 f1 0.683 || val_loss 1.1519 acc 0.405 f1 0.344\n",
            "[W3] RNN Epoch 22 | train_loss 0.6194 acc 0.705 f1 0.701 || val_loss 1.1748 acc 0.430 f1 0.374\n",
            "[W3] RNN Epoch 23 | train_loss 0.6046 acc 0.717 f1 0.712 || val_loss 1.1913 acc 0.428 f1 0.369\n",
            "[W3] RNN Epoch 24 | train_loss 0.5852 acc 0.718 f1 0.714 || val_loss 1.2133 acc 0.416 f1 0.368\n",
            "[W3] RNN Epoch 25 | train_loss 0.5701 acc 0.725 f1 0.720 || val_loss 1.2169 acc 0.440 f1 0.377\n",
            "[W3] RNN Epoch 26 | train_loss 0.5454 acc 0.745 f1 0.741 || val_loss 1.2225 acc 0.451 f1 0.391\n",
            "[W3] RNN Epoch 27 | train_loss 0.5349 acc 0.744 f1 0.741 || val_loss 1.2622 acc 0.436 f1 0.381\n",
            "[W3] RNN Epoch 28 | train_loss 0.5251 acc 0.753 f1 0.749 || val_loss 1.2722 acc 0.447 f1 0.367\n",
            "[W3] RNN Epoch 29 | train_loss 0.4999 acc 0.770 f1 0.767 || val_loss 1.2868 acc 0.449 f1 0.382\n",
            "[W3] RNN Epoch 30 | train_loss 0.4940 acc 0.771 f1 0.768 || val_loss 1.3036 acc 0.420 f1 0.358\n",
            "[W3] RNN Epoch 31 | train_loss 0.4783 acc 0.780 f1 0.778 || val_loss 1.3366 acc 0.440 f1 0.378\n",
            "[W3] RNN Epoch 32 | train_loss 0.4749 acc 0.768 f1 0.765 || val_loss 1.3380 acc 0.463 f1 0.398\n",
            "[W3] RNN Epoch 33 | train_loss 0.4611 acc 0.772 f1 0.770 || val_loss 1.3791 acc 0.455 f1 0.391\n",
            "[W3] RNN Epoch 34 | train_loss 0.4384 acc 0.798 f1 0.796 || val_loss 1.3881 acc 0.469 f1 0.407\n",
            "[W3] RNN Epoch 35 | train_loss 0.4379 acc 0.793 f1 0.792 || val_loss 1.4168 acc 0.455 f1 0.387\n",
            "[W3] RNN Epoch 36 | train_loss 0.4268 acc 0.797 f1 0.795 || val_loss 1.4527 acc 0.447 f1 0.378\n",
            "[W3] RNN Epoch 37 | train_loss 0.4203 acc 0.798 f1 0.797 || val_loss 1.4728 acc 0.447 f1 0.376\n",
            "[W3] RNN Epoch 38 | train_loss 0.4178 acc 0.807 f1 0.806 || val_loss 1.4871 acc 0.444 f1 0.378\n",
            "[W3] RNN Epoch 39 | train_loss 0.4054 acc 0.799 f1 0.798 || val_loss 1.4879 acc 0.442 f1 0.378\n",
            "[W3] RNN Epoch 40 | train_loss 0.4141 acc 0.805 f1 0.803 || val_loss 1.5364 acc 0.457 f1 0.379\n",
            "[W3] RNN Epoch 41 | train_loss 0.3976 acc 0.818 f1 0.817 || val_loss 1.5338 acc 0.457 f1 0.384\n",
            "[W3] RNN Epoch 42 | train_loss 0.3988 acc 0.816 f1 0.815 || val_loss 1.5619 acc 0.461 f1 0.392\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=68\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 989, np.int64(1): 924, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 989, np.int64(2): 989, np.int64(0): 989})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0986 acc 0.347 f1 0.264 || val_loss 1.1063 acc 0.255 f1 0.228\n",
            "[W3] GRU Epoch 02 | train_loss 1.0907 acc 0.386 f1 0.366 || val_loss 1.0952 acc 0.323 f1 0.284\n",
            "[W3] GRU Epoch 03 | train_loss 1.0811 acc 0.414 f1 0.399 || val_loss 1.0930 acc 0.298 f1 0.283\n",
            "[W3] GRU Epoch 04 | train_loss 1.0663 acc 0.430 f1 0.425 || val_loss 1.1057 acc 0.292 f1 0.285\n",
            "[W3] GRU Epoch 05 | train_loss 1.0402 acc 0.466 f1 0.454 || val_loss 1.0620 acc 0.360 f1 0.319\n",
            "[W3] GRU Epoch 06 | train_loss 0.9900 acc 0.509 f1 0.499 || val_loss 1.1066 acc 0.317 f1 0.299\n",
            "[W3] GRU Epoch 07 | train_loss 0.9102 acc 0.556 f1 0.544 || val_loss 1.1038 acc 0.362 f1 0.313\n",
            "[W3] GRU Epoch 08 | train_loss 0.8432 acc 0.577 f1 0.569 || val_loss 1.1329 acc 0.358 f1 0.315\n",
            "[W3] GRU Epoch 09 | train_loss 0.7895 acc 0.606 f1 0.597 || val_loss 1.2044 acc 0.337 f1 0.300\n",
            "[W3] GRU Epoch 10 | train_loss 0.7513 acc 0.628 f1 0.622 || val_loss 1.1789 acc 0.364 f1 0.314\n",
            "[W3] GRU Epoch 11 | train_loss 0.7216 acc 0.635 f1 0.627 || val_loss 1.1977 acc 0.352 f1 0.301\n",
            "[W3] GRU Epoch 12 | train_loss 0.6806 acc 0.657 f1 0.652 || val_loss 1.1980 acc 0.372 f1 0.305\n",
            "[W3] GRU Epoch 13 | train_loss 0.6624 acc 0.662 f1 0.657 || val_loss 1.2379 acc 0.372 f1 0.315\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=68\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 989, np.int64(1): 924, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 989, np.int64(2): 989, np.int64(0): 989})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0992 acc 0.337 f1 0.264 || val_loss 1.0878 acc 0.432 f1 0.308\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0951 acc 0.356 f1 0.295 || val_loss 1.0903 acc 0.395 f1 0.327\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0870 acc 0.398 f1 0.397 || val_loss 1.0921 acc 0.340 f1 0.326\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0702 acc 0.447 f1 0.445 || val_loss 1.1135 acc 0.270 f1 0.266\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0345 acc 0.475 f1 0.459 || val_loss 1.0603 acc 0.379 f1 0.325\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9637 acc 0.535 f1 0.531 || val_loss 1.1158 acc 0.325 f1 0.291\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8624 acc 0.575 f1 0.569 || val_loss 1.1523 acc 0.346 f1 0.298\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8062 acc 0.611 f1 0.605 || val_loss 1.1039 acc 0.399 f1 0.320\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7582 acc 0.640 f1 0.635 || val_loss 1.1492 acc 0.389 f1 0.325\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7185 acc 0.650 f1 0.648 || val_loss 1.1733 acc 0.381 f1 0.315\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  68%|   | 68/100 [32:29<13:58, 26.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=69 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=69\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 933, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1214 acc 0.406 f1 0.405 || val_loss 1.1375 acc 0.309 f1 0.279\n",
            "[W3] ANN Epoch 02 | train_loss 0.9589 acc 0.525 f1 0.517 || val_loss 1.1294 acc 0.350 f1 0.306\n",
            "[W3] ANN Epoch 03 | train_loss 0.8670 acc 0.566 f1 0.560 || val_loss 1.1061 acc 0.395 f1 0.341\n",
            "[W3] ANN Epoch 04 | train_loss 0.7937 acc 0.618 f1 0.611 || val_loss 1.1350 acc 0.374 f1 0.325\n",
            "[W3] ANN Epoch 05 | train_loss 0.7269 acc 0.647 f1 0.642 || val_loss 1.1201 acc 0.393 f1 0.329\n",
            "[W3] ANN Epoch 06 | train_loss 0.6867 acc 0.669 f1 0.665 || val_loss 1.1431 acc 0.401 f1 0.329\n",
            "[W3] ANN Epoch 07 | train_loss 0.6357 acc 0.694 f1 0.690 || val_loss 1.1394 acc 0.401 f1 0.343\n",
            "[W3] ANN Epoch 08 | train_loss 0.6047 acc 0.710 f1 0.707 || val_loss 1.1745 acc 0.395 f1 0.310\n",
            "[W3] ANN Epoch 09 | train_loss 0.5796 acc 0.731 f1 0.729 || val_loss 1.1832 acc 0.385 f1 0.316\n",
            "[W3] ANN Epoch 10 | train_loss 0.5721 acc 0.736 f1 0.734 || val_loss 1.2441 acc 0.407 f1 0.341\n",
            "[W3] ANN Epoch 11 | train_loss 0.5261 acc 0.756 f1 0.754 || val_loss 1.2144 acc 0.403 f1 0.319\n",
            "[W3] ANN Epoch 12 | train_loss 0.5149 acc 0.763 f1 0.762 || val_loss 1.2266 acc 0.434 f1 0.360\n",
            "[W3] ANN Epoch 13 | train_loss 0.5004 acc 0.770 f1 0.769 || val_loss 1.2843 acc 0.397 f1 0.318\n",
            "[W3] ANN Epoch 14 | train_loss 0.4731 acc 0.785 f1 0.784 || val_loss 1.2680 acc 0.407 f1 0.318\n",
            "[W3] ANN Epoch 15 | train_loss 0.4600 acc 0.797 f1 0.798 || val_loss 1.2826 acc 0.451 f1 0.352\n",
            "[W3] ANN Epoch 16 | train_loss 0.4540 acc 0.797 f1 0.797 || val_loss 1.3089 acc 0.442 f1 0.354\n",
            "[W3] ANN Epoch 17 | train_loss 0.4385 acc 0.812 f1 0.811 || val_loss 1.3310 acc 0.418 f1 0.325\n",
            "[W3] ANN Epoch 18 | train_loss 0.4355 acc 0.814 f1 0.813 || val_loss 1.3357 acc 0.393 f1 0.306\n",
            "[W3] ANN Epoch 19 | train_loss 0.4314 acc 0.810 f1 0.810 || val_loss 1.3278 acc 0.430 f1 0.326\n",
            "[W3] ANN Epoch 20 | train_loss 0.4030 acc 0.837 f1 0.837 || val_loss 1.3769 acc 0.416 f1 0.321\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=69\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 933, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0808 acc 0.376 f1 0.380 || val_loss 1.0264 acc 0.416 f1 0.316\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9536 acc 0.522 f1 0.518 || val_loss 1.0225 acc 0.393 f1 0.313\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8207 acc 0.605 f1 0.600 || val_loss 1.0717 acc 0.395 f1 0.336\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7238 acc 0.645 f1 0.641 || val_loss 1.1693 acc 0.399 f1 0.340\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6545 acc 0.684 f1 0.680 || val_loss 1.1679 acc 0.416 f1 0.359\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6116 acc 0.706 f1 0.704 || val_loss 1.1797 acc 0.438 f1 0.362\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5462 acc 0.755 f1 0.755 || val_loss 1.2243 acc 0.438 f1 0.365\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5014 acc 0.772 f1 0.771 || val_loss 1.2734 acc 0.436 f1 0.374\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4620 acc 0.798 f1 0.798 || val_loss 1.3048 acc 0.438 f1 0.365\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4455 acc 0.808 f1 0.808 || val_loss 1.3617 acc 0.442 f1 0.375\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.4148 acc 0.816 f1 0.816 || val_loss 1.4118 acc 0.422 f1 0.353\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3727 acc 0.845 f1 0.844 || val_loss 1.4675 acc 0.449 f1 0.364\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3846 acc 0.846 f1 0.845 || val_loss 1.4713 acc 0.442 f1 0.355\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3450 acc 0.867 f1 0.866 || val_loss 1.4623 acc 0.438 f1 0.356\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.3178 acc 0.874 f1 0.874 || val_loss 1.5155 acc 0.459 f1 0.377\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2975 acc 0.886 f1 0.885 || val_loss 1.5499 acc 0.426 f1 0.352\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.2336 acc 0.920 f1 0.920 || val_loss 1.6530 acc 0.438 f1 0.357\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.2612 acc 0.902 f1 0.901 || val_loss 1.6546 acc 0.444 f1 0.357\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.2555 acc 0.906 f1 0.906 || val_loss 1.6756 acc 0.467 f1 0.374\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.2389 acc 0.908 f1 0.908 || val_loss 1.7439 acc 0.432 f1 0.357\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.2038 acc 0.931 f1 0.931 || val_loss 1.8213 acc 0.459 f1 0.385\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.2207 acc 0.917 f1 0.917 || val_loss 1.7349 acc 0.449 f1 0.368\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.2103 acc 0.921 f1 0.921 || val_loss 1.8428 acc 0.438 f1 0.357\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.1994 acc 0.929 f1 0.929 || val_loss 1.8187 acc 0.426 f1 0.358\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.2001 acc 0.929 f1 0.929 || val_loss 1.8295 acc 0.457 f1 0.384\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.1697 acc 0.942 f1 0.942 || val_loss 1.9003 acc 0.459 f1 0.373\n",
            "[W3] CNN1D Epoch 27 | train_loss 0.1473 acc 0.951 f1 0.951 || val_loss 2.0237 acc 0.457 f1 0.381\n",
            "[W3] CNN1D Epoch 28 | train_loss 0.1400 acc 0.958 f1 0.957 || val_loss 1.9839 acc 0.438 f1 0.357\n",
            "[W3] CNN1D Epoch 29 | train_loss 0.1587 acc 0.949 f1 0.949 || val_loss 1.9639 acc 0.428 f1 0.343\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=69\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 933, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1020 acc 0.343 f1 0.279 || val_loss 1.1103 acc 0.259 f1 0.259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0841 acc 0.416 f1 0.389 || val_loss 1.0994 acc 0.313 f1 0.299\n",
            "[W3] RNN Epoch 03 | train_loss 1.0727 acc 0.421 f1 0.417 || val_loss 1.0888 acc 0.325 f1 0.303\n",
            "[W3] RNN Epoch 04 | train_loss 1.0560 acc 0.463 f1 0.459 || val_loss 1.0883 acc 0.325 f1 0.305\n",
            "[W3] RNN Epoch 05 | train_loss 1.0473 acc 0.455 f1 0.452 || val_loss 1.0949 acc 0.305 f1 0.287\n",
            "[W3] RNN Epoch 06 | train_loss 1.0252 acc 0.495 f1 0.492 || val_loss 1.0898 acc 0.335 f1 0.314\n",
            "[W3] RNN Epoch 07 | train_loss 1.0099 acc 0.495 f1 0.492 || val_loss 1.1063 acc 0.298 f1 0.278\n",
            "[W3] RNN Epoch 08 | train_loss 0.9976 acc 0.501 f1 0.494 || val_loss 1.0847 acc 0.354 f1 0.321\n",
            "[W3] RNN Epoch 09 | train_loss 0.9774 acc 0.524 f1 0.520 || val_loss 1.0947 acc 0.350 f1 0.321\n",
            "[W3] RNN Epoch 10 | train_loss 0.9549 acc 0.536 f1 0.528 || val_loss 1.1155 acc 0.348 f1 0.323\n",
            "[W3] RNN Epoch 11 | train_loss 0.9372 acc 0.547 f1 0.540 || val_loss 1.1130 acc 0.360 f1 0.333\n",
            "[W3] RNN Epoch 12 | train_loss 0.9148 acc 0.552 f1 0.544 || val_loss 1.1120 acc 0.358 f1 0.329\n",
            "[W3] RNN Epoch 13 | train_loss 0.8965 acc 0.568 f1 0.561 || val_loss 1.1247 acc 0.370 f1 0.336\n",
            "[W3] RNN Epoch 14 | train_loss 0.8851 acc 0.585 f1 0.576 || val_loss 1.1080 acc 0.366 f1 0.322\n",
            "[W3] RNN Epoch 15 | train_loss 0.8636 acc 0.592 f1 0.585 || val_loss 1.1143 acc 0.377 f1 0.334\n",
            "[W3] RNN Epoch 16 | train_loss 0.8256 acc 0.618 f1 0.611 || val_loss 1.1407 acc 0.377 f1 0.338\n",
            "[W3] RNN Epoch 17 | train_loss 0.8130 acc 0.619 f1 0.611 || val_loss 1.1228 acc 0.387 f1 0.333\n",
            "[W3] RNN Epoch 18 | train_loss 0.8089 acc 0.619 f1 0.613 || val_loss 1.1302 acc 0.399 f1 0.349\n",
            "[W3] RNN Epoch 19 | train_loss 0.7914 acc 0.634 f1 0.628 || val_loss 1.1425 acc 0.399 f1 0.347\n",
            "[W3] RNN Epoch 20 | train_loss 0.7530 acc 0.657 f1 0.651 || val_loss 1.1469 acc 0.403 f1 0.340\n",
            "[W3] RNN Epoch 21 | train_loss 0.7446 acc 0.653 f1 0.645 || val_loss 1.1647 acc 0.395 f1 0.348\n",
            "[W3] RNN Epoch 22 | train_loss 0.7131 acc 0.669 f1 0.662 || val_loss 1.1638 acc 0.387 f1 0.335\n",
            "[W3] RNN Epoch 23 | train_loss 0.6965 acc 0.679 f1 0.672 || val_loss 1.1938 acc 0.403 f1 0.348\n",
            "[W3] RNN Epoch 24 | train_loss 0.6988 acc 0.667 f1 0.659 || val_loss 1.1793 acc 0.403 f1 0.352\n",
            "[W3] RNN Epoch 25 | train_loss 0.6703 acc 0.690 f1 0.683 || val_loss 1.1840 acc 0.428 f1 0.356\n",
            "[W3] RNN Epoch 26 | train_loss 0.6524 acc 0.699 f1 0.695 || val_loss 1.2072 acc 0.420 f1 0.368\n",
            "[W3] RNN Epoch 27 | train_loss 0.6281 acc 0.714 f1 0.709 || val_loss 1.2122 acc 0.426 f1 0.363\n",
            "[W3] RNN Epoch 28 | train_loss 0.6090 acc 0.723 f1 0.717 || val_loss 1.2074 acc 0.426 f1 0.363\n",
            "[W3] RNN Epoch 29 | train_loss 0.6121 acc 0.714 f1 0.708 || val_loss 1.2291 acc 0.436 f1 0.375\n",
            "[W3] RNN Epoch 30 | train_loss 0.6058 acc 0.715 f1 0.712 || val_loss 1.2688 acc 0.430 f1 0.366\n",
            "[W3] RNN Epoch 31 | train_loss 0.5715 acc 0.749 f1 0.745 || val_loss 1.2617 acc 0.412 f1 0.354\n",
            "[W3] RNN Epoch 32 | train_loss 0.5663 acc 0.738 f1 0.734 || val_loss 1.2899 acc 0.424 f1 0.363\n",
            "[W3] RNN Epoch 33 | train_loss 0.5436 acc 0.746 f1 0.742 || val_loss 1.2950 acc 0.426 f1 0.356\n",
            "[W3] RNN Epoch 34 | train_loss 0.5593 acc 0.732 f1 0.728 || val_loss 1.2819 acc 0.426 f1 0.367\n",
            "[W3] RNN Epoch 35 | train_loss 0.5268 acc 0.750 f1 0.747 || val_loss 1.3040 acc 0.453 f1 0.370\n",
            "[W3] RNN Epoch 36 | train_loss 0.5150 acc 0.753 f1 0.751 || val_loss 1.3301 acc 0.449 f1 0.362\n",
            "[W3] RNN Epoch 37 | train_loss 0.5273 acc 0.755 f1 0.752 || val_loss 1.3573 acc 0.442 f1 0.368\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=69\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 933, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0982 acc 0.340 f1 0.284 || val_loss 1.0940 acc 0.340 f1 0.274\n",
            "[W3] GRU Epoch 02 | train_loss 1.0904 acc 0.396 f1 0.371 || val_loss 1.0945 acc 0.337 f1 0.305\n",
            "[W3] GRU Epoch 03 | train_loss 1.0801 acc 0.428 f1 0.424 || val_loss 1.0931 acc 0.340 f1 0.314\n",
            "[W3] GRU Epoch 04 | train_loss 1.0683 acc 0.443 f1 0.437 || val_loss 1.0887 acc 0.327 f1 0.299\n",
            "[W3] GRU Epoch 05 | train_loss 1.0470 acc 0.475 f1 0.471 || val_loss 1.0991 acc 0.327 f1 0.307\n",
            "[W3] GRU Epoch 06 | train_loss 1.0205 acc 0.504 f1 0.498 || val_loss 1.0785 acc 0.348 f1 0.305\n",
            "[W3] GRU Epoch 07 | train_loss 0.9781 acc 0.537 f1 0.530 || val_loss 1.0963 acc 0.315 f1 0.285\n",
            "[W3] GRU Epoch 08 | train_loss 0.9234 acc 0.560 f1 0.553 || val_loss 1.0659 acc 0.385 f1 0.315\n",
            "[W3] GRU Epoch 09 | train_loss 0.8619 acc 0.591 f1 0.585 || val_loss 1.1186 acc 0.350 f1 0.297\n",
            "[W3] GRU Epoch 10 | train_loss 0.8067 acc 0.605 f1 0.599 || val_loss 1.1648 acc 0.356 f1 0.310\n",
            "[W3] GRU Epoch 11 | train_loss 0.7611 acc 0.630 f1 0.626 || val_loss 1.1936 acc 0.368 f1 0.312\n",
            "[W3] GRU Epoch 12 | train_loss 0.7242 acc 0.653 f1 0.649 || val_loss 1.2127 acc 0.383 f1 0.318\n",
            "[W3] GRU Epoch 13 | train_loss 0.7005 acc 0.663 f1 0.659 || val_loss 1.2372 acc 0.389 f1 0.322\n",
            "[W3] GRU Epoch 14 | train_loss 0.6662 acc 0.678 f1 0.673 || val_loss 1.2387 acc 0.401 f1 0.337\n",
            "[W3] GRU Epoch 15 | train_loss 0.6512 acc 0.689 f1 0.688 || val_loss 1.2662 acc 0.389 f1 0.322\n",
            "[W3] GRU Epoch 16 | train_loss 0.6241 acc 0.698 f1 0.693 || val_loss 1.2539 acc 0.409 f1 0.333\n",
            "[W3] GRU Epoch 17 | train_loss 0.5955 acc 0.710 f1 0.707 || val_loss 1.3172 acc 0.381 f1 0.313\n",
            "[W3] GRU Epoch 18 | train_loss 0.5749 acc 0.720 f1 0.718 || val_loss 1.3617 acc 0.407 f1 0.330\n",
            "[W3] GRU Epoch 19 | train_loss 0.5809 acc 0.717 f1 0.714 || val_loss 1.3661 acc 0.414 f1 0.330\n",
            "[W3] GRU Epoch 20 | train_loss 0.5410 acc 0.736 f1 0.734 || val_loss 1.3795 acc 0.422 f1 0.341\n",
            "[W3] GRU Epoch 21 | train_loss 0.5362 acc 0.740 f1 0.739 || val_loss 1.4082 acc 0.416 f1 0.328\n",
            "[W3] GRU Epoch 22 | train_loss 0.5068 acc 0.759 f1 0.756 || val_loss 1.4415 acc 0.418 f1 0.329\n",
            "[W3] GRU Epoch 23 | train_loss 0.4861 acc 0.776 f1 0.775 || val_loss 1.4594 acc 0.426 f1 0.333\n",
            "[W3] GRU Epoch 24 | train_loss 0.4615 acc 0.788 f1 0.787 || val_loss 1.5005 acc 0.426 f1 0.344\n",
            "[W3] GRU Epoch 25 | train_loss 0.4764 acc 0.775 f1 0.774 || val_loss 1.5233 acc 0.416 f1 0.331\n",
            "[W3] GRU Epoch 26 | train_loss 0.4479 acc 0.785 f1 0.784 || val_loss 1.5491 acc 0.422 f1 0.341\n",
            "[W3] GRU Epoch 27 | train_loss 0.4287 acc 0.799 f1 0.798 || val_loss 1.5732 acc 0.416 f1 0.331\n",
            "[W3] GRU Epoch 28 | train_loss 0.4104 acc 0.817 f1 0.817 || val_loss 1.6191 acc 0.414 f1 0.326\n",
            "[W3] GRU Epoch 29 | train_loss 0.3996 acc 0.817 f1 0.817 || val_loss 1.6492 acc 0.418 f1 0.332\n",
            "[W3] GRU Epoch 30 | train_loss 0.3826 acc 0.820 f1 0.820 || val_loss 1.6841 acc 0.409 f1 0.327\n",
            "[W3] GRU Epoch 31 | train_loss 0.3678 acc 0.839 f1 0.839 || val_loss 1.7332 acc 0.409 f1 0.327\n",
            "[W3] GRU Epoch 32 | train_loss 0.3632 acc 0.838 f1 0.837 || val_loss 1.7504 acc 0.401 f1 0.320\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=69\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 933, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1019 acc 0.334 f1 0.172 || val_loss 1.1004 acc 0.405 f1 0.196\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0933 acc 0.379 f1 0.349 || val_loss 1.1058 acc 0.284 f1 0.281\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0836 acc 0.412 f1 0.407 || val_loss 1.1022 acc 0.282 f1 0.274\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0719 acc 0.415 f1 0.409 || val_loss 1.1090 acc 0.282 f1 0.276\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0399 acc 0.470 f1 0.460 || val_loss 1.1132 acc 0.327 f1 0.319\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9704 acc 0.524 f1 0.510 || val_loss 1.0647 acc 0.364 f1 0.296\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8887 acc 0.558 f1 0.549 || val_loss 1.0766 acc 0.403 f1 0.317\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8381 acc 0.585 f1 0.580 || val_loss 1.1388 acc 0.377 f1 0.328\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7813 acc 0.620 f1 0.610 || val_loss 1.1430 acc 0.381 f1 0.303\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7449 acc 0.636 f1 0.632 || val_loss 1.1959 acc 0.374 f1 0.322\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7430 acc 0.633 f1 0.627 || val_loss 1.2129 acc 0.372 f1 0.325\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6904 acc 0.674 f1 0.669 || val_loss 1.2223 acc 0.391 f1 0.326\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6637 acc 0.684 f1 0.680 || val_loss 1.2261 acc 0.407 f1 0.333\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6396 acc 0.683 f1 0.680 || val_loss 1.2875 acc 0.395 f1 0.326\n",
            "[W3] LSTM Epoch 15 | train_loss 0.6106 acc 0.712 f1 0.708 || val_loss 1.2767 acc 0.442 f1 0.350\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5914 acc 0.708 f1 0.705 || val_loss 1.3289 acc 0.432 f1 0.344\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5748 acc 0.720 f1 0.718 || val_loss 1.3741 acc 0.414 f1 0.336\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5964 acc 0.713 f1 0.709 || val_loss 1.3337 acc 0.412 f1 0.337\n",
            "[W3] LSTM Epoch 19 | train_loss 0.5365 acc 0.739 f1 0.737 || val_loss 1.4137 acc 0.424 f1 0.339\n",
            "[W3] LSTM Epoch 20 | train_loss 0.5073 acc 0.755 f1 0.754 || val_loss 1.4478 acc 0.426 f1 0.352\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4946 acc 0.760 f1 0.758 || val_loss 1.5024 acc 0.432 f1 0.357\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4798 acc 0.759 f1 0.758 || val_loss 1.5277 acc 0.414 f1 0.330\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4630 acc 0.778 f1 0.777 || val_loss 1.5727 acc 0.399 f1 0.316\n",
            "[W3] LSTM Epoch 24 | train_loss 0.4386 acc 0.795 f1 0.794 || val_loss 1.6365 acc 0.412 f1 0.334\n",
            "[W3] LSTM Epoch 25 | train_loss 0.4212 acc 0.801 f1 0.800 || val_loss 1.6883 acc 0.405 f1 0.324\n",
            "[W3] LSTM Epoch 26 | train_loss 0.4118 acc 0.804 f1 0.803 || val_loss 1.7222 acc 0.416 f1 0.331\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3991 acc 0.804 f1 0.804 || val_loss 1.8037 acc 0.403 f1 0.327\n",
            "[W3] LSTM Epoch 28 | train_loss 0.3806 acc 0.821 f1 0.822 || val_loss 1.8409 acc 0.397 f1 0.311\n",
            "[W3] LSTM Epoch 29 | train_loss 0.3643 acc 0.830 f1 0.830 || val_loss 1.8969 acc 0.414 f1 0.335\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  69%|   | 69/100 [33:10<15:47, 30.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=70 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=70\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 979, np.int64(1): 933, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 979, np.int64(2): 979, np.int64(0): 979})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1375 acc 0.404 f1 0.405 || val_loss 1.1415 acc 0.352 f1 0.322\n",
            "[W3] ANN Epoch 02 | train_loss 0.9737 acc 0.518 f1 0.510 || val_loss 1.1530 acc 0.356 f1 0.324\n",
            "[W3] ANN Epoch 03 | train_loss 0.8771 acc 0.580 f1 0.573 || val_loss 1.1260 acc 0.405 f1 0.359\n",
            "[W3] ANN Epoch 04 | train_loss 0.7913 acc 0.627 f1 0.620 || val_loss 1.1427 acc 0.412 f1 0.355\n",
            "[W3] ANN Epoch 05 | train_loss 0.7228 acc 0.662 f1 0.657 || val_loss 1.1104 acc 0.428 f1 0.360\n",
            "[W3] ANN Epoch 06 | train_loss 0.6751 acc 0.672 f1 0.670 || val_loss 1.1372 acc 0.432 f1 0.352\n",
            "[W3] ANN Epoch 07 | train_loss 0.6382 acc 0.696 f1 0.693 || val_loss 1.1351 acc 0.422 f1 0.361\n",
            "[W3] ANN Epoch 08 | train_loss 0.6051 acc 0.722 f1 0.720 || val_loss 1.1739 acc 0.424 f1 0.352\n",
            "[W3] ANN Epoch 09 | train_loss 0.5646 acc 0.741 f1 0.740 || val_loss 1.1534 acc 0.444 f1 0.366\n",
            "[W3] ANN Epoch 10 | train_loss 0.5304 acc 0.758 f1 0.758 || val_loss 1.2016 acc 0.477 f1 0.390\n",
            "[W3] ANN Epoch 11 | train_loss 0.5203 acc 0.769 f1 0.768 || val_loss 1.2142 acc 0.424 f1 0.350\n",
            "[W3] ANN Epoch 12 | train_loss 0.5167 acc 0.768 f1 0.767 || val_loss 1.2554 acc 0.453 f1 0.361\n",
            "[W3] ANN Epoch 13 | train_loss 0.4801 acc 0.788 f1 0.787 || val_loss 1.2467 acc 0.440 f1 0.355\n",
            "[W3] ANN Epoch 14 | train_loss 0.4482 acc 0.800 f1 0.800 || val_loss 1.3093 acc 0.447 f1 0.355\n",
            "[W3] ANN Epoch 15 | train_loss 0.4237 acc 0.818 f1 0.818 || val_loss 1.3383 acc 0.442 f1 0.353\n",
            "[W3] ANN Epoch 16 | train_loss 0.4162 acc 0.821 f1 0.821 || val_loss 1.3431 acc 0.457 f1 0.370\n",
            "[W3] ANN Epoch 17 | train_loss 0.4002 acc 0.829 f1 0.828 || val_loss 1.4492 acc 0.430 f1 0.338\n",
            "[W3] ANN Epoch 18 | train_loss 0.3991 acc 0.824 f1 0.824 || val_loss 1.4333 acc 0.436 f1 0.350\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=70\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 979, np.int64(1): 933, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 979, np.int64(2): 979, np.int64(0): 979})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0700 acc 0.413 f1 0.415 || val_loss 1.0108 acc 0.426 f1 0.323\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9249 acc 0.553 f1 0.549 || val_loss 1.0536 acc 0.401 f1 0.344\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8050 acc 0.613 f1 0.608 || val_loss 1.0872 acc 0.401 f1 0.336\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7097 acc 0.664 f1 0.661 || val_loss 1.1705 acc 0.414 f1 0.350\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6380 acc 0.701 f1 0.697 || val_loss 1.2087 acc 0.405 f1 0.350\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5604 acc 0.740 f1 0.738 || val_loss 1.2678 acc 0.424 f1 0.345\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5037 acc 0.769 f1 0.769 || val_loss 1.3331 acc 0.428 f1 0.365\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4621 acc 0.795 f1 0.795 || val_loss 1.4200 acc 0.407 f1 0.355\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4055 acc 0.829 f1 0.829 || val_loss 1.4863 acc 0.428 f1 0.357\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3648 acc 0.845 f1 0.845 || val_loss 1.5763 acc 0.430 f1 0.354\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3252 acc 0.870 f1 0.870 || val_loss 1.6026 acc 0.430 f1 0.347\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2758 acc 0.893 f1 0.893 || val_loss 1.7452 acc 0.412 f1 0.348\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2322 acc 0.910 f1 0.910 || val_loss 1.8618 acc 0.416 f1 0.349\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.1950 acc 0.932 f1 0.932 || val_loss 1.9343 acc 0.434 f1 0.348\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1829 acc 0.929 f1 0.929 || val_loss 2.0284 acc 0.428 f1 0.322\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=70\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 979, np.int64(1): 933, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 979, np.int64(2): 979, np.int64(0): 979})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0979 acc 0.347 f1 0.337 || val_loss 1.0905 acc 0.356 f1 0.302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0819 acc 0.424 f1 0.419 || val_loss 1.0922 acc 0.321 f1 0.305\n",
            "[W3] RNN Epoch 03 | train_loss 1.0682 acc 0.437 f1 0.430 || val_loss 1.0855 acc 0.352 f1 0.329\n",
            "[W3] RNN Epoch 04 | train_loss 1.0499 acc 0.460 f1 0.453 || val_loss 1.0864 acc 0.340 f1 0.320\n",
            "[W3] RNN Epoch 05 | train_loss 1.0279 acc 0.484 f1 0.475 || val_loss 1.0846 acc 0.350 f1 0.321\n",
            "[W3] RNN Epoch 06 | train_loss 1.0014 acc 0.511 f1 0.504 || val_loss 1.0924 acc 0.360 f1 0.333\n",
            "[W3] RNN Epoch 07 | train_loss 0.9752 acc 0.524 f1 0.516 || val_loss 1.0991 acc 0.383 f1 0.357\n",
            "[W3] RNN Epoch 08 | train_loss 0.9458 acc 0.539 f1 0.529 || val_loss 1.0667 acc 0.432 f1 0.372\n",
            "[W3] RNN Epoch 09 | train_loss 0.9235 acc 0.562 f1 0.554 || val_loss 1.0722 acc 0.428 f1 0.374\n",
            "[W3] RNN Epoch 10 | train_loss 0.8922 acc 0.566 f1 0.558 || val_loss 1.0969 acc 0.395 f1 0.353\n",
            "[W3] RNN Epoch 11 | train_loss 0.8753 acc 0.590 f1 0.580 || val_loss 1.1054 acc 0.426 f1 0.380\n",
            "[W3] RNN Epoch 12 | train_loss 0.8516 acc 0.588 f1 0.579 || val_loss 1.0957 acc 0.434 f1 0.376\n",
            "[W3] RNN Epoch 13 | train_loss 0.8266 acc 0.622 f1 0.614 || val_loss 1.1304 acc 0.414 f1 0.373\n",
            "[W3] RNN Epoch 14 | train_loss 0.8016 acc 0.629 f1 0.620 || val_loss 1.1161 acc 0.428 f1 0.357\n",
            "[W3] RNN Epoch 15 | train_loss 0.7814 acc 0.641 f1 0.633 || val_loss 1.1314 acc 0.436 f1 0.369\n",
            "[W3] RNN Epoch 16 | train_loss 0.7569 acc 0.639 f1 0.631 || val_loss 1.1490 acc 0.436 f1 0.380\n",
            "[W3] RNN Epoch 17 | train_loss 0.7344 acc 0.660 f1 0.651 || val_loss 1.1613 acc 0.426 f1 0.363\n",
            "[W3] RNN Epoch 18 | train_loss 0.7175 acc 0.677 f1 0.670 || val_loss 1.1762 acc 0.424 f1 0.364\n",
            "[W3] RNN Epoch 19 | train_loss 0.6993 acc 0.676 f1 0.668 || val_loss 1.1779 acc 0.453 f1 0.371\n",
            "[W3] RNN Epoch 20 | train_loss 0.6780 acc 0.679 f1 0.671 || val_loss 1.1955 acc 0.444 f1 0.370\n",
            "[W3] RNN Epoch 21 | train_loss 0.6540 acc 0.705 f1 0.700 || val_loss 1.2128 acc 0.432 f1 0.348\n",
            "[W3] RNN Epoch 22 | train_loss 0.6393 acc 0.703 f1 0.697 || val_loss 1.2359 acc 0.414 f1 0.343\n",
            "[W3] RNN Epoch 23 | train_loss 0.6135 acc 0.710 f1 0.704 || val_loss 1.2653 acc 0.426 f1 0.354\n",
            "[W3] RNN Epoch 24 | train_loss 0.5980 acc 0.715 f1 0.709 || val_loss 1.2991 acc 0.424 f1 0.345\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=70\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 979, np.int64(1): 933, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 979, np.int64(2): 979, np.int64(0): 979})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0994 acc 0.344 f1 0.278 || val_loss 1.1022 acc 0.294 f1 0.230\n",
            "[W3] GRU Epoch 02 | train_loss 1.0922 acc 0.382 f1 0.357 || val_loss 1.0928 acc 0.348 f1 0.310\n",
            "[W3] GRU Epoch 03 | train_loss 1.0831 acc 0.411 f1 0.395 || val_loss 1.0890 acc 0.346 f1 0.320\n",
            "[W3] GRU Epoch 04 | train_loss 1.0685 acc 0.447 f1 0.439 || val_loss 1.0852 acc 0.374 f1 0.343\n",
            "[W3] GRU Epoch 05 | train_loss 1.0477 acc 0.476 f1 0.468 || val_loss 1.0770 acc 0.368 f1 0.332\n",
            "[W3] GRU Epoch 06 | train_loss 0.9993 acc 0.513 f1 0.506 || val_loss 1.0895 acc 0.389 f1 0.351\n",
            "[W3] GRU Epoch 07 | train_loss 0.9235 acc 0.543 f1 0.533 || val_loss 1.0892 acc 0.387 f1 0.323\n",
            "[W3] GRU Epoch 08 | train_loss 0.8411 acc 0.595 f1 0.589 || val_loss 1.0932 acc 0.416 f1 0.339\n",
            "[W3] GRU Epoch 09 | train_loss 0.7887 acc 0.607 f1 0.602 || val_loss 1.1315 acc 0.385 f1 0.315\n",
            "[W3] GRU Epoch 10 | train_loss 0.7394 acc 0.635 f1 0.631 || val_loss 1.1532 acc 0.397 f1 0.330\n",
            "[W3] GRU Epoch 11 | train_loss 0.7019 acc 0.656 f1 0.651 || val_loss 1.1814 acc 0.387 f1 0.319\n",
            "[W3] GRU Epoch 12 | train_loss 0.6652 acc 0.668 f1 0.664 || val_loss 1.1908 acc 0.401 f1 0.324\n",
            "[W3] GRU Epoch 13 | train_loss 0.6381 acc 0.687 f1 0.684 || val_loss 1.2293 acc 0.395 f1 0.319\n",
            "[W3] GRU Epoch 14 | train_loss 0.6008 acc 0.710 f1 0.708 || val_loss 1.2762 acc 0.379 f1 0.312\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=70\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 979, np.int64(1): 933, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 979, np.int64(2): 979, np.int64(0): 979})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0989 acc 0.344 f1 0.277 || val_loss 1.1025 acc 0.298 f1 0.296\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0935 acc 0.363 f1 0.343 || val_loss 1.1030 acc 0.305 f1 0.303\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0845 acc 0.416 f1 0.405 || val_loss 1.0940 acc 0.337 f1 0.317\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0680 acc 0.448 f1 0.439 || val_loss 1.1025 acc 0.311 f1 0.303\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0249 acc 0.486 f1 0.463 || val_loss 1.0775 acc 0.342 f1 0.309\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9480 acc 0.531 f1 0.519 || val_loss 1.0704 acc 0.403 f1 0.349\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8544 acc 0.571 f1 0.562 || val_loss 1.0883 acc 0.368 f1 0.306\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7983 acc 0.600 f1 0.592 || val_loss 1.1201 acc 0.416 f1 0.315\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7585 acc 0.624 f1 0.618 || val_loss 1.1572 acc 0.391 f1 0.315\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7217 acc 0.649 f1 0.643 || val_loss 1.1782 acc 0.409 f1 0.315\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6813 acc 0.673 f1 0.668 || val_loss 1.2129 acc 0.401 f1 0.321\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6591 acc 0.681 f1 0.678 || val_loss 1.2439 acc 0.407 f1 0.320\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6392 acc 0.692 f1 0.687 || val_loss 1.2525 acc 0.418 f1 0.328\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5973 acc 0.705 f1 0.702 || val_loss 1.2869 acc 0.405 f1 0.318\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  70%|   | 70/100 [33:34<14:22, 28.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=71 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=71\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 930, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1281 acc 0.391 f1 0.390 || val_loss 1.1399 acc 0.311 f1 0.302\n",
            "[W3] ANN Epoch 02 | train_loss 0.9566 acc 0.513 f1 0.505 || val_loss 1.1497 acc 0.331 f1 0.323\n",
            "[W3] ANN Epoch 03 | train_loss 0.8577 acc 0.569 f1 0.560 || val_loss 1.1337 acc 0.358 f1 0.327\n",
            "[W3] ANN Epoch 04 | train_loss 0.7745 acc 0.625 f1 0.620 || val_loss 1.1089 acc 0.401 f1 0.356\n",
            "[W3] ANN Epoch 05 | train_loss 0.7221 acc 0.643 f1 0.636 || val_loss 1.1418 acc 0.391 f1 0.344\n",
            "[W3] ANN Epoch 06 | train_loss 0.6565 acc 0.694 f1 0.690 || val_loss 1.1371 acc 0.409 f1 0.366\n",
            "[W3] ANN Epoch 07 | train_loss 0.6282 acc 0.706 f1 0.703 || val_loss 1.1358 acc 0.422 f1 0.368\n",
            "[W3] ANN Epoch 08 | train_loss 0.5858 acc 0.724 f1 0.722 || val_loss 1.1698 acc 0.434 f1 0.373\n",
            "[W3] ANN Epoch 09 | train_loss 0.5608 acc 0.739 f1 0.737 || val_loss 1.1923 acc 0.420 f1 0.365\n",
            "[W3] ANN Epoch 10 | train_loss 0.5341 acc 0.759 f1 0.757 || val_loss 1.1928 acc 0.449 f1 0.385\n",
            "[W3] ANN Epoch 11 | train_loss 0.4998 acc 0.774 f1 0.773 || val_loss 1.2363 acc 0.434 f1 0.367\n",
            "[W3] ANN Epoch 12 | train_loss 0.4802 acc 0.779 f1 0.779 || val_loss 1.2598 acc 0.434 f1 0.353\n",
            "[W3] ANN Epoch 13 | train_loss 0.4514 acc 0.792 f1 0.792 || val_loss 1.2930 acc 0.461 f1 0.383\n",
            "[W3] ANN Epoch 14 | train_loss 0.4639 acc 0.804 f1 0.804 || val_loss 1.3088 acc 0.465 f1 0.396\n",
            "[W3] ANN Epoch 15 | train_loss 0.4373 acc 0.817 f1 0.816 || val_loss 1.2943 acc 0.434 f1 0.360\n",
            "[W3] ANN Epoch 16 | train_loss 0.4022 acc 0.828 f1 0.828 || val_loss 1.3706 acc 0.418 f1 0.363\n",
            "[W3] ANN Epoch 17 | train_loss 0.4089 acc 0.828 f1 0.827 || val_loss 1.4022 acc 0.412 f1 0.341\n",
            "[W3] ANN Epoch 18 | train_loss 0.3734 acc 0.849 f1 0.848 || val_loss 1.4519 acc 0.442 f1 0.355\n",
            "[W3] ANN Epoch 19 | train_loss 0.3656 acc 0.837 f1 0.836 || val_loss 1.4781 acc 0.424 f1 0.356\n",
            "[W3] ANN Epoch 20 | train_loss 0.3373 acc 0.860 f1 0.859 || val_loss 1.5459 acc 0.414 f1 0.332\n",
            "[W3] ANN Epoch 21 | train_loss 0.3549 acc 0.853 f1 0.853 || val_loss 1.4769 acc 0.424 f1 0.351\n",
            "[W3] ANN Epoch 22 | train_loss 0.3303 acc 0.861 f1 0.861 || val_loss 1.5583 acc 0.418 f1 0.346\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=71\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 930, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0776 acc 0.394 f1 0.393 || val_loss 1.0360 acc 0.385 f1 0.302\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9535 acc 0.533 f1 0.526 || val_loss 1.0365 acc 0.428 f1 0.325\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8210 acc 0.602 f1 0.599 || val_loss 1.1342 acc 0.389 f1 0.332\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7233 acc 0.658 f1 0.653 || val_loss 1.1684 acc 0.401 f1 0.326\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6363 acc 0.699 f1 0.698 || val_loss 1.2474 acc 0.377 f1 0.311\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5672 acc 0.730 f1 0.729 || val_loss 1.3109 acc 0.418 f1 0.360\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.4886 acc 0.784 f1 0.782 || val_loss 1.4176 acc 0.391 f1 0.316\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4421 acc 0.804 f1 0.803 || val_loss 1.4598 acc 0.387 f1 0.300\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4185 acc 0.809 f1 0.809 || val_loss 1.5655 acc 0.403 f1 0.327\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3471 acc 0.852 f1 0.852 || val_loss 1.6674 acc 0.385 f1 0.310\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3170 acc 0.871 f1 0.871 || val_loss 1.7141 acc 0.405 f1 0.332\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2879 acc 0.887 f1 0.886 || val_loss 1.7747 acc 0.416 f1 0.339\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2561 acc 0.902 f1 0.901 || val_loss 1.9346 acc 0.383 f1 0.316\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2432 acc 0.901 f1 0.901 || val_loss 1.9176 acc 0.453 f1 0.345\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=71\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 930, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0974 acc 0.358 f1 0.332 || val_loss 1.0890 acc 0.360 f1 0.329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0798 acc 0.424 f1 0.424 || val_loss 1.0866 acc 0.342 f1 0.324\n",
            "[W3] RNN Epoch 03 | train_loss 1.0637 acc 0.443 f1 0.439 || val_loss 1.0883 acc 0.352 f1 0.337\n",
            "[W3] RNN Epoch 04 | train_loss 1.0485 acc 0.467 f1 0.458 || val_loss 1.0820 acc 0.370 f1 0.350\n",
            "[W3] RNN Epoch 05 | train_loss 1.0262 acc 0.491 f1 0.482 || val_loss 1.0758 acc 0.372 f1 0.351\n",
            "[W3] RNN Epoch 06 | train_loss 1.0108 acc 0.498 f1 0.490 || val_loss 1.0710 acc 0.366 f1 0.341\n",
            "[W3] RNN Epoch 07 | train_loss 0.9890 acc 0.511 f1 0.501 || val_loss 1.0735 acc 0.383 f1 0.354\n",
            "[W3] RNN Epoch 08 | train_loss 0.9663 acc 0.536 f1 0.526 || val_loss 1.0859 acc 0.364 f1 0.337\n",
            "[W3] RNN Epoch 09 | train_loss 0.9424 acc 0.551 f1 0.543 || val_loss 1.0738 acc 0.366 f1 0.330\n",
            "[W3] RNN Epoch 10 | train_loss 0.9190 acc 0.557 f1 0.547 || val_loss 1.0837 acc 0.379 f1 0.344\n",
            "[W3] RNN Epoch 11 | train_loss 0.8982 acc 0.578 f1 0.568 || val_loss 1.0898 acc 0.356 f1 0.322\n",
            "[W3] RNN Epoch 12 | train_loss 0.8803 acc 0.581 f1 0.571 || val_loss 1.1022 acc 0.372 f1 0.342\n",
            "[W3] RNN Epoch 13 | train_loss 0.8660 acc 0.582 f1 0.573 || val_loss 1.1164 acc 0.352 f1 0.322\n",
            "[W3] RNN Epoch 14 | train_loss 0.8377 acc 0.603 f1 0.593 || val_loss 1.1046 acc 0.366 f1 0.325\n",
            "[W3] RNN Epoch 15 | train_loss 0.8091 acc 0.617 f1 0.608 || val_loss 1.1007 acc 0.387 f1 0.338\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=71\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 930, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0968 acc 0.345 f1 0.297 || val_loss 1.0920 acc 0.393 f1 0.349\n",
            "[W3] GRU Epoch 02 | train_loss 1.0870 acc 0.425 f1 0.422 || val_loss 1.0899 acc 0.368 f1 0.333\n",
            "[W3] GRU Epoch 03 | train_loss 1.0756 acc 0.438 f1 0.438 || val_loss 1.0870 acc 0.372 f1 0.343\n",
            "[W3] GRU Epoch 04 | train_loss 1.0568 acc 0.464 f1 0.464 || val_loss 1.0796 acc 0.383 f1 0.341\n",
            "[W3] GRU Epoch 05 | train_loss 1.0320 acc 0.488 f1 0.485 || val_loss 1.0789 acc 0.379 f1 0.341\n",
            "[W3] GRU Epoch 06 | train_loss 0.9863 acc 0.528 f1 0.526 || val_loss 1.0853 acc 0.374 f1 0.334\n",
            "[W3] GRU Epoch 07 | train_loss 0.9276 acc 0.558 f1 0.552 || val_loss 1.1183 acc 0.364 f1 0.330\n",
            "[W3] GRU Epoch 08 | train_loss 0.8494 acc 0.590 f1 0.582 || val_loss 1.1058 acc 0.381 f1 0.327\n",
            "[W3] GRU Epoch 09 | train_loss 0.7960 acc 0.619 f1 0.612 || val_loss 1.1267 acc 0.389 f1 0.325\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=71\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 987, np.int64(1): 930, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 987, np.int64(2): 987, np.int64(0): 987})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0979 acc 0.335 f1 0.309 || val_loss 1.0958 acc 0.389 f1 0.327\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0926 acc 0.393 f1 0.379 || val_loss 1.0916 acc 0.358 f1 0.320\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0844 acc 0.427 f1 0.425 || val_loss 1.0965 acc 0.327 f1 0.307\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0683 acc 0.447 f1 0.447 || val_loss 1.0984 acc 0.323 f1 0.308\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0311 acc 0.489 f1 0.485 || val_loss 1.0863 acc 0.340 f1 0.311\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9446 acc 0.553 f1 0.546 || val_loss 1.0817 acc 0.407 f1 0.338\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8486 acc 0.589 f1 0.583 || val_loss 1.1149 acc 0.420 f1 0.324\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7914 acc 0.617 f1 0.612 || val_loss 1.1888 acc 0.374 f1 0.329\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7500 acc 0.645 f1 0.639 || val_loss 1.2002 acc 0.385 f1 0.329\n",
            "[W3] LSTM Epoch 10 | train_loss 0.6989 acc 0.656 f1 0.650 || val_loss 1.1947 acc 0.395 f1 0.310\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6677 acc 0.682 f1 0.679 || val_loss 1.2586 acc 0.372 f1 0.309\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6399 acc 0.695 f1 0.692 || val_loss 1.2781 acc 0.383 f1 0.312\n",
            "[W3] LSTM Epoch 13 | train_loss 0.5993 acc 0.715 f1 0.711 || val_loss 1.2870 acc 0.405 f1 0.321\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5661 acc 0.735 f1 0.733 || val_loss 1.3419 acc 0.391 f1 0.321\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  71%|   | 71/100 [33:57<13:04, 27.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=72 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=72\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 931, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1314 acc 0.400 f1 0.398 || val_loss 1.1341 acc 0.350 f1 0.279\n",
            "[W3] ANN Epoch 02 | train_loss 0.9616 acc 0.525 f1 0.517 || val_loss 1.1501 acc 0.358 f1 0.302\n",
            "[W3] ANN Epoch 03 | train_loss 0.8564 acc 0.570 f1 0.562 || val_loss 1.1579 acc 0.366 f1 0.314\n",
            "[W3] ANN Epoch 04 | train_loss 0.7883 acc 0.609 f1 0.603 || val_loss 1.1125 acc 0.393 f1 0.325\n",
            "[W3] ANN Epoch 05 | train_loss 0.7307 acc 0.649 f1 0.644 || val_loss 1.1514 acc 0.397 f1 0.328\n",
            "[W3] ANN Epoch 06 | train_loss 0.6789 acc 0.669 f1 0.665 || val_loss 1.1445 acc 0.407 f1 0.344\n",
            "[W3] ANN Epoch 07 | train_loss 0.6397 acc 0.689 f1 0.687 || val_loss 1.1413 acc 0.409 f1 0.319\n",
            "[W3] ANN Epoch 08 | train_loss 0.6139 acc 0.703 f1 0.702 || val_loss 1.1748 acc 0.432 f1 0.339\n",
            "[W3] ANN Epoch 09 | train_loss 0.5765 acc 0.736 f1 0.735 || val_loss 1.2231 acc 0.416 f1 0.342\n",
            "[W3] ANN Epoch 10 | train_loss 0.5493 acc 0.738 f1 0.737 || val_loss 1.2270 acc 0.409 f1 0.319\n",
            "[W3] ANN Epoch 11 | train_loss 0.5390 acc 0.753 f1 0.752 || val_loss 1.2829 acc 0.403 f1 0.316\n",
            "[W3] ANN Epoch 12 | train_loss 0.5258 acc 0.771 f1 0.770 || val_loss 1.2787 acc 0.399 f1 0.314\n",
            "[W3] ANN Epoch 13 | train_loss 0.5054 acc 0.766 f1 0.765 || val_loss 1.2915 acc 0.440 f1 0.343\n",
            "[W3] ANN Epoch 14 | train_loss 0.5009 acc 0.771 f1 0.770 || val_loss 1.3587 acc 0.391 f1 0.316\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=72\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 931, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0761 acc 0.410 f1 0.411 || val_loss 1.0202 acc 0.438 f1 0.314\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9338 acc 0.557 f1 0.553 || val_loss 1.0429 acc 0.414 f1 0.312\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.7991 acc 0.621 f1 0.618 || val_loss 1.1473 acc 0.358 f1 0.308\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.6980 acc 0.682 f1 0.680 || val_loss 1.1730 acc 0.377 f1 0.311\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6156 acc 0.721 f1 0.719 || val_loss 1.2316 acc 0.370 f1 0.313\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5479 acc 0.762 f1 0.761 || val_loss 1.2957 acc 0.377 f1 0.323\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.4832 acc 0.791 f1 0.791 || val_loss 1.4053 acc 0.372 f1 0.307\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4525 acc 0.801 f1 0.800 || val_loss 1.3904 acc 0.372 f1 0.302\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4112 acc 0.832 f1 0.832 || val_loss 1.4995 acc 0.364 f1 0.304\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3551 acc 0.854 f1 0.854 || val_loss 1.5715 acc 0.393 f1 0.333\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3389 acc 0.867 f1 0.867 || val_loss 1.6501 acc 0.377 f1 0.298\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2759 acc 0.894 f1 0.893 || val_loss 1.8050 acc 0.389 f1 0.308\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2574 acc 0.898 f1 0.898 || val_loss 1.8290 acc 0.407 f1 0.321\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2253 acc 0.915 f1 0.915 || val_loss 1.9437 acc 0.364 f1 0.311\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2214 acc 0.916 f1 0.916 || val_loss 1.9727 acc 0.393 f1 0.317\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2197 acc 0.920 f1 0.919 || val_loss 2.0265 acc 0.354 f1 0.299\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1803 acc 0.930 f1 0.929 || val_loss 2.1522 acc 0.374 f1 0.298\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1585 acc 0.942 f1 0.942 || val_loss 2.2383 acc 0.372 f1 0.301\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=72\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 931, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0978 acc 0.340 f1 0.340 || val_loss 1.0960 acc 0.319 f1 0.288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0813 acc 0.403 f1 0.396 || val_loss 1.0960 acc 0.337 f1 0.315\n",
            "[W3] RNN Epoch 03 | train_loss 1.0626 acc 0.441 f1 0.437 || val_loss 1.0912 acc 0.340 f1 0.315\n",
            "[W3] RNN Epoch 04 | train_loss 1.0438 acc 0.460 f1 0.450 || val_loss 1.0812 acc 0.340 f1 0.303\n",
            "[W3] RNN Epoch 05 | train_loss 1.0253 acc 0.478 f1 0.473 || val_loss 1.0907 acc 0.364 f1 0.332\n",
            "[W3] RNN Epoch 06 | train_loss 0.9982 acc 0.505 f1 0.500 || val_loss 1.0865 acc 0.352 f1 0.323\n",
            "[W3] RNN Epoch 07 | train_loss 0.9730 acc 0.520 f1 0.512 || val_loss 1.1214 acc 0.346 f1 0.321\n",
            "[W3] RNN Epoch 08 | train_loss 0.9508 acc 0.533 f1 0.523 || val_loss 1.0500 acc 0.422 f1 0.363\n",
            "[W3] RNN Epoch 09 | train_loss 0.9267 acc 0.557 f1 0.547 || val_loss 1.1045 acc 0.397 f1 0.364\n",
            "[W3] RNN Epoch 10 | train_loss 0.9030 acc 0.559 f1 0.550 || val_loss 1.0845 acc 0.407 f1 0.366\n",
            "[W3] RNN Epoch 11 | train_loss 0.8777 acc 0.591 f1 0.582 || val_loss 1.1260 acc 0.356 f1 0.329\n",
            "[W3] RNN Epoch 12 | train_loss 0.8601 acc 0.586 f1 0.577 || val_loss 1.0984 acc 0.379 f1 0.340\n",
            "[W3] RNN Epoch 13 | train_loss 0.8275 acc 0.600 f1 0.591 || val_loss 1.1436 acc 0.356 f1 0.328\n",
            "[W3] RNN Epoch 14 | train_loss 0.8098 acc 0.622 f1 0.612 || val_loss 1.1032 acc 0.370 f1 0.325\n",
            "[W3] RNN Epoch 15 | train_loss 0.7883 acc 0.626 f1 0.617 || val_loss 1.1119 acc 0.389 f1 0.330\n",
            "[W3] RNN Epoch 16 | train_loss 0.7681 acc 0.628 f1 0.620 || val_loss 1.1429 acc 0.385 f1 0.347\n",
            "[W3] RNN Epoch 17 | train_loss 0.7335 acc 0.652 f1 0.645 || val_loss 1.1420 acc 0.387 f1 0.348\n",
            "[W3] RNN Epoch 18 | train_loss 0.7096 acc 0.656 f1 0.648 || val_loss 1.1334 acc 0.379 f1 0.327\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=72\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 931, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0982 acc 0.346 f1 0.308 || val_loss 1.0983 acc 0.350 f1 0.321\n",
            "[W3] GRU Epoch 02 | train_loss 1.0917 acc 0.393 f1 0.391 || val_loss 1.1014 acc 0.300 f1 0.289\n",
            "[W3] GRU Epoch 03 | train_loss 1.0829 acc 0.418 f1 0.415 || val_loss 1.0939 acc 0.321 f1 0.300\n",
            "[W3] GRU Epoch 04 | train_loss 1.0689 acc 0.452 f1 0.447 || val_loss 1.0865 acc 0.350 f1 0.317\n",
            "[W3] GRU Epoch 05 | train_loss 1.0450 acc 0.483 f1 0.476 || val_loss 1.0805 acc 0.346 f1 0.309\n",
            "[W3] GRU Epoch 06 | train_loss 1.0030 acc 0.512 f1 0.506 || val_loss 1.1017 acc 0.329 f1 0.300\n",
            "[W3] GRU Epoch 07 | train_loss 0.9362 acc 0.541 f1 0.530 || val_loss 1.1108 acc 0.335 f1 0.306\n",
            "[W3] GRU Epoch 08 | train_loss 0.8582 acc 0.589 f1 0.582 || val_loss 1.1736 acc 0.323 f1 0.293\n",
            "[W3] GRU Epoch 09 | train_loss 0.8053 acc 0.610 f1 0.601 || val_loss 1.1741 acc 0.356 f1 0.310\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=72\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 931, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1022 acc 0.335 f1 0.183 || val_loss 1.0910 acc 0.444 f1 0.250\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0951 acc 0.358 f1 0.314 || val_loss 1.0963 acc 0.364 f1 0.328\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0905 acc 0.403 f1 0.400 || val_loss 1.0994 acc 0.325 f1 0.309\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0806 acc 0.435 f1 0.430 || val_loss 1.1002 acc 0.323 f1 0.310\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0621 acc 0.459 f1 0.450 || val_loss 1.1027 acc 0.321 f1 0.306\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0207 acc 0.502 f1 0.486 || val_loss 1.0912 acc 0.348 f1 0.312\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9362 acc 0.556 f1 0.547 || val_loss 1.0613 acc 0.381 f1 0.315\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8427 acc 0.585 f1 0.577 || val_loss 1.1121 acc 0.403 f1 0.337\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7814 acc 0.618 f1 0.612 || val_loss 1.1952 acc 0.364 f1 0.314\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7354 acc 0.649 f1 0.643 || val_loss 1.1675 acc 0.383 f1 0.322\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7072 acc 0.653 f1 0.650 || val_loss 1.2051 acc 0.372 f1 0.313\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6704 acc 0.672 f1 0.668 || val_loss 1.2091 acc 0.409 f1 0.328\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6317 acc 0.694 f1 0.692 || val_loss 1.2750 acc 0.387 f1 0.327\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6036 acc 0.710 f1 0.708 || val_loss 1.3011 acc 0.399 f1 0.329\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5843 acc 0.718 f1 0.717 || val_loss 1.3100 acc 0.397 f1 0.317\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5515 acc 0.739 f1 0.737 || val_loss 1.3628 acc 0.401 f1 0.324\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  72%|  | 72/100 [34:20<12:01, 25.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=73 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=73\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 932, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1308 acc 0.424 f1 0.416 || val_loss 1.1401 acc 0.313 f1 0.306\n",
            "[W3] ANN Epoch 02 | train_loss 0.9533 acc 0.521 f1 0.512 || val_loss 1.1121 acc 0.333 f1 0.313\n",
            "[W3] ANN Epoch 03 | train_loss 0.8632 acc 0.575 f1 0.566 || val_loss 1.1144 acc 0.333 f1 0.299\n",
            "[W3] ANN Epoch 04 | train_loss 0.8012 acc 0.619 f1 0.614 || val_loss 1.0931 acc 0.412 f1 0.361\n",
            "[W3] ANN Epoch 05 | train_loss 0.7472 acc 0.640 f1 0.635 || val_loss 1.1023 acc 0.412 f1 0.356\n",
            "[W3] ANN Epoch 06 | train_loss 0.6813 acc 0.673 f1 0.670 || val_loss 1.1116 acc 0.416 f1 0.359\n",
            "[W3] ANN Epoch 07 | train_loss 0.6343 acc 0.706 f1 0.703 || val_loss 1.1194 acc 0.453 f1 0.367\n",
            "[W3] ANN Epoch 08 | train_loss 0.6222 acc 0.708 f1 0.707 || val_loss 1.1429 acc 0.434 f1 0.351\n",
            "[W3] ANN Epoch 09 | train_loss 0.5839 acc 0.734 f1 0.733 || val_loss 1.1520 acc 0.451 f1 0.369\n",
            "[W3] ANN Epoch 10 | train_loss 0.5411 acc 0.746 f1 0.744 || val_loss 1.1488 acc 0.469 f1 0.384\n",
            "[W3] ANN Epoch 11 | train_loss 0.5375 acc 0.745 f1 0.744 || val_loss 1.2138 acc 0.442 f1 0.364\n",
            "[W3] ANN Epoch 12 | train_loss 0.5199 acc 0.765 f1 0.765 || val_loss 1.2054 acc 0.457 f1 0.365\n",
            "[W3] ANN Epoch 13 | train_loss 0.4941 acc 0.787 f1 0.786 || val_loss 1.2677 acc 0.447 f1 0.363\n",
            "[W3] ANN Epoch 14 | train_loss 0.4636 acc 0.797 f1 0.796 || val_loss 1.2830 acc 0.449 f1 0.359\n",
            "[W3] ANN Epoch 15 | train_loss 0.4634 acc 0.796 f1 0.796 || val_loss 1.2885 acc 0.432 f1 0.351\n",
            "[W3] ANN Epoch 16 | train_loss 0.4513 acc 0.802 f1 0.802 || val_loss 1.2671 acc 0.453 f1 0.359\n",
            "[W3] ANN Epoch 17 | train_loss 0.4458 acc 0.805 f1 0.805 || val_loss 1.2964 acc 0.444 f1 0.360\n",
            "[W3] ANN Epoch 18 | train_loss 0.4257 acc 0.816 f1 0.815 || val_loss 1.3097 acc 0.430 f1 0.351\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=73\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 932, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0735 acc 0.407 f1 0.409 || val_loss 1.0288 acc 0.407 f1 0.311\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9419 acc 0.535 f1 0.532 || val_loss 1.0531 acc 0.387 f1 0.299\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8008 acc 0.596 f1 0.593 || val_loss 1.1355 acc 0.364 f1 0.308\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.6981 acc 0.668 f1 0.665 || val_loss 1.1916 acc 0.360 f1 0.305\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6411 acc 0.696 f1 0.694 || val_loss 1.2460 acc 0.360 f1 0.299\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5904 acc 0.720 f1 0.718 || val_loss 1.3339 acc 0.350 f1 0.293\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5328 acc 0.752 f1 0.751 || val_loss 1.3066 acc 0.395 f1 0.327\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4727 acc 0.795 f1 0.794 || val_loss 1.3852 acc 0.387 f1 0.303\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4323 acc 0.812 f1 0.811 || val_loss 1.4834 acc 0.393 f1 0.334\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3865 acc 0.840 f1 0.840 || val_loss 1.5648 acc 0.374 f1 0.319\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3501 acc 0.854 f1 0.854 || val_loss 1.6491 acc 0.395 f1 0.325\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3299 acc 0.863 f1 0.863 || val_loss 1.7208 acc 0.385 f1 0.311\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2879 acc 0.891 f1 0.891 || val_loss 1.7534 acc 0.389 f1 0.301\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2530 acc 0.909 f1 0.909 || val_loss 1.9068 acc 0.393 f1 0.302\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2480 acc 0.905 f1 0.904 || val_loss 1.9846 acc 0.405 f1 0.333\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1917 acc 0.936 f1 0.936 || val_loss 2.0944 acc 0.395 f1 0.323\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1736 acc 0.940 f1 0.940 || val_loss 2.2101 acc 0.407 f1 0.334\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=73\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 932, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0961 acc 0.365 f1 0.353 || val_loss 1.1039 acc 0.319 f1 0.311\n",
            "[W3] RNN Epoch 02 | train_loss 1.0799 acc 0.419 f1 0.408 || val_loss 1.1009 acc 0.327 f1 0.312\n",
            "[W3] RNN Epoch 03 | train_loss 1.0634 acc 0.447 f1 0.436 || val_loss 1.0778 acc 0.379 f1 0.351\n",
            "[W3] RNN Epoch 04 | train_loss 1.0464 acc 0.473 f1 0.470 || val_loss 1.0826 acc 0.356 f1 0.336\n",
            "[W3] RNN Epoch 05 | train_loss 1.0262 acc 0.483 f1 0.475 || val_loss 1.0522 acc 0.395 f1 0.351\n",
            "[W3] RNN Epoch 06 | train_loss 1.0032 acc 0.507 f1 0.503 || val_loss 1.0534 acc 0.397 f1 0.361\n",
            "[W3] RNN Epoch 07 | train_loss 0.9761 acc 0.529 f1 0.524 || val_loss 1.0475 acc 0.405 f1 0.369\n",
            "[W3] RNN Epoch 08 | train_loss 0.9506 acc 0.544 f1 0.538 || val_loss 1.0546 acc 0.368 f1 0.341\n",
            "[W3] RNN Epoch 09 | train_loss 0.9282 acc 0.570 f1 0.565 || val_loss 1.0692 acc 0.374 f1 0.352\n",
            "[W3] RNN Epoch 10 | train_loss 0.8985 acc 0.571 f1 0.563 || val_loss 1.0592 acc 0.387 f1 0.357\n",
            "[W3] RNN Epoch 11 | train_loss 0.8747 acc 0.592 f1 0.585 || val_loss 1.0703 acc 0.405 f1 0.383\n",
            "[W3] RNN Epoch 12 | train_loss 0.8479 acc 0.596 f1 0.589 || val_loss 1.0642 acc 0.420 f1 0.387\n",
            "[W3] RNN Epoch 13 | train_loss 0.8144 acc 0.623 f1 0.615 || val_loss 1.0803 acc 0.379 f1 0.354\n",
            "[W3] RNN Epoch 14 | train_loss 0.7935 acc 0.629 f1 0.622 || val_loss 1.0674 acc 0.405 f1 0.364\n",
            "[W3] RNN Epoch 15 | train_loss 0.7625 acc 0.647 f1 0.640 || val_loss 1.0842 acc 0.401 f1 0.365\n",
            "[W3] RNN Epoch 16 | train_loss 0.7367 acc 0.664 f1 0.657 || val_loss 1.0826 acc 0.403 f1 0.352\n",
            "[W3] RNN Epoch 17 | train_loss 0.7124 acc 0.672 f1 0.665 || val_loss 1.1011 acc 0.426 f1 0.359\n",
            "[W3] RNN Epoch 18 | train_loss 0.6961 acc 0.676 f1 0.670 || val_loss 1.1016 acc 0.407 f1 0.356\n",
            "[W3] RNN Epoch 19 | train_loss 0.6709 acc 0.692 f1 0.686 || val_loss 1.1026 acc 0.424 f1 0.365\n",
            "[W3] RNN Epoch 20 | train_loss 0.6521 acc 0.696 f1 0.690 || val_loss 1.1168 acc 0.401 f1 0.351\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=73\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 932, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1016 acc 0.333 f1 0.201 || val_loss 1.0765 acc 0.442 f1 0.296\n",
            "[W3] GRU Epoch 02 | train_loss 1.0902 acc 0.393 f1 0.386 || val_loss 1.0953 acc 0.333 f1 0.324\n",
            "[W3] GRU Epoch 03 | train_loss 1.0768 acc 0.447 f1 0.446 || val_loss 1.0901 acc 0.331 f1 0.315\n",
            "[W3] GRU Epoch 04 | train_loss 1.0584 acc 0.466 f1 0.461 || val_loss 1.0674 acc 0.391 f1 0.349\n",
            "[W3] GRU Epoch 05 | train_loss 1.0270 acc 0.491 f1 0.484 || val_loss 1.0685 acc 0.385 f1 0.342\n",
            "[W3] GRU Epoch 06 | train_loss 0.9801 acc 0.535 f1 0.526 || val_loss 1.0804 acc 0.352 f1 0.326\n",
            "[W3] GRU Epoch 07 | train_loss 0.9097 acc 0.569 f1 0.563 || val_loss 1.0677 acc 0.393 f1 0.328\n",
            "[W3] GRU Epoch 08 | train_loss 0.8458 acc 0.597 f1 0.591 || val_loss 1.1144 acc 0.379 f1 0.317\n",
            "[W3] GRU Epoch 09 | train_loss 0.7970 acc 0.618 f1 0.613 || val_loss 1.1222 acc 0.399 f1 0.332\n",
            "[W3] GRU Epoch 10 | train_loss 0.7632 acc 0.638 f1 0.633 || val_loss 1.1526 acc 0.385 f1 0.325\n",
            "[W3] GRU Epoch 11 | train_loss 0.7227 acc 0.654 f1 0.650 || val_loss 1.1961 acc 0.370 f1 0.318\n",
            "[W3] GRU Epoch 12 | train_loss 0.6945 acc 0.666 f1 0.662 || val_loss 1.1758 acc 0.389 f1 0.319\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=73\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 985, np.int64(1): 932, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 985, np.int64(2): 985, np.int64(0): 985})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0990 acc 0.332 f1 0.177 || val_loss 1.0940 acc 0.432 f1 0.263\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0938 acc 0.379 f1 0.364 || val_loss 1.0917 acc 0.335 f1 0.309\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0835 acc 0.418 f1 0.418 || val_loss 1.0807 acc 0.356 f1 0.331\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0656 acc 0.434 f1 0.430 || val_loss 1.0685 acc 0.360 f1 0.328\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0305 acc 0.460 f1 0.450 || val_loss 1.0502 acc 0.377 f1 0.340\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9624 acc 0.528 f1 0.512 || val_loss 1.0611 acc 0.348 f1 0.302\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8819 acc 0.564 f1 0.554 || val_loss 1.0791 acc 0.391 f1 0.320\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8057 acc 0.615 f1 0.608 || val_loss 1.1186 acc 0.362 f1 0.307\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7575 acc 0.624 f1 0.616 || val_loss 1.1710 acc 0.372 f1 0.300\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7285 acc 0.640 f1 0.634 || val_loss 1.1818 acc 0.374 f1 0.309\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6804 acc 0.672 f1 0.668 || val_loss 1.1999 acc 0.379 f1 0.299\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6633 acc 0.671 f1 0.666 || val_loss 1.2268 acc 0.397 f1 0.317\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6206 acc 0.696 f1 0.693 || val_loss 1.2802 acc 0.407 f1 0.338\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  73%|  | 73/100 [34:43<11:12, 24.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=74 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=74\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 928, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1120 acc 0.408 f1 0.404 || val_loss 1.1655 acc 0.267 f1 0.263\n",
            "[W3] ANN Epoch 02 | train_loss 0.9746 acc 0.508 f1 0.497 || val_loss 1.1590 acc 0.309 f1 0.295\n",
            "[W3] ANN Epoch 03 | train_loss 0.8697 acc 0.571 f1 0.560 || val_loss 1.1417 acc 0.331 f1 0.307\n",
            "[W3] ANN Epoch 04 | train_loss 0.7893 acc 0.615 f1 0.606 || val_loss 1.1389 acc 0.395 f1 0.352\n",
            "[W3] ANN Epoch 05 | train_loss 0.7339 acc 0.636 f1 0.630 || val_loss 1.1509 acc 0.377 f1 0.332\n",
            "[W3] ANN Epoch 06 | train_loss 0.6953 acc 0.658 f1 0.653 || val_loss 1.1649 acc 0.397 f1 0.332\n",
            "[W3] ANN Epoch 07 | train_loss 0.6572 acc 0.680 f1 0.676 || val_loss 1.1396 acc 0.399 f1 0.345\n",
            "[W3] ANN Epoch 08 | train_loss 0.6303 acc 0.702 f1 0.699 || val_loss 1.1737 acc 0.409 f1 0.343\n",
            "[W3] ANN Epoch 09 | train_loss 0.5961 acc 0.707 f1 0.705 || val_loss 1.1701 acc 0.428 f1 0.349\n",
            "[W3] ANN Epoch 10 | train_loss 0.5847 acc 0.724 f1 0.722 || val_loss 1.2069 acc 0.418 f1 0.349\n",
            "[W3] ANN Epoch 11 | train_loss 0.5750 acc 0.726 f1 0.726 || val_loss 1.2088 acc 0.412 f1 0.343\n",
            "[W3] ANN Epoch 12 | train_loss 0.5392 acc 0.752 f1 0.751 || val_loss 1.2205 acc 0.416 f1 0.344\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=74\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 928, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0679 acc 0.415 f1 0.418 || val_loss 1.0294 acc 0.391 f1 0.317\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9215 acc 0.565 f1 0.560 || val_loss 1.0715 acc 0.358 f1 0.277\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.7956 acc 0.623 f1 0.621 || val_loss 1.1187 acc 0.360 f1 0.305\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.6939 acc 0.682 f1 0.678 || val_loss 1.1727 acc 0.383 f1 0.335\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6152 acc 0.705 f1 0.703 || val_loss 1.2052 acc 0.399 f1 0.346\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5608 acc 0.754 f1 0.753 || val_loss 1.3165 acc 0.401 f1 0.347\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5052 acc 0.776 f1 0.775 || val_loss 1.3427 acc 0.391 f1 0.343\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4742 acc 0.790 f1 0.788 || val_loss 1.3863 acc 0.401 f1 0.341\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4482 acc 0.812 f1 0.811 || val_loss 1.4850 acc 0.387 f1 0.340\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4020 acc 0.828 f1 0.827 || val_loss 1.5318 acc 0.401 f1 0.340\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3490 acc 0.856 f1 0.856 || val_loss 1.5649 acc 0.416 f1 0.350\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3383 acc 0.862 f1 0.862 || val_loss 1.6122 acc 0.409 f1 0.348\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2747 acc 0.895 f1 0.895 || val_loss 1.7110 acc 0.424 f1 0.361\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2429 acc 0.901 f1 0.901 || val_loss 1.8201 acc 0.409 f1 0.345\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2445 acc 0.911 f1 0.911 || val_loss 1.8194 acc 0.412 f1 0.345\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2118 acc 0.922 f1 0.922 || val_loss 1.8913 acc 0.412 f1 0.360\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1997 acc 0.925 f1 0.925 || val_loss 2.0528 acc 0.409 f1 0.350\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1449 acc 0.953 f1 0.953 || val_loss 2.1131 acc 0.430 f1 0.377\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1581 acc 0.943 f1 0.943 || val_loss 2.1639 acc 0.428 f1 0.375\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1465 acc 0.951 f1 0.950 || val_loss 2.2579 acc 0.407 f1 0.353\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.1430 acc 0.952 f1 0.952 || val_loss 2.3275 acc 0.414 f1 0.363\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1725 acc 0.937 f1 0.937 || val_loss 2.2459 acc 0.416 f1 0.349\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.1628 acc 0.948 f1 0.948 || val_loss 2.2393 acc 0.426 f1 0.361\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.1127 acc 0.964 f1 0.964 || val_loss 2.3983 acc 0.422 f1 0.361\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.0899 acc 0.970 f1 0.970 || val_loss 2.5770 acc 0.407 f1 0.349\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.0777 acc 0.977 f1 0.977 || val_loss 2.6661 acc 0.426 f1 0.369\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=74\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 928, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0964 acc 0.350 f1 0.336 || val_loss 1.0945 acc 0.377 f1 0.341\n",
            "[W3] RNN Epoch 02 | train_loss 1.0798 acc 0.422 f1 0.419 || val_loss 1.1027 acc 0.313 f1 0.299\n",
            "[W3] RNN Epoch 03 | train_loss 1.0626 acc 0.436 f1 0.430 || val_loss 1.0892 acc 0.340 f1 0.304\n",
            "[W3] RNN Epoch 04 | train_loss 1.0428 acc 0.461 f1 0.457 || val_loss 1.0937 acc 0.358 f1 0.336\n",
            "[W3] RNN Epoch 05 | train_loss 1.0181 acc 0.483 f1 0.478 || val_loss 1.0880 acc 0.366 f1 0.338\n",
            "[W3] RNN Epoch 06 | train_loss 0.9952 acc 0.502 f1 0.498 || val_loss 1.0752 acc 0.362 f1 0.323\n",
            "[W3] RNN Epoch 07 | train_loss 0.9705 acc 0.518 f1 0.513 || val_loss 1.0716 acc 0.377 f1 0.334\n",
            "[W3] RNN Epoch 08 | train_loss 0.9431 acc 0.544 f1 0.537 || val_loss 1.0818 acc 0.405 f1 0.372\n",
            "[W3] RNN Epoch 09 | train_loss 0.9188 acc 0.550 f1 0.540 || val_loss 1.0702 acc 0.385 f1 0.340\n",
            "[W3] RNN Epoch 10 | train_loss 0.8980 acc 0.564 f1 0.556 || val_loss 1.0616 acc 0.403 f1 0.349\n",
            "[W3] RNN Epoch 11 | train_loss 0.8824 acc 0.561 f1 0.550 || val_loss 1.0620 acc 0.399 f1 0.351\n",
            "[W3] RNN Epoch 12 | train_loss 0.8591 acc 0.592 f1 0.581 || val_loss 1.0730 acc 0.403 f1 0.349\n",
            "[W3] RNN Epoch 13 | train_loss 0.8322 acc 0.607 f1 0.598 || val_loss 1.0757 acc 0.414 f1 0.361\n",
            "[W3] RNN Epoch 14 | train_loss 0.8102 acc 0.602 f1 0.592 || val_loss 1.0938 acc 0.403 f1 0.357\n",
            "[W3] RNN Epoch 15 | train_loss 0.7925 acc 0.621 f1 0.612 || val_loss 1.1010 acc 0.414 f1 0.354\n",
            "[W3] RNN Epoch 16 | train_loss 0.7694 acc 0.628 f1 0.618 || val_loss 1.1084 acc 0.432 f1 0.368\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=74\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 928, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0973 acc 0.354 f1 0.331 || val_loss 1.0957 acc 0.352 f1 0.320\n",
            "[W3] GRU Epoch 02 | train_loss 1.0900 acc 0.402 f1 0.399 || val_loss 1.0920 acc 0.335 f1 0.308\n",
            "[W3] GRU Epoch 03 | train_loss 1.0809 acc 0.419 f1 0.416 || val_loss 1.0920 acc 0.323 f1 0.306\n",
            "[W3] GRU Epoch 04 | train_loss 1.0686 acc 0.434 f1 0.431 || val_loss 1.0873 acc 0.340 f1 0.311\n",
            "[W3] GRU Epoch 05 | train_loss 1.0475 acc 0.474 f1 0.475 || val_loss 1.0871 acc 0.331 f1 0.308\n",
            "[W3] GRU Epoch 06 | train_loss 1.0129 acc 0.502 f1 0.490 || val_loss 1.0914 acc 0.358 f1 0.325\n",
            "[W3] GRU Epoch 07 | train_loss 0.9494 acc 0.556 f1 0.547 || val_loss 1.0758 acc 0.360 f1 0.301\n",
            "[W3] GRU Epoch 08 | train_loss 0.8706 acc 0.587 f1 0.582 || val_loss 1.1149 acc 0.383 f1 0.328\n",
            "[W3] GRU Epoch 09 | train_loss 0.8156 acc 0.600 f1 0.594 || val_loss 1.1334 acc 0.370 f1 0.300\n",
            "[W3] GRU Epoch 10 | train_loss 0.7623 acc 0.639 f1 0.634 || val_loss 1.1759 acc 0.358 f1 0.303\n",
            "[W3] GRU Epoch 11 | train_loss 0.7324 acc 0.650 f1 0.646 || val_loss 1.1691 acc 0.368 f1 0.305\n",
            "[W3] GRU Epoch 12 | train_loss 0.6929 acc 0.672 f1 0.669 || val_loss 1.1903 acc 0.377 f1 0.304\n",
            "[W3] GRU Epoch 13 | train_loss 0.6667 acc 0.691 f1 0.688 || val_loss 1.1993 acc 0.399 f1 0.323\n",
            "[W3] GRU Epoch 14 | train_loss 0.6376 acc 0.696 f1 0.694 || val_loss 1.2041 acc 0.422 f1 0.339\n",
            "[W3] GRU Epoch 15 | train_loss 0.6037 acc 0.721 f1 0.718 || val_loss 1.2434 acc 0.414 f1 0.334\n",
            "[W3] GRU Epoch 16 | train_loss 0.5837 acc 0.728 f1 0.726 || val_loss 1.2695 acc 0.393 f1 0.323\n",
            "[W3] GRU Epoch 17 | train_loss 0.5557 acc 0.741 f1 0.740 || val_loss 1.3171 acc 0.389 f1 0.316\n",
            "[W3] GRU Epoch 18 | train_loss 0.5341 acc 0.751 f1 0.750 || val_loss 1.3489 acc 0.383 f1 0.316\n",
            "[W3] GRU Epoch 19 | train_loss 0.5114 acc 0.760 f1 0.759 || val_loss 1.3679 acc 0.387 f1 0.314\n",
            "[W3] GRU Epoch 20 | train_loss 0.4938 acc 0.770 f1 0.769 || val_loss 1.3788 acc 0.416 f1 0.329\n",
            "[W3] GRU Epoch 21 | train_loss 0.4632 acc 0.786 f1 0.786 || val_loss 1.4373 acc 0.385 f1 0.310\n",
            "[W3] GRU Epoch 22 | train_loss 0.4458 acc 0.801 f1 0.800 || val_loss 1.4440 acc 0.399 f1 0.322\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=74\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 928, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0997 acc 0.332 f1 0.226 || val_loss 1.1079 acc 0.202 f1 0.194\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0945 acc 0.381 f1 0.373 || val_loss 1.0980 acc 0.305 f1 0.272\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0870 acc 0.404 f1 0.378 || val_loss 1.0893 acc 0.331 f1 0.297\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0741 acc 0.431 f1 0.416 || val_loss 1.1026 acc 0.282 f1 0.275\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0475 acc 0.464 f1 0.451 || val_loss 1.1218 acc 0.288 f1 0.282\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9840 acc 0.524 f1 0.512 || val_loss 1.0886 acc 0.311 f1 0.266\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8742 acc 0.579 f1 0.570 || val_loss 1.1160 acc 0.374 f1 0.307\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8019 acc 0.605 f1 0.597 || val_loss 1.1982 acc 0.350 f1 0.302\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7469 acc 0.631 f1 0.625 || val_loss 1.1942 acc 0.389 f1 0.323\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7154 acc 0.642 f1 0.636 || val_loss 1.2204 acc 0.374 f1 0.307\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6696 acc 0.669 f1 0.663 || val_loss 1.2620 acc 0.372 f1 0.304\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6464 acc 0.681 f1 0.677 || val_loss 1.2800 acc 0.393 f1 0.327\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6142 acc 0.699 f1 0.696 || val_loss 1.3287 acc 0.391 f1 0.328\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5914 acc 0.712 f1 0.708 || val_loss 1.3535 acc 0.395 f1 0.345\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5716 acc 0.713 f1 0.711 || val_loss 1.3697 acc 0.387 f1 0.325\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5372 acc 0.740 f1 0.738 || val_loss 1.4124 acc 0.401 f1 0.333\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5170 acc 0.751 f1 0.749 || val_loss 1.4868 acc 0.379 f1 0.319\n",
            "[W3] LSTM Epoch 18 | train_loss 0.4952 acc 0.762 f1 0.761 || val_loss 1.5297 acc 0.377 f1 0.314\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4771 acc 0.767 f1 0.766 || val_loss 1.6008 acc 0.387 f1 0.322\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4540 acc 0.778 f1 0.777 || val_loss 1.6891 acc 0.370 f1 0.317\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4454 acc 0.784 f1 0.782 || val_loss 1.6897 acc 0.385 f1 0.320\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4186 acc 0.805 f1 0.804 || val_loss 1.7685 acc 0.395 f1 0.328\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  74%|  | 74/100 [35:14<11:31, 26.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=75 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=75\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 935, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1205 acc 0.411 f1 0.408 || val_loss 1.1795 acc 0.284 f1 0.276\n",
            "[W3] ANN Epoch 02 | train_loss 0.9470 acc 0.525 f1 0.516 || val_loss 1.1599 acc 0.354 f1 0.327\n",
            "[W3] ANN Epoch 03 | train_loss 0.8591 acc 0.582 f1 0.572 || val_loss 1.1297 acc 0.381 f1 0.340\n",
            "[W3] ANN Epoch 04 | train_loss 0.7865 acc 0.616 f1 0.609 || val_loss 1.1077 acc 0.401 f1 0.346\n",
            "[W3] ANN Epoch 05 | train_loss 0.7190 acc 0.659 f1 0.653 || val_loss 1.1726 acc 0.414 f1 0.360\n",
            "[W3] ANN Epoch 06 | train_loss 0.6781 acc 0.673 f1 0.669 || val_loss 1.1482 acc 0.449 f1 0.379\n",
            "[W3] ANN Epoch 07 | train_loss 0.6357 acc 0.695 f1 0.691 || val_loss 1.1767 acc 0.418 f1 0.351\n",
            "[W3] ANN Epoch 08 | train_loss 0.5955 acc 0.720 f1 0.716 || val_loss 1.2007 acc 0.418 f1 0.345\n",
            "[W3] ANN Epoch 09 | train_loss 0.5725 acc 0.730 f1 0.729 || val_loss 1.2230 acc 0.428 f1 0.357\n",
            "[W3] ANN Epoch 10 | train_loss 0.5505 acc 0.747 f1 0.746 || val_loss 1.2498 acc 0.430 f1 0.351\n",
            "[W3] ANN Epoch 11 | train_loss 0.5209 acc 0.765 f1 0.763 || val_loss 1.2929 acc 0.436 f1 0.361\n",
            "[W3] ANN Epoch 12 | train_loss 0.4951 acc 0.779 f1 0.778 || val_loss 1.3437 acc 0.420 f1 0.338\n",
            "[W3] ANN Epoch 13 | train_loss 0.5054 acc 0.765 f1 0.764 || val_loss 1.3007 acc 0.434 f1 0.354\n",
            "[W3] ANN Epoch 14 | train_loss 0.4675 acc 0.788 f1 0.787 || val_loss 1.3349 acc 0.436 f1 0.377\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=75\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 935, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0741 acc 0.404 f1 0.405 || val_loss 1.0424 acc 0.414 f1 0.356\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9322 acc 0.563 f1 0.555 || val_loss 1.0285 acc 0.391 f1 0.318\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8222 acc 0.605 f1 0.599 || val_loss 1.0859 acc 0.389 f1 0.334\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7358 acc 0.644 f1 0.639 || val_loss 1.1432 acc 0.397 f1 0.315\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6564 acc 0.696 f1 0.692 || val_loss 1.1613 acc 0.393 f1 0.327\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6031 acc 0.718 f1 0.714 || val_loss 1.3159 acc 0.377 f1 0.328\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5465 acc 0.747 f1 0.746 || val_loss 1.2992 acc 0.348 f1 0.299\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4994 acc 0.774 f1 0.772 || val_loss 1.4036 acc 0.364 f1 0.312\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4708 acc 0.780 f1 0.780 || val_loss 1.4409 acc 0.358 f1 0.315\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=75\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 935, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0991 acc 0.345 f1 0.340 || val_loss 1.0993 acc 0.329 f1 0.313\n",
            "[W3] RNN Epoch 02 | train_loss 1.0841 acc 0.406 f1 0.405 || val_loss 1.0887 acc 0.372 f1 0.345\n",
            "[W3] RNN Epoch 03 | train_loss 1.0681 acc 0.440 f1 0.439 || val_loss 1.0807 acc 0.370 f1 0.347\n",
            "[W3] RNN Epoch 04 | train_loss 1.0473 acc 0.469 f1 0.458 || val_loss 1.0651 acc 0.393 f1 0.360\n",
            "[W3] RNN Epoch 05 | train_loss 1.0266 acc 0.475 f1 0.472 || val_loss 1.0775 acc 0.370 f1 0.347\n",
            "[W3] RNN Epoch 06 | train_loss 1.0060 acc 0.489 f1 0.482 || val_loss 1.1196 acc 0.337 f1 0.328\n",
            "[W3] RNN Epoch 07 | train_loss 0.9899 acc 0.502 f1 0.488 || val_loss 1.0913 acc 0.366 f1 0.340\n",
            "[W3] RNN Epoch 08 | train_loss 0.9743 acc 0.513 f1 0.504 || val_loss 1.1319 acc 0.323 f1 0.309\n",
            "[W3] RNN Epoch 09 | train_loss 0.9547 acc 0.521 f1 0.507 || val_loss 1.1241 acc 0.358 f1 0.337\n",
            "[W3] RNN Epoch 10 | train_loss 0.9427 acc 0.544 f1 0.535 || val_loss 1.1017 acc 0.379 f1 0.347\n",
            "[W3] RNN Epoch 11 | train_loss 0.9174 acc 0.566 f1 0.557 || val_loss 1.1279 acc 0.381 f1 0.352\n",
            "[W3] RNN Epoch 12 | train_loss 0.9011 acc 0.572 f1 0.560 || val_loss 1.1241 acc 0.381 f1 0.347\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=75\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 935, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0969 acc 0.353 f1 0.327 || val_loss 1.0969 acc 0.325 f1 0.311\n",
            "[W3] GRU Epoch 02 | train_loss 1.0904 acc 0.397 f1 0.397 || val_loss 1.0877 acc 0.356 f1 0.318\n",
            "[W3] GRU Epoch 03 | train_loss 1.0824 acc 0.413 f1 0.414 || val_loss 1.0841 acc 0.360 f1 0.312\n",
            "[W3] GRU Epoch 04 | train_loss 1.0706 acc 0.431 f1 0.432 || val_loss 1.0828 acc 0.356 f1 0.321\n",
            "[W3] GRU Epoch 05 | train_loss 1.0549 acc 0.460 f1 0.455 || val_loss 1.0874 acc 0.337 f1 0.319\n",
            "[W3] GRU Epoch 06 | train_loss 1.0361 acc 0.473 f1 0.464 || val_loss 1.0706 acc 0.368 f1 0.333\n",
            "[W3] GRU Epoch 07 | train_loss 0.9984 acc 0.517 f1 0.510 || val_loss 1.0728 acc 0.364 f1 0.326\n",
            "[W3] GRU Epoch 08 | train_loss 0.9421 acc 0.551 f1 0.544 || val_loss 1.0664 acc 0.377 f1 0.330\n",
            "[W3] GRU Epoch 09 | train_loss 0.8791 acc 0.586 f1 0.578 || val_loss 1.1010 acc 0.350 f1 0.309\n",
            "[W3] GRU Epoch 10 | train_loss 0.8225 acc 0.605 f1 0.596 || val_loss 1.1195 acc 0.374 f1 0.329\n",
            "[W3] GRU Epoch 11 | train_loss 0.7679 acc 0.631 f1 0.624 || val_loss 1.1650 acc 0.379 f1 0.340\n",
            "[W3] GRU Epoch 12 | train_loss 0.7314 acc 0.642 f1 0.635 || val_loss 1.1701 acc 0.377 f1 0.328\n",
            "[W3] GRU Epoch 13 | train_loss 0.7012 acc 0.658 f1 0.653 || val_loss 1.1828 acc 0.391 f1 0.337\n",
            "[W3] GRU Epoch 14 | train_loss 0.6710 acc 0.678 f1 0.674 || val_loss 1.2059 acc 0.385 f1 0.327\n",
            "[W3] GRU Epoch 15 | train_loss 0.6360 acc 0.699 f1 0.692 || val_loss 1.2154 acc 0.385 f1 0.324\n",
            "[W3] GRU Epoch 16 | train_loss 0.6014 acc 0.709 f1 0.706 || val_loss 1.2373 acc 0.379 f1 0.316\n",
            "[W3] GRU Epoch 17 | train_loss 0.5814 acc 0.724 f1 0.720 || val_loss 1.2823 acc 0.397 f1 0.334\n",
            "[W3] GRU Epoch 18 | train_loss 0.5552 acc 0.732 f1 0.729 || val_loss 1.3125 acc 0.393 f1 0.330\n",
            "[W3] GRU Epoch 19 | train_loss 0.5330 acc 0.741 f1 0.740 || val_loss 1.3224 acc 0.399 f1 0.334\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=75\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 935, np.int64(0): 270})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0983 acc 0.347 f1 0.334 || val_loss 1.0982 acc 0.352 f1 0.305\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0941 acc 0.385 f1 0.383 || val_loss 1.0975 acc 0.329 f1 0.305\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0900 acc 0.405 f1 0.398 || val_loss 1.0906 acc 0.358 f1 0.319\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0823 acc 0.428 f1 0.428 || val_loss 1.0991 acc 0.309 f1 0.295\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0724 acc 0.436 f1 0.425 || val_loss 1.0818 acc 0.356 f1 0.318\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0481 acc 0.466 f1 0.452 || val_loss 1.0879 acc 0.360 f1 0.338\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9943 acc 0.516 f1 0.506 || val_loss 1.0770 acc 0.352 f1 0.297\n",
            "[W3] LSTM Epoch 08 | train_loss 0.9116 acc 0.561 f1 0.544 || val_loss 1.0667 acc 0.395 f1 0.312\n",
            "[W3] LSTM Epoch 09 | train_loss 0.8407 acc 0.600 f1 0.592 || val_loss 1.0925 acc 0.372 f1 0.298\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7802 acc 0.620 f1 0.611 || val_loss 1.1049 acc 0.389 f1 0.319\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7443 acc 0.632 f1 0.626 || val_loss 1.1228 acc 0.401 f1 0.315\n",
            "[W3] LSTM Epoch 12 | train_loss 0.7110 acc 0.652 f1 0.645 || val_loss 1.1557 acc 0.401 f1 0.321\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6671 acc 0.686 f1 0.683 || val_loss 1.1959 acc 0.403 f1 0.317\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6417 acc 0.700 f1 0.698 || val_loss 1.2352 acc 0.387 f1 0.313\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  75%|  | 75/100 [35:34<10:16, 24.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=76 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=76\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1462 acc 0.372 f1 0.369 || val_loss 1.1135 acc 0.333 f1 0.303\n",
            "[W3] ANN Epoch 02 | train_loss 0.9658 acc 0.514 f1 0.507 || val_loss 1.0995 acc 0.360 f1 0.313\n",
            "[W3] ANN Epoch 03 | train_loss 0.8781 acc 0.563 f1 0.556 || val_loss 1.0835 acc 0.393 f1 0.321\n",
            "[W3] ANN Epoch 04 | train_loss 0.8121 acc 0.597 f1 0.592 || val_loss 1.0837 acc 0.412 f1 0.339\n",
            "[W3] ANN Epoch 05 | train_loss 0.7263 acc 0.658 f1 0.656 || val_loss 1.0695 acc 0.453 f1 0.377\n",
            "[W3] ANN Epoch 06 | train_loss 0.6984 acc 0.662 f1 0.659 || val_loss 1.0801 acc 0.455 f1 0.381\n",
            "[W3] ANN Epoch 07 | train_loss 0.6940 acc 0.667 f1 0.665 || val_loss 1.0810 acc 0.449 f1 0.352\n",
            "[W3] ANN Epoch 08 | train_loss 0.6311 acc 0.688 f1 0.688 || val_loss 1.1298 acc 0.453 f1 0.379\n",
            "[W3] ANN Epoch 09 | train_loss 0.6266 acc 0.708 f1 0.707 || val_loss 1.1396 acc 0.440 f1 0.351\n",
            "[W3] ANN Epoch 10 | train_loss 0.5813 acc 0.725 f1 0.724 || val_loss 1.1386 acc 0.449 f1 0.350\n",
            "[W3] ANN Epoch 11 | train_loss 0.5737 acc 0.728 f1 0.726 || val_loss 1.1675 acc 0.447 f1 0.350\n",
            "[W3] ANN Epoch 12 | train_loss 0.5482 acc 0.749 f1 0.748 || val_loss 1.1981 acc 0.430 f1 0.349\n",
            "[W3] ANN Epoch 13 | train_loss 0.5749 acc 0.733 f1 0.733 || val_loss 1.1988 acc 0.426 f1 0.332\n",
            "[W3] ANN Epoch 14 | train_loss 0.5288 acc 0.755 f1 0.755 || val_loss 1.2186 acc 0.457 f1 0.364\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=76\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0754 acc 0.403 f1 0.405 || val_loss 1.0403 acc 0.412 f1 0.311\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9392 acc 0.542 f1 0.537 || val_loss 1.0562 acc 0.416 f1 0.305\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8131 acc 0.612 f1 0.609 || val_loss 1.1212 acc 0.385 f1 0.309\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7226 acc 0.664 f1 0.662 || val_loss 1.2010 acc 0.368 f1 0.305\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6561 acc 0.695 f1 0.695 || val_loss 1.2469 acc 0.389 f1 0.327\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6004 acc 0.719 f1 0.718 || val_loss 1.2257 acc 0.403 f1 0.314\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5395 acc 0.757 f1 0.755 || val_loss 1.3250 acc 0.379 f1 0.317\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4922 acc 0.788 f1 0.787 || val_loss 1.3583 acc 0.405 f1 0.332\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4577 acc 0.797 f1 0.796 || val_loss 1.3698 acc 0.409 f1 0.339\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4279 acc 0.817 f1 0.816 || val_loss 1.3869 acc 0.393 f1 0.319\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.4028 acc 0.827 f1 0.827 || val_loss 1.4680 acc 0.418 f1 0.339\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3653 acc 0.854 f1 0.853 || val_loss 1.5110 acc 0.418 f1 0.333\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3134 acc 0.875 f1 0.875 || val_loss 1.5847 acc 0.436 f1 0.338\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2692 acc 0.896 f1 0.896 || val_loss 1.6758 acc 0.422 f1 0.329\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2832 acc 0.885 f1 0.885 || val_loss 1.6421 acc 0.428 f1 0.338\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2898 acc 0.885 f1 0.885 || val_loss 1.7086 acc 0.438 f1 0.348\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.2412 acc 0.913 f1 0.912 || val_loss 1.8153 acc 0.385 f1 0.319\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1955 acc 0.930 f1 0.930 || val_loss 1.8530 acc 0.418 f1 0.323\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1910 acc 0.931 f1 0.931 || val_loss 1.9037 acc 0.412 f1 0.319\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1710 acc 0.942 f1 0.942 || val_loss 2.0109 acc 0.409 f1 0.332\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.1821 acc 0.938 f1 0.938 || val_loss 2.0222 acc 0.416 f1 0.332\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1420 acc 0.956 f1 0.956 || val_loss 2.0588 acc 0.412 f1 0.318\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.1249 acc 0.959 f1 0.959 || val_loss 2.1453 acc 0.424 f1 0.336\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.1375 acc 0.957 f1 0.957 || val_loss 2.2828 acc 0.409 f1 0.339\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=76\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0978 acc 0.353 f1 0.333 || val_loss 1.1062 acc 0.307 f1 0.295\n",
            "[W3] RNN Epoch 02 | train_loss 1.0818 acc 0.415 f1 0.401 || val_loss 1.0844 acc 0.377 f1 0.319\n",
            "[W3] RNN Epoch 03 | train_loss 1.0653 acc 0.435 f1 0.431 || val_loss 1.0952 acc 0.329 f1 0.308\n",
            "[W3] RNN Epoch 04 | train_loss 1.0475 acc 0.458 f1 0.449 || val_loss 1.0751 acc 0.385 f1 0.342\n",
            "[W3] RNN Epoch 05 | train_loss 1.0248 acc 0.488 f1 0.484 || val_loss 1.1024 acc 0.333 f1 0.313\n",
            "[W3] RNN Epoch 06 | train_loss 1.0057 acc 0.499 f1 0.491 || val_loss 1.1094 acc 0.337 f1 0.318\n",
            "[W3] RNN Epoch 07 | train_loss 0.9723 acc 0.527 f1 0.520 || val_loss 1.0929 acc 0.385 f1 0.346\n",
            "[W3] RNN Epoch 08 | train_loss 0.9472 acc 0.537 f1 0.529 || val_loss 1.1057 acc 0.366 f1 0.328\n",
            "[W3] RNN Epoch 09 | train_loss 0.9233 acc 0.558 f1 0.550 || val_loss 1.1227 acc 0.362 f1 0.330\n",
            "[W3] RNN Epoch 10 | train_loss 0.9006 acc 0.574 f1 0.564 || val_loss 1.1012 acc 0.374 f1 0.329\n",
            "[W3] RNN Epoch 11 | train_loss 0.8690 acc 0.594 f1 0.587 || val_loss 1.1064 acc 0.389 f1 0.340\n",
            "[W3] RNN Epoch 12 | train_loss 0.8587 acc 0.599 f1 0.592 || val_loss 1.1176 acc 0.385 f1 0.344\n",
            "[W3] RNN Epoch 13 | train_loss 0.8220 acc 0.623 f1 0.615 || val_loss 1.1481 acc 0.372 f1 0.335\n",
            "[W3] RNN Epoch 14 | train_loss 0.8223 acc 0.621 f1 0.610 || val_loss 1.1154 acc 0.412 f1 0.363\n",
            "[W3] RNN Epoch 15 | train_loss 0.7925 acc 0.625 f1 0.618 || val_loss 1.1406 acc 0.372 f1 0.330\n",
            "[W3] RNN Epoch 16 | train_loss 0.7682 acc 0.641 f1 0.633 || val_loss 1.1334 acc 0.399 f1 0.357\n",
            "[W3] RNN Epoch 17 | train_loss 0.7476 acc 0.648 f1 0.640 || val_loss 1.1412 acc 0.399 f1 0.356\n",
            "[W3] RNN Epoch 18 | train_loss 0.7352 acc 0.653 f1 0.645 || val_loss 1.1621 acc 0.391 f1 0.352\n",
            "[W3] RNN Epoch 19 | train_loss 0.7041 acc 0.677 f1 0.671 || val_loss 1.1799 acc 0.399 f1 0.354\n",
            "[W3] RNN Epoch 20 | train_loss 0.6910 acc 0.671 f1 0.663 || val_loss 1.1766 acc 0.414 f1 0.357\n",
            "[W3] RNN Epoch 21 | train_loss 0.6649 acc 0.685 f1 0.679 || val_loss 1.1837 acc 0.399 f1 0.351\n",
            "[W3] RNN Epoch 22 | train_loss 0.6535 acc 0.693 f1 0.687 || val_loss 1.1959 acc 0.399 f1 0.349\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=76\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0977 acc 0.356 f1 0.301 || val_loss 1.1031 acc 0.296 f1 0.254\n",
            "[W3] GRU Epoch 02 | train_loss 1.0857 acc 0.423 f1 0.413 || val_loss 1.0875 acc 0.344 f1 0.297\n",
            "[W3] GRU Epoch 03 | train_loss 1.0753 acc 0.449 f1 0.447 || val_loss 1.0795 acc 0.335 f1 0.290\n",
            "[W3] GRU Epoch 04 | train_loss 1.0628 acc 0.471 f1 0.468 || val_loss 1.0776 acc 0.327 f1 0.296\n",
            "[W3] GRU Epoch 05 | train_loss 1.0399 acc 0.486 f1 0.485 || val_loss 1.0856 acc 0.323 f1 0.302\n",
            "[W3] GRU Epoch 06 | train_loss 1.0074 acc 0.521 f1 0.514 || val_loss 1.0830 acc 0.350 f1 0.322\n",
            "[W3] GRU Epoch 07 | train_loss 0.9637 acc 0.562 f1 0.554 || val_loss 1.0871 acc 0.352 f1 0.316\n",
            "[W3] GRU Epoch 08 | train_loss 0.8973 acc 0.583 f1 0.575 || val_loss 1.1456 acc 0.356 f1 0.328\n",
            "[W3] GRU Epoch 09 | train_loss 0.8241 acc 0.612 f1 0.604 || val_loss 1.1535 acc 0.374 f1 0.328\n",
            "[W3] GRU Epoch 10 | train_loss 0.7712 acc 0.636 f1 0.630 || val_loss 1.1834 acc 0.383 f1 0.330\n",
            "[W3] GRU Epoch 11 | train_loss 0.7267 acc 0.659 f1 0.653 || val_loss 1.2060 acc 0.407 f1 0.338\n",
            "[W3] GRU Epoch 12 | train_loss 0.6902 acc 0.681 f1 0.676 || val_loss 1.2247 acc 0.397 f1 0.333\n",
            "[W3] GRU Epoch 13 | train_loss 0.6497 acc 0.698 f1 0.694 || val_loss 1.2751 acc 0.414 f1 0.346\n",
            "[W3] GRU Epoch 14 | train_loss 0.6306 acc 0.693 f1 0.691 || val_loss 1.2999 acc 0.405 f1 0.347\n",
            "[W3] GRU Epoch 15 | train_loss 0.6097 acc 0.703 f1 0.698 || val_loss 1.3240 acc 0.389 f1 0.319\n",
            "[W3] GRU Epoch 16 | train_loss 0.5756 acc 0.732 f1 0.730 || val_loss 1.3673 acc 0.395 f1 0.324\n",
            "[W3] GRU Epoch 17 | train_loss 0.5513 acc 0.734 f1 0.732 || val_loss 1.4259 acc 0.391 f1 0.318\n",
            "[W3] GRU Epoch 18 | train_loss 0.5231 acc 0.753 f1 0.752 || val_loss 1.4561 acc 0.409 f1 0.330\n",
            "[W3] GRU Epoch 19 | train_loss 0.5055 acc 0.757 f1 0.756 || val_loss 1.4807 acc 0.405 f1 0.327\n",
            "[W3] GRU Epoch 20 | train_loss 0.4846 acc 0.770 f1 0.769 || val_loss 1.5363 acc 0.395 f1 0.324\n",
            "[W3] GRU Epoch 21 | train_loss 0.4610 acc 0.782 f1 0.781 || val_loss 1.5794 acc 0.405 f1 0.332\n",
            "[W3] GRU Epoch 22 | train_loss 0.4509 acc 0.793 f1 0.792 || val_loss 1.6252 acc 0.385 f1 0.312\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=76\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 930, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0985 acc 0.341 f1 0.314 || val_loss 1.0972 acc 0.356 f1 0.282\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0948 acc 0.381 f1 0.371 || val_loss 1.0957 acc 0.309 f1 0.273\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0907 acc 0.401 f1 0.391 || val_loss 1.0875 acc 0.329 f1 0.293\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0784 acc 0.439 f1 0.432 || val_loss 1.0906 acc 0.309 f1 0.296\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0540 acc 0.472 f1 0.467 || val_loss 1.0845 acc 0.327 f1 0.304\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9982 acc 0.519 f1 0.502 || val_loss 1.0695 acc 0.383 f1 0.348\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9144 acc 0.552 f1 0.544 || val_loss 1.0732 acc 0.377 f1 0.333\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8388 acc 0.598 f1 0.592 || val_loss 1.0887 acc 0.389 f1 0.335\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7796 acc 0.621 f1 0.613 || val_loss 1.0846 acc 0.389 f1 0.312\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7428 acc 0.639 f1 0.633 || val_loss 1.1393 acc 0.387 f1 0.321\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7084 acc 0.648 f1 0.642 || val_loss 1.1312 acc 0.399 f1 0.315\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6928 acc 0.665 f1 0.660 || val_loss 1.1401 acc 0.409 f1 0.323\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6488 acc 0.677 f1 0.673 || val_loss 1.1833 acc 0.409 f1 0.333\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6308 acc 0.685 f1 0.681 || val_loss 1.2035 acc 0.397 f1 0.317\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  76%|  | 76/100 [36:04<10:32, 26.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=77 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=77\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1149 acc 0.415 f1 0.408 || val_loss 1.1744 acc 0.274 f1 0.266\n",
            "[W3] ANN Epoch 02 | train_loss 0.9389 acc 0.519 f1 0.507 || val_loss 1.1357 acc 0.325 f1 0.301\n",
            "[W3] ANN Epoch 03 | train_loss 0.8411 acc 0.577 f1 0.566 || val_loss 1.1313 acc 0.344 f1 0.306\n",
            "[W3] ANN Epoch 04 | train_loss 0.7859 acc 0.609 f1 0.600 || val_loss 1.1352 acc 0.358 f1 0.314\n",
            "[W3] ANN Epoch 05 | train_loss 0.7316 acc 0.641 f1 0.633 || val_loss 1.1200 acc 0.397 f1 0.340\n",
            "[W3] ANN Epoch 06 | train_loss 0.6996 acc 0.660 f1 0.656 || val_loss 1.1322 acc 0.416 f1 0.352\n",
            "[W3] ANN Epoch 07 | train_loss 0.6431 acc 0.687 f1 0.685 || val_loss 1.1342 acc 0.418 f1 0.354\n",
            "[W3] ANN Epoch 08 | train_loss 0.6025 acc 0.713 f1 0.710 || val_loss 1.1875 acc 0.399 f1 0.342\n",
            "[W3] ANN Epoch 09 | train_loss 0.6062 acc 0.709 f1 0.707 || val_loss 1.1472 acc 0.401 f1 0.329\n",
            "[W3] ANN Epoch 10 | train_loss 0.5644 acc 0.734 f1 0.732 || val_loss 1.2487 acc 0.389 f1 0.319\n",
            "[W3] ANN Epoch 11 | train_loss 0.5569 acc 0.737 f1 0.735 || val_loss 1.2289 acc 0.393 f1 0.327\n",
            "[W3] ANN Epoch 12 | train_loss 0.5473 acc 0.740 f1 0.739 || val_loss 1.2176 acc 0.397 f1 0.329\n",
            "[W3] ANN Epoch 13 | train_loss 0.5244 acc 0.755 f1 0.753 || val_loss 1.2588 acc 0.407 f1 0.333\n",
            "[W3] ANN Epoch 14 | train_loss 0.5267 acc 0.749 f1 0.748 || val_loss 1.2607 acc 0.403 f1 0.330\n",
            "[W3] ANN Epoch 15 | train_loss 0.4767 acc 0.779 f1 0.778 || val_loss 1.2949 acc 0.403 f1 0.323\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=77\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0798 acc 0.397 f1 0.399 || val_loss 1.0330 acc 0.426 f1 0.304\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9450 acc 0.535 f1 0.527 || val_loss 1.0273 acc 0.426 f1 0.346\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8123 acc 0.614 f1 0.611 || val_loss 1.0796 acc 0.409 f1 0.352\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7209 acc 0.656 f1 0.652 || val_loss 1.1409 acc 0.412 f1 0.345\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6447 acc 0.685 f1 0.683 || val_loss 1.1600 acc 0.405 f1 0.313\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5765 acc 0.737 f1 0.735 || val_loss 1.2420 acc 0.399 f1 0.327\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5357 acc 0.762 f1 0.760 || val_loss 1.2751 acc 0.409 f1 0.330\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5018 acc 0.773 f1 0.772 || val_loss 1.3503 acc 0.418 f1 0.347\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4362 acc 0.801 f1 0.801 || val_loss 1.4161 acc 0.403 f1 0.330\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3858 acc 0.834 f1 0.834 || val_loss 1.5308 acc 0.395 f1 0.335\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3627 acc 0.849 f1 0.848 || val_loss 1.5996 acc 0.416 f1 0.334\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=77\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0963 acc 0.357 f1 0.344 || val_loss 1.0900 acc 0.362 f1 0.327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0790 acc 0.425 f1 0.420 || val_loss 1.0896 acc 0.335 f1 0.313\n",
            "[W3] RNN Epoch 03 | train_loss 1.0641 acc 0.449 f1 0.445 || val_loss 1.0916 acc 0.342 f1 0.322\n",
            "[W3] RNN Epoch 04 | train_loss 1.0488 acc 0.460 f1 0.456 || val_loss 1.0780 acc 0.391 f1 0.356\n",
            "[W3] RNN Epoch 05 | train_loss 1.0262 acc 0.484 f1 0.484 || val_loss 1.0958 acc 0.364 f1 0.340\n",
            "[W3] RNN Epoch 06 | train_loss 1.0026 acc 0.510 f1 0.506 || val_loss 1.0848 acc 0.362 f1 0.333\n",
            "[W3] RNN Epoch 07 | train_loss 0.9804 acc 0.522 f1 0.516 || val_loss 1.0681 acc 0.389 f1 0.348\n",
            "[W3] RNN Epoch 08 | train_loss 0.9529 acc 0.535 f1 0.530 || val_loss 1.0981 acc 0.348 f1 0.321\n",
            "[W3] RNN Epoch 09 | train_loss 0.9238 acc 0.559 f1 0.550 || val_loss 1.0887 acc 0.389 f1 0.350\n",
            "[W3] RNN Epoch 10 | train_loss 0.9071 acc 0.564 f1 0.558 || val_loss 1.1098 acc 0.362 f1 0.327\n",
            "[W3] RNN Epoch 11 | train_loss 0.8747 acc 0.591 f1 0.583 || val_loss 1.1192 acc 0.383 f1 0.343\n",
            "[W3] RNN Epoch 12 | train_loss 0.8523 acc 0.596 f1 0.588 || val_loss 1.1285 acc 0.350 f1 0.316\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=77\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1005 acc 0.349 f1 0.283 || val_loss 1.1080 acc 0.261 f1 0.243\n",
            "[W3] GRU Epoch 02 | train_loss 1.0907 acc 0.413 f1 0.397 || val_loss 1.1003 acc 0.296 f1 0.282\n",
            "[W3] GRU Epoch 03 | train_loss 1.0835 acc 0.421 f1 0.404 || val_loss 1.0881 acc 0.346 f1 0.310\n",
            "[W3] GRU Epoch 04 | train_loss 1.0696 acc 0.438 f1 0.438 || val_loss 1.0993 acc 0.307 f1 0.295\n",
            "[W3] GRU Epoch 05 | train_loss 1.0481 acc 0.465 f1 0.461 || val_loss 1.1026 acc 0.333 f1 0.315\n",
            "[W3] GRU Epoch 06 | train_loss 1.0074 acc 0.511 f1 0.502 || val_loss 1.1164 acc 0.354 f1 0.338\n",
            "[W3] GRU Epoch 07 | train_loss 0.9358 acc 0.546 f1 0.539 || val_loss 1.0912 acc 0.395 f1 0.353\n",
            "[W3] GRU Epoch 08 | train_loss 0.8687 acc 0.581 f1 0.576 || val_loss 1.1379 acc 0.397 f1 0.359\n",
            "[W3] GRU Epoch 09 | train_loss 0.8132 acc 0.608 f1 0.603 || val_loss 1.1019 acc 0.383 f1 0.344\n",
            "[W3] GRU Epoch 10 | train_loss 0.7642 acc 0.624 f1 0.618 || val_loss 1.0970 acc 0.395 f1 0.324\n",
            "[W3] GRU Epoch 11 | train_loss 0.7326 acc 0.651 f1 0.647 || val_loss 1.1200 acc 0.426 f1 0.348\n",
            "[W3] GRU Epoch 12 | train_loss 0.7022 acc 0.659 f1 0.654 || val_loss 1.1322 acc 0.403 f1 0.322\n",
            "[W3] GRU Epoch 13 | train_loss 0.6571 acc 0.687 f1 0.684 || val_loss 1.1675 acc 0.428 f1 0.349\n",
            "[W3] GRU Epoch 14 | train_loss 0.6345 acc 0.692 f1 0.689 || val_loss 1.2084 acc 0.405 f1 0.332\n",
            "[W3] GRU Epoch 15 | train_loss 0.6019 acc 0.705 f1 0.702 || val_loss 1.2326 acc 0.420 f1 0.343\n",
            "[W3] GRU Epoch 16 | train_loss 0.5794 acc 0.721 f1 0.719 || val_loss 1.2887 acc 0.414 f1 0.348\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=77\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 984, np.int64(1): 929, np.int64(0): 274})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 984, np.int64(2): 984, np.int64(0): 984})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1026 acc 0.334 f1 0.170 || val_loss 1.1192 acc 0.126 f1 0.081\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0951 acc 0.376 f1 0.327 || val_loss 1.0974 acc 0.340 f1 0.312\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0890 acc 0.399 f1 0.399 || val_loss 1.0962 acc 0.329 f1 0.313\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0795 acc 0.421 f1 0.419 || val_loss 1.0963 acc 0.327 f1 0.315\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0561 acc 0.462 f1 0.456 || val_loss 1.0856 acc 0.329 f1 0.306\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0089 acc 0.516 f1 0.510 || val_loss 1.0884 acc 0.340 f1 0.314\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9172 acc 0.560 f1 0.551 || val_loss 1.1248 acc 0.350 f1 0.315\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8434 acc 0.590 f1 0.582 || val_loss 1.1180 acc 0.368 f1 0.317\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7837 acc 0.616 f1 0.609 || val_loss 1.1253 acc 0.366 f1 0.309\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7365 acc 0.632 f1 0.627 || val_loss 1.1654 acc 0.364 f1 0.302\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6952 acc 0.665 f1 0.658 || val_loss 1.1911 acc 0.407 f1 0.346\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6572 acc 0.687 f1 0.683 || val_loss 1.2451 acc 0.389 f1 0.334\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6211 acc 0.686 f1 0.682 || val_loss 1.2659 acc 0.401 f1 0.341\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5972 acc 0.703 f1 0.700 || val_loss 1.3082 acc 0.395 f1 0.320\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5697 acc 0.719 f1 0.716 || val_loss 1.3338 acc 0.412 f1 0.337\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5424 acc 0.725 f1 0.723 || val_loss 1.3945 acc 0.391 f1 0.329\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5206 acc 0.742 f1 0.740 || val_loss 1.4017 acc 0.397 f1 0.321\n",
            "[W3] LSTM Epoch 18 | train_loss 0.4962 acc 0.758 f1 0.757 || val_loss 1.4583 acc 0.391 f1 0.321\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4776 acc 0.766 f1 0.764 || val_loss 1.5067 acc 0.389 f1 0.313\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  77%|  | 77/100 [36:26<09:36, 25.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=78 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=78\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 932, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1471 acc 0.391 f1 0.387 || val_loss 1.1445 acc 0.323 f1 0.315\n",
            "[W3] ANN Epoch 02 | train_loss 0.9826 acc 0.504 f1 0.495 || val_loss 1.1313 acc 0.337 f1 0.307\n",
            "[W3] ANN Epoch 03 | train_loss 0.9126 acc 0.547 f1 0.536 || val_loss 1.1178 acc 0.356 f1 0.317\n",
            "[W3] ANN Epoch 04 | train_loss 0.8267 acc 0.609 f1 0.604 || val_loss 1.1052 acc 0.385 f1 0.330\n",
            "[W3] ANN Epoch 05 | train_loss 0.7569 acc 0.634 f1 0.630 || val_loss 1.1135 acc 0.389 f1 0.337\n",
            "[W3] ANN Epoch 06 | train_loss 0.7272 acc 0.659 f1 0.655 || val_loss 1.1203 acc 0.405 f1 0.343\n",
            "[W3] ANN Epoch 07 | train_loss 0.6882 acc 0.669 f1 0.666 || val_loss 1.1353 acc 0.420 f1 0.372\n",
            "[W3] ANN Epoch 08 | train_loss 0.6302 acc 0.702 f1 0.700 || val_loss 1.1362 acc 0.428 f1 0.365\n",
            "[W3] ANN Epoch 09 | train_loss 0.6305 acc 0.699 f1 0.697 || val_loss 1.1769 acc 0.401 f1 0.346\n",
            "[W3] ANN Epoch 10 | train_loss 0.6156 acc 0.715 f1 0.713 || val_loss 1.1756 acc 0.420 f1 0.361\n",
            "[W3] ANN Epoch 11 | train_loss 0.5722 acc 0.730 f1 0.729 || val_loss 1.1779 acc 0.418 f1 0.364\n",
            "[W3] ANN Epoch 12 | train_loss 0.5659 acc 0.739 f1 0.738 || val_loss 1.1969 acc 0.405 f1 0.351\n",
            "[W3] ANN Epoch 13 | train_loss 0.5323 acc 0.755 f1 0.754 || val_loss 1.2237 acc 0.409 f1 0.353\n",
            "[W3] ANN Epoch 14 | train_loss 0.5241 acc 0.749 f1 0.749 || val_loss 1.2305 acc 0.432 f1 0.381\n",
            "[W3] ANN Epoch 15 | train_loss 0.5412 acc 0.764 f1 0.763 || val_loss 1.2000 acc 0.442 f1 0.382\n",
            "[W3] ANN Epoch 16 | train_loss 0.5146 acc 0.773 f1 0.772 || val_loss 1.2239 acc 0.422 f1 0.354\n",
            "[W3] ANN Epoch 17 | train_loss 0.4783 acc 0.784 f1 0.782 || val_loss 1.2505 acc 0.473 f1 0.401\n",
            "[W3] ANN Epoch 18 | train_loss 0.5074 acc 0.785 f1 0.784 || val_loss 1.2779 acc 0.412 f1 0.340\n",
            "[W3] ANN Epoch 19 | train_loss 0.4855 acc 0.775 f1 0.775 || val_loss 1.2596 acc 0.432 f1 0.358\n",
            "[W3] ANN Epoch 20 | train_loss 0.4722 acc 0.795 f1 0.794 || val_loss 1.2780 acc 0.438 f1 0.367\n",
            "[W3] ANN Epoch 21 | train_loss 0.4511 acc 0.793 f1 0.793 || val_loss 1.3017 acc 0.453 f1 0.373\n",
            "[W3] ANN Epoch 22 | train_loss 0.4321 acc 0.812 f1 0.811 || val_loss 1.3399 acc 0.418 f1 0.351\n",
            "[W3] ANN Epoch 23 | train_loss 0.4428 acc 0.809 f1 0.808 || val_loss 1.3184 acc 0.444 f1 0.373\n",
            "[W3] ANN Epoch 24 | train_loss 0.4146 acc 0.818 f1 0.818 || val_loss 1.3637 acc 0.440 f1 0.359\n",
            "[W3] ANN Epoch 25 | train_loss 0.4126 acc 0.828 f1 0.828 || val_loss 1.3825 acc 0.440 f1 0.365\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=78\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 932, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0739 acc 0.398 f1 0.399 || val_loss 1.0315 acc 0.444 f1 0.337\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9365 acc 0.543 f1 0.535 || val_loss 1.0432 acc 0.422 f1 0.322\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8162 acc 0.601 f1 0.598 || val_loss 1.0920 acc 0.399 f1 0.312\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7229 acc 0.645 f1 0.642 || val_loss 1.1988 acc 0.379 f1 0.311\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6416 acc 0.691 f1 0.688 || val_loss 1.2114 acc 0.409 f1 0.335\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5881 acc 0.713 f1 0.710 || val_loss 1.2762 acc 0.407 f1 0.327\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5226 acc 0.766 f1 0.765 || val_loss 1.3542 acc 0.418 f1 0.333\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4887 acc 0.782 f1 0.781 || val_loss 1.4245 acc 0.403 f1 0.323\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4457 acc 0.796 f1 0.796 || val_loss 1.4566 acc 0.416 f1 0.327\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=78\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 932, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0995 acc 0.358 f1 0.320 || val_loss 1.1094 acc 0.272 f1 0.269\n",
            "[W3] RNN Epoch 02 | train_loss 1.0831 acc 0.418 f1 0.407 || val_loss 1.0979 acc 0.307 f1 0.293\n",
            "[W3] RNN Epoch 03 | train_loss 1.0685 acc 0.443 f1 0.437 || val_loss 1.1038 acc 0.321 f1 0.307\n",
            "[W3] RNN Epoch 04 | train_loss 1.0554 acc 0.465 f1 0.461 || val_loss 1.0970 acc 0.325 f1 0.303\n",
            "[W3] RNN Epoch 05 | train_loss 1.0408 acc 0.471 f1 0.468 || val_loss 1.0998 acc 0.344 f1 0.309\n",
            "[W3] RNN Epoch 06 | train_loss 1.0238 acc 0.493 f1 0.486 || val_loss 1.0899 acc 0.387 f1 0.342\n",
            "[W3] RNN Epoch 07 | train_loss 1.0096 acc 0.505 f1 0.499 || val_loss 1.1192 acc 0.356 f1 0.329\n",
            "[W3] RNN Epoch 08 | train_loss 0.9855 acc 0.535 f1 0.530 || val_loss 1.1076 acc 0.366 f1 0.330\n",
            "[W3] RNN Epoch 09 | train_loss 0.9663 acc 0.539 f1 0.531 || val_loss 1.1064 acc 0.372 f1 0.337\n",
            "[W3] RNN Epoch 10 | train_loss 0.9498 acc 0.546 f1 0.538 || val_loss 1.1215 acc 0.358 f1 0.326\n",
            "[W3] RNN Epoch 11 | train_loss 0.9251 acc 0.557 f1 0.548 || val_loss 1.0934 acc 0.364 f1 0.311\n",
            "[W3] RNN Epoch 12 | train_loss 0.9094 acc 0.568 f1 0.561 || val_loss 1.0911 acc 0.374 f1 0.334\n",
            "[W3] RNN Epoch 13 | train_loss 0.8748 acc 0.606 f1 0.597 || val_loss 1.1142 acc 0.370 f1 0.324\n",
            "[W3] RNN Epoch 14 | train_loss 0.8534 acc 0.602 f1 0.593 || val_loss 1.1097 acc 0.379 f1 0.340\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=78\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 932, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1019 acc 0.341 f1 0.272 || val_loss 1.0813 acc 0.414 f1 0.310\n",
            "[W3] GRU Epoch 02 | train_loss 1.0930 acc 0.363 f1 0.339 || val_loss 1.0904 acc 0.377 f1 0.322\n",
            "[W3] GRU Epoch 03 | train_loss 1.0839 acc 0.418 f1 0.418 || val_loss 1.0878 acc 0.337 f1 0.287\n",
            "[W3] GRU Epoch 04 | train_loss 1.0720 acc 0.428 f1 0.427 || val_loss 1.1007 acc 0.300 f1 0.283\n",
            "[W3] GRU Epoch 05 | train_loss 1.0522 acc 0.472 f1 0.469 || val_loss 1.1258 acc 0.274 f1 0.267\n",
            "[W3] GRU Epoch 06 | train_loss 1.0160 acc 0.503 f1 0.497 || val_loss 1.1160 acc 0.302 f1 0.280\n",
            "[W3] GRU Epoch 07 | train_loss 0.9557 acc 0.540 f1 0.531 || val_loss 1.0765 acc 0.352 f1 0.286\n",
            "[W3] GRU Epoch 08 | train_loss 0.8809 acc 0.582 f1 0.574 || val_loss 1.0945 acc 0.368 f1 0.303\n",
            "[W3] GRU Epoch 09 | train_loss 0.8159 acc 0.611 f1 0.605 || val_loss 1.1118 acc 0.397 f1 0.319\n",
            "[W3] GRU Epoch 10 | train_loss 0.7708 acc 0.638 f1 0.632 || val_loss 1.1514 acc 0.368 f1 0.306\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=78\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 932, np.int64(0): 272})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0993 acc 0.336 f1 0.227 || val_loss 1.0941 acc 0.414 f1 0.277\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0952 acc 0.364 f1 0.321 || val_loss 1.0952 acc 0.381 f1 0.337\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0896 acc 0.421 f1 0.421 || val_loss 1.1046 acc 0.305 f1 0.294\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0786 acc 0.451 f1 0.439 || val_loss 1.0978 acc 0.340 f1 0.315\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0531 acc 0.474 f1 0.470 || val_loss 1.1259 acc 0.302 f1 0.293\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9876 acc 0.518 f1 0.508 || val_loss 1.1069 acc 0.356 f1 0.317\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9026 acc 0.557 f1 0.548 || val_loss 1.1052 acc 0.360 f1 0.306\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8428 acc 0.603 f1 0.597 || val_loss 1.1074 acc 0.368 f1 0.309\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7848 acc 0.626 f1 0.622 || val_loss 1.1774 acc 0.356 f1 0.317\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7595 acc 0.637 f1 0.633 || val_loss 1.1427 acc 0.397 f1 0.334\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  78%|  | 78/100 [36:48<08:48, 24.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=79 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=79\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 929, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1323 acc 0.409 f1 0.408 || val_loss 1.2150 acc 0.253 f1 0.254\n",
            "[W3] ANN Epoch 02 | train_loss 0.9588 acc 0.517 f1 0.509 || val_loss 1.1861 acc 0.317 f1 0.309\n",
            "[W3] ANN Epoch 03 | train_loss 0.8695 acc 0.574 f1 0.565 || val_loss 1.1654 acc 0.362 f1 0.342\n",
            "[W3] ANN Epoch 04 | train_loss 0.8247 acc 0.589 f1 0.580 || val_loss 1.1367 acc 0.364 f1 0.330\n",
            "[W3] ANN Epoch 05 | train_loss 0.7723 acc 0.617 f1 0.611 || val_loss 1.1212 acc 0.377 f1 0.340\n",
            "[W3] ANN Epoch 06 | train_loss 0.7210 acc 0.650 f1 0.644 || val_loss 1.1361 acc 0.387 f1 0.341\n",
            "[W3] ANN Epoch 07 | train_loss 0.6851 acc 0.661 f1 0.656 || val_loss 1.1520 acc 0.379 f1 0.323\n",
            "[W3] ANN Epoch 08 | train_loss 0.6566 acc 0.689 f1 0.685 || val_loss 1.1481 acc 0.391 f1 0.333\n",
            "[W3] ANN Epoch 09 | train_loss 0.6360 acc 0.689 f1 0.686 || val_loss 1.1998 acc 0.383 f1 0.332\n",
            "[W3] ANN Epoch 10 | train_loss 0.6062 acc 0.716 f1 0.713 || val_loss 1.1889 acc 0.405 f1 0.347\n",
            "[W3] ANN Epoch 11 | train_loss 0.5866 acc 0.716 f1 0.715 || val_loss 1.1896 acc 0.393 f1 0.331\n",
            "[W3] ANN Epoch 12 | train_loss 0.5680 acc 0.724 f1 0.722 || val_loss 1.2130 acc 0.401 f1 0.337\n",
            "[W3] ANN Epoch 13 | train_loss 0.5766 acc 0.727 f1 0.726 || val_loss 1.1759 acc 0.424 f1 0.358\n",
            "[W3] ANN Epoch 14 | train_loss 0.5588 acc 0.736 f1 0.734 || val_loss 1.2086 acc 0.407 f1 0.342\n",
            "[W3] ANN Epoch 15 | train_loss 0.5248 acc 0.752 f1 0.750 || val_loss 1.2362 acc 0.424 f1 0.359\n",
            "[W3] ANN Epoch 16 | train_loss 0.5025 acc 0.777 f1 0.775 || val_loss 1.2504 acc 0.436 f1 0.362\n",
            "[W3] ANN Epoch 17 | train_loss 0.4902 acc 0.774 f1 0.774 || val_loss 1.3109 acc 0.409 f1 0.350\n",
            "[W3] ANN Epoch 18 | train_loss 0.5071 acc 0.761 f1 0.760 || val_loss 1.2956 acc 0.389 f1 0.319\n",
            "[W3] ANN Epoch 19 | train_loss 0.4857 acc 0.789 f1 0.788 || val_loss 1.2688 acc 0.436 f1 0.373\n",
            "[W3] ANN Epoch 20 | train_loss 0.4821 acc 0.777 f1 0.776 || val_loss 1.3177 acc 0.399 f1 0.346\n",
            "[W3] ANN Epoch 21 | train_loss 0.4650 acc 0.800 f1 0.800 || val_loss 1.3513 acc 0.414 f1 0.353\n",
            "[W3] ANN Epoch 22 | train_loss 0.4521 acc 0.797 f1 0.797 || val_loss 1.3641 acc 0.420 f1 0.359\n",
            "[W3] ANN Epoch 23 | train_loss 0.4712 acc 0.797 f1 0.796 || val_loss 1.3850 acc 0.418 f1 0.357\n",
            "[W3] ANN Epoch 24 | train_loss 0.4426 acc 0.805 f1 0.805 || val_loss 1.3756 acc 0.428 f1 0.366\n",
            "[W3] ANN Epoch 25 | train_loss 0.4491 acc 0.811 f1 0.810 || val_loss 1.3709 acc 0.453 f1 0.379\n",
            "[W3] ANN Epoch 26 | train_loss 0.4267 acc 0.815 f1 0.814 || val_loss 1.4204 acc 0.442 f1 0.363\n",
            "[W3] ANN Epoch 27 | train_loss 0.4074 acc 0.828 f1 0.827 || val_loss 1.4443 acc 0.424 f1 0.345\n",
            "[W3] ANN Epoch 28 | train_loss 0.4242 acc 0.816 f1 0.815 || val_loss 1.4404 acc 0.449 f1 0.364\n",
            "[W3] ANN Epoch 29 | train_loss 0.3901 acc 0.839 f1 0.839 || val_loss 1.4356 acc 0.430 f1 0.359\n",
            "[W3] ANN Epoch 30 | train_loss 0.4221 acc 0.818 f1 0.817 || val_loss 1.4927 acc 0.420 f1 0.351\n",
            "[W3] ANN Epoch 31 | train_loss 0.4040 acc 0.837 f1 0.836 || val_loss 1.5000 acc 0.420 f1 0.355\n",
            "[W3] ANN Epoch 32 | train_loss 0.3906 acc 0.831 f1 0.831 || val_loss 1.4624 acc 0.426 f1 0.356\n",
            "[W3] ANN Epoch 33 | train_loss 0.3785 acc 0.842 f1 0.841 || val_loss 1.5139 acc 0.424 f1 0.362\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=79\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 929, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0731 acc 0.404 f1 0.399 || val_loss 1.0341 acc 0.424 f1 0.330\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9310 acc 0.547 f1 0.542 || val_loss 1.0506 acc 0.397 f1 0.313\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8112 acc 0.595 f1 0.591 || val_loss 1.1198 acc 0.385 f1 0.309\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7191 acc 0.653 f1 0.650 || val_loss 1.1520 acc 0.399 f1 0.323\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6522 acc 0.689 f1 0.686 || val_loss 1.2356 acc 0.389 f1 0.329\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5909 acc 0.713 f1 0.711 || val_loss 1.2730 acc 0.414 f1 0.347\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5483 acc 0.739 f1 0.737 || val_loss 1.3138 acc 0.414 f1 0.338\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5122 acc 0.765 f1 0.765 || val_loss 1.3371 acc 0.430 f1 0.346\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4932 acc 0.780 f1 0.778 || val_loss 1.3878 acc 0.418 f1 0.337\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4301 acc 0.814 f1 0.814 || val_loss 1.4761 acc 0.412 f1 0.339\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3853 acc 0.829 f1 0.828 || val_loss 1.5048 acc 0.416 f1 0.317\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3681 acc 0.839 f1 0.838 || val_loss 1.6591 acc 0.383 f1 0.330\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3755 acc 0.837 f1 0.836 || val_loss 1.6479 acc 0.412 f1 0.336\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3133 acc 0.873 f1 0.872 || val_loss 1.6958 acc 0.405 f1 0.327\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=79\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 929, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0986 acc 0.348 f1 0.329 || val_loss 1.0936 acc 0.360 f1 0.335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0845 acc 0.409 f1 0.404 || val_loss 1.0920 acc 0.333 f1 0.309\n",
            "[W3] RNN Epoch 03 | train_loss 1.0683 acc 0.437 f1 0.432 || val_loss 1.0878 acc 0.350 f1 0.327\n",
            "[W3] RNN Epoch 04 | train_loss 1.0506 acc 0.453 f1 0.447 || val_loss 1.0806 acc 0.346 f1 0.323\n",
            "[W3] RNN Epoch 05 | train_loss 1.0317 acc 0.469 f1 0.464 || val_loss 1.0742 acc 0.352 f1 0.321\n",
            "[W3] RNN Epoch 06 | train_loss 1.0115 acc 0.487 f1 0.483 || val_loss 1.0882 acc 0.342 f1 0.323\n",
            "[W3] RNN Epoch 07 | train_loss 0.9895 acc 0.521 f1 0.515 || val_loss 1.0771 acc 0.342 f1 0.321\n",
            "[W3] RNN Epoch 08 | train_loss 0.9691 acc 0.523 f1 0.514 || val_loss 1.0600 acc 0.383 f1 0.347\n",
            "[W3] RNN Epoch 09 | train_loss 0.9445 acc 0.545 f1 0.536 || val_loss 1.0641 acc 0.379 f1 0.344\n",
            "[W3] RNN Epoch 10 | train_loss 0.9158 acc 0.558 f1 0.546 || val_loss 1.0765 acc 0.358 f1 0.325\n",
            "[W3] RNN Epoch 11 | train_loss 0.8965 acc 0.568 f1 0.559 || val_loss 1.1016 acc 0.374 f1 0.349\n",
            "[W3] RNN Epoch 12 | train_loss 0.8728 acc 0.585 f1 0.574 || val_loss 1.0526 acc 0.399 f1 0.344\n",
            "[W3] RNN Epoch 13 | train_loss 0.8517 acc 0.595 f1 0.585 || val_loss 1.0577 acc 0.405 f1 0.352\n",
            "[W3] RNN Epoch 14 | train_loss 0.8257 acc 0.601 f1 0.591 || val_loss 1.0724 acc 0.405 f1 0.363\n",
            "[W3] RNN Epoch 15 | train_loss 0.8079 acc 0.614 f1 0.604 || val_loss 1.0929 acc 0.412 f1 0.376\n",
            "[W3] RNN Epoch 16 | train_loss 0.7854 acc 0.623 f1 0.612 || val_loss 1.0874 acc 0.418 f1 0.360\n",
            "[W3] RNN Epoch 17 | train_loss 0.7675 acc 0.633 f1 0.625 || val_loss 1.0941 acc 0.397 f1 0.358\n",
            "[W3] RNN Epoch 18 | train_loss 0.7468 acc 0.646 f1 0.638 || val_loss 1.1173 acc 0.403 f1 0.364\n",
            "[W3] RNN Epoch 19 | train_loss 0.7210 acc 0.659 f1 0.649 || val_loss 1.1105 acc 0.428 f1 0.383\n",
            "[W3] RNN Epoch 20 | train_loss 0.7011 acc 0.674 f1 0.666 || val_loss 1.1448 acc 0.403 f1 0.361\n",
            "[W3] RNN Epoch 21 | train_loss 0.6736 acc 0.680 f1 0.673 || val_loss 1.1511 acc 0.407 f1 0.364\n",
            "[W3] RNN Epoch 22 | train_loss 0.6699 acc 0.685 f1 0.678 || val_loss 1.1472 acc 0.418 f1 0.369\n",
            "[W3] RNN Epoch 23 | train_loss 0.6527 acc 0.683 f1 0.675 || val_loss 1.1535 acc 0.414 f1 0.362\n",
            "[W3] RNN Epoch 24 | train_loss 0.6193 acc 0.707 f1 0.701 || val_loss 1.1565 acc 0.414 f1 0.361\n",
            "[W3] RNN Epoch 25 | train_loss 0.6054 acc 0.708 f1 0.703 || val_loss 1.1757 acc 0.438 f1 0.390\n",
            "[W3] RNN Epoch 26 | train_loss 0.5973 acc 0.709 f1 0.704 || val_loss 1.1954 acc 0.422 f1 0.353\n",
            "[W3] RNN Epoch 27 | train_loss 0.5777 acc 0.729 f1 0.725 || val_loss 1.2065 acc 0.449 f1 0.391\n",
            "[W3] RNN Epoch 28 | train_loss 0.5741 acc 0.716 f1 0.712 || val_loss 1.2418 acc 0.440 f1 0.363\n",
            "[W3] RNN Epoch 29 | train_loss 0.5688 acc 0.723 f1 0.719 || val_loss 1.2262 acc 0.444 f1 0.381\n",
            "[W3] RNN Epoch 30 | train_loss 0.5428 acc 0.740 f1 0.736 || val_loss 1.2496 acc 0.453 f1 0.388\n",
            "[W3] RNN Epoch 31 | train_loss 0.5244 acc 0.742 f1 0.738 || val_loss 1.2802 acc 0.444 f1 0.377\n",
            "[W3] RNN Epoch 32 | train_loss 0.5187 acc 0.744 f1 0.741 || val_loss 1.3113 acc 0.444 f1 0.361\n",
            "[W3] RNN Epoch 33 | train_loss 0.5036 acc 0.757 f1 0.753 || val_loss 1.3240 acc 0.453 f1 0.381\n",
            "[W3] RNN Epoch 34 | train_loss 0.5063 acc 0.751 f1 0.748 || val_loss 1.3439 acc 0.442 f1 0.373\n",
            "[W3] RNN Epoch 35 | train_loss 0.4884 acc 0.765 f1 0.762 || val_loss 1.3781 acc 0.444 f1 0.381\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=79\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 929, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0983 acc 0.354 f1 0.329 || val_loss 1.1045 acc 0.278 f1 0.279\n",
            "[W3] GRU Epoch 02 | train_loss 1.0890 acc 0.402 f1 0.399 || val_loss 1.0905 acc 0.368 f1 0.332\n",
            "[W3] GRU Epoch 03 | train_loss 1.0813 acc 0.417 f1 0.414 || val_loss 1.0918 acc 0.366 f1 0.342\n",
            "[W3] GRU Epoch 04 | train_loss 1.0689 acc 0.442 f1 0.441 || val_loss 1.0756 acc 0.381 f1 0.344\n",
            "[W3] GRU Epoch 05 | train_loss 1.0503 acc 0.478 f1 0.476 || val_loss 1.0769 acc 0.381 f1 0.348\n",
            "[W3] GRU Epoch 06 | train_loss 1.0187 acc 0.508 f1 0.503 || val_loss 1.0799 acc 0.374 f1 0.339\n",
            "[W3] GRU Epoch 07 | train_loss 0.9690 acc 0.536 f1 0.527 || val_loss 1.0878 acc 0.391 f1 0.337\n",
            "[W3] GRU Epoch 08 | train_loss 0.9013 acc 0.573 f1 0.565 || val_loss 1.0948 acc 0.407 f1 0.338\n",
            "[W3] GRU Epoch 09 | train_loss 0.8495 acc 0.597 f1 0.591 || val_loss 1.1480 acc 0.383 f1 0.319\n",
            "[W3] GRU Epoch 10 | train_loss 0.7883 acc 0.624 f1 0.618 || val_loss 1.1470 acc 0.395 f1 0.329\n",
            "[W3] GRU Epoch 11 | train_loss 0.7486 acc 0.641 f1 0.636 || val_loss 1.2131 acc 0.385 f1 0.333\n",
            "[W3] GRU Epoch 12 | train_loss 0.7170 acc 0.662 f1 0.656 || val_loss 1.2296 acc 0.385 f1 0.319\n",
            "[W3] GRU Epoch 13 | train_loss 0.6897 acc 0.667 f1 0.662 || val_loss 1.2469 acc 0.374 f1 0.313\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=79\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 929, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1008 acc 0.337 f1 0.187 || val_loss 1.1109 acc 0.160 f1 0.142\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0954 acc 0.367 f1 0.360 || val_loss 1.1002 acc 0.307 f1 0.297\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0906 acc 0.406 f1 0.393 || val_loss 1.0926 acc 0.340 f1 0.318\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0810 acc 0.426 f1 0.420 || val_loss 1.0888 acc 0.337 f1 0.320\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0648 acc 0.442 f1 0.438 || val_loss 1.1028 acc 0.317 f1 0.309\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0338 acc 0.489 f1 0.479 || val_loss 1.0879 acc 0.364 f1 0.340\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9647 acc 0.533 f1 0.517 || val_loss 1.1348 acc 0.325 f1 0.308\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8735 acc 0.575 f1 0.566 || val_loss 1.1183 acc 0.360 f1 0.327\n",
            "[W3] LSTM Epoch 09 | train_loss 0.8128 acc 0.609 f1 0.602 || val_loss 1.1041 acc 0.393 f1 0.338\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7584 acc 0.637 f1 0.632 || val_loss 1.1186 acc 0.407 f1 0.358\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7204 acc 0.654 f1 0.649 || val_loss 1.1480 acc 0.385 f1 0.327\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6772 acc 0.676 f1 0.672 || val_loss 1.1788 acc 0.368 f1 0.315\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6471 acc 0.692 f1 0.689 || val_loss 1.2043 acc 0.399 f1 0.336\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6173 acc 0.707 f1 0.705 || val_loss 1.2335 acc 0.395 f1 0.329\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5862 acc 0.727 f1 0.725 || val_loss 1.2734 acc 0.374 f1 0.315\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5551 acc 0.737 f1 0.735 || val_loss 1.3042 acc 0.377 f1 0.325\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5306 acc 0.751 f1 0.750 || val_loss 1.3343 acc 0.381 f1 0.317\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5110 acc 0.767 f1 0.766 || val_loss 1.3735 acc 0.387 f1 0.305\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  79%|  | 79/100 [37:19<09:13, 26.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=80 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=80\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1337 acc 0.403 f1 0.400 || val_loss 1.1357 acc 0.300 f1 0.288\n",
            "[W3] ANN Epoch 02 | train_loss 0.9579 acc 0.530 f1 0.521 || val_loss 1.1254 acc 0.356 f1 0.328\n",
            "[W3] ANN Epoch 03 | train_loss 0.8803 acc 0.572 f1 0.563 || val_loss 1.1139 acc 0.385 f1 0.341\n",
            "[W3] ANN Epoch 04 | train_loss 0.8084 acc 0.609 f1 0.602 || val_loss 1.1007 acc 0.372 f1 0.324\n",
            "[W3] ANN Epoch 05 | train_loss 0.7476 acc 0.638 f1 0.633 || val_loss 1.1121 acc 0.385 f1 0.331\n",
            "[W3] ANN Epoch 06 | train_loss 0.7067 acc 0.663 f1 0.659 || val_loss 1.1360 acc 0.401 f1 0.353\n",
            "[W3] ANN Epoch 07 | train_loss 0.6814 acc 0.675 f1 0.672 || val_loss 1.1206 acc 0.377 f1 0.310\n",
            "[W3] ANN Epoch 08 | train_loss 0.6492 acc 0.688 f1 0.685 || val_loss 1.1383 acc 0.393 f1 0.323\n",
            "[W3] ANN Epoch 09 | train_loss 0.6230 acc 0.702 f1 0.700 || val_loss 1.1386 acc 0.405 f1 0.336\n",
            "[W3] ANN Epoch 10 | train_loss 0.5961 acc 0.718 f1 0.717 || val_loss 1.1771 acc 0.416 f1 0.335\n",
            "[W3] ANN Epoch 11 | train_loss 0.6028 acc 0.723 f1 0.721 || val_loss 1.1714 acc 0.405 f1 0.341\n",
            "[W3] ANN Epoch 12 | train_loss 0.5654 acc 0.741 f1 0.740 || val_loss 1.1667 acc 0.442 f1 0.354\n",
            "[W3] ANN Epoch 13 | train_loss 0.5581 acc 0.756 f1 0.756 || val_loss 1.2025 acc 0.412 f1 0.333\n",
            "[W3] ANN Epoch 14 | train_loss 0.5483 acc 0.755 f1 0.754 || val_loss 1.2083 acc 0.422 f1 0.344\n",
            "[W3] ANN Epoch 15 | train_loss 0.5146 acc 0.766 f1 0.766 || val_loss 1.2479 acc 0.397 f1 0.325\n",
            "[W3] ANN Epoch 16 | train_loss 0.5057 acc 0.770 f1 0.769 || val_loss 1.2239 acc 0.420 f1 0.338\n",
            "[W3] ANN Epoch 17 | train_loss 0.5151 acc 0.762 f1 0.761 || val_loss 1.2347 acc 0.430 f1 0.355\n",
            "[W3] ANN Epoch 18 | train_loss 0.4911 acc 0.769 f1 0.768 || val_loss 1.2625 acc 0.430 f1 0.333\n",
            "[W3] ANN Epoch 19 | train_loss 0.4795 acc 0.797 f1 0.797 || val_loss 1.3134 acc 0.399 f1 0.327\n",
            "[W3] ANN Epoch 20 | train_loss 0.4513 acc 0.803 f1 0.802 || val_loss 1.3173 acc 0.430 f1 0.346\n",
            "[W3] ANN Epoch 21 | train_loss 0.4859 acc 0.785 f1 0.783 || val_loss 1.3076 acc 0.432 f1 0.346\n",
            "[W3] ANN Epoch 22 | train_loss 0.4315 acc 0.812 f1 0.811 || val_loss 1.3466 acc 0.403 f1 0.330\n",
            "[W3] ANN Epoch 23 | train_loss 0.4333 acc 0.812 f1 0.812 || val_loss 1.3445 acc 0.409 f1 0.338\n",
            "[W3] ANN Epoch 24 | train_loss 0.4195 acc 0.823 f1 0.822 || val_loss 1.3741 acc 0.412 f1 0.350\n",
            "[W3] ANN Epoch 25 | train_loss 0.3923 acc 0.836 f1 0.836 || val_loss 1.4068 acc 0.424 f1 0.341\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=80\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0659 acc 0.410 f1 0.412 || val_loss 1.0277 acc 0.447 f1 0.352\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9409 acc 0.529 f1 0.521 || val_loss 1.0382 acc 0.420 f1 0.351\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8186 acc 0.611 f1 0.608 || val_loss 1.0962 acc 0.407 f1 0.342\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7357 acc 0.645 f1 0.643 || val_loss 1.1461 acc 0.395 f1 0.323\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6472 acc 0.708 f1 0.704 || val_loss 1.2000 acc 0.438 f1 0.363\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5874 acc 0.738 f1 0.738 || val_loss 1.2619 acc 0.409 f1 0.340\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5530 acc 0.753 f1 0.752 || val_loss 1.2579 acc 0.416 f1 0.340\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4866 acc 0.786 f1 0.785 || val_loss 1.3255 acc 0.412 f1 0.345\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4646 acc 0.801 f1 0.801 || val_loss 1.3835 acc 0.409 f1 0.335\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4092 acc 0.821 f1 0.820 || val_loss 1.4890 acc 0.409 f1 0.340\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3855 acc 0.832 f1 0.832 || val_loss 1.4870 acc 0.395 f1 0.311\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3391 acc 0.858 f1 0.857 || val_loss 1.5861 acc 0.418 f1 0.336\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.3216 acc 0.871 f1 0.871 || val_loss 1.6303 acc 0.420 f1 0.324\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=80\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0982 acc 0.353 f1 0.342 || val_loss 1.1039 acc 0.296 f1 0.285\n",
            "[W3] RNN Epoch 02 | train_loss 1.0792 acc 0.421 f1 0.421 || val_loss 1.1030 acc 0.311 f1 0.287\n",
            "[W3] RNN Epoch 03 | train_loss 1.0644 acc 0.433 f1 0.421 || val_loss 1.0827 acc 0.348 f1 0.304\n",
            "[W3] RNN Epoch 04 | train_loss 1.0446 acc 0.460 f1 0.449 || val_loss 1.0900 acc 0.346 f1 0.308\n",
            "[W3] RNN Epoch 05 | train_loss 1.0248 acc 0.477 f1 0.473 || val_loss 1.1035 acc 0.319 f1 0.293\n",
            "[W3] RNN Epoch 06 | train_loss 0.9961 acc 0.509 f1 0.498 || val_loss 1.0977 acc 0.342 f1 0.308\n",
            "[W3] RNN Epoch 07 | train_loss 0.9808 acc 0.518 f1 0.513 || val_loss 1.0668 acc 0.362 f1 0.311\n",
            "[W3] RNN Epoch 08 | train_loss 0.9555 acc 0.536 f1 0.527 || val_loss 1.0972 acc 0.331 f1 0.307\n",
            "[W3] RNN Epoch 09 | train_loss 0.9335 acc 0.557 f1 0.548 || val_loss 1.0849 acc 0.360 f1 0.329\n",
            "[W3] RNN Epoch 10 | train_loss 0.9033 acc 0.562 f1 0.553 || val_loss 1.0952 acc 0.342 f1 0.305\n",
            "[W3] RNN Epoch 11 | train_loss 0.8877 acc 0.587 f1 0.579 || val_loss 1.1001 acc 0.356 f1 0.317\n",
            "[W3] RNN Epoch 12 | train_loss 0.8586 acc 0.600 f1 0.591 || val_loss 1.0879 acc 0.374 f1 0.323\n",
            "[W3] RNN Epoch 13 | train_loss 0.8343 acc 0.606 f1 0.598 || val_loss 1.1365 acc 0.337 f1 0.301\n",
            "[W3] RNN Epoch 14 | train_loss 0.8183 acc 0.604 f1 0.594 || val_loss 1.1112 acc 0.362 f1 0.316\n",
            "[W3] RNN Epoch 15 | train_loss 0.8079 acc 0.612 f1 0.603 || val_loss 1.1178 acc 0.383 f1 0.341\n",
            "[W3] RNN Epoch 16 | train_loss 0.7820 acc 0.624 f1 0.616 || val_loss 1.1298 acc 0.360 f1 0.320\n",
            "[W3] RNN Epoch 17 | train_loss 0.7456 acc 0.649 f1 0.640 || val_loss 1.1365 acc 0.385 f1 0.332\n",
            "[W3] RNN Epoch 18 | train_loss 0.7265 acc 0.655 f1 0.648 || val_loss 1.1490 acc 0.385 f1 0.331\n",
            "[W3] RNN Epoch 19 | train_loss 0.7221 acc 0.651 f1 0.641 || val_loss 1.1362 acc 0.399 f1 0.339\n",
            "[W3] RNN Epoch 20 | train_loss 0.6972 acc 0.663 f1 0.656 || val_loss 1.1728 acc 0.383 f1 0.336\n",
            "[W3] RNN Epoch 21 | train_loss 0.6762 acc 0.669 f1 0.662 || val_loss 1.1770 acc 0.389 f1 0.334\n",
            "[W3] RNN Epoch 22 | train_loss 0.6683 acc 0.676 f1 0.669 || val_loss 1.1799 acc 0.399 f1 0.330\n",
            "[W3] RNN Epoch 23 | train_loss 0.6455 acc 0.685 f1 0.677 || val_loss 1.1978 acc 0.397 f1 0.325\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=80\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0994 acc 0.335 f1 0.245 || val_loss 1.0959 acc 0.362 f1 0.311\n",
            "[W3] GRU Epoch 02 | train_loss 1.0915 acc 0.387 f1 0.371 || val_loss 1.0908 acc 0.337 f1 0.284\n",
            "[W3] GRU Epoch 03 | train_loss 1.0843 acc 0.413 f1 0.409 || val_loss 1.0864 acc 0.340 f1 0.290\n",
            "[W3] GRU Epoch 04 | train_loss 1.0753 acc 0.428 f1 0.423 || val_loss 1.0977 acc 0.278 f1 0.262\n",
            "[W3] GRU Epoch 05 | train_loss 1.0633 acc 0.453 f1 0.453 || val_loss 1.0915 acc 0.327 f1 0.301\n",
            "[W3] GRU Epoch 06 | train_loss 1.0411 acc 0.482 f1 0.478 || val_loss 1.0934 acc 0.331 f1 0.303\n",
            "[W3] GRU Epoch 07 | train_loss 0.9992 acc 0.516 f1 0.510 || val_loss 1.0683 acc 0.377 f1 0.319\n",
            "[W3] GRU Epoch 08 | train_loss 0.9298 acc 0.560 f1 0.554 || val_loss 1.1190 acc 0.358 f1 0.314\n",
            "[W3] GRU Epoch 09 | train_loss 0.8720 acc 0.583 f1 0.576 || val_loss 1.1596 acc 0.352 f1 0.329\n",
            "[W3] GRU Epoch 10 | train_loss 0.8192 acc 0.613 f1 0.607 || val_loss 1.1364 acc 0.374 f1 0.329\n",
            "[W3] GRU Epoch 11 | train_loss 0.7816 acc 0.627 f1 0.622 || val_loss 1.1786 acc 0.364 f1 0.321\n",
            "[W3] GRU Epoch 12 | train_loss 0.7340 acc 0.658 f1 0.653 || val_loss 1.1821 acc 0.364 f1 0.326\n",
            "[W3] GRU Epoch 13 | train_loss 0.6939 acc 0.677 f1 0.672 || val_loss 1.1692 acc 0.391 f1 0.333\n",
            "[W3] GRU Epoch 14 | train_loss 0.6661 acc 0.694 f1 0.692 || val_loss 1.2276 acc 0.374 f1 0.325\n",
            "[W3] GRU Epoch 15 | train_loss 0.6302 acc 0.708 f1 0.706 || val_loss 1.2467 acc 0.387 f1 0.326\n",
            "[W3] GRU Epoch 16 | train_loss 0.6019 acc 0.717 f1 0.714 || val_loss 1.3075 acc 0.387 f1 0.339\n",
            "[W3] GRU Epoch 17 | train_loss 0.5739 acc 0.734 f1 0.732 || val_loss 1.3178 acc 0.381 f1 0.330\n",
            "[W3] GRU Epoch 18 | train_loss 0.5691 acc 0.737 f1 0.734 || val_loss 1.3191 acc 0.391 f1 0.316\n",
            "[W3] GRU Epoch 19 | train_loss 0.5155 acc 0.762 f1 0.760 || val_loss 1.3798 acc 0.385 f1 0.334\n",
            "[W3] GRU Epoch 20 | train_loss 0.4936 acc 0.775 f1 0.774 || val_loss 1.4162 acc 0.397 f1 0.326\n",
            "[W3] GRU Epoch 21 | train_loss 0.4727 acc 0.791 f1 0.790 || val_loss 1.4398 acc 0.399 f1 0.322\n",
            "[W3] GRU Epoch 22 | train_loss 0.4530 acc 0.795 f1 0.794 || val_loss 1.5081 acc 0.389 f1 0.311\n",
            "[W3] GRU Epoch 23 | train_loss 0.4268 acc 0.808 f1 0.807 || val_loss 1.5325 acc 0.385 f1 0.306\n",
            "[W3] GRU Epoch 24 | train_loss 0.4001 acc 0.818 f1 0.817 || val_loss 1.6073 acc 0.387 f1 0.314\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=80\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 983, np.int64(1): 931, np.int64(0): 273})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 983, np.int64(2): 983, np.int64(0): 983})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1014 acc 0.333 f1 0.168 || val_loss 1.0953 acc 0.447 f1 0.226\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0956 acc 0.355 f1 0.286 || val_loss 1.0956 acc 0.337 f1 0.268\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0876 acc 0.398 f1 0.377 || val_loss 1.0937 acc 0.296 f1 0.283\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0765 acc 0.424 f1 0.413 || val_loss 1.0996 acc 0.300 f1 0.289\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0557 acc 0.465 f1 0.454 || val_loss 1.1016 acc 0.319 f1 0.311\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0074 acc 0.504 f1 0.490 || val_loss 1.0831 acc 0.362 f1 0.327\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9231 acc 0.544 f1 0.532 || val_loss 1.0557 acc 0.418 f1 0.341\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8542 acc 0.578 f1 0.568 || val_loss 1.1089 acc 0.383 f1 0.337\n",
            "[W3] LSTM Epoch 09 | train_loss 0.8095 acc 0.606 f1 0.598 || val_loss 1.0918 acc 0.383 f1 0.304\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7562 acc 0.634 f1 0.627 || val_loss 1.1379 acc 0.385 f1 0.302\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7245 acc 0.644 f1 0.639 || val_loss 1.1648 acc 0.364 f1 0.302\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6915 acc 0.664 f1 0.658 || val_loss 1.1731 acc 0.377 f1 0.281\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6547 acc 0.679 f1 0.674 || val_loss 1.2179 acc 0.381 f1 0.300\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6343 acc 0.690 f1 0.687 || val_loss 1.2404 acc 0.391 f1 0.306\n",
            "[W3] LSTM Epoch 15 | train_loss 0.6148 acc 0.702 f1 0.699 || val_loss 1.2644 acc 0.407 f1 0.306\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  80%|  | 80/100 [37:48<09:01, 27.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=81 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=81\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1180 acc 0.401 f1 0.399 || val_loss 1.1491 acc 0.282 f1 0.268\n",
            "[W3] ANN Epoch 02 | train_loss 0.9779 acc 0.508 f1 0.501 || val_loss 1.1322 acc 0.346 f1 0.305\n",
            "[W3] ANN Epoch 03 | train_loss 0.8806 acc 0.579 f1 0.573 || val_loss 1.1035 acc 0.368 f1 0.315\n",
            "[W3] ANN Epoch 04 | train_loss 0.7803 acc 0.632 f1 0.626 || val_loss 1.1247 acc 0.393 f1 0.337\n",
            "[W3] ANN Epoch 05 | train_loss 0.7376 acc 0.648 f1 0.646 || val_loss 1.1287 acc 0.387 f1 0.313\n",
            "[W3] ANN Epoch 06 | train_loss 0.6843 acc 0.682 f1 0.678 || val_loss 1.1177 acc 0.424 f1 0.341\n",
            "[W3] ANN Epoch 07 | train_loss 0.6406 acc 0.702 f1 0.701 || val_loss 1.1542 acc 0.426 f1 0.352\n",
            "[W3] ANN Epoch 08 | train_loss 0.5944 acc 0.730 f1 0.730 || val_loss 1.1589 acc 0.449 f1 0.349\n",
            "[W3] ANN Epoch 09 | train_loss 0.5760 acc 0.732 f1 0.730 || val_loss 1.1852 acc 0.449 f1 0.364\n",
            "[W3] ANN Epoch 10 | train_loss 0.5409 acc 0.757 f1 0.756 || val_loss 1.1992 acc 0.463 f1 0.359\n",
            "[W3] ANN Epoch 11 | train_loss 0.5097 acc 0.775 f1 0.775 || val_loss 1.2612 acc 0.449 f1 0.364\n",
            "[W3] ANN Epoch 12 | train_loss 0.5078 acc 0.772 f1 0.771 || val_loss 1.3183 acc 0.453 f1 0.367\n",
            "[W3] ANN Epoch 13 | train_loss 0.4712 acc 0.795 f1 0.795 || val_loss 1.3053 acc 0.455 f1 0.358\n",
            "[W3] ANN Epoch 14 | train_loss 0.4483 acc 0.807 f1 0.807 || val_loss 1.3327 acc 0.467 f1 0.378\n",
            "[W3] ANN Epoch 15 | train_loss 0.4436 acc 0.817 f1 0.817 || val_loss 1.3190 acc 0.451 f1 0.348\n",
            "[W3] ANN Epoch 16 | train_loss 0.4252 acc 0.819 f1 0.819 || val_loss 1.3638 acc 0.430 f1 0.345\n",
            "[W3] ANN Epoch 17 | train_loss 0.3895 acc 0.837 f1 0.836 || val_loss 1.3943 acc 0.449 f1 0.364\n",
            "[W3] ANN Epoch 18 | train_loss 0.3935 acc 0.827 f1 0.827 || val_loss 1.4110 acc 0.481 f1 0.384\n",
            "[W3] ANN Epoch 19 | train_loss 0.3756 acc 0.849 f1 0.849 || val_loss 1.4329 acc 0.463 f1 0.364\n",
            "[W3] ANN Epoch 20 | train_loss 0.3737 acc 0.847 f1 0.847 || val_loss 1.4130 acc 0.442 f1 0.364\n",
            "[W3] ANN Epoch 21 | train_loss 0.3687 acc 0.848 f1 0.848 || val_loss 1.4702 acc 0.453 f1 0.387\n",
            "[W3] ANN Epoch 22 | train_loss 0.3419 acc 0.859 f1 0.859 || val_loss 1.4095 acc 0.459 f1 0.373\n",
            "[W3] ANN Epoch 23 | train_loss 0.3222 acc 0.873 f1 0.873 || val_loss 1.5089 acc 0.481 f1 0.400\n",
            "[W3] ANN Epoch 24 | train_loss 0.3194 acc 0.873 f1 0.873 || val_loss 1.5510 acc 0.469 f1 0.379\n",
            "[W3] ANN Epoch 25 | train_loss 0.2884 acc 0.888 f1 0.888 || val_loss 1.5769 acc 0.457 f1 0.379\n",
            "[W3] ANN Epoch 26 | train_loss 0.2935 acc 0.880 f1 0.880 || val_loss 1.5888 acc 0.467 f1 0.373\n",
            "[W3] ANN Epoch 27 | train_loss 0.3119 acc 0.874 f1 0.874 || val_loss 1.5965 acc 0.449 f1 0.373\n",
            "[W3] ANN Epoch 28 | train_loss 0.2966 acc 0.881 f1 0.881 || val_loss 1.6108 acc 0.481 f1 0.401\n",
            "[W3] ANN Epoch 29 | train_loss 0.2812 acc 0.891 f1 0.891 || val_loss 1.6105 acc 0.471 f1 0.387\n",
            "[W3] ANN Epoch 30 | train_loss 0.2812 acc 0.888 f1 0.888 || val_loss 1.7014 acc 0.442 f1 0.364\n",
            "[W3] ANN Epoch 31 | train_loss 0.2731 acc 0.888 f1 0.888 || val_loss 1.6786 acc 0.463 f1 0.381\n",
            "[W3] ANN Epoch 32 | train_loss 0.2900 acc 0.885 f1 0.884 || val_loss 1.6797 acc 0.434 f1 0.362\n",
            "[W3] ANN Epoch 33 | train_loss 0.2620 acc 0.899 f1 0.899 || val_loss 1.6235 acc 0.469 f1 0.374\n",
            "[W3] ANN Epoch 34 | train_loss 0.2939 acc 0.883 f1 0.882 || val_loss 1.6196 acc 0.459 f1 0.377\n",
            "[W3] ANN Epoch 35 | train_loss 0.2681 acc 0.895 f1 0.895 || val_loss 1.6342 acc 0.469 f1 0.375\n",
            "[W3] ANN Epoch 36 | train_loss 0.2457 acc 0.904 f1 0.904 || val_loss 1.6833 acc 0.469 f1 0.374\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=81\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0761 acc 0.401 f1 0.403 || val_loss 1.0486 acc 0.391 f1 0.303\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9497 acc 0.543 f1 0.537 || val_loss 1.0821 acc 0.385 f1 0.324\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8373 acc 0.598 f1 0.592 || val_loss 1.1305 acc 0.401 f1 0.339\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7405 acc 0.655 f1 0.653 || val_loss 1.1899 acc 0.393 f1 0.344\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6817 acc 0.675 f1 0.673 || val_loss 1.1837 acc 0.389 f1 0.336\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.6179 acc 0.724 f1 0.723 || val_loss 1.2493 acc 0.387 f1 0.338\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5843 acc 0.738 f1 0.735 || val_loss 1.3196 acc 0.348 f1 0.311\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.5226 acc 0.768 f1 0.766 || val_loss 1.3243 acc 0.374 f1 0.331\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4871 acc 0.803 f1 0.803 || val_loss 1.3274 acc 0.442 f1 0.367\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.4520 acc 0.810 f1 0.809 || val_loss 1.3641 acc 0.414 f1 0.354\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.4123 acc 0.822 f1 0.822 || val_loss 1.4154 acc 0.432 f1 0.368\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3915 acc 0.843 f1 0.843 || val_loss 1.4746 acc 0.428 f1 0.364\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.4020 acc 0.838 f1 0.837 || val_loss 1.4799 acc 0.389 f1 0.328\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.3416 acc 0.872 f1 0.872 || val_loss 1.5060 acc 0.428 f1 0.368\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.3401 acc 0.866 f1 0.865 || val_loss 1.5082 acc 0.449 f1 0.374\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.2938 acc 0.893 f1 0.893 || val_loss 1.5910 acc 0.432 f1 0.355\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.2757 acc 0.895 f1 0.895 || val_loss 1.6664 acc 0.405 f1 0.346\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.2554 acc 0.914 f1 0.914 || val_loss 1.6454 acc 0.440 f1 0.369\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.2555 acc 0.907 f1 0.906 || val_loss 1.7563 acc 0.409 f1 0.358\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.2384 acc 0.919 f1 0.918 || val_loss 1.7067 acc 0.420 f1 0.365\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.2115 acc 0.929 f1 0.929 || val_loss 1.8538 acc 0.428 f1 0.349\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1877 acc 0.938 f1 0.938 || val_loss 1.8821 acc 0.434 f1 0.381\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.1764 acc 0.943 f1 0.943 || val_loss 1.8897 acc 0.438 f1 0.369\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.2066 acc 0.925 f1 0.925 || val_loss 1.9244 acc 0.438 f1 0.373\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.2345 acc 0.914 f1 0.914 || val_loss 2.1255 acc 0.383 f1 0.318\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.2270 acc 0.916 f1 0.916 || val_loss 2.1390 acc 0.397 f1 0.350\n",
            "[W3] CNN1D Epoch 27 | train_loss 0.1967 acc 0.925 f1 0.925 || val_loss 2.0223 acc 0.407 f1 0.336\n",
            "[W3] CNN1D Epoch 28 | train_loss 0.1836 acc 0.935 f1 0.935 || val_loss 2.0658 acc 0.393 f1 0.334\n",
            "[W3] CNN1D Epoch 29 | train_loss 0.1347 acc 0.957 f1 0.957 || val_loss 2.1230 acc 0.409 f1 0.343\n",
            "[W3] CNN1D Epoch 30 | train_loss 0.1246 acc 0.960 f1 0.960 || val_loss 2.1962 acc 0.389 f1 0.331\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=81\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0998 acc 0.342 f1 0.312 || val_loss 1.0955 acc 0.331 f1 0.302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0827 acc 0.411 f1 0.407 || val_loss 1.0988 acc 0.309 f1 0.289\n",
            "[W3] RNN Epoch 03 | train_loss 1.0715 acc 0.426 f1 0.424 || val_loss 1.1113 acc 0.296 f1 0.283\n",
            "[W3] RNN Epoch 04 | train_loss 1.0618 acc 0.443 f1 0.435 || val_loss 1.0945 acc 0.319 f1 0.295\n",
            "[W3] RNN Epoch 05 | train_loss 1.0485 acc 0.459 f1 0.455 || val_loss 1.1089 acc 0.305 f1 0.291\n",
            "[W3] RNN Epoch 06 | train_loss 1.0360 acc 0.481 f1 0.478 || val_loss 1.0799 acc 0.360 f1 0.323\n",
            "[W3] RNN Epoch 07 | train_loss 1.0217 acc 0.484 f1 0.481 || val_loss 1.0810 acc 0.344 f1 0.305\n",
            "[W3] RNN Epoch 08 | train_loss 1.0039 acc 0.496 f1 0.493 || val_loss 1.0868 acc 0.337 f1 0.304\n",
            "[W3] RNN Epoch 09 | train_loss 0.9825 acc 0.516 f1 0.512 || val_loss 1.0842 acc 0.350 f1 0.320\n",
            "[W3] RNN Epoch 10 | train_loss 0.9588 acc 0.536 f1 0.528 || val_loss 1.0797 acc 0.350 f1 0.322\n",
            "[W3] RNN Epoch 11 | train_loss 0.9452 acc 0.554 f1 0.549 || val_loss 1.0775 acc 0.377 f1 0.347\n",
            "[W3] RNN Epoch 12 | train_loss 0.9253 acc 0.555 f1 0.546 || val_loss 1.0814 acc 0.372 f1 0.335\n",
            "[W3] RNN Epoch 13 | train_loss 0.8998 acc 0.580 f1 0.574 || val_loss 1.0913 acc 0.366 f1 0.339\n",
            "[W3] RNN Epoch 14 | train_loss 0.8792 acc 0.586 f1 0.579 || val_loss 1.0861 acc 0.366 f1 0.338\n",
            "[W3] RNN Epoch 15 | train_loss 0.8623 acc 0.597 f1 0.591 || val_loss 1.0890 acc 0.387 f1 0.351\n",
            "[W3] RNN Epoch 16 | train_loss 0.8416 acc 0.610 f1 0.602 || val_loss 1.0753 acc 0.391 f1 0.350\n",
            "[W3] RNN Epoch 17 | train_loss 0.8361 acc 0.608 f1 0.602 || val_loss 1.0781 acc 0.383 f1 0.341\n",
            "[W3] RNN Epoch 18 | train_loss 0.8212 acc 0.609 f1 0.600 || val_loss 1.0837 acc 0.385 f1 0.346\n",
            "[W3] RNN Epoch 19 | train_loss 0.7945 acc 0.634 f1 0.627 || val_loss 1.1033 acc 0.395 f1 0.360\n",
            "[W3] RNN Epoch 20 | train_loss 0.7771 acc 0.643 f1 0.636 || val_loss 1.1026 acc 0.397 f1 0.360\n",
            "[W3] RNN Epoch 21 | train_loss 0.7599 acc 0.650 f1 0.643 || val_loss 1.0946 acc 0.395 f1 0.358\n",
            "[W3] RNN Epoch 22 | train_loss 0.7382 acc 0.653 f1 0.646 || val_loss 1.0972 acc 0.399 f1 0.363\n",
            "[W3] RNN Epoch 23 | train_loss 0.7241 acc 0.669 f1 0.663 || val_loss 1.1085 acc 0.391 f1 0.355\n",
            "[W3] RNN Epoch 24 | train_loss 0.7044 acc 0.679 f1 0.672 || val_loss 1.1284 acc 0.397 f1 0.355\n",
            "[W3] RNN Epoch 25 | train_loss 0.6867 acc 0.684 f1 0.677 || val_loss 1.1094 acc 0.393 f1 0.347\n",
            "[W3] RNN Epoch 26 | train_loss 0.6715 acc 0.691 f1 0.685 || val_loss 1.1068 acc 0.399 f1 0.354\n",
            "[W3] RNN Epoch 27 | train_loss 0.6910 acc 0.665 f1 0.659 || val_loss 1.1300 acc 0.395 f1 0.354\n",
            "[W3] RNN Epoch 28 | train_loss 0.6505 acc 0.694 f1 0.688 || val_loss 1.1256 acc 0.414 f1 0.368\n",
            "[W3] RNN Epoch 29 | train_loss 0.6285 acc 0.713 f1 0.708 || val_loss 1.1677 acc 0.395 f1 0.360\n",
            "[W3] RNN Epoch 30 | train_loss 0.6125 acc 0.709 f1 0.703 || val_loss 1.1681 acc 0.393 f1 0.345\n",
            "[W3] RNN Epoch 31 | train_loss 0.6028 acc 0.716 f1 0.712 || val_loss 1.1733 acc 0.407 f1 0.362\n",
            "[W3] RNN Epoch 32 | train_loss 0.5867 acc 0.722 f1 0.717 || val_loss 1.1888 acc 0.383 f1 0.342\n",
            "[W3] RNN Epoch 33 | train_loss 0.6131 acc 0.711 f1 0.705 || val_loss 1.1922 acc 0.387 f1 0.333\n",
            "[W3] RNN Epoch 34 | train_loss 0.6005 acc 0.721 f1 0.717 || val_loss 1.2021 acc 0.374 f1 0.323\n",
            "[W3] RNN Epoch 35 | train_loss 0.5601 acc 0.735 f1 0.730 || val_loss 1.2010 acc 0.385 f1 0.332\n",
            "[W3] RNN Epoch 36 | train_loss 0.5453 acc 0.734 f1 0.730 || val_loss 1.2184 acc 0.403 f1 0.356\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=81\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1011 acc 0.345 f1 0.275 || val_loss 1.1061 acc 0.280 f1 0.233\n",
            "[W3] GRU Epoch 02 | train_loss 1.0921 acc 0.371 f1 0.343 || val_loss 1.1008 acc 0.319 f1 0.294\n",
            "[W3] GRU Epoch 03 | train_loss 1.0838 acc 0.407 f1 0.405 || val_loss 1.1010 acc 0.302 f1 0.298\n",
            "[W3] GRU Epoch 04 | train_loss 1.0744 acc 0.423 f1 0.412 || val_loss 1.0891 acc 0.329 f1 0.316\n",
            "[W3] GRU Epoch 05 | train_loss 1.0597 acc 0.444 f1 0.434 || val_loss 1.0914 acc 0.317 f1 0.299\n",
            "[W3] GRU Epoch 06 | train_loss 1.0414 acc 0.462 f1 0.454 || val_loss 1.0783 acc 0.352 f1 0.328\n",
            "[W3] GRU Epoch 07 | train_loss 1.0127 acc 0.495 f1 0.490 || val_loss 1.0670 acc 0.385 f1 0.347\n",
            "[W3] GRU Epoch 08 | train_loss 0.9599 acc 0.538 f1 0.532 || val_loss 1.0774 acc 0.387 f1 0.349\n",
            "[W3] GRU Epoch 09 | train_loss 0.8966 acc 0.570 f1 0.563 || val_loss 1.0862 acc 0.407 f1 0.346\n",
            "[W3] GRU Epoch 10 | train_loss 0.8465 acc 0.589 f1 0.581 || val_loss 1.1032 acc 0.399 f1 0.343\n",
            "[W3] GRU Epoch 11 | train_loss 0.7939 acc 0.623 f1 0.618 || val_loss 1.1196 acc 0.401 f1 0.329\n",
            "[W3] GRU Epoch 12 | train_loss 0.7427 acc 0.640 f1 0.635 || val_loss 1.1499 acc 0.420 f1 0.347\n",
            "[W3] GRU Epoch 13 | train_loss 0.7454 acc 0.647 f1 0.642 || val_loss 1.1482 acc 0.399 f1 0.339\n",
            "[W3] GRU Epoch 14 | train_loss 0.6957 acc 0.666 f1 0.661 || val_loss 1.1844 acc 0.403 f1 0.335\n",
            "[W3] GRU Epoch 15 | train_loss 0.6565 acc 0.687 f1 0.684 || val_loss 1.2212 acc 0.370 f1 0.310\n",
            "[W3] GRU Epoch 16 | train_loss 0.6348 acc 0.694 f1 0.689 || val_loss 1.2416 acc 0.385 f1 0.318\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=81\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 982, np.int64(1): 930, np.int64(0): 275})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 982, np.int64(2): 982, np.int64(0): 982})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1023 acc 0.333 f1 0.172 || val_loss 1.0826 acc 0.428 f1 0.208\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0942 acc 0.363 f1 0.303 || val_loss 1.0996 acc 0.305 f1 0.275\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0873 acc 0.391 f1 0.355 || val_loss 1.0967 acc 0.319 f1 0.302\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0717 acc 0.438 f1 0.428 || val_loss 1.0813 acc 0.350 f1 0.329\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0400 acc 0.461 f1 0.448 || val_loss 1.0724 acc 0.377 f1 0.347\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9737 acc 0.518 f1 0.505 || val_loss 1.1091 acc 0.331 f1 0.301\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8879 acc 0.570 f1 0.562 || val_loss 1.0767 acc 0.381 f1 0.315\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8321 acc 0.583 f1 0.575 || val_loss 1.0674 acc 0.379 f1 0.297\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7908 acc 0.617 f1 0.613 || val_loss 1.1210 acc 0.389 f1 0.338\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7774 acc 0.617 f1 0.610 || val_loss 1.1145 acc 0.393 f1 0.347\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7301 acc 0.639 f1 0.633 || val_loss 1.1238 acc 0.397 f1 0.338\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6967 acc 0.655 f1 0.652 || val_loss 1.1850 acc 0.383 f1 0.329\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6772 acc 0.671 f1 0.667 || val_loss 1.2304 acc 0.374 f1 0.328\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  81%|  | 81/100 [38:25<09:32, 30.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=82 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=82\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1373 acc 0.406 f1 0.400 || val_loss 1.1575 acc 0.296 f1 0.291\n",
            "[W3] ANN Epoch 02 | train_loss 0.9725 acc 0.520 f1 0.510 || val_loss 1.1524 acc 0.333 f1 0.316\n",
            "[W3] ANN Epoch 03 | train_loss 0.8671 acc 0.584 f1 0.575 || val_loss 1.1099 acc 0.387 f1 0.343\n",
            "[W3] ANN Epoch 04 | train_loss 0.7801 acc 0.632 f1 0.625 || val_loss 1.1129 acc 0.393 f1 0.346\n",
            "[W3] ANN Epoch 05 | train_loss 0.7259 acc 0.650 f1 0.646 || val_loss 1.1078 acc 0.416 f1 0.368\n",
            "[W3] ANN Epoch 06 | train_loss 0.6687 acc 0.692 f1 0.690 || val_loss 1.1318 acc 0.407 f1 0.350\n",
            "[W3] ANN Epoch 07 | train_loss 0.6293 acc 0.700 f1 0.698 || val_loss 1.1479 acc 0.420 f1 0.352\n",
            "[W3] ANN Epoch 08 | train_loss 0.5930 acc 0.717 f1 0.715 || val_loss 1.1588 acc 0.440 f1 0.386\n",
            "[W3] ANN Epoch 09 | train_loss 0.5641 acc 0.742 f1 0.741 || val_loss 1.1661 acc 0.465 f1 0.395\n",
            "[W3] ANN Epoch 10 | train_loss 0.5474 acc 0.750 f1 0.748 || val_loss 1.1999 acc 0.451 f1 0.399\n",
            "[W3] ANN Epoch 11 | train_loss 0.5195 acc 0.760 f1 0.759 || val_loss 1.2245 acc 0.447 f1 0.388\n",
            "[W3] ANN Epoch 12 | train_loss 0.5038 acc 0.774 f1 0.774 || val_loss 1.2391 acc 0.426 f1 0.347\n",
            "[W3] ANN Epoch 13 | train_loss 0.4810 acc 0.788 f1 0.788 || val_loss 1.2464 acc 0.436 f1 0.355\n",
            "[W3] ANN Epoch 14 | train_loss 0.4498 acc 0.802 f1 0.802 || val_loss 1.3290 acc 0.426 f1 0.352\n",
            "[W3] ANN Epoch 15 | train_loss 0.4261 acc 0.821 f1 0.819 || val_loss 1.3224 acc 0.442 f1 0.365\n",
            "[W3] ANN Epoch 16 | train_loss 0.4173 acc 0.811 f1 0.811 || val_loss 1.3625 acc 0.442 f1 0.360\n",
            "[W3] ANN Epoch 17 | train_loss 0.3891 acc 0.829 f1 0.828 || val_loss 1.4304 acc 0.440 f1 0.353\n",
            "[W3] ANN Epoch 18 | train_loss 0.3928 acc 0.830 f1 0.830 || val_loss 1.4341 acc 0.436 f1 0.359\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=82\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0798 acc 0.392 f1 0.393 || val_loss 1.0502 acc 0.407 f1 0.335\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9477 acc 0.543 f1 0.539 || val_loss 1.0512 acc 0.414 f1 0.327\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8092 acc 0.607 f1 0.602 || val_loss 1.1390 acc 0.366 f1 0.311\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7063 acc 0.683 f1 0.681 || val_loss 1.1967 acc 0.379 f1 0.313\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6329 acc 0.709 f1 0.707 || val_loss 1.2369 acc 0.393 f1 0.322\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5584 acc 0.746 f1 0.745 || val_loss 1.2754 acc 0.409 f1 0.339\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5050 acc 0.779 f1 0.779 || val_loss 1.3454 acc 0.399 f1 0.330\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4472 acc 0.807 f1 0.806 || val_loss 1.4109 acc 0.395 f1 0.315\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.3988 acc 0.839 f1 0.839 || val_loss 1.4655 acc 0.397 f1 0.334\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3587 acc 0.851 f1 0.851 || val_loss 1.5859 acc 0.377 f1 0.290\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3053 acc 0.883 f1 0.883 || val_loss 1.7022 acc 0.416 f1 0.329\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2766 acc 0.889 f1 0.888 || val_loss 1.8255 acc 0.407 f1 0.292\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2424 acc 0.908 f1 0.907 || val_loss 1.9112 acc 0.389 f1 0.309\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2160 acc 0.922 f1 0.922 || val_loss 2.0384 acc 0.401 f1 0.323\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=82\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0972 acc 0.355 f1 0.339 || val_loss 1.0971 acc 0.364 f1 0.343\n",
            "[W3] RNN Epoch 02 | train_loss 1.0819 acc 0.429 f1 0.427 || val_loss 1.0948 acc 0.350 f1 0.317\n",
            "[W3] RNN Epoch 03 | train_loss 1.0659 acc 0.459 f1 0.456 || val_loss 1.0894 acc 0.348 f1 0.324\n",
            "[W3] RNN Epoch 04 | train_loss 1.0435 acc 0.470 f1 0.466 || val_loss 1.0860 acc 0.352 f1 0.321\n",
            "[W3] RNN Epoch 05 | train_loss 1.0210 acc 0.491 f1 0.485 || val_loss 1.0940 acc 0.346 f1 0.323\n",
            "[W3] RNN Epoch 06 | train_loss 1.0013 acc 0.503 f1 0.496 || val_loss 1.0909 acc 0.329 f1 0.306\n",
            "[W3] RNN Epoch 07 | train_loss 0.9790 acc 0.511 f1 0.503 || val_loss 1.0755 acc 0.364 f1 0.330\n",
            "[W3] RNN Epoch 08 | train_loss 0.9554 acc 0.540 f1 0.531 || val_loss 1.0977 acc 0.364 f1 0.340\n",
            "[W3] RNN Epoch 09 | train_loss 0.9327 acc 0.542 f1 0.533 || val_loss 1.1060 acc 0.364 f1 0.336\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=82\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0985 acc 0.339 f1 0.333 || val_loss 1.1028 acc 0.290 f1 0.278\n",
            "[W3] GRU Epoch 02 | train_loss 1.0907 acc 0.399 f1 0.382 || val_loss 1.0980 acc 0.337 f1 0.307\n",
            "[W3] GRU Epoch 03 | train_loss 1.0844 acc 0.423 f1 0.415 || val_loss 1.0986 acc 0.315 f1 0.299\n",
            "[W3] GRU Epoch 04 | train_loss 1.0696 acc 0.439 f1 0.435 || val_loss 1.0896 acc 0.319 f1 0.296\n",
            "[W3] GRU Epoch 05 | train_loss 1.0425 acc 0.472 f1 0.469 || val_loss 1.1235 acc 0.311 f1 0.303\n",
            "[W3] GRU Epoch 06 | train_loss 0.9964 acc 0.510 f1 0.499 || val_loss 1.0637 acc 0.354 f1 0.303\n",
            "[W3] GRU Epoch 07 | train_loss 0.9246 acc 0.550 f1 0.543 || val_loss 1.0878 acc 0.335 f1 0.288\n",
            "[W3] GRU Epoch 08 | train_loss 0.8664 acc 0.585 f1 0.578 || val_loss 1.1117 acc 0.372 f1 0.318\n",
            "[W3] GRU Epoch 09 | train_loss 0.7956 acc 0.619 f1 0.613 || val_loss 1.1722 acc 0.335 f1 0.300\n",
            "[W3] GRU Epoch 10 | train_loss 0.7538 acc 0.642 f1 0.637 || val_loss 1.1546 acc 0.379 f1 0.312\n",
            "[W3] GRU Epoch 11 | train_loss 0.7120 acc 0.661 f1 0.656 || val_loss 1.1353 acc 0.395 f1 0.319\n",
            "[W3] GRU Epoch 12 | train_loss 0.6815 acc 0.670 f1 0.667 || val_loss 1.1789 acc 0.407 f1 0.333\n",
            "[W3] GRU Epoch 13 | train_loss 0.6448 acc 0.685 f1 0.682 || val_loss 1.2292 acc 0.387 f1 0.322\n",
            "[W3] GRU Epoch 14 | train_loss 0.6130 acc 0.705 f1 0.703 || val_loss 1.2642 acc 0.407 f1 0.345\n",
            "[W3] GRU Epoch 15 | train_loss 0.5898 acc 0.709 f1 0.707 || val_loss 1.3197 acc 0.395 f1 0.336\n",
            "[W3] GRU Epoch 16 | train_loss 0.5570 acc 0.732 f1 0.729 || val_loss 1.3011 acc 0.420 f1 0.347\n",
            "[W3] GRU Epoch 17 | train_loss 0.5441 acc 0.740 f1 0.739 || val_loss 1.3643 acc 0.407 f1 0.342\n",
            "[W3] GRU Epoch 18 | train_loss 0.5157 acc 0.755 f1 0.753 || val_loss 1.4267 acc 0.401 f1 0.337\n",
            "[W3] GRU Epoch 19 | train_loss 0.4896 acc 0.760 f1 0.759 || val_loss 1.4260 acc 0.407 f1 0.333\n",
            "[W3] GRU Epoch 20 | train_loss 0.4721 acc 0.766 f1 0.765 || val_loss 1.5181 acc 0.409 f1 0.343\n",
            "[W3] GRU Epoch 21 | train_loss 0.4604 acc 0.778 f1 0.778 || val_loss 1.5681 acc 0.407 f1 0.338\n",
            "[W3] GRU Epoch 22 | train_loss 0.4399 acc 0.786 f1 0.785 || val_loss 1.6032 acc 0.418 f1 0.349\n",
            "[W3] GRU Epoch 23 | train_loss 0.4189 acc 0.801 f1 0.801 || val_loss 1.6200 acc 0.414 f1 0.342\n",
            "[W3] GRU Epoch 24 | train_loss 0.4057 acc 0.808 f1 0.807 || val_loss 1.6887 acc 0.412 f1 0.342\n",
            "[W3] GRU Epoch 25 | train_loss 0.3851 acc 0.821 f1 0.820 || val_loss 1.7497 acc 0.409 f1 0.339\n",
            "[W3] GRU Epoch 26 | train_loss 0.3702 acc 0.832 f1 0.832 || val_loss 1.7541 acc 0.405 f1 0.345\n",
            "[W3] GRU Epoch 27 | train_loss 0.3608 acc 0.832 f1 0.832 || val_loss 1.8195 acc 0.414 f1 0.347\n",
            "[W3] GRU Epoch 28 | train_loss 0.3384 acc 0.849 f1 0.849 || val_loss 1.8606 acc 0.412 f1 0.345\n",
            "[W3] GRU Epoch 29 | train_loss 0.3215 acc 0.855 f1 0.855 || val_loss 1.9462 acc 0.420 f1 0.351\n",
            "[W3] GRU Epoch 30 | train_loss 0.3143 acc 0.863 f1 0.863 || val_loss 1.9193 acc 0.418 f1 0.350\n",
            "[W3] GRU Epoch 31 | train_loss 0.2939 acc 0.877 f1 0.877 || val_loss 2.0292 acc 0.424 f1 0.350\n",
            "[W3] GRU Epoch 32 | train_loss 0.2799 acc 0.882 f1 0.882 || val_loss 2.0823 acc 0.414 f1 0.346\n",
            "[W3] GRU Epoch 33 | train_loss 0.2658 acc 0.882 f1 0.881 || val_loss 2.1808 acc 0.424 f1 0.354\n",
            "[W3] GRU Epoch 34 | train_loss 0.2524 acc 0.893 f1 0.893 || val_loss 2.2250 acc 0.407 f1 0.341\n",
            "[W3] GRU Epoch 35 | train_loss 0.2295 acc 0.911 f1 0.911 || val_loss 2.2549 acc 0.416 f1 0.352\n",
            "[W3] GRU Epoch 36 | train_loss 0.2130 acc 0.920 f1 0.920 || val_loss 2.3446 acc 0.397 f1 0.334\n",
            "[W3] GRU Epoch 37 | train_loss 0.1908 acc 0.932 f1 0.932 || val_loss 2.4131 acc 0.403 f1 0.349\n",
            "[W3] GRU Epoch 38 | train_loss 0.1821 acc 0.939 f1 0.939 || val_loss 2.4848 acc 0.403 f1 0.347\n",
            "[W3] GRU Epoch 39 | train_loss 0.1647 acc 0.946 f1 0.946 || val_loss 2.5114 acc 0.401 f1 0.341\n",
            "[W3] GRU Epoch 40 | train_loss 0.1529 acc 0.954 f1 0.954 || val_loss 2.6007 acc 0.405 f1 0.345\n",
            "[W3] GRU Epoch 41 | train_loss 0.1332 acc 0.959 f1 0.959 || val_loss 2.6782 acc 0.412 f1 0.345\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=82\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0995 acc 0.337 f1 0.216 || val_loss 1.0930 acc 0.391 f1 0.248\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0937 acc 0.374 f1 0.375 || val_loss 1.0976 acc 0.323 f1 0.309\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0866 acc 0.393 f1 0.392 || val_loss 1.0975 acc 0.325 f1 0.313\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0703 acc 0.431 f1 0.417 || val_loss 1.0942 acc 0.315 f1 0.295\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0323 acc 0.486 f1 0.474 || val_loss 1.0996 acc 0.321 f1 0.299\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9449 acc 0.530 f1 0.521 || val_loss 1.1024 acc 0.366 f1 0.303\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8612 acc 0.579 f1 0.574 || val_loss 1.1354 acc 0.362 f1 0.299\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8134 acc 0.593 f1 0.587 || val_loss 1.1267 acc 0.385 f1 0.310\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7635 acc 0.629 f1 0.623 || val_loss 1.1645 acc 0.377 f1 0.298\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7240 acc 0.648 f1 0.642 || val_loss 1.2271 acc 0.372 f1 0.303\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6841 acc 0.664 f1 0.660 || val_loss 1.2289 acc 0.370 f1 0.295\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  82%| | 82/100 [38:53<08:46, 29.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=83 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=83\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1256 acc 0.398 f1 0.397 || val_loss 1.1461 acc 0.309 f1 0.293\n",
            "[W3] ANN Epoch 02 | train_loss 0.9641 acc 0.512 f1 0.505 || val_loss 1.1698 acc 0.296 f1 0.282\n",
            "[W3] ANN Epoch 03 | train_loss 0.8653 acc 0.572 f1 0.561 || val_loss 1.1470 acc 0.323 f1 0.285\n",
            "[W3] ANN Epoch 04 | train_loss 0.7888 acc 0.611 f1 0.604 || val_loss 1.1521 acc 0.350 f1 0.307\n",
            "[W3] ANN Epoch 05 | train_loss 0.7137 acc 0.667 f1 0.664 || val_loss 1.1302 acc 0.403 f1 0.335\n",
            "[W3] ANN Epoch 06 | train_loss 0.6750 acc 0.678 f1 0.675 || val_loss 1.2036 acc 0.374 f1 0.323\n",
            "[W3] ANN Epoch 07 | train_loss 0.6387 acc 0.695 f1 0.694 || val_loss 1.2178 acc 0.379 f1 0.309\n",
            "[W3] ANN Epoch 08 | train_loss 0.6102 acc 0.711 f1 0.709 || val_loss 1.2017 acc 0.387 f1 0.318\n",
            "[W3] ANN Epoch 09 | train_loss 0.5697 acc 0.736 f1 0.735 || val_loss 1.2546 acc 0.379 f1 0.312\n",
            "[W3] ANN Epoch 10 | train_loss 0.5447 acc 0.746 f1 0.745 || val_loss 1.2648 acc 0.409 f1 0.335\n",
            "[W3] ANN Epoch 11 | train_loss 0.5249 acc 0.762 f1 0.761 || val_loss 1.2504 acc 0.407 f1 0.342\n",
            "[W3] ANN Epoch 12 | train_loss 0.5088 acc 0.762 f1 0.761 || val_loss 1.3092 acc 0.403 f1 0.347\n",
            "[W3] ANN Epoch 13 | train_loss 0.4838 acc 0.782 f1 0.782 || val_loss 1.2893 acc 0.424 f1 0.341\n",
            "[W3] ANN Epoch 14 | train_loss 0.4741 acc 0.781 f1 0.780 || val_loss 1.3357 acc 0.418 f1 0.343\n",
            "[W3] ANN Epoch 15 | train_loss 0.4487 acc 0.807 f1 0.807 || val_loss 1.3491 acc 0.422 f1 0.340\n",
            "[W3] ANN Epoch 16 | train_loss 0.4302 acc 0.812 f1 0.811 || val_loss 1.3856 acc 0.438 f1 0.356\n",
            "[W3] ANN Epoch 17 | train_loss 0.4127 acc 0.823 f1 0.822 || val_loss 1.4306 acc 0.409 f1 0.339\n",
            "[W3] ANN Epoch 18 | train_loss 0.3963 acc 0.834 f1 0.833 || val_loss 1.4150 acc 0.434 f1 0.353\n",
            "[W3] ANN Epoch 19 | train_loss 0.3839 acc 0.838 f1 0.838 || val_loss 1.3834 acc 0.442 f1 0.359\n",
            "[W3] ANN Epoch 20 | train_loss 0.3973 acc 0.834 f1 0.834 || val_loss 1.4767 acc 0.407 f1 0.333\n",
            "[W3] ANN Epoch 21 | train_loss 0.3393 acc 0.857 f1 0.857 || val_loss 1.4622 acc 0.426 f1 0.342\n",
            "[W3] ANN Epoch 22 | train_loss 0.3568 acc 0.850 f1 0.850 || val_loss 1.5465 acc 0.401 f1 0.329\n",
            "[W3] ANN Epoch 23 | train_loss 0.3411 acc 0.861 f1 0.861 || val_loss 1.5644 acc 0.391 f1 0.330\n",
            "[W3] ANN Epoch 24 | train_loss 0.3175 acc 0.869 f1 0.869 || val_loss 1.5857 acc 0.414 f1 0.338\n",
            "[W3] ANN Epoch 25 | train_loss 0.3064 acc 0.874 f1 0.874 || val_loss 1.6260 acc 0.416 f1 0.340\n",
            "[W3] ANN Epoch 26 | train_loss 0.2877 acc 0.882 f1 0.882 || val_loss 1.6831 acc 0.428 f1 0.344\n",
            "[W3] ANN Epoch 27 | train_loss 0.2921 acc 0.880 f1 0.880 || val_loss 1.6434 acc 0.424 f1 0.360\n",
            "[W3] ANN Epoch 28 | train_loss 0.2767 acc 0.887 f1 0.886 || val_loss 1.6174 acc 0.447 f1 0.367\n",
            "[W3] ANN Epoch 29 | train_loss 0.2707 acc 0.894 f1 0.894 || val_loss 1.6712 acc 0.430 f1 0.355\n",
            "[W3] ANN Epoch 30 | train_loss 0.2676 acc 0.899 f1 0.899 || val_loss 1.6775 acc 0.436 f1 0.358\n",
            "[W3] ANN Epoch 31 | train_loss 0.2632 acc 0.890 f1 0.890 || val_loss 1.7409 acc 0.420 f1 0.347\n",
            "[W3] ANN Epoch 32 | train_loss 0.2675 acc 0.893 f1 0.893 || val_loss 1.7325 acc 0.409 f1 0.334\n",
            "[W3] ANN Epoch 33 | train_loss 0.2414 acc 0.895 f1 0.895 || val_loss 1.7028 acc 0.451 f1 0.376\n",
            "[W3] ANN Epoch 34 | train_loss 0.2509 acc 0.906 f1 0.905 || val_loss 1.7861 acc 0.428 f1 0.354\n",
            "[W3] ANN Epoch 35 | train_loss 0.2281 acc 0.908 f1 0.908 || val_loss 1.8061 acc 0.438 f1 0.370\n",
            "[W3] ANN Epoch 36 | train_loss 0.2232 acc 0.911 f1 0.911 || val_loss 1.8032 acc 0.449 f1 0.364\n",
            "[W3] ANN Epoch 37 | train_loss 0.2339 acc 0.906 f1 0.905 || val_loss 1.8663 acc 0.426 f1 0.357\n",
            "[W3] ANN Epoch 38 | train_loss 0.2272 acc 0.910 f1 0.910 || val_loss 1.8212 acc 0.432 f1 0.358\n",
            "[W3] ANN Epoch 39 | train_loss 0.2212 acc 0.917 f1 0.917 || val_loss 1.8448 acc 0.428 f1 0.349\n",
            "[W3] ANN Epoch 40 | train_loss 0.2229 acc 0.919 f1 0.919 || val_loss 1.8374 acc 0.438 f1 0.365\n",
            "[W3] ANN Epoch 41 | train_loss 0.2055 acc 0.922 f1 0.922 || val_loss 1.8427 acc 0.434 f1 0.369\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=83\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0789 acc 0.391 f1 0.393 || val_loss 1.0214 acc 0.461 f1 0.325\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9487 acc 0.542 f1 0.539 || val_loss 1.0568 acc 0.368 f1 0.283\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8215 acc 0.604 f1 0.599 || val_loss 1.0965 acc 0.372 f1 0.305\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7087 acc 0.670 f1 0.667 || val_loss 1.1714 acc 0.379 f1 0.320\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6315 acc 0.712 f1 0.710 || val_loss 1.2289 acc 0.387 f1 0.329\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5596 acc 0.753 f1 0.751 || val_loss 1.2816 acc 0.383 f1 0.321\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.4977 acc 0.785 f1 0.784 || val_loss 1.2931 acc 0.391 f1 0.320\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4485 acc 0.802 f1 0.800 || val_loss 1.3841 acc 0.403 f1 0.328\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4061 acc 0.837 f1 0.836 || val_loss 1.4496 acc 0.389 f1 0.329\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3556 acc 0.855 f1 0.854 || val_loss 1.5852 acc 0.372 f1 0.303\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3149 acc 0.870 f1 0.870 || val_loss 1.6490 acc 0.401 f1 0.334\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2674 acc 0.899 f1 0.899 || val_loss 1.7908 acc 0.377 f1 0.321\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2335 acc 0.912 f1 0.912 || val_loss 1.8998 acc 0.399 f1 0.337\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2128 acc 0.921 f1 0.921 || val_loss 1.9135 acc 0.393 f1 0.327\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1987 acc 0.931 f1 0.931 || val_loss 2.1192 acc 0.391 f1 0.306\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1678 acc 0.940 f1 0.940 || val_loss 2.1764 acc 0.409 f1 0.351\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1527 acc 0.947 f1 0.947 || val_loss 2.2166 acc 0.379 f1 0.295\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1307 acc 0.957 f1 0.957 || val_loss 2.2412 acc 0.420 f1 0.342\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1128 acc 0.966 f1 0.966 || val_loss 2.3721 acc 0.412 f1 0.324\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.0946 acc 0.972 f1 0.972 || val_loss 2.5676 acc 0.403 f1 0.333\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.0910 acc 0.973 f1 0.973 || val_loss 2.5588 acc 0.416 f1 0.344\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.0756 acc 0.976 f1 0.976 || val_loss 2.6483 acc 0.407 f1 0.338\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.0686 acc 0.979 f1 0.979 || val_loss 2.7193 acc 0.409 f1 0.332\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.0779 acc 0.971 f1 0.971 || val_loss 2.7980 acc 0.426 f1 0.352\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.0645 acc 0.979 f1 0.979 || val_loss 2.8707 acc 0.381 f1 0.311\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.0563 acc 0.980 f1 0.980 || val_loss 2.8713 acc 0.395 f1 0.320\n",
            "[W3] CNN1D Epoch 27 | train_loss 0.0528 acc 0.983 f1 0.983 || val_loss 2.9213 acc 0.420 f1 0.358\n",
            "[W3] CNN1D Epoch 28 | train_loss 0.0543 acc 0.982 f1 0.982 || val_loss 2.9429 acc 0.401 f1 0.323\n",
            "[W3] CNN1D Epoch 29 | train_loss 0.0574 acc 0.984 f1 0.984 || val_loss 3.1298 acc 0.399 f1 0.336\n",
            "[W3] CNN1D Epoch 30 | train_loss 0.0638 acc 0.976 f1 0.976 || val_loss 3.1648 acc 0.412 f1 0.341\n",
            "[W3] CNN1D Epoch 31 | train_loss 0.0430 acc 0.987 f1 0.987 || val_loss 3.1274 acc 0.412 f1 0.318\n",
            "[W3] CNN1D Epoch 32 | train_loss 0.0654 acc 0.979 f1 0.979 || val_loss 3.1810 acc 0.403 f1 0.329\n",
            "[W3] CNN1D Epoch 33 | train_loss 0.0510 acc 0.981 f1 0.981 || val_loss 3.2338 acc 0.395 f1 0.336\n",
            "[W3] CNN1D Epoch 34 | train_loss 0.0464 acc 0.986 f1 0.986 || val_loss 3.3127 acc 0.399 f1 0.325\n",
            "[W3] CNN1D Epoch 35 | train_loss 0.0443 acc 0.983 f1 0.983 || val_loss 3.3930 acc 0.407 f1 0.350\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=83\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0960 acc 0.358 f1 0.344 || val_loss 1.0968 acc 0.300 f1 0.283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0773 acc 0.410 f1 0.402 || val_loss 1.0980 acc 0.298 f1 0.280\n",
            "[W3] RNN Epoch 03 | train_loss 1.0607 acc 0.426 f1 0.418 || val_loss 1.0919 acc 0.325 f1 0.305\n",
            "[W3] RNN Epoch 04 | train_loss 1.0456 acc 0.461 f1 0.456 || val_loss 1.0846 acc 0.321 f1 0.289\n",
            "[W3] RNN Epoch 05 | train_loss 1.0251 acc 0.464 f1 0.459 || val_loss 1.0978 acc 0.309 f1 0.283\n",
            "[W3] RNN Epoch 06 | train_loss 1.0099 acc 0.487 f1 0.483 || val_loss 1.0997 acc 0.325 f1 0.291\n",
            "[W3] RNN Epoch 07 | train_loss 0.9886 acc 0.503 f1 0.495 || val_loss 1.0856 acc 0.348 f1 0.306\n",
            "[W3] RNN Epoch 08 | train_loss 0.9653 acc 0.533 f1 0.526 || val_loss 1.0881 acc 0.356 f1 0.310\n",
            "[W3] RNN Epoch 09 | train_loss 0.9344 acc 0.547 f1 0.540 || val_loss 1.1018 acc 0.354 f1 0.315\n",
            "[W3] RNN Epoch 10 | train_loss 0.9172 acc 0.566 f1 0.557 || val_loss 1.0903 acc 0.350 f1 0.304\n",
            "[W3] RNN Epoch 11 | train_loss 0.8910 acc 0.571 f1 0.562 || val_loss 1.0966 acc 0.372 f1 0.322\n",
            "[W3] RNN Epoch 12 | train_loss 0.8611 acc 0.605 f1 0.596 || val_loss 1.1051 acc 0.362 f1 0.311\n",
            "[W3] RNN Epoch 13 | train_loss 0.8433 acc 0.604 f1 0.596 || val_loss 1.1078 acc 0.370 f1 0.308\n",
            "[W3] RNN Epoch 14 | train_loss 0.8190 acc 0.617 f1 0.608 || val_loss 1.1154 acc 0.374 f1 0.310\n",
            "[W3] RNN Epoch 15 | train_loss 0.8123 acc 0.616 f1 0.608 || val_loss 1.1234 acc 0.364 f1 0.298\n",
            "[W3] RNN Epoch 16 | train_loss 0.7730 acc 0.643 f1 0.635 || val_loss 1.1488 acc 0.344 f1 0.297\n",
            "[W3] RNN Epoch 17 | train_loss 0.7638 acc 0.652 f1 0.647 || val_loss 1.1603 acc 0.364 f1 0.295\n",
            "[W3] RNN Epoch 18 | train_loss 0.7325 acc 0.660 f1 0.653 || val_loss 1.1583 acc 0.368 f1 0.301\n",
            "[W3] RNN Epoch 19 | train_loss 0.7112 acc 0.672 f1 0.666 || val_loss 1.1654 acc 0.360 f1 0.276\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=83\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1001 acc 0.334 f1 0.170 || val_loss 1.0838 acc 0.430 f1 0.211\n",
            "[W3] GRU Epoch 02 | train_loss 1.0889 acc 0.369 f1 0.329 || val_loss 1.0810 acc 0.372 f1 0.332\n",
            "[W3] GRU Epoch 03 | train_loss 1.0769 acc 0.414 f1 0.415 || val_loss 1.0785 acc 0.344 f1 0.321\n",
            "[W3] GRU Epoch 04 | train_loss 1.0543 acc 0.455 f1 0.452 || val_loss 1.0646 acc 0.381 f1 0.348\n",
            "[W3] GRU Epoch 05 | train_loss 1.0159 acc 0.511 f1 0.503 || val_loss 1.0937 acc 0.333 f1 0.313\n",
            "[W3] GRU Epoch 06 | train_loss 0.9499 acc 0.545 f1 0.535 || val_loss 1.0965 acc 0.368 f1 0.335\n",
            "[W3] GRU Epoch 07 | train_loss 0.8775 acc 0.573 f1 0.564 || val_loss 1.1099 acc 0.385 f1 0.335\n",
            "[W3] GRU Epoch 08 | train_loss 0.8223 acc 0.594 f1 0.586 || val_loss 1.1261 acc 0.387 f1 0.339\n",
            "[W3] GRU Epoch 09 | train_loss 0.7759 acc 0.617 f1 0.610 || val_loss 1.1581 acc 0.385 f1 0.337\n",
            "[W3] GRU Epoch 10 | train_loss 0.7398 acc 0.639 f1 0.633 || val_loss 1.1717 acc 0.374 f1 0.326\n",
            "[W3] GRU Epoch 11 | train_loss 0.7092 acc 0.654 f1 0.649 || val_loss 1.2444 acc 0.381 f1 0.337\n",
            "[W3] GRU Epoch 12 | train_loss 0.6765 acc 0.670 f1 0.665 || val_loss 1.2390 acc 0.395 f1 0.345\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=83\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0992 acc 0.338 f1 0.277 || val_loss 1.1047 acc 0.247 f1 0.208\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0936 acc 0.391 f1 0.378 || val_loss 1.0973 acc 0.333 f1 0.302\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0872 acc 0.405 f1 0.405 || val_loss 1.0966 acc 0.321 f1 0.293\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0763 acc 0.432 f1 0.429 || val_loss 1.0849 acc 0.340 f1 0.305\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0599 acc 0.455 f1 0.449 || val_loss 1.0702 acc 0.364 f1 0.324\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0299 acc 0.490 f1 0.486 || val_loss 1.1201 acc 0.321 f1 0.306\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9701 acc 0.545 f1 0.537 || val_loss 1.0789 acc 0.372 f1 0.322\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8838 acc 0.575 f1 0.570 || val_loss 1.1128 acc 0.379 f1 0.291\n",
            "[W3] LSTM Epoch 09 | train_loss 0.8059 acc 0.622 f1 0.617 || val_loss 1.1531 acc 0.346 f1 0.272\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7619 acc 0.637 f1 0.633 || val_loss 1.1653 acc 0.356 f1 0.276\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7250 acc 0.667 f1 0.664 || val_loss 1.1895 acc 0.348 f1 0.271\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6926 acc 0.681 f1 0.677 || val_loss 1.2280 acc 0.377 f1 0.299\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6524 acc 0.699 f1 0.696 || val_loss 1.2639 acc 0.360 f1 0.282\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  83%| | 83/100 [39:31<09:03, 31.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=84 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=84\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1388 acc 0.417 f1 0.409 || val_loss 1.1948 acc 0.257 f1 0.249\n",
            "[W3] ANN Epoch 02 | train_loss 0.9838 acc 0.490 f1 0.478 || val_loss 1.2036 acc 0.311 f1 0.298\n",
            "[W3] ANN Epoch 03 | train_loss 0.8714 acc 0.577 f1 0.567 || val_loss 1.1737 acc 0.350 f1 0.319\n",
            "[W3] ANN Epoch 04 | train_loss 0.8152 acc 0.592 f1 0.583 || val_loss 1.1573 acc 0.333 f1 0.298\n",
            "[W3] ANN Epoch 05 | train_loss 0.7161 acc 0.656 f1 0.650 || val_loss 1.1547 acc 0.362 f1 0.324\n",
            "[W3] ANN Epoch 06 | train_loss 0.6816 acc 0.667 f1 0.663 || val_loss 1.1751 acc 0.362 f1 0.306\n",
            "[W3] ANN Epoch 07 | train_loss 0.6339 acc 0.703 f1 0.701 || val_loss 1.1846 acc 0.387 f1 0.336\n",
            "[W3] ANN Epoch 08 | train_loss 0.5888 acc 0.733 f1 0.731 || val_loss 1.1545 acc 0.379 f1 0.320\n",
            "[W3] ANN Epoch 09 | train_loss 0.5874 acc 0.714 f1 0.712 || val_loss 1.1805 acc 0.399 f1 0.343\n",
            "[W3] ANN Epoch 10 | train_loss 0.5448 acc 0.755 f1 0.754 || val_loss 1.1980 acc 0.409 f1 0.352\n",
            "[W3] ANN Epoch 11 | train_loss 0.5114 acc 0.757 f1 0.756 || val_loss 1.2351 acc 0.393 f1 0.339\n",
            "[W3] ANN Epoch 12 | train_loss 0.4908 acc 0.780 f1 0.779 || val_loss 1.2901 acc 0.414 f1 0.347\n",
            "[W3] ANN Epoch 13 | train_loss 0.4806 acc 0.782 f1 0.781 || val_loss 1.2931 acc 0.420 f1 0.351\n",
            "[W3] ANN Epoch 14 | train_loss 0.4650 acc 0.798 f1 0.797 || val_loss 1.3393 acc 0.399 f1 0.329\n",
            "[W3] ANN Epoch 15 | train_loss 0.4512 acc 0.799 f1 0.799 || val_loss 1.2711 acc 0.449 f1 0.375\n",
            "[W3] ANN Epoch 16 | train_loss 0.4251 acc 0.810 f1 0.809 || val_loss 1.3476 acc 0.422 f1 0.363\n",
            "[W3] ANN Epoch 17 | train_loss 0.4012 acc 0.827 f1 0.827 || val_loss 1.3635 acc 0.422 f1 0.343\n",
            "[W3] ANN Epoch 18 | train_loss 0.3875 acc 0.836 f1 0.835 || val_loss 1.3697 acc 0.436 f1 0.371\n",
            "[W3] ANN Epoch 19 | train_loss 0.3698 acc 0.845 f1 0.845 || val_loss 1.3970 acc 0.434 f1 0.348\n",
            "[W3] ANN Epoch 20 | train_loss 0.3676 acc 0.844 f1 0.844 || val_loss 1.4112 acc 0.467 f1 0.390\n",
            "[W3] ANN Epoch 21 | train_loss 0.3406 acc 0.859 f1 0.858 || val_loss 1.4361 acc 0.428 f1 0.354\n",
            "[W3] ANN Epoch 22 | train_loss 0.3370 acc 0.864 f1 0.864 || val_loss 1.4790 acc 0.447 f1 0.381\n",
            "[W3] ANN Epoch 23 | train_loss 0.3411 acc 0.856 f1 0.856 || val_loss 1.4694 acc 0.449 f1 0.377\n",
            "[W3] ANN Epoch 24 | train_loss 0.3028 acc 0.875 f1 0.874 || val_loss 1.5253 acc 0.442 f1 0.377\n",
            "[W3] ANN Epoch 25 | train_loss 0.3197 acc 0.866 f1 0.866 || val_loss 1.5920 acc 0.416 f1 0.349\n",
            "[W3] ANN Epoch 26 | train_loss 0.3125 acc 0.877 f1 0.878 || val_loss 1.5890 acc 0.473 f1 0.398\n",
            "[W3] ANN Epoch 27 | train_loss 0.2847 acc 0.883 f1 0.883 || val_loss 1.6131 acc 0.444 f1 0.370\n",
            "[W3] ANN Epoch 28 | train_loss 0.2719 acc 0.891 f1 0.891 || val_loss 1.6799 acc 0.461 f1 0.408\n",
            "[W3] ANN Epoch 29 | train_loss 0.2770 acc 0.893 f1 0.893 || val_loss 1.6388 acc 0.436 f1 0.368\n",
            "[W3] ANN Epoch 30 | train_loss 0.2490 acc 0.904 f1 0.904 || val_loss 1.7044 acc 0.447 f1 0.375\n",
            "[W3] ANN Epoch 31 | train_loss 0.2559 acc 0.897 f1 0.896 || val_loss 1.6802 acc 0.438 f1 0.356\n",
            "[W3] ANN Epoch 32 | train_loss 0.2398 acc 0.905 f1 0.905 || val_loss 1.6880 acc 0.451 f1 0.395\n",
            "[W3] ANN Epoch 33 | train_loss 0.2554 acc 0.900 f1 0.900 || val_loss 1.7351 acc 0.447 f1 0.374\n",
            "[W3] ANN Epoch 34 | train_loss 0.2443 acc 0.910 f1 0.910 || val_loss 1.7087 acc 0.432 f1 0.362\n",
            "[W3] ANN Epoch 35 | train_loss 0.2313 acc 0.910 f1 0.910 || val_loss 1.7398 acc 0.436 f1 0.360\n",
            "[W3] ANN Epoch 36 | train_loss 0.2289 acc 0.907 f1 0.906 || val_loss 1.7564 acc 0.459 f1 0.398\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=84\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0668 acc 0.408 f1 0.410 || val_loss 1.0215 acc 0.397 f1 0.324\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9200 acc 0.541 f1 0.538 || val_loss 1.0639 acc 0.360 f1 0.308\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.7923 acc 0.603 f1 0.597 || val_loss 1.1041 acc 0.379 f1 0.311\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7072 acc 0.663 f1 0.661 || val_loss 1.1946 acc 0.350 f1 0.305\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6309 acc 0.704 f1 0.702 || val_loss 1.2368 acc 0.381 f1 0.336\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5636 acc 0.743 f1 0.741 || val_loss 1.2959 acc 0.340 f1 0.283\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5027 acc 0.752 f1 0.751 || val_loss 1.3012 acc 0.412 f1 0.344\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4531 acc 0.789 f1 0.789 || val_loss 1.3993 acc 0.401 f1 0.324\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4222 acc 0.804 f1 0.803 || val_loss 1.4379 acc 0.397 f1 0.321\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3872 acc 0.833 f1 0.833 || val_loss 1.5077 acc 0.399 f1 0.335\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3462 acc 0.857 f1 0.857 || val_loss 1.5647 acc 0.389 f1 0.292\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2923 acc 0.888 f1 0.887 || val_loss 1.6950 acc 0.387 f1 0.318\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2674 acc 0.897 f1 0.897 || val_loss 1.7980 acc 0.403 f1 0.342\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2324 acc 0.910 f1 0.910 || val_loss 1.8535 acc 0.395 f1 0.329\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2055 acc 0.925 f1 0.925 || val_loss 1.9451 acc 0.405 f1 0.321\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=84\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0977 acc 0.340 f1 0.297 || val_loss 1.0916 acc 0.364 f1 0.299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0795 acc 0.438 f1 0.437 || val_loss 1.0864 acc 0.366 f1 0.320\n",
            "[W3] RNN Epoch 03 | train_loss 1.0626 acc 0.441 f1 0.440 || val_loss 1.0848 acc 0.323 f1 0.294\n",
            "[W3] RNN Epoch 04 | train_loss 1.0351 acc 0.464 f1 0.461 || val_loss 1.0865 acc 0.317 f1 0.281\n",
            "[W3] RNN Epoch 05 | train_loss 1.0093 acc 0.502 f1 0.496 || val_loss 1.0656 acc 0.354 f1 0.305\n",
            "[W3] RNN Epoch 06 | train_loss 0.9806 acc 0.517 f1 0.509 || val_loss 1.0739 acc 0.358 f1 0.308\n",
            "[W3] RNN Epoch 07 | train_loss 0.9557 acc 0.536 f1 0.528 || val_loss 1.1079 acc 0.333 f1 0.296\n",
            "[W3] RNN Epoch 08 | train_loss 0.9268 acc 0.552 f1 0.543 || val_loss 1.0829 acc 0.352 f1 0.306\n",
            "[W3] RNN Epoch 09 | train_loss 0.9005 acc 0.573 f1 0.562 || val_loss 1.0674 acc 0.399 f1 0.343\n",
            "[W3] RNN Epoch 10 | train_loss 0.8796 acc 0.577 f1 0.568 || val_loss 1.0590 acc 0.397 f1 0.331\n",
            "[W3] RNN Epoch 11 | train_loss 0.8597 acc 0.590 f1 0.581 || val_loss 1.1159 acc 0.387 f1 0.345\n",
            "[W3] RNN Epoch 12 | train_loss 0.8381 acc 0.596 f1 0.588 || val_loss 1.1045 acc 0.397 f1 0.338\n",
            "[W3] RNN Epoch 13 | train_loss 0.8156 acc 0.623 f1 0.616 || val_loss 1.1124 acc 0.385 f1 0.332\n",
            "[W3] RNN Epoch 14 | train_loss 0.8004 acc 0.622 f1 0.614 || val_loss 1.1046 acc 0.403 f1 0.342\n",
            "[W3] RNN Epoch 15 | train_loss 0.7757 acc 0.634 f1 0.628 || val_loss 1.1585 acc 0.370 f1 0.326\n",
            "[W3] RNN Epoch 16 | train_loss 0.7662 acc 0.636 f1 0.629 || val_loss 1.1232 acc 0.385 f1 0.340\n",
            "[W3] RNN Epoch 17 | train_loss 0.7390 acc 0.643 f1 0.636 || val_loss 1.1562 acc 0.383 f1 0.328\n",
            "[W3] RNN Epoch 18 | train_loss 0.7088 acc 0.665 f1 0.658 || val_loss 1.1407 acc 0.416 f1 0.352\n",
            "[W3] RNN Epoch 19 | train_loss 0.6840 acc 0.674 f1 0.668 || val_loss 1.1605 acc 0.397 f1 0.328\n",
            "[W3] RNN Epoch 20 | train_loss 0.6817 acc 0.671 f1 0.666 || val_loss 1.1611 acc 0.412 f1 0.341\n",
            "[W3] RNN Epoch 21 | train_loss 0.6563 acc 0.695 f1 0.689 || val_loss 1.1862 acc 0.409 f1 0.339\n",
            "[W3] RNN Epoch 22 | train_loss 0.6452 acc 0.696 f1 0.691 || val_loss 1.1975 acc 0.391 f1 0.319\n",
            "[W3] RNN Epoch 23 | train_loss 0.6239 acc 0.701 f1 0.697 || val_loss 1.2166 acc 0.414 f1 0.333\n",
            "[W3] RNN Epoch 24 | train_loss 0.5960 acc 0.715 f1 0.711 || val_loss 1.2428 acc 0.389 f1 0.311\n",
            "[W3] RNN Epoch 25 | train_loss 0.5906 acc 0.715 f1 0.711 || val_loss 1.2677 acc 0.395 f1 0.322\n",
            "[W3] RNN Epoch 26 | train_loss 0.5778 acc 0.723 f1 0.719 || val_loss 1.2902 acc 0.381 f1 0.317\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=84\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1000 acc 0.344 f1 0.251 || val_loss 1.0838 acc 0.422 f1 0.313\n",
            "[W3] GRU Epoch 02 | train_loss 1.0902 acc 0.374 f1 0.353 || val_loss 1.0938 acc 0.321 f1 0.292\n",
            "[W3] GRU Epoch 03 | train_loss 1.0797 acc 0.432 f1 0.429 || val_loss 1.0893 acc 0.342 f1 0.318\n",
            "[W3] GRU Epoch 04 | train_loss 1.0615 acc 0.458 f1 0.456 || val_loss 1.1088 acc 0.280 f1 0.275\n",
            "[W3] GRU Epoch 05 | train_loss 1.0348 acc 0.485 f1 0.481 || val_loss 1.1036 acc 0.296 f1 0.285\n",
            "[W3] GRU Epoch 06 | train_loss 0.9880 acc 0.524 f1 0.518 || val_loss 1.0824 acc 0.321 f1 0.298\n",
            "[W3] GRU Epoch 07 | train_loss 0.9125 acc 0.572 f1 0.565 || val_loss 1.0583 acc 0.374 f1 0.325\n",
            "[W3] GRU Epoch 08 | train_loss 0.8376 acc 0.600 f1 0.593 || val_loss 1.0975 acc 0.366 f1 0.304\n",
            "[W3] GRU Epoch 09 | train_loss 0.7864 acc 0.623 f1 0.619 || val_loss 1.1305 acc 0.360 f1 0.305\n",
            "[W3] GRU Epoch 10 | train_loss 0.7428 acc 0.650 f1 0.646 || val_loss 1.1618 acc 0.366 f1 0.313\n",
            "[W3] GRU Epoch 11 | train_loss 0.7033 acc 0.676 f1 0.672 || val_loss 1.1635 acc 0.354 f1 0.296\n",
            "[W3] GRU Epoch 12 | train_loss 0.6669 acc 0.686 f1 0.683 || val_loss 1.1879 acc 0.368 f1 0.299\n",
            "[W3] GRU Epoch 13 | train_loss 0.6381 acc 0.702 f1 0.699 || val_loss 1.2176 acc 0.362 f1 0.299\n",
            "[W3] GRU Epoch 14 | train_loss 0.6084 acc 0.710 f1 0.708 || val_loss 1.2631 acc 0.368 f1 0.308\n",
            "[W3] GRU Epoch 15 | train_loss 0.5867 acc 0.723 f1 0.720 || val_loss 1.2536 acc 0.391 f1 0.316\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=84\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0993 acc 0.335 f1 0.189 || val_loss 1.0892 acc 0.428 f1 0.265\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0938 acc 0.384 f1 0.364 || val_loss 1.0950 acc 0.327 f1 0.309\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0847 acc 0.409 f1 0.403 || val_loss 1.0930 acc 0.311 f1 0.297\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0649 acc 0.440 f1 0.433 || val_loss 1.0914 acc 0.313 f1 0.296\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0187 acc 0.487 f1 0.471 || val_loss 1.0870 acc 0.344 f1 0.312\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9147 acc 0.549 f1 0.539 || val_loss 1.1220 acc 0.323 f1 0.270\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8266 acc 0.596 f1 0.582 || val_loss 1.0822 acc 0.395 f1 0.314\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7807 acc 0.610 f1 0.601 || val_loss 1.1537 acc 0.360 f1 0.303\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7339 acc 0.641 f1 0.636 || val_loss 1.1402 acc 0.381 f1 0.295\n",
            "[W3] LSTM Epoch 10 | train_loss 0.6996 acc 0.653 f1 0.647 || val_loss 1.1537 acc 0.385 f1 0.301\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6680 acc 0.673 f1 0.671 || val_loss 1.1750 acc 0.387 f1 0.300\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6413 acc 0.692 f1 0.687 || val_loss 1.2167 acc 0.374 f1 0.294\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6084 acc 0.700 f1 0.698 || val_loss 1.2562 acc 0.381 f1 0.305\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5806 acc 0.711 f1 0.709 || val_loss 1.2911 acc 0.377 f1 0.301\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5623 acc 0.727 f1 0.726 || val_loss 1.3150 acc 0.366 f1 0.294\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  84%| | 84/100 [40:01<08:20, 31.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=85 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=85\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1100 acc 0.411 f1 0.408 || val_loss 1.1475 acc 0.300 f1 0.282\n",
            "[W3] ANN Epoch 02 | train_loss 0.9497 acc 0.528 f1 0.520 || val_loss 1.1496 acc 0.329 f1 0.303\n",
            "[W3] ANN Epoch 03 | train_loss 0.8434 acc 0.585 f1 0.576 || val_loss 1.1269 acc 0.354 f1 0.304\n",
            "[W3] ANN Epoch 04 | train_loss 0.7758 acc 0.619 f1 0.612 || val_loss 1.1039 acc 0.405 f1 0.328\n",
            "[W3] ANN Epoch 05 | train_loss 0.7359 acc 0.635 f1 0.630 || val_loss 1.1262 acc 0.409 f1 0.331\n",
            "[W3] ANN Epoch 06 | train_loss 0.6684 acc 0.686 f1 0.685 || val_loss 1.1490 acc 0.426 f1 0.358\n",
            "[W3] ANN Epoch 07 | train_loss 0.6319 acc 0.700 f1 0.696 || val_loss 1.1518 acc 0.426 f1 0.344\n",
            "[W3] ANN Epoch 08 | train_loss 0.5914 acc 0.720 f1 0.718 || val_loss 1.1646 acc 0.412 f1 0.338\n",
            "[W3] ANN Epoch 09 | train_loss 0.5652 acc 0.732 f1 0.730 || val_loss 1.2051 acc 0.432 f1 0.351\n",
            "[W3] ANN Epoch 10 | train_loss 0.5342 acc 0.747 f1 0.745 || val_loss 1.2270 acc 0.426 f1 0.329\n",
            "[W3] ANN Epoch 11 | train_loss 0.5011 acc 0.770 f1 0.769 || val_loss 1.2365 acc 0.418 f1 0.338\n",
            "[W3] ANN Epoch 12 | train_loss 0.4924 acc 0.777 f1 0.777 || val_loss 1.3149 acc 0.440 f1 0.349\n",
            "[W3] ANN Epoch 13 | train_loss 0.4863 acc 0.775 f1 0.775 || val_loss 1.2799 acc 0.449 f1 0.360\n",
            "[W3] ANN Epoch 14 | train_loss 0.4552 acc 0.796 f1 0.795 || val_loss 1.3227 acc 0.438 f1 0.338\n",
            "[W3] ANN Epoch 15 | train_loss 0.4397 acc 0.808 f1 0.808 || val_loss 1.2705 acc 0.447 f1 0.370\n",
            "[W3] ANN Epoch 16 | train_loss 0.4198 acc 0.817 f1 0.817 || val_loss 1.3569 acc 0.424 f1 0.343\n",
            "[W3] ANN Epoch 17 | train_loss 0.4082 acc 0.829 f1 0.828 || val_loss 1.3992 acc 0.455 f1 0.376\n",
            "[W3] ANN Epoch 18 | train_loss 0.3859 acc 0.840 f1 0.840 || val_loss 1.4249 acc 0.414 f1 0.330\n",
            "[W3] ANN Epoch 19 | train_loss 0.3807 acc 0.844 f1 0.843 || val_loss 1.4344 acc 0.424 f1 0.345\n",
            "[W3] ANN Epoch 20 | train_loss 0.3602 acc 0.846 f1 0.846 || val_loss 1.4931 acc 0.422 f1 0.343\n",
            "[W3] ANN Epoch 21 | train_loss 0.3586 acc 0.848 f1 0.848 || val_loss 1.4549 acc 0.457 f1 0.371\n",
            "[W3] ANN Epoch 22 | train_loss 0.3446 acc 0.857 f1 0.856 || val_loss 1.5559 acc 0.434 f1 0.368\n",
            "[W3] ANN Epoch 23 | train_loss 0.3503 acc 0.855 f1 0.854 || val_loss 1.4989 acc 0.418 f1 0.334\n",
            "[W3] ANN Epoch 24 | train_loss 0.3298 acc 0.871 f1 0.871 || val_loss 1.5377 acc 0.444 f1 0.363\n",
            "[W3] ANN Epoch 25 | train_loss 0.3268 acc 0.863 f1 0.863 || val_loss 1.6311 acc 0.438 f1 0.356\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=85\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0769 acc 0.393 f1 0.394 || val_loss 1.0383 acc 0.383 f1 0.303\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9546 acc 0.539 f1 0.534 || val_loss 1.0417 acc 0.383 f1 0.288\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8256 acc 0.612 f1 0.609 || val_loss 1.1317 acc 0.366 f1 0.304\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7286 acc 0.665 f1 0.661 || val_loss 1.2140 acc 0.344 f1 0.285\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6512 acc 0.699 f1 0.698 || val_loss 1.2368 acc 0.385 f1 0.301\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5800 acc 0.730 f1 0.729 || val_loss 1.2963 acc 0.387 f1 0.308\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5275 acc 0.759 f1 0.758 || val_loss 1.3602 acc 0.395 f1 0.311\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4649 acc 0.803 f1 0.803 || val_loss 1.4408 acc 0.391 f1 0.309\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4198 acc 0.817 f1 0.817 || val_loss 1.5635 acc 0.360 f1 0.297\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3717 acc 0.848 f1 0.847 || val_loss 1.5942 acc 0.393 f1 0.324\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3268 acc 0.866 f1 0.866 || val_loss 1.6656 acc 0.412 f1 0.331\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2975 acc 0.877 f1 0.876 || val_loss 1.7660 acc 0.424 f1 0.329\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2686 acc 0.897 f1 0.897 || val_loss 1.8754 acc 0.412 f1 0.337\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2230 acc 0.916 f1 0.916 || val_loss 1.9436 acc 0.399 f1 0.321\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2012 acc 0.924 f1 0.924 || val_loss 2.0355 acc 0.428 f1 0.357\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1759 acc 0.939 f1 0.939 || val_loss 2.1388 acc 0.409 f1 0.339\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1520 acc 0.951 f1 0.951 || val_loss 2.2064 acc 0.405 f1 0.339\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1325 acc 0.956 f1 0.956 || val_loss 2.3303 acc 0.397 f1 0.330\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1233 acc 0.958 f1 0.958 || val_loss 2.4941 acc 0.385 f1 0.324\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.0963 acc 0.971 f1 0.971 || val_loss 2.4743 acc 0.412 f1 0.321\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.0840 acc 0.976 f1 0.976 || val_loss 2.6782 acc 0.391 f1 0.320\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.0723 acc 0.976 f1 0.976 || val_loss 2.6683 acc 0.426 f1 0.347\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.0793 acc 0.970 f1 0.970 || val_loss 2.8114 acc 0.364 f1 0.312\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=85\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0995 acc 0.352 f1 0.308 || val_loss 1.0973 acc 0.350 f1 0.320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0812 acc 0.430 f1 0.427 || val_loss 1.0895 acc 0.340 f1 0.312\n",
            "[W3] RNN Epoch 03 | train_loss 1.0629 acc 0.451 f1 0.445 || val_loss 1.0833 acc 0.340 f1 0.318\n",
            "[W3] RNN Epoch 04 | train_loss 1.0431 acc 0.473 f1 0.471 || val_loss 1.1046 acc 0.325 f1 0.315\n",
            "[W3] RNN Epoch 05 | train_loss 1.0228 acc 0.488 f1 0.481 || val_loss 1.0686 acc 0.377 f1 0.346\n",
            "[W3] RNN Epoch 06 | train_loss 1.0028 acc 0.505 f1 0.500 || val_loss 1.0867 acc 0.358 f1 0.329\n",
            "[W3] RNN Epoch 07 | train_loss 0.9750 acc 0.529 f1 0.523 || val_loss 1.0891 acc 0.374 f1 0.339\n",
            "[W3] RNN Epoch 08 | train_loss 0.9534 acc 0.544 f1 0.535 || val_loss 1.0711 acc 0.374 f1 0.328\n",
            "[W3] RNN Epoch 09 | train_loss 0.9236 acc 0.562 f1 0.552 || val_loss 1.0755 acc 0.366 f1 0.311\n",
            "[W3] RNN Epoch 10 | train_loss 0.8899 acc 0.573 f1 0.564 || val_loss 1.0822 acc 0.356 f1 0.310\n",
            "[W3] RNN Epoch 11 | train_loss 0.8687 acc 0.594 f1 0.585 || val_loss 1.0858 acc 0.364 f1 0.315\n",
            "[W3] RNN Epoch 12 | train_loss 0.8424 acc 0.604 f1 0.594 || val_loss 1.0915 acc 0.360 f1 0.314\n",
            "[W3] RNN Epoch 13 | train_loss 0.8159 acc 0.629 f1 0.621 || val_loss 1.0859 acc 0.364 f1 0.308\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=85\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0990 acc 0.343 f1 0.322 || val_loss 1.0978 acc 0.362 f1 0.307\n",
            "[W3] GRU Epoch 02 | train_loss 1.0912 acc 0.395 f1 0.394 || val_loss 1.1001 acc 0.327 f1 0.308\n",
            "[W3] GRU Epoch 03 | train_loss 1.0809 acc 0.433 f1 0.432 || val_loss 1.0919 acc 0.342 f1 0.315\n",
            "[W3] GRU Epoch 04 | train_loss 1.0617 acc 0.462 f1 0.458 || val_loss 1.0908 acc 0.340 f1 0.317\n",
            "[W3] GRU Epoch 05 | train_loss 1.0265 acc 0.502 f1 0.494 || val_loss 1.0958 acc 0.329 f1 0.295\n",
            "[W3] GRU Epoch 06 | train_loss 0.9732 acc 0.531 f1 0.525 || val_loss 1.1618 acc 0.302 f1 0.285\n",
            "[W3] GRU Epoch 07 | train_loss 0.9040 acc 0.567 f1 0.559 || val_loss 1.1234 acc 0.358 f1 0.311\n",
            "[W3] GRU Epoch 08 | train_loss 0.8347 acc 0.601 f1 0.594 || val_loss 1.1273 acc 0.383 f1 0.311\n",
            "[W3] GRU Epoch 09 | train_loss 0.7829 acc 0.620 f1 0.614 || val_loss 1.1705 acc 0.358 f1 0.297\n",
            "[W3] GRU Epoch 10 | train_loss 0.7563 acc 0.633 f1 0.628 || val_loss 1.1805 acc 0.379 f1 0.306\n",
            "[W3] GRU Epoch 11 | train_loss 0.7285 acc 0.646 f1 0.642 || val_loss 1.2111 acc 0.350 f1 0.296\n",
            "[W3] GRU Epoch 12 | train_loss 0.6961 acc 0.660 f1 0.656 || val_loss 1.2151 acc 0.368 f1 0.302\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=85\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0996 acc 0.346 f1 0.262 || val_loss 1.1032 acc 0.296 f1 0.289\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0930 acc 0.382 f1 0.371 || val_loss 1.0929 acc 0.344 f1 0.322\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0840 acc 0.406 f1 0.388 || val_loss 1.0887 acc 0.329 f1 0.298\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0675 acc 0.430 f1 0.420 || val_loss 1.0885 acc 0.319 f1 0.302\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0343 acc 0.477 f1 0.461 || val_loss 1.0576 acc 0.350 f1 0.301\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9540 acc 0.534 f1 0.523 || val_loss 1.1052 acc 0.354 f1 0.317\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8591 acc 0.561 f1 0.552 || val_loss 1.1459 acc 0.344 f1 0.301\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8011 acc 0.603 f1 0.597 || val_loss 1.1571 acc 0.356 f1 0.295\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7610 acc 0.618 f1 0.612 || val_loss 1.2060 acc 0.346 f1 0.301\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7163 acc 0.639 f1 0.634 || val_loss 1.1972 acc 0.368 f1 0.318\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  85%| | 85/100 [40:27<07:24, 29.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=86 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=86\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1460 acc 0.401 f1 0.394 || val_loss 1.1657 acc 0.292 f1 0.283\n",
            "[W3] ANN Epoch 02 | train_loss 0.9702 acc 0.513 f1 0.503 || val_loss 1.1383 acc 0.325 f1 0.306\n",
            "[W3] ANN Epoch 03 | train_loss 0.8810 acc 0.583 f1 0.576 || val_loss 1.1292 acc 0.374 f1 0.326\n",
            "[W3] ANN Epoch 04 | train_loss 0.7814 acc 0.624 f1 0.620 || val_loss 1.1422 acc 0.374 f1 0.327\n",
            "[W3] ANN Epoch 05 | train_loss 0.7237 acc 0.645 f1 0.641 || val_loss 1.1208 acc 0.387 f1 0.317\n",
            "[W3] ANN Epoch 06 | train_loss 0.6731 acc 0.681 f1 0.679 || val_loss 1.1451 acc 0.383 f1 0.322\n",
            "[W3] ANN Epoch 07 | train_loss 0.6357 acc 0.697 f1 0.694 || val_loss 1.1859 acc 0.401 f1 0.319\n",
            "[W3] ANN Epoch 08 | train_loss 0.6156 acc 0.704 f1 0.703 || val_loss 1.1775 acc 0.395 f1 0.333\n",
            "[W3] ANN Epoch 09 | train_loss 0.5731 acc 0.728 f1 0.726 || val_loss 1.2011 acc 0.399 f1 0.328\n",
            "[W3] ANN Epoch 10 | train_loss 0.5389 acc 0.747 f1 0.746 || val_loss 1.1968 acc 0.403 f1 0.325\n",
            "[W3] ANN Epoch 11 | train_loss 0.5162 acc 0.761 f1 0.760 || val_loss 1.2599 acc 0.414 f1 0.347\n",
            "[W3] ANN Epoch 12 | train_loss 0.5054 acc 0.773 f1 0.771 || val_loss 1.2657 acc 0.438 f1 0.366\n",
            "[W3] ANN Epoch 13 | train_loss 0.4784 acc 0.780 f1 0.780 || val_loss 1.3207 acc 0.426 f1 0.346\n",
            "[W3] ANN Epoch 14 | train_loss 0.4536 acc 0.795 f1 0.795 || val_loss 1.3166 acc 0.418 f1 0.347\n",
            "[W3] ANN Epoch 15 | train_loss 0.4371 acc 0.817 f1 0.816 || val_loss 1.3748 acc 0.430 f1 0.333\n",
            "[W3] ANN Epoch 16 | train_loss 0.4248 acc 0.820 f1 0.820 || val_loss 1.3788 acc 0.426 f1 0.371\n",
            "[W3] ANN Epoch 17 | train_loss 0.4232 acc 0.808 f1 0.808 || val_loss 1.4059 acc 0.414 f1 0.337\n",
            "[W3] ANN Epoch 18 | train_loss 0.3915 acc 0.834 f1 0.833 || val_loss 1.4049 acc 0.420 f1 0.343\n",
            "[W3] ANN Epoch 19 | train_loss 0.3594 acc 0.841 f1 0.841 || val_loss 1.4674 acc 0.426 f1 0.336\n",
            "[W3] ANN Epoch 20 | train_loss 0.3690 acc 0.839 f1 0.838 || val_loss 1.4726 acc 0.457 f1 0.358\n",
            "[W3] ANN Epoch 21 | train_loss 0.3532 acc 0.845 f1 0.845 || val_loss 1.4530 acc 0.455 f1 0.369\n",
            "[W3] ANN Epoch 22 | train_loss 0.3412 acc 0.861 f1 0.861 || val_loss 1.5231 acc 0.436 f1 0.360\n",
            "[W3] ANN Epoch 23 | train_loss 0.3251 acc 0.871 f1 0.870 || val_loss 1.5287 acc 0.440 f1 0.353\n",
            "[W3] ANN Epoch 24 | train_loss 0.2984 acc 0.882 f1 0.882 || val_loss 1.5406 acc 0.418 f1 0.318\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=86\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0678 acc 0.411 f1 0.412 || val_loss 1.0144 acc 0.436 f1 0.344\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9401 acc 0.525 f1 0.520 || val_loss 1.0483 acc 0.381 f1 0.316\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8174 acc 0.598 f1 0.594 || val_loss 1.0587 acc 0.393 f1 0.310\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7112 acc 0.660 f1 0.658 || val_loss 1.1323 acc 0.381 f1 0.324\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6314 acc 0.699 f1 0.698 || val_loss 1.1526 acc 0.389 f1 0.325\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5610 acc 0.732 f1 0.730 || val_loss 1.2201 acc 0.403 f1 0.327\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5077 acc 0.761 f1 0.760 || val_loss 1.2658 acc 0.391 f1 0.317\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4477 acc 0.796 f1 0.796 || val_loss 1.3749 acc 0.414 f1 0.346\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4138 acc 0.812 f1 0.811 || val_loss 1.3849 acc 0.424 f1 0.351\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3656 acc 0.837 f1 0.836 || val_loss 1.4779 acc 0.387 f1 0.318\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3353 acc 0.862 f1 0.862 || val_loss 1.5623 acc 0.407 f1 0.338\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3054 acc 0.874 f1 0.874 || val_loss 1.6731 acc 0.409 f1 0.333\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2703 acc 0.896 f1 0.895 || val_loss 1.7284 acc 0.424 f1 0.341\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2397 acc 0.907 f1 0.907 || val_loss 1.8103 acc 0.414 f1 0.338\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1992 acc 0.930 f1 0.930 || val_loss 1.9439 acc 0.405 f1 0.336\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1709 acc 0.942 f1 0.942 || val_loss 1.9859 acc 0.424 f1 0.361\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1563 acc 0.947 f1 0.947 || val_loss 2.0904 acc 0.422 f1 0.339\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1324 acc 0.955 f1 0.956 || val_loss 2.1644 acc 0.436 f1 0.376\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1163 acc 0.962 f1 0.962 || val_loss 2.2836 acc 0.416 f1 0.339\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1145 acc 0.961 f1 0.961 || val_loss 2.4073 acc 0.401 f1 0.312\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.1084 acc 0.967 f1 0.967 || val_loss 2.4963 acc 0.403 f1 0.334\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1133 acc 0.959 f1 0.959 || val_loss 2.3913 acc 0.414 f1 0.339\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.0962 acc 0.968 f1 0.968 || val_loss 2.5156 acc 0.409 f1 0.348\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.0776 acc 0.979 f1 0.979 || val_loss 2.5532 acc 0.422 f1 0.347\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.0608 acc 0.982 f1 0.982 || val_loss 2.7396 acc 0.407 f1 0.328\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.0516 acc 0.985 f1 0.985 || val_loss 2.8099 acc 0.424 f1 0.345\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=86\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0979 acc 0.354 f1 0.320 || val_loss 1.1014 acc 0.280 f1 0.260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0790 acc 0.417 f1 0.398 || val_loss 1.1072 acc 0.284 f1 0.270\n",
            "[W3] RNN Epoch 03 | train_loss 1.0636 acc 0.441 f1 0.421 || val_loss 1.0882 acc 0.321 f1 0.287\n",
            "[W3] RNN Epoch 04 | train_loss 1.0463 acc 0.456 f1 0.451 || val_loss 1.0907 acc 0.360 f1 0.329\n",
            "[W3] RNN Epoch 05 | train_loss 1.0269 acc 0.472 f1 0.461 || val_loss 1.0742 acc 0.352 f1 0.312\n",
            "[W3] RNN Epoch 06 | train_loss 1.0036 acc 0.504 f1 0.498 || val_loss 1.0907 acc 0.348 f1 0.318\n",
            "[W3] RNN Epoch 07 | train_loss 0.9850 acc 0.519 f1 0.510 || val_loss 1.1224 acc 0.313 f1 0.291\n",
            "[W3] RNN Epoch 08 | train_loss 0.9465 acc 0.536 f1 0.527 || val_loss 1.1014 acc 0.350 f1 0.310\n",
            "[W3] RNN Epoch 09 | train_loss 0.9187 acc 0.556 f1 0.546 || val_loss 1.1268 acc 0.346 f1 0.319\n",
            "[W3] RNN Epoch 10 | train_loss 0.8981 acc 0.564 f1 0.554 || val_loss 1.1179 acc 0.354 f1 0.320\n",
            "[W3] RNN Epoch 11 | train_loss 0.8648 acc 0.592 f1 0.582 || val_loss 1.1224 acc 0.358 f1 0.321\n",
            "[W3] RNN Epoch 12 | train_loss 0.8469 acc 0.587 f1 0.575 || val_loss 1.1297 acc 0.368 f1 0.327\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=86\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0995 acc 0.335 f1 0.246 || val_loss 1.0935 acc 0.405 f1 0.318\n",
            "[W3] GRU Epoch 02 | train_loss 1.0921 acc 0.403 f1 0.398 || val_loss 1.1028 acc 0.286 f1 0.279\n",
            "[W3] GRU Epoch 03 | train_loss 1.0830 acc 0.419 f1 0.413 || val_loss 1.0941 acc 0.321 f1 0.298\n",
            "[W3] GRU Epoch 04 | train_loss 1.0682 acc 0.450 f1 0.444 || val_loss 1.0890 acc 0.317 f1 0.286\n",
            "[W3] GRU Epoch 05 | train_loss 1.0432 acc 0.484 f1 0.472 || val_loss 1.0915 acc 0.331 f1 0.301\n",
            "[W3] GRU Epoch 06 | train_loss 0.9815 acc 0.516 f1 0.506 || val_loss 1.0723 acc 0.362 f1 0.308\n",
            "[W3] GRU Epoch 07 | train_loss 0.8920 acc 0.558 f1 0.549 || val_loss 1.1107 acc 0.352 f1 0.318\n",
            "[W3] GRU Epoch 08 | train_loss 0.8243 acc 0.592 f1 0.585 || val_loss 1.1064 acc 0.379 f1 0.324\n",
            "[W3] GRU Epoch 09 | train_loss 0.7762 acc 0.616 f1 0.613 || val_loss 1.1436 acc 0.372 f1 0.329\n",
            "[W3] GRU Epoch 10 | train_loss 0.7398 acc 0.636 f1 0.631 || val_loss 1.1257 acc 0.372 f1 0.303\n",
            "[W3] GRU Epoch 11 | train_loss 0.7000 acc 0.655 f1 0.650 || val_loss 1.1236 acc 0.395 f1 0.324\n",
            "[W3] GRU Epoch 12 | train_loss 0.6695 acc 0.668 f1 0.665 || val_loss 1.1707 acc 0.391 f1 0.332\n",
            "[W3] GRU Epoch 13 | train_loss 0.6370 acc 0.681 f1 0.678 || val_loss 1.1927 acc 0.391 f1 0.321\n",
            "[W3] GRU Epoch 14 | train_loss 0.6080 acc 0.704 f1 0.702 || val_loss 1.2178 acc 0.389 f1 0.322\n",
            "[W3] GRU Epoch 15 | train_loss 0.5803 acc 0.715 f1 0.712 || val_loss 1.2806 acc 0.387 f1 0.330\n",
            "[W3] GRU Epoch 16 | train_loss 0.5614 acc 0.731 f1 0.729 || val_loss 1.2491 acc 0.409 f1 0.332\n",
            "[W3] GRU Epoch 17 | train_loss 0.5312 acc 0.744 f1 0.744 || val_loss 1.2848 acc 0.395 f1 0.325\n",
            "[W3] GRU Epoch 18 | train_loss 0.5120 acc 0.747 f1 0.746 || val_loss 1.3356 acc 0.409 f1 0.335\n",
            "[W3] GRU Epoch 19 | train_loss 0.4906 acc 0.770 f1 0.769 || val_loss 1.3519 acc 0.401 f1 0.338\n",
            "[W3] GRU Epoch 20 | train_loss 0.4716 acc 0.773 f1 0.773 || val_loss 1.4179 acc 0.407 f1 0.332\n",
            "[W3] GRU Epoch 21 | train_loss 0.4464 acc 0.790 f1 0.790 || val_loss 1.4431 acc 0.412 f1 0.328\n",
            "[W3] GRU Epoch 22 | train_loss 0.4316 acc 0.797 f1 0.797 || val_loss 1.4771 acc 0.428 f1 0.357\n",
            "[W3] GRU Epoch 23 | train_loss 0.4090 acc 0.809 f1 0.809 || val_loss 1.5063 acc 0.422 f1 0.343\n",
            "[W3] GRU Epoch 24 | train_loss 0.4001 acc 0.813 f1 0.813 || val_loss 1.5716 acc 0.407 f1 0.333\n",
            "[W3] GRU Epoch 25 | train_loss 0.3773 acc 0.823 f1 0.823 || val_loss 1.5774 acc 0.414 f1 0.333\n",
            "[W3] GRU Epoch 26 | train_loss 0.3575 acc 0.834 f1 0.833 || val_loss 1.6117 acc 0.422 f1 0.344\n",
            "[W3] GRU Epoch 27 | train_loss 0.3465 acc 0.843 f1 0.843 || val_loss 1.6707 acc 0.432 f1 0.351\n",
            "[W3] GRU Epoch 28 | train_loss 0.3288 acc 0.850 f1 0.850 || val_loss 1.7239 acc 0.428 f1 0.354\n",
            "[W3] GRU Epoch 29 | train_loss 0.3121 acc 0.859 f1 0.859 || val_loss 1.7865 acc 0.422 f1 0.333\n",
            "[W3] GRU Epoch 30 | train_loss 0.2953 acc 0.877 f1 0.877 || val_loss 1.8466 acc 0.422 f1 0.343\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=86\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1001 acc 0.329 f1 0.227 || val_loss 1.0934 acc 0.372 f1 0.278\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0937 acc 0.384 f1 0.383 || val_loss 1.0990 acc 0.290 f1 0.277\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0851 acc 0.410 f1 0.400 || val_loss 1.1128 acc 0.288 f1 0.286\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0699 acc 0.431 f1 0.425 || val_loss 1.1077 acc 0.298 f1 0.288\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0312 acc 0.478 f1 0.467 || val_loss 1.0882 acc 0.350 f1 0.309\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9450 acc 0.546 f1 0.535 || val_loss 1.1333 acc 0.340 f1 0.295\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8635 acc 0.582 f1 0.574 || val_loss 1.0798 acc 0.412 f1 0.348\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7985 acc 0.621 f1 0.617 || val_loss 1.1113 acc 0.383 f1 0.309\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7552 acc 0.635 f1 0.631 || val_loss 1.1716 acc 0.372 f1 0.313\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7113 acc 0.665 f1 0.662 || val_loss 1.1785 acc 0.387 f1 0.323\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6778 acc 0.681 f1 0.677 || val_loss 1.2055 acc 0.374 f1 0.314\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6462 acc 0.691 f1 0.688 || val_loss 1.2398 acc 0.366 f1 0.310\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6164 acc 0.715 f1 0.713 || val_loss 1.2492 acc 0.395 f1 0.330\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5786 acc 0.725 f1 0.723 || val_loss 1.2748 acc 0.389 f1 0.304\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5584 acc 0.734 f1 0.733 || val_loss 1.3359 acc 0.377 f1 0.311\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  86%| | 86/100 [40:58<07:02, 30.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=87 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=87\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1489 acc 0.390 f1 0.386 || val_loss 1.1644 acc 0.294 f1 0.266\n",
            "[W3] ANN Epoch 02 | train_loss 0.9725 acc 0.511 f1 0.503 || val_loss 1.1315 acc 0.350 f1 0.311\n",
            "[W3] ANN Epoch 03 | train_loss 0.8842 acc 0.577 f1 0.570 || val_loss 1.1353 acc 0.348 f1 0.303\n",
            "[W3] ANN Epoch 04 | train_loss 0.7871 acc 0.617 f1 0.611 || val_loss 1.1227 acc 0.372 f1 0.312\n",
            "[W3] ANN Epoch 05 | train_loss 0.7365 acc 0.646 f1 0.642 || val_loss 1.1299 acc 0.403 f1 0.330\n",
            "[W3] ANN Epoch 06 | train_loss 0.6874 acc 0.672 f1 0.669 || val_loss 1.1226 acc 0.409 f1 0.331\n",
            "[W3] ANN Epoch 07 | train_loss 0.6346 acc 0.702 f1 0.699 || val_loss 1.1623 acc 0.397 f1 0.313\n",
            "[W3] ANN Epoch 08 | train_loss 0.6030 acc 0.717 f1 0.716 || val_loss 1.1916 acc 0.440 f1 0.354\n",
            "[W3] ANN Epoch 09 | train_loss 0.5828 acc 0.721 f1 0.720 || val_loss 1.1834 acc 0.430 f1 0.342\n",
            "[W3] ANN Epoch 10 | train_loss 0.5657 acc 0.741 f1 0.740 || val_loss 1.2396 acc 0.426 f1 0.335\n",
            "[W3] ANN Epoch 11 | train_loss 0.5264 acc 0.757 f1 0.756 || val_loss 1.2222 acc 0.451 f1 0.352\n",
            "[W3] ANN Epoch 12 | train_loss 0.4961 acc 0.777 f1 0.776 || val_loss 1.2362 acc 0.440 f1 0.351\n",
            "[W3] ANN Epoch 13 | train_loss 0.4642 acc 0.793 f1 0.793 || val_loss 1.2927 acc 0.455 f1 0.367\n",
            "[W3] ANN Epoch 14 | train_loss 0.4609 acc 0.798 f1 0.798 || val_loss 1.2754 acc 0.440 f1 0.352\n",
            "[W3] ANN Epoch 15 | train_loss 0.4316 acc 0.817 f1 0.816 || val_loss 1.3005 acc 0.447 f1 0.350\n",
            "[W3] ANN Epoch 16 | train_loss 0.4379 acc 0.806 f1 0.806 || val_loss 1.3273 acc 0.432 f1 0.345\n",
            "[W3] ANN Epoch 17 | train_loss 0.3918 acc 0.829 f1 0.829 || val_loss 1.3664 acc 0.436 f1 0.328\n",
            "[W3] ANN Epoch 18 | train_loss 0.3768 acc 0.842 f1 0.842 || val_loss 1.4230 acc 0.432 f1 0.340\n",
            "[W3] ANN Epoch 19 | train_loss 0.3713 acc 0.845 f1 0.845 || val_loss 1.4246 acc 0.461 f1 0.360\n",
            "[W3] ANN Epoch 20 | train_loss 0.3656 acc 0.842 f1 0.842 || val_loss 1.4410 acc 0.447 f1 0.356\n",
            "[W3] ANN Epoch 21 | train_loss 0.3822 acc 0.837 f1 0.837 || val_loss 1.4619 acc 0.432 f1 0.340\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=87\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0701 acc 0.417 f1 0.417 || val_loss 1.0386 acc 0.418 f1 0.284\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9353 acc 0.550 f1 0.546 || val_loss 1.0527 acc 0.405 f1 0.327\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8137 acc 0.608 f1 0.604 || val_loss 1.1393 acc 0.372 f1 0.303\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7049 acc 0.667 f1 0.665 || val_loss 1.1970 acc 0.385 f1 0.313\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6208 acc 0.713 f1 0.711 || val_loss 1.2962 acc 0.383 f1 0.314\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5715 acc 0.748 f1 0.746 || val_loss 1.3088 acc 0.397 f1 0.322\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5061 acc 0.771 f1 0.771 || val_loss 1.4026 acc 0.389 f1 0.310\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4634 acc 0.793 f1 0.792 || val_loss 1.4487 acc 0.422 f1 0.333\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4121 acc 0.824 f1 0.824 || val_loss 1.6177 acc 0.385 f1 0.305\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3677 acc 0.843 f1 0.842 || val_loss 1.6314 acc 0.397 f1 0.308\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3171 acc 0.875 f1 0.875 || val_loss 1.7659 acc 0.440 f1 0.333\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2865 acc 0.882 f1 0.882 || val_loss 1.8916 acc 0.414 f1 0.315\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2309 acc 0.911 f1 0.910 || val_loss 1.9599 acc 0.420 f1 0.313\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2266 acc 0.913 f1 0.913 || val_loss 2.0036 acc 0.414 f1 0.312\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1958 acc 0.932 f1 0.932 || val_loss 2.2265 acc 0.418 f1 0.320\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1854 acc 0.932 f1 0.932 || val_loss 2.2374 acc 0.397 f1 0.313\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=87\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0956 acc 0.362 f1 0.353 || val_loss 1.0953 acc 0.352 f1 0.328\n",
            "[W3] RNN Epoch 02 | train_loss 1.0763 acc 0.414 f1 0.414 || val_loss 1.0945 acc 0.337 f1 0.316\n",
            "[W3] RNN Epoch 03 | train_loss 1.0567 acc 0.442 f1 0.432 || val_loss 1.0711 acc 0.374 f1 0.332\n",
            "[W3] RNN Epoch 04 | train_loss 1.0379 acc 0.468 f1 0.463 || val_loss 1.0758 acc 0.360 f1 0.325\n",
            "[W3] RNN Epoch 05 | train_loss 1.0131 acc 0.494 f1 0.487 || val_loss 1.0851 acc 0.344 f1 0.316\n",
            "[W3] RNN Epoch 06 | train_loss 0.9873 acc 0.519 f1 0.511 || val_loss 1.0860 acc 0.360 f1 0.329\n",
            "[W3] RNN Epoch 07 | train_loss 0.9533 acc 0.540 f1 0.530 || val_loss 1.0624 acc 0.391 f1 0.354\n",
            "[W3] RNN Epoch 08 | train_loss 0.9250 acc 0.555 f1 0.544 || val_loss 1.0415 acc 0.420 f1 0.357\n",
            "[W3] RNN Epoch 09 | train_loss 0.8961 acc 0.577 f1 0.569 || val_loss 1.0855 acc 0.391 f1 0.351\n",
            "[W3] RNN Epoch 10 | train_loss 0.8647 acc 0.583 f1 0.572 || val_loss 1.0528 acc 0.430 f1 0.379\n",
            "[W3] RNN Epoch 11 | train_loss 0.8352 acc 0.597 f1 0.588 || val_loss 1.0816 acc 0.414 f1 0.365\n",
            "[W3] RNN Epoch 12 | train_loss 0.8106 acc 0.608 f1 0.598 || val_loss 1.0881 acc 0.420 f1 0.372\n",
            "[W3] RNN Epoch 13 | train_loss 0.7951 acc 0.626 f1 0.619 || val_loss 1.0639 acc 0.430 f1 0.362\n",
            "[W3] RNN Epoch 14 | train_loss 0.7658 acc 0.629 f1 0.621 || val_loss 1.0773 acc 0.449 f1 0.384\n",
            "[W3] RNN Epoch 15 | train_loss 0.7483 acc 0.638 f1 0.630 || val_loss 1.1084 acc 0.426 f1 0.375\n",
            "[W3] RNN Epoch 16 | train_loss 0.7329 acc 0.648 f1 0.641 || val_loss 1.0944 acc 0.455 f1 0.386\n",
            "[W3] RNN Epoch 17 | train_loss 0.7001 acc 0.666 f1 0.659 || val_loss 1.1335 acc 0.432 f1 0.382\n",
            "[W3] RNN Epoch 18 | train_loss 0.6869 acc 0.671 f1 0.665 || val_loss 1.1381 acc 0.440 f1 0.389\n",
            "[W3] RNN Epoch 19 | train_loss 0.6632 acc 0.680 f1 0.672 || val_loss 1.1498 acc 0.428 f1 0.380\n",
            "[W3] RNN Epoch 20 | train_loss 0.6602 acc 0.681 f1 0.674 || val_loss 1.1503 acc 0.436 f1 0.386\n",
            "[W3] RNN Epoch 21 | train_loss 0.6350 acc 0.691 f1 0.687 || val_loss 1.1647 acc 0.430 f1 0.371\n",
            "[W3] RNN Epoch 22 | train_loss 0.6140 acc 0.702 f1 0.696 || val_loss 1.1773 acc 0.442 f1 0.391\n",
            "[W3] RNN Epoch 23 | train_loss 0.5973 acc 0.714 f1 0.709 || val_loss 1.1941 acc 0.434 f1 0.384\n",
            "[W3] RNN Epoch 24 | train_loss 0.5855 acc 0.725 f1 0.721 || val_loss 1.2164 acc 0.430 f1 0.376\n",
            "[W3] RNN Epoch 25 | train_loss 0.5666 acc 0.725 f1 0.721 || val_loss 1.2205 acc 0.440 f1 0.380\n",
            "[W3] RNN Epoch 26 | train_loss 0.5486 acc 0.732 f1 0.729 || val_loss 1.2654 acc 0.401 f1 0.359\n",
            "[W3] RNN Epoch 27 | train_loss 0.5470 acc 0.730 f1 0.726 || val_loss 1.2578 acc 0.416 f1 0.366\n",
            "[W3] RNN Epoch 28 | train_loss 0.5264 acc 0.745 f1 0.743 || val_loss 1.2694 acc 0.422 f1 0.366\n",
            "[W3] RNN Epoch 29 | train_loss 0.5221 acc 0.740 f1 0.737 || val_loss 1.2891 acc 0.420 f1 0.372\n",
            "[W3] RNN Epoch 30 | train_loss 0.5077 acc 0.753 f1 0.751 || val_loss 1.2937 acc 0.426 f1 0.364\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=87\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0984 acc 0.338 f1 0.264 || val_loss 1.0819 acc 0.432 f1 0.300\n",
            "[W3] GRU Epoch 02 | train_loss 1.0886 acc 0.400 f1 0.391 || val_loss 1.0909 acc 0.335 f1 0.306\n",
            "[W3] GRU Epoch 03 | train_loss 1.0777 acc 0.428 f1 0.425 || val_loss 1.0917 acc 0.337 f1 0.317\n",
            "[W3] GRU Epoch 04 | train_loss 1.0595 acc 0.454 f1 0.448 || val_loss 1.0968 acc 0.325 f1 0.307\n",
            "[W3] GRU Epoch 05 | train_loss 1.0301 acc 0.494 f1 0.485 || val_loss 1.0769 acc 0.348 f1 0.308\n",
            "[W3] GRU Epoch 06 | train_loss 0.9714 acc 0.534 f1 0.527 || val_loss 1.0529 acc 0.389 f1 0.328\n",
            "[W3] GRU Epoch 07 | train_loss 0.8898 acc 0.579 f1 0.572 || val_loss 1.0698 acc 0.385 f1 0.327\n",
            "[W3] GRU Epoch 08 | train_loss 0.8237 acc 0.598 f1 0.592 || val_loss 1.0926 acc 0.389 f1 0.330\n",
            "[W3] GRU Epoch 09 | train_loss 0.7740 acc 0.634 f1 0.629 || val_loss 1.1292 acc 0.385 f1 0.315\n",
            "[W3] GRU Epoch 10 | train_loss 0.7341 acc 0.643 f1 0.639 || val_loss 1.1464 acc 0.393 f1 0.328\n",
            "[W3] GRU Epoch 11 | train_loss 0.6955 acc 0.670 f1 0.666 || val_loss 1.1821 acc 0.362 f1 0.307\n",
            "[W3] GRU Epoch 12 | train_loss 0.6642 acc 0.680 f1 0.676 || val_loss 1.1911 acc 0.379 f1 0.316\n",
            "[W3] GRU Epoch 13 | train_loss 0.6406 acc 0.701 f1 0.697 || val_loss 1.2086 acc 0.405 f1 0.330\n",
            "[W3] GRU Epoch 14 | train_loss 0.6079 acc 0.711 f1 0.707 || val_loss 1.2457 acc 0.387 f1 0.326\n",
            "[W3] GRU Epoch 15 | train_loss 0.5847 acc 0.720 f1 0.717 || val_loss 1.2730 acc 0.385 f1 0.316\n",
            "[W3] GRU Epoch 16 | train_loss 0.5578 acc 0.736 f1 0.733 || val_loss 1.2860 acc 0.387 f1 0.318\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=87\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0989 acc 0.333 f1 0.250 || val_loss 1.1051 acc 0.247 f1 0.246\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0943 acc 0.375 f1 0.375 || val_loss 1.1011 acc 0.311 f1 0.301\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0865 acc 0.405 f1 0.388 || val_loss 1.0986 acc 0.317 f1 0.308\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0724 acc 0.428 f1 0.420 || val_loss 1.0954 acc 0.309 f1 0.295\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0446 acc 0.455 f1 0.441 || val_loss 1.0858 acc 0.344 f1 0.316\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9787 acc 0.515 f1 0.507 || val_loss 1.0714 acc 0.366 f1 0.321\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8907 acc 0.564 f1 0.557 || val_loss 1.1507 acc 0.344 f1 0.305\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8145 acc 0.609 f1 0.603 || val_loss 1.1504 acc 0.358 f1 0.309\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7731 acc 0.625 f1 0.619 || val_loss 1.1298 acc 0.379 f1 0.300\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7374 acc 0.643 f1 0.638 || val_loss 1.1989 acc 0.368 f1 0.315\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7077 acc 0.648 f1 0.643 || val_loss 1.1979 acc 0.370 f1 0.307\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6761 acc 0.668 f1 0.665 || val_loss 1.1911 acc 0.372 f1 0.300\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6485 acc 0.701 f1 0.697 || val_loss 1.2182 acc 0.381 f1 0.306\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6220 acc 0.702 f1 0.699 || val_loss 1.2469 acc 0.399 f1 0.332\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5874 acc 0.720 f1 0.717 || val_loss 1.2858 acc 0.401 f1 0.325\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5651 acc 0.733 f1 0.732 || val_loss 1.3329 acc 0.414 f1 0.344\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5362 acc 0.746 f1 0.744 || val_loss 1.3928 acc 0.403 f1 0.349\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5139 acc 0.770 f1 0.769 || val_loss 1.3864 acc 0.428 f1 0.350\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4918 acc 0.773 f1 0.772 || val_loss 1.4432 acc 0.405 f1 0.328\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4687 acc 0.788 f1 0.787 || val_loss 1.4901 acc 0.414 f1 0.335\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4513 acc 0.794 f1 0.794 || val_loss 1.4844 acc 0.438 f1 0.360\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4282 acc 0.803 f1 0.802 || val_loss 1.5506 acc 0.422 f1 0.345\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4019 acc 0.825 f1 0.825 || val_loss 1.6131 acc 0.428 f1 0.358\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3863 acc 0.834 f1 0.834 || val_loss 1.6635 acc 0.424 f1 0.351\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3659 acc 0.841 f1 0.840 || val_loss 1.7102 acc 0.428 f1 0.351\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3500 acc 0.847 f1 0.847 || val_loss 1.7722 acc 0.412 f1 0.340\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3252 acc 0.864 f1 0.864 || val_loss 1.8277 acc 0.424 f1 0.350\n",
            "[W3] LSTM Epoch 28 | train_loss 0.3080 acc 0.875 f1 0.875 || val_loss 1.8651 acc 0.416 f1 0.335\n",
            "[W3] LSTM Epoch 29 | train_loss 0.2888 acc 0.881 f1 0.881 || val_loss 1.9390 acc 0.412 f1 0.341\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  87%| | 87/100 [41:30<06:40, 30.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=88 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=88\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1456 acc 0.397 f1 0.395 || val_loss 1.1404 acc 0.294 f1 0.290\n",
            "[W3] ANN Epoch 02 | train_loss 0.9853 acc 0.491 f1 0.480 || val_loss 1.1269 acc 0.340 f1 0.329\n",
            "[W3] ANN Epoch 03 | train_loss 0.8698 acc 0.574 f1 0.563 || val_loss 1.1045 acc 0.366 f1 0.330\n",
            "[W3] ANN Epoch 04 | train_loss 0.7960 acc 0.611 f1 0.604 || val_loss 1.0789 acc 0.409 f1 0.354\n",
            "[W3] ANN Epoch 05 | train_loss 0.7187 acc 0.662 f1 0.658 || val_loss 1.0993 acc 0.407 f1 0.349\n",
            "[W3] ANN Epoch 06 | train_loss 0.6767 acc 0.682 f1 0.678 || val_loss 1.0865 acc 0.436 f1 0.359\n",
            "[W3] ANN Epoch 07 | train_loss 0.6217 acc 0.706 f1 0.704 || val_loss 1.1308 acc 0.428 f1 0.349\n",
            "[W3] ANN Epoch 08 | train_loss 0.5968 acc 0.717 f1 0.715 || val_loss 1.1283 acc 0.426 f1 0.356\n",
            "[W3] ANN Epoch 09 | train_loss 0.5601 acc 0.741 f1 0.740 || val_loss 1.1643 acc 0.432 f1 0.361\n",
            "[W3] ANN Epoch 10 | train_loss 0.5406 acc 0.744 f1 0.743 || val_loss 1.1710 acc 0.432 f1 0.351\n",
            "[W3] ANN Epoch 11 | train_loss 0.5048 acc 0.762 f1 0.761 || val_loss 1.2113 acc 0.432 f1 0.341\n",
            "[W3] ANN Epoch 12 | train_loss 0.5061 acc 0.771 f1 0.770 || val_loss 1.2360 acc 0.432 f1 0.352\n",
            "[W3] ANN Epoch 13 | train_loss 0.4760 acc 0.777 f1 0.776 || val_loss 1.2407 acc 0.426 f1 0.336\n",
            "[W3] ANN Epoch 14 | train_loss 0.4377 acc 0.810 f1 0.809 || val_loss 1.2717 acc 0.424 f1 0.355\n",
            "[W3] ANN Epoch 15 | train_loss 0.4495 acc 0.796 f1 0.796 || val_loss 1.3199 acc 0.449 f1 0.359\n",
            "[W3] ANN Epoch 16 | train_loss 0.4017 acc 0.822 f1 0.822 || val_loss 1.3472 acc 0.426 f1 0.353\n",
            "[W3] ANN Epoch 17 | train_loss 0.4206 acc 0.822 f1 0.822 || val_loss 1.3632 acc 0.438 f1 0.357\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=88\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0766 acc 0.404 f1 0.405 || val_loss 1.0259 acc 0.418 f1 0.317\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9489 acc 0.542 f1 0.538 || val_loss 1.0423 acc 0.389 f1 0.311\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8203 acc 0.611 f1 0.607 || val_loss 1.0750 acc 0.383 f1 0.323\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7290 acc 0.658 f1 0.655 || val_loss 1.1537 acc 0.360 f1 0.313\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6500 acc 0.696 f1 0.695 || val_loss 1.2570 acc 0.366 f1 0.328\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5737 acc 0.739 f1 0.737 || val_loss 1.2632 acc 0.383 f1 0.320\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5033 acc 0.780 f1 0.779 || val_loss 1.3248 acc 0.393 f1 0.334\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4493 acc 0.804 f1 0.803 || val_loss 1.4083 acc 0.352 f1 0.291\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.3992 acc 0.832 f1 0.831 || val_loss 1.4920 acc 0.395 f1 0.314\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3795 acc 0.843 f1 0.842 || val_loss 1.5992 acc 0.389 f1 0.319\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3181 acc 0.875 f1 0.875 || val_loss 1.6861 acc 0.370 f1 0.290\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2841 acc 0.889 f1 0.888 || val_loss 1.7711 acc 0.397 f1 0.330\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2447 acc 0.909 f1 0.909 || val_loss 1.8901 acc 0.395 f1 0.315\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2119 acc 0.921 f1 0.921 || val_loss 1.9349 acc 0.395 f1 0.318\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1792 acc 0.932 f1 0.932 || val_loss 2.1007 acc 0.393 f1 0.325\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=88\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1034 acc 0.340 f1 0.259 || val_loss 1.1186 acc 0.243 f1 0.234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0853 acc 0.407 f1 0.384 || val_loss 1.0964 acc 0.340 f1 0.312\n",
            "[W3] RNN Epoch 03 | train_loss 1.0689 acc 0.450 f1 0.445 || val_loss 1.0901 acc 0.344 f1 0.315\n",
            "[W3] RNN Epoch 04 | train_loss 1.0500 acc 0.469 f1 0.466 || val_loss 1.0856 acc 0.354 f1 0.315\n",
            "[W3] RNN Epoch 05 | train_loss 1.0320 acc 0.483 f1 0.478 || val_loss 1.0813 acc 0.377 f1 0.331\n",
            "[W3] RNN Epoch 06 | train_loss 1.0074 acc 0.507 f1 0.504 || val_loss 1.0981 acc 0.348 f1 0.314\n",
            "[W3] RNN Epoch 07 | train_loss 0.9907 acc 0.521 f1 0.514 || val_loss 1.0767 acc 0.381 f1 0.328\n",
            "[W3] RNN Epoch 08 | train_loss 0.9636 acc 0.533 f1 0.527 || val_loss 1.0818 acc 0.391 f1 0.341\n",
            "[W3] RNN Epoch 09 | train_loss 0.9400 acc 0.553 f1 0.549 || val_loss 1.0815 acc 0.381 f1 0.327\n",
            "[W3] RNN Epoch 10 | train_loss 0.9184 acc 0.558 f1 0.551 || val_loss 1.1053 acc 0.366 f1 0.320\n",
            "[W3] RNN Epoch 11 | train_loss 0.8893 acc 0.586 f1 0.582 || val_loss 1.0976 acc 0.370 f1 0.324\n",
            "[W3] RNN Epoch 12 | train_loss 0.8616 acc 0.597 f1 0.590 || val_loss 1.0964 acc 0.362 f1 0.315\n",
            "[W3] RNN Epoch 13 | train_loss 0.8415 acc 0.614 f1 0.608 || val_loss 1.1089 acc 0.372 f1 0.324\n",
            "[W3] RNN Epoch 14 | train_loss 0.8119 acc 0.615 f1 0.609 || val_loss 1.1107 acc 0.360 f1 0.311\n",
            "[W3] RNN Epoch 15 | train_loss 0.7959 acc 0.625 f1 0.620 || val_loss 1.1223 acc 0.379 f1 0.327\n",
            "[W3] RNN Epoch 16 | train_loss 0.7771 acc 0.641 f1 0.635 || val_loss 1.1166 acc 0.391 f1 0.336\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=88\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1003 acc 0.316 f1 0.284 || val_loss 1.0891 acc 0.391 f1 0.334\n",
            "[W3] GRU Epoch 02 | train_loss 1.0916 acc 0.374 f1 0.368 || val_loss 1.1034 acc 0.290 f1 0.280\n",
            "[W3] GRU Epoch 03 | train_loss 1.0817 acc 0.416 f1 0.413 || val_loss 1.1027 acc 0.309 f1 0.285\n",
            "[W3] GRU Epoch 04 | train_loss 1.0672 acc 0.452 f1 0.443 || val_loss 1.1076 acc 0.309 f1 0.296\n",
            "[W3] GRU Epoch 05 | train_loss 1.0399 acc 0.483 f1 0.472 || val_loss 1.0731 acc 0.356 f1 0.321\n",
            "[W3] GRU Epoch 06 | train_loss 0.9948 acc 0.508 f1 0.498 || val_loss 1.0755 acc 0.366 f1 0.328\n",
            "[W3] GRU Epoch 07 | train_loss 0.9202 acc 0.559 f1 0.552 || val_loss 1.0805 acc 0.360 f1 0.314\n",
            "[W3] GRU Epoch 08 | train_loss 0.8548 acc 0.574 f1 0.566 || val_loss 1.0622 acc 0.393 f1 0.327\n",
            "[W3] GRU Epoch 09 | train_loss 0.8011 acc 0.617 f1 0.611 || val_loss 1.0741 acc 0.409 f1 0.343\n",
            "[W3] GRU Epoch 10 | train_loss 0.7538 acc 0.639 f1 0.635 || val_loss 1.1169 acc 0.405 f1 0.336\n",
            "[W3] GRU Epoch 11 | train_loss 0.7150 acc 0.657 f1 0.652 || val_loss 1.1254 acc 0.409 f1 0.337\n",
            "[W3] GRU Epoch 12 | train_loss 0.6795 acc 0.676 f1 0.672 || val_loss 1.1341 acc 0.418 f1 0.336\n",
            "[W3] GRU Epoch 13 | train_loss 0.6554 acc 0.683 f1 0.680 || val_loss 1.1693 acc 0.405 f1 0.326\n",
            "[W3] GRU Epoch 14 | train_loss 0.6191 acc 0.702 f1 0.700 || val_loss 1.2151 acc 0.405 f1 0.331\n",
            "[W3] GRU Epoch 15 | train_loss 0.5938 acc 0.707 f1 0.705 || val_loss 1.3690 acc 0.379 f1 0.322\n",
            "[W3] GRU Epoch 16 | train_loss 0.5778 acc 0.714 f1 0.711 || val_loss 1.2835 acc 0.391 f1 0.322\n",
            "[W3] GRU Epoch 17 | train_loss 0.5414 acc 0.740 f1 0.739 || val_loss 1.3257 acc 0.409 f1 0.339\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=88\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1043 acc 0.334 f1 0.170 || val_loss 1.1244 acc 0.115 f1 0.078\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0943 acc 0.378 f1 0.334 || val_loss 1.1044 acc 0.280 f1 0.274\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0861 acc 0.401 f1 0.392 || val_loss 1.0956 acc 0.292 f1 0.287\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0715 acc 0.436 f1 0.430 || val_loss 1.0833 acc 0.331 f1 0.311\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0487 acc 0.463 f1 0.455 || val_loss 1.0627 acc 0.364 f1 0.320\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9969 acc 0.530 f1 0.525 || val_loss 1.0971 acc 0.362 f1 0.330\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8945 acc 0.575 f1 0.565 || val_loss 1.0811 acc 0.379 f1 0.315\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8198 acc 0.601 f1 0.591 || val_loss 1.1222 acc 0.374 f1 0.314\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7564 acc 0.639 f1 0.633 || val_loss 1.1345 acc 0.366 f1 0.310\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7089 acc 0.650 f1 0.642 || val_loss 1.1689 acc 0.374 f1 0.315\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6826 acc 0.666 f1 0.660 || val_loss 1.1931 acc 0.383 f1 0.323\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6485 acc 0.687 f1 0.683 || val_loss 1.2114 acc 0.370 f1 0.309\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6184 acc 0.699 f1 0.695 || val_loss 1.2456 acc 0.383 f1 0.323\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5897 acc 0.716 f1 0.713 || val_loss 1.2858 acc 0.368 f1 0.303\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  88%| | 88/100 [41:55<05:46, 28.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=89 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=89\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1454 acc 0.395 f1 0.390 || val_loss 1.1793 acc 0.307 f1 0.279\n",
            "[W3] ANN Epoch 02 | train_loss 0.9657 acc 0.520 f1 0.512 || val_loss 1.1530 acc 0.348 f1 0.315\n",
            "[W3] ANN Epoch 03 | train_loss 0.8730 acc 0.577 f1 0.569 || val_loss 1.1363 acc 0.364 f1 0.316\n",
            "[W3] ANN Epoch 04 | train_loss 0.7933 acc 0.625 f1 0.619 || val_loss 1.1431 acc 0.368 f1 0.315\n",
            "[W3] ANN Epoch 05 | train_loss 0.7245 acc 0.657 f1 0.653 || val_loss 1.1524 acc 0.360 f1 0.303\n",
            "[W3] ANN Epoch 06 | train_loss 0.6831 acc 0.677 f1 0.674 || val_loss 1.1368 acc 0.401 f1 0.335\n",
            "[W3] ANN Epoch 07 | train_loss 0.6176 acc 0.701 f1 0.699 || val_loss 1.1632 acc 0.383 f1 0.323\n",
            "[W3] ANN Epoch 08 | train_loss 0.6065 acc 0.723 f1 0.721 || val_loss 1.1865 acc 0.409 f1 0.339\n",
            "[W3] ANN Epoch 09 | train_loss 0.5695 acc 0.742 f1 0.741 || val_loss 1.2271 acc 0.393 f1 0.321\n",
            "[W3] ANN Epoch 10 | train_loss 0.5426 acc 0.759 f1 0.758 || val_loss 1.2218 acc 0.407 f1 0.350\n",
            "[W3] ANN Epoch 11 | train_loss 0.5123 acc 0.773 f1 0.772 || val_loss 1.2482 acc 0.436 f1 0.364\n",
            "[W3] ANN Epoch 12 | train_loss 0.4882 acc 0.775 f1 0.775 || val_loss 1.2546 acc 0.455 f1 0.367\n",
            "[W3] ANN Epoch 13 | train_loss 0.4693 acc 0.778 f1 0.777 || val_loss 1.3127 acc 0.449 f1 0.369\n",
            "[W3] ANN Epoch 14 | train_loss 0.4359 acc 0.819 f1 0.819 || val_loss 1.3513 acc 0.426 f1 0.330\n",
            "[W3] ANN Epoch 15 | train_loss 0.4296 acc 0.816 f1 0.815 || val_loss 1.4094 acc 0.414 f1 0.343\n",
            "[W3] ANN Epoch 16 | train_loss 0.4236 acc 0.820 f1 0.819 || val_loss 1.3790 acc 0.420 f1 0.353\n",
            "[W3] ANN Epoch 17 | train_loss 0.4105 acc 0.821 f1 0.821 || val_loss 1.4075 acc 0.426 f1 0.352\n",
            "[W3] ANN Epoch 18 | train_loss 0.3979 acc 0.834 f1 0.833 || val_loss 1.4259 acc 0.418 f1 0.342\n",
            "[W3] ANN Epoch 19 | train_loss 0.3816 acc 0.837 f1 0.837 || val_loss 1.4633 acc 0.414 f1 0.355\n",
            "[W3] ANN Epoch 20 | train_loss 0.3563 acc 0.863 f1 0.862 || val_loss 1.5322 acc 0.414 f1 0.348\n",
            "[W3] ANN Epoch 21 | train_loss 0.3527 acc 0.849 f1 0.849 || val_loss 1.5569 acc 0.401 f1 0.337\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=89\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0678 acc 0.411 f1 0.414 || val_loss 1.0416 acc 0.416 f1 0.253\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9290 acc 0.545 f1 0.538 || val_loss 1.0553 acc 0.399 f1 0.307\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8062 acc 0.602 f1 0.599 || val_loss 1.1560 acc 0.397 f1 0.327\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7107 acc 0.660 f1 0.657 || val_loss 1.1816 acc 0.397 f1 0.329\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6390 acc 0.697 f1 0.694 || val_loss 1.2329 acc 0.368 f1 0.293\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5680 acc 0.727 f1 0.725 || val_loss 1.3012 acc 0.393 f1 0.319\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5275 acc 0.754 f1 0.753 || val_loss 1.3084 acc 0.391 f1 0.311\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4685 acc 0.800 f1 0.799 || val_loss 1.4213 acc 0.383 f1 0.310\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4169 acc 0.817 f1 0.816 || val_loss 1.4601 acc 0.416 f1 0.326\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3793 acc 0.839 f1 0.839 || val_loss 1.5370 acc 0.391 f1 0.307\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3274 acc 0.868 f1 0.868 || val_loss 1.6089 acc 0.407 f1 0.322\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2903 acc 0.888 f1 0.887 || val_loss 1.7595 acc 0.424 f1 0.321\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=89\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.1040 acc 0.343 f1 0.279 || val_loss 1.1118 acc 0.280 f1 0.274\n",
            "[W3] RNN Epoch 02 | train_loss 1.0846 acc 0.406 f1 0.394 || val_loss 1.0934 acc 0.327 f1 0.303\n",
            "[W3] RNN Epoch 03 | train_loss 1.0708 acc 0.426 f1 0.424 || val_loss 1.0929 acc 0.319 f1 0.301\n",
            "[W3] RNN Epoch 04 | train_loss 1.0564 acc 0.449 f1 0.444 || val_loss 1.0910 acc 0.342 f1 0.316\n",
            "[W3] RNN Epoch 05 | train_loss 1.0387 acc 0.465 f1 0.455 || val_loss 1.0919 acc 0.344 f1 0.318\n",
            "[W3] RNN Epoch 06 | train_loss 1.0261 acc 0.472 f1 0.465 || val_loss 1.0884 acc 0.360 f1 0.330\n",
            "[W3] RNN Epoch 07 | train_loss 0.9987 acc 0.513 f1 0.505 || val_loss 1.0851 acc 0.350 f1 0.313\n",
            "[W3] RNN Epoch 08 | train_loss 0.9739 acc 0.536 f1 0.528 || val_loss 1.0775 acc 0.366 f1 0.325\n",
            "[W3] RNN Epoch 09 | train_loss 0.9478 acc 0.541 f1 0.534 || val_loss 1.0598 acc 0.420 f1 0.355\n",
            "[W3] RNN Epoch 10 | train_loss 0.9336 acc 0.548 f1 0.542 || val_loss 1.0894 acc 0.366 f1 0.329\n",
            "[W3] RNN Epoch 11 | train_loss 0.9054 acc 0.577 f1 0.569 || val_loss 1.1052 acc 0.374 f1 0.333\n",
            "[W3] RNN Epoch 12 | train_loss 0.8805 acc 0.593 f1 0.585 || val_loss 1.1151 acc 0.379 f1 0.336\n",
            "[W3] RNN Epoch 13 | train_loss 0.8593 acc 0.598 f1 0.589 || val_loss 1.1153 acc 0.377 f1 0.337\n",
            "[W3] RNN Epoch 14 | train_loss 0.8312 acc 0.618 f1 0.610 || val_loss 1.1004 acc 0.412 f1 0.361\n",
            "[W3] RNN Epoch 15 | train_loss 0.8141 acc 0.627 f1 0.619 || val_loss 1.1131 acc 0.397 f1 0.347\n",
            "[W3] RNN Epoch 16 | train_loss 0.7916 acc 0.636 f1 0.629 || val_loss 1.1301 acc 0.379 f1 0.342\n",
            "[W3] RNN Epoch 17 | train_loss 0.7650 acc 0.650 f1 0.642 || val_loss 1.1232 acc 0.405 f1 0.344\n",
            "[W3] RNN Epoch 18 | train_loss 0.7380 acc 0.661 f1 0.654 || val_loss 1.1389 acc 0.416 f1 0.352\n",
            "[W3] RNN Epoch 19 | train_loss 0.7194 acc 0.667 f1 0.661 || val_loss 1.1401 acc 0.407 f1 0.346\n",
            "[W3] RNN Epoch 20 | train_loss 0.7037 acc 0.673 f1 0.665 || val_loss 1.1432 acc 0.428 f1 0.357\n",
            "[W3] RNN Epoch 21 | train_loss 0.6880 acc 0.685 f1 0.680 || val_loss 1.1585 acc 0.424 f1 0.351\n",
            "[W3] RNN Epoch 22 | train_loss 0.6616 acc 0.700 f1 0.695 || val_loss 1.1698 acc 0.424 f1 0.347\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=89\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1018 acc 0.350 f1 0.259 || val_loss 1.1056 acc 0.323 f1 0.225\n",
            "[W3] GRU Epoch 02 | train_loss 1.0906 acc 0.399 f1 0.350 || val_loss 1.0936 acc 0.323 f1 0.277\n",
            "[W3] GRU Epoch 03 | train_loss 1.0806 acc 0.426 f1 0.406 || val_loss 1.0758 acc 0.364 f1 0.317\n",
            "[W3] GRU Epoch 04 | train_loss 1.0647 acc 0.456 f1 0.448 || val_loss 1.0648 acc 0.383 f1 0.332\n",
            "[W3] GRU Epoch 05 | train_loss 1.0439 acc 0.462 f1 0.457 || val_loss 1.0798 acc 0.368 f1 0.330\n",
            "[W3] GRU Epoch 06 | train_loss 1.0078 acc 0.506 f1 0.502 || val_loss 1.0890 acc 0.360 f1 0.326\n",
            "[W3] GRU Epoch 07 | train_loss 0.9498 acc 0.552 f1 0.545 || val_loss 1.0947 acc 0.360 f1 0.311\n",
            "[W3] GRU Epoch 08 | train_loss 0.8752 acc 0.577 f1 0.568 || val_loss 1.1166 acc 0.368 f1 0.311\n",
            "[W3] GRU Epoch 09 | train_loss 0.8110 acc 0.614 f1 0.605 || val_loss 1.1301 acc 0.395 f1 0.332\n",
            "[W3] GRU Epoch 10 | train_loss 0.7589 acc 0.644 f1 0.639 || val_loss 1.1416 acc 0.389 f1 0.315\n",
            "[W3] GRU Epoch 11 | train_loss 0.7155 acc 0.663 f1 0.658 || val_loss 1.1964 acc 0.389 f1 0.324\n",
            "[W3] GRU Epoch 12 | train_loss 0.6812 acc 0.683 f1 0.679 || val_loss 1.2054 acc 0.387 f1 0.313\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=89\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1029 acc 0.333 f1 0.171 || val_loss 1.1091 acc 0.451 f1 0.207\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0964 acc 0.357 f1 0.273 || val_loss 1.1034 acc 0.280 f1 0.240\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0895 acc 0.412 f1 0.410 || val_loss 1.1004 acc 0.315 f1 0.301\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0750 acc 0.432 f1 0.423 || val_loss 1.0937 acc 0.335 f1 0.316\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0469 acc 0.456 f1 0.450 || val_loss 1.0887 acc 0.348 f1 0.323\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9823 acc 0.527 f1 0.515 || val_loss 1.0858 acc 0.368 f1 0.329\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8921 acc 0.567 f1 0.558 || val_loss 1.0763 acc 0.368 f1 0.293\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8204 acc 0.596 f1 0.590 || val_loss 1.1280 acc 0.364 f1 0.306\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7756 acc 0.618 f1 0.613 || val_loss 1.1640 acc 0.362 f1 0.304\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7411 acc 0.644 f1 0.639 || val_loss 1.1858 acc 0.360 f1 0.313\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6891 acc 0.664 f1 0.659 || val_loss 1.1968 acc 0.381 f1 0.318\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6674 acc 0.681 f1 0.677 || val_loss 1.1997 acc 0.374 f1 0.312\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6359 acc 0.689 f1 0.685 || val_loss 1.2402 acc 0.372 f1 0.295\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6006 acc 0.717 f1 0.714 || val_loss 1.2680 acc 0.372 f1 0.287\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  89%| | 89/100 [42:20<05:04, 27.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=90 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=90\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1407 acc 0.405 f1 0.401 || val_loss 1.1718 acc 0.305 f1 0.293\n",
            "[W3] ANN Epoch 02 | train_loss 0.9724 acc 0.507 f1 0.499 || val_loss 1.1557 acc 0.323 f1 0.299\n",
            "[W3] ANN Epoch 03 | train_loss 0.8640 acc 0.574 f1 0.564 || val_loss 1.1373 acc 0.354 f1 0.318\n",
            "[W3] ANN Epoch 04 | train_loss 0.7836 acc 0.615 f1 0.609 || val_loss 1.1361 acc 0.368 f1 0.324\n",
            "[W3] ANN Epoch 05 | train_loss 0.7143 acc 0.650 f1 0.643 || val_loss 1.1420 acc 0.379 f1 0.315\n",
            "[W3] ANN Epoch 06 | train_loss 0.6865 acc 0.671 f1 0.669 || val_loss 1.1690 acc 0.374 f1 0.307\n",
            "[W3] ANN Epoch 07 | train_loss 0.6237 acc 0.702 f1 0.700 || val_loss 1.1865 acc 0.381 f1 0.307\n",
            "[W3] ANN Epoch 08 | train_loss 0.6049 acc 0.710 f1 0.709 || val_loss 1.2181 acc 0.403 f1 0.325\n",
            "[W3] ANN Epoch 09 | train_loss 0.5670 acc 0.739 f1 0.738 || val_loss 1.2166 acc 0.405 f1 0.336\n",
            "[W3] ANN Epoch 10 | train_loss 0.5516 acc 0.746 f1 0.745 || val_loss 1.2515 acc 0.395 f1 0.312\n",
            "[W3] ANN Epoch 11 | train_loss 0.4997 acc 0.774 f1 0.773 || val_loss 1.2811 acc 0.403 f1 0.332\n",
            "[W3] ANN Epoch 12 | train_loss 0.4948 acc 0.771 f1 0.771 || val_loss 1.2970 acc 0.412 f1 0.343\n",
            "[W3] ANN Epoch 13 | train_loss 0.4673 acc 0.785 f1 0.785 || val_loss 1.3286 acc 0.407 f1 0.341\n",
            "[W3] ANN Epoch 14 | train_loss 0.4452 acc 0.806 f1 0.805 || val_loss 1.3833 acc 0.387 f1 0.318\n",
            "[W3] ANN Epoch 15 | train_loss 0.4418 acc 0.810 f1 0.809 || val_loss 1.4397 acc 0.401 f1 0.324\n",
            "[W3] ANN Epoch 16 | train_loss 0.4241 acc 0.819 f1 0.818 || val_loss 1.4494 acc 0.426 f1 0.352\n",
            "[W3] ANN Epoch 17 | train_loss 0.3906 acc 0.835 f1 0.835 || val_loss 1.4991 acc 0.426 f1 0.336\n",
            "[W3] ANN Epoch 18 | train_loss 0.3618 acc 0.849 f1 0.849 || val_loss 1.5474 acc 0.424 f1 0.345\n",
            "[W3] ANN Epoch 19 | train_loss 0.3799 acc 0.841 f1 0.841 || val_loss 1.5277 acc 0.405 f1 0.309\n",
            "[W3] ANN Epoch 20 | train_loss 0.3590 acc 0.851 f1 0.851 || val_loss 1.5471 acc 0.416 f1 0.334\n",
            "[W3] ANN Epoch 21 | train_loss 0.3559 acc 0.852 f1 0.852 || val_loss 1.5758 acc 0.420 f1 0.353\n",
            "[W3] ANN Epoch 22 | train_loss 0.3386 acc 0.865 f1 0.865 || val_loss 1.5870 acc 0.428 f1 0.348\n",
            "[W3] ANN Epoch 23 | train_loss 0.3094 acc 0.875 f1 0.875 || val_loss 1.6557 acc 0.418 f1 0.330\n",
            "[W3] ANN Epoch 24 | train_loss 0.3161 acc 0.875 f1 0.875 || val_loss 1.6484 acc 0.414 f1 0.333\n",
            "[W3] ANN Epoch 25 | train_loss 0.3183 acc 0.874 f1 0.874 || val_loss 1.6496 acc 0.442 f1 0.365\n",
            "[W3] ANN Epoch 26 | train_loss 0.3128 acc 0.875 f1 0.875 || val_loss 1.6919 acc 0.424 f1 0.333\n",
            "[W3] ANN Epoch 27 | train_loss 0.2748 acc 0.890 f1 0.890 || val_loss 1.7371 acc 0.422 f1 0.333\n",
            "[W3] ANN Epoch 28 | train_loss 0.2750 acc 0.890 f1 0.889 || val_loss 1.7064 acc 0.403 f1 0.330\n",
            "[W3] ANN Epoch 29 | train_loss 0.2759 acc 0.898 f1 0.898 || val_loss 1.7024 acc 0.432 f1 0.358\n",
            "[W3] ANN Epoch 30 | train_loss 0.2638 acc 0.896 f1 0.896 || val_loss 1.8214 acc 0.416 f1 0.334\n",
            "[W3] ANN Epoch 31 | train_loss 0.2629 acc 0.897 f1 0.897 || val_loss 1.8492 acc 0.434 f1 0.354\n",
            "[W3] ANN Epoch 32 | train_loss 0.2813 acc 0.889 f1 0.889 || val_loss 1.8751 acc 0.416 f1 0.340\n",
            "[W3] ANN Epoch 33 | train_loss 0.2387 acc 0.908 f1 0.907 || val_loss 1.7926 acc 0.432 f1 0.345\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=90\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0828 acc 0.384 f1 0.384 || val_loss 1.0597 acc 0.364 f1 0.313\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9582 acc 0.533 f1 0.528 || val_loss 1.0325 acc 0.374 f1 0.303\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8130 acc 0.625 f1 0.621 || val_loss 1.1185 acc 0.360 f1 0.305\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7148 acc 0.665 f1 0.663 || val_loss 1.1915 acc 0.356 f1 0.307\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6319 acc 0.705 f1 0.703 || val_loss 1.2665 acc 0.379 f1 0.324\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5650 acc 0.747 f1 0.745 || val_loss 1.2961 acc 0.412 f1 0.362\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5063 acc 0.769 f1 0.768 || val_loss 1.3782 acc 0.387 f1 0.327\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4378 acc 0.805 f1 0.804 || val_loss 1.4691 acc 0.372 f1 0.312\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.3886 acc 0.826 f1 0.826 || val_loss 1.5580 acc 0.393 f1 0.335\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3409 acc 0.855 f1 0.855 || val_loss 1.6366 acc 0.407 f1 0.342\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.2941 acc 0.882 f1 0.881 || val_loss 1.7154 acc 0.418 f1 0.350\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2576 acc 0.902 f1 0.902 || val_loss 1.8382 acc 0.426 f1 0.347\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2349 acc 0.907 f1 0.907 || val_loss 1.8824 acc 0.414 f1 0.353\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2093 acc 0.921 f1 0.921 || val_loss 2.1265 acc 0.370 f1 0.326\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=90\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0978 acc 0.366 f1 0.318 || val_loss 1.1057 acc 0.294 f1 0.288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0811 acc 0.434 f1 0.426 || val_loss 1.0861 acc 0.354 f1 0.314\n",
            "[W3] RNN Epoch 03 | train_loss 1.0639 acc 0.446 f1 0.443 || val_loss 1.0782 acc 0.354 f1 0.317\n",
            "[W3] RNN Epoch 04 | train_loss 1.0434 acc 0.478 f1 0.470 || val_loss 1.0708 acc 0.333 f1 0.291\n",
            "[W3] RNN Epoch 05 | train_loss 1.0155 acc 0.503 f1 0.497 || val_loss 1.0823 acc 0.329 f1 0.302\n",
            "[W3] RNN Epoch 06 | train_loss 0.9860 acc 0.514 f1 0.506 || val_loss 1.0803 acc 0.348 f1 0.317\n",
            "[W3] RNN Epoch 07 | train_loss 0.9575 acc 0.544 f1 0.534 || val_loss 1.0968 acc 0.344 f1 0.314\n",
            "[W3] RNN Epoch 08 | train_loss 0.9291 acc 0.560 f1 0.550 || val_loss 1.0904 acc 0.348 f1 0.317\n",
            "[W3] RNN Epoch 09 | train_loss 0.9012 acc 0.565 f1 0.555 || val_loss 1.0933 acc 0.340 f1 0.314\n",
            "[W3] RNN Epoch 10 | train_loss 0.8720 acc 0.590 f1 0.581 || val_loss 1.1129 acc 0.356 f1 0.324\n",
            "[W3] RNN Epoch 11 | train_loss 0.8482 acc 0.603 f1 0.595 || val_loss 1.1187 acc 0.352 f1 0.320\n",
            "[W3] RNN Epoch 12 | train_loss 0.8288 acc 0.600 f1 0.591 || val_loss 1.1167 acc 0.366 f1 0.326\n",
            "[W3] RNN Epoch 13 | train_loss 0.8058 acc 0.622 f1 0.613 || val_loss 1.1109 acc 0.370 f1 0.317\n",
            "[W3] RNN Epoch 14 | train_loss 0.7744 acc 0.631 f1 0.623 || val_loss 1.1518 acc 0.370 f1 0.325\n",
            "[W3] RNN Epoch 15 | train_loss 0.7605 acc 0.644 f1 0.635 || val_loss 1.1448 acc 0.377 f1 0.326\n",
            "[W3] RNN Epoch 16 | train_loss 0.7455 acc 0.651 f1 0.643 || val_loss 1.1372 acc 0.385 f1 0.329\n",
            "[W3] RNN Epoch 17 | train_loss 0.7204 acc 0.661 f1 0.654 || val_loss 1.1657 acc 0.387 f1 0.341\n",
            "[W3] RNN Epoch 18 | train_loss 0.6987 acc 0.668 f1 0.662 || val_loss 1.1670 acc 0.399 f1 0.354\n",
            "[W3] RNN Epoch 19 | train_loss 0.6899 acc 0.669 f1 0.661 || val_loss 1.1678 acc 0.412 f1 0.356\n",
            "[W3] RNN Epoch 20 | train_loss 0.6593 acc 0.679 f1 0.673 || val_loss 1.1773 acc 0.416 f1 0.363\n",
            "[W3] RNN Epoch 21 | train_loss 0.6440 acc 0.694 f1 0.689 || val_loss 1.2068 acc 0.407 f1 0.355\n",
            "[W3] RNN Epoch 22 | train_loss 0.6212 acc 0.701 f1 0.696 || val_loss 1.2183 acc 0.407 f1 0.351\n",
            "[W3] RNN Epoch 23 | train_loss 0.6132 acc 0.702 f1 0.697 || val_loss 1.2555 acc 0.399 f1 0.349\n",
            "[W3] RNN Epoch 24 | train_loss 0.5920 acc 0.721 f1 0.717 || val_loss 1.2483 acc 0.416 f1 0.352\n",
            "[W3] RNN Epoch 25 | train_loss 0.5832 acc 0.721 f1 0.717 || val_loss 1.2665 acc 0.401 f1 0.350\n",
            "[W3] RNN Epoch 26 | train_loss 0.5686 acc 0.730 f1 0.727 || val_loss 1.2726 acc 0.414 f1 0.357\n",
            "[W3] RNN Epoch 27 | train_loss 0.5394 acc 0.743 f1 0.740 || val_loss 1.3000 acc 0.414 f1 0.358\n",
            "[W3] RNN Epoch 28 | train_loss 0.5459 acc 0.730 f1 0.726 || val_loss 1.3118 acc 0.399 f1 0.323\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=90\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0983 acc 0.343 f1 0.243 || val_loss 1.0922 acc 0.401 f1 0.311\n",
            "[W3] GRU Epoch 02 | train_loss 1.0886 acc 0.394 f1 0.394 || val_loss 1.0953 acc 0.340 f1 0.314\n",
            "[W3] GRU Epoch 03 | train_loss 1.0788 acc 0.418 f1 0.418 || val_loss 1.0920 acc 0.335 f1 0.314\n",
            "[W3] GRU Epoch 04 | train_loss 1.0595 acc 0.454 f1 0.452 || val_loss 1.0868 acc 0.348 f1 0.319\n",
            "[W3] GRU Epoch 05 | train_loss 1.0282 acc 0.488 f1 0.481 || val_loss 1.0920 acc 0.335 f1 0.305\n",
            "[W3] GRU Epoch 06 | train_loss 0.9755 acc 0.529 f1 0.519 || val_loss 1.0729 acc 0.387 f1 0.341\n",
            "[W3] GRU Epoch 07 | train_loss 0.9064 acc 0.567 f1 0.559 || val_loss 1.0964 acc 0.374 f1 0.324\n",
            "[W3] GRU Epoch 08 | train_loss 0.8460 acc 0.594 f1 0.588 || val_loss 1.1218 acc 0.372 f1 0.322\n",
            "[W3] GRU Epoch 09 | train_loss 0.7939 acc 0.618 f1 0.613 || val_loss 1.1071 acc 0.397 f1 0.322\n",
            "[W3] GRU Epoch 10 | train_loss 0.7519 acc 0.649 f1 0.643 || val_loss 1.1272 acc 0.395 f1 0.343\n",
            "[W3] GRU Epoch 11 | train_loss 0.7143 acc 0.663 f1 0.659 || val_loss 1.1523 acc 0.381 f1 0.313\n",
            "[W3] GRU Epoch 12 | train_loss 0.6746 acc 0.681 f1 0.678 || val_loss 1.1799 acc 0.389 f1 0.323\n",
            "[W3] GRU Epoch 13 | train_loss 0.6449 acc 0.702 f1 0.699 || val_loss 1.2005 acc 0.407 f1 0.329\n",
            "[W3] GRU Epoch 14 | train_loss 0.6181 acc 0.707 f1 0.704 || val_loss 1.2261 acc 0.374 f1 0.301\n",
            "[W3] GRU Epoch 15 | train_loss 0.5875 acc 0.724 f1 0.722 || val_loss 1.2465 acc 0.385 f1 0.311\n",
            "[W3] GRU Epoch 16 | train_loss 0.5567 acc 0.740 f1 0.738 || val_loss 1.2966 acc 0.399 f1 0.336\n",
            "[W3] GRU Epoch 17 | train_loss 0.5308 acc 0.750 f1 0.749 || val_loss 1.3240 acc 0.401 f1 0.321\n",
            "[W3] GRU Epoch 18 | train_loss 0.5049 acc 0.770 f1 0.770 || val_loss 1.3771 acc 0.393 f1 0.323\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=90\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0990 acc 0.345 f1 0.285 || val_loss 1.1029 acc 0.307 f1 0.257\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0939 acc 0.397 f1 0.358 || val_loss 1.0930 acc 0.348 f1 0.309\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0851 acc 0.408 f1 0.407 || val_loss 1.0860 acc 0.337 f1 0.307\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0665 acc 0.448 f1 0.443 || val_loss 1.0880 acc 0.327 f1 0.312\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0275 acc 0.485 f1 0.474 || val_loss 1.1161 acc 0.315 f1 0.303\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9553 acc 0.524 f1 0.513 || val_loss 1.0981 acc 0.348 f1 0.298\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8605 acc 0.574 f1 0.566 || val_loss 1.1241 acc 0.350 f1 0.289\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8061 acc 0.608 f1 0.602 || val_loss 1.1311 acc 0.389 f1 0.290\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7518 acc 0.632 f1 0.627 || val_loss 1.1527 acc 0.368 f1 0.292\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7190 acc 0.651 f1 0.648 || val_loss 1.1565 acc 0.403 f1 0.311\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6826 acc 0.665 f1 0.661 || val_loss 1.1866 acc 0.405 f1 0.327\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6583 acc 0.673 f1 0.670 || val_loss 1.2230 acc 0.393 f1 0.312\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6206 acc 0.691 f1 0.689 || val_loss 1.2485 acc 0.403 f1 0.313\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5931 acc 0.721 f1 0.719 || val_loss 1.3027 acc 0.393 f1 0.309\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5682 acc 0.726 f1 0.723 || val_loss 1.3384 acc 0.416 f1 0.328\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5382 acc 0.734 f1 0.733 || val_loss 1.3675 acc 0.401 f1 0.311\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5206 acc 0.752 f1 0.750 || val_loss 1.4017 acc 0.424 f1 0.328\n",
            "[W3] LSTM Epoch 18 | train_loss 0.4923 acc 0.761 f1 0.760 || val_loss 1.4601 acc 0.424 f1 0.335\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4649 acc 0.776 f1 0.776 || val_loss 1.5079 acc 0.432 f1 0.345\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4470 acc 0.789 f1 0.789 || val_loss 1.5535 acc 0.426 f1 0.344\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4263 acc 0.804 f1 0.803 || val_loss 1.5993 acc 0.405 f1 0.327\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4028 acc 0.816 f1 0.816 || val_loss 1.6629 acc 0.420 f1 0.347\n",
            "[W3] LSTM Epoch 23 | train_loss 0.3785 acc 0.829 f1 0.829 || val_loss 1.7313 acc 0.403 f1 0.334\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3595 acc 0.841 f1 0.840 || val_loss 1.7987 acc 0.418 f1 0.332\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3441 acc 0.852 f1 0.851 || val_loss 1.8246 acc 0.416 f1 0.334\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3220 acc 0.865 f1 0.864 || val_loss 1.8767 acc 0.416 f1 0.340\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3103 acc 0.864 f1 0.864 || val_loss 1.9674 acc 0.395 f1 0.324\n",
            "[W3] LSTM Epoch 28 | train_loss 0.2842 acc 0.891 f1 0.891 || val_loss 2.0266 acc 0.424 f1 0.340\n",
            "[W3] LSTM Epoch 29 | train_loss 0.2688 acc 0.888 f1 0.888 || val_loss 2.0974 acc 0.426 f1 0.342\n",
            "[W3] LSTM Epoch 30 | train_loss 0.2492 acc 0.900 f1 0.900 || val_loss 2.1625 acc 0.440 f1 0.352\n",
            "[W3] LSTM Epoch 31 | train_loss 0.2258 acc 0.912 f1 0.912 || val_loss 2.2458 acc 0.430 f1 0.350\n",
            "[W3] LSTM Epoch 32 | train_loss 0.2051 acc 0.922 f1 0.922 || val_loss 2.3247 acc 0.444 f1 0.354\n",
            "[W3] LSTM Epoch 33 | train_loss 0.1971 acc 0.924 f1 0.924 || val_loss 2.4019 acc 0.432 f1 0.347\n",
            "[W3] LSTM Epoch 34 | train_loss 0.1763 acc 0.941 f1 0.941 || val_loss 2.4746 acc 0.418 f1 0.330\n",
            "[W3] LSTM Epoch 35 | train_loss 0.1587 acc 0.946 f1 0.946 || val_loss 2.5740 acc 0.434 f1 0.351\n",
            "[W3] LSTM Epoch 36 | train_loss 0.1318 acc 0.963 f1 0.963 || val_loss 2.6209 acc 0.422 f1 0.338\n",
            "[W3] LSTM Epoch 37 | train_loss 0.1202 acc 0.961 f1 0.961 || val_loss 2.7548 acc 0.422 f1 0.337\n",
            "[W3] LSTM Epoch 38 | train_loss 0.1067 acc 0.970 f1 0.970 || val_loss 2.8231 acc 0.432 f1 0.347\n",
            "[W3] LSTM Epoch 39 | train_loss 0.0957 acc 0.971 f1 0.971 || val_loss 2.9411 acc 0.405 f1 0.327\n",
            "[W3] LSTM Epoch 40 | train_loss 0.0802 acc 0.982 f1 0.982 || val_loss 3.0067 acc 0.418 f1 0.341\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  90%| | 90/100 [42:56<05:03, 30.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=91 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=91\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1240 acc 0.406 f1 0.402 || val_loss 1.1596 acc 0.288 f1 0.281\n",
            "[W3] ANN Epoch 02 | train_loss 0.9664 acc 0.512 f1 0.503 || val_loss 1.1910 acc 0.302 f1 0.290\n",
            "[W3] ANN Epoch 03 | train_loss 0.8856 acc 0.564 f1 0.553 || val_loss 1.1795 acc 0.305 f1 0.280\n",
            "[W3] ANN Epoch 04 | train_loss 0.7946 acc 0.616 f1 0.609 || val_loss 1.1402 acc 0.372 f1 0.321\n",
            "[W3] ANN Epoch 05 | train_loss 0.7292 acc 0.646 f1 0.638 || val_loss 1.1820 acc 0.356 f1 0.307\n",
            "[W3] ANN Epoch 06 | train_loss 0.6622 acc 0.685 f1 0.681 || val_loss 1.1973 acc 0.368 f1 0.310\n",
            "[W3] ANN Epoch 07 | train_loss 0.6564 acc 0.683 f1 0.680 || val_loss 1.1545 acc 0.412 f1 0.340\n",
            "[W3] ANN Epoch 08 | train_loss 0.6077 acc 0.718 f1 0.716 || val_loss 1.2023 acc 0.385 f1 0.316\n",
            "[W3] ANN Epoch 09 | train_loss 0.5555 acc 0.740 f1 0.738 || val_loss 1.2834 acc 0.391 f1 0.326\n",
            "[W3] ANN Epoch 10 | train_loss 0.5572 acc 0.740 f1 0.738 || val_loss 1.2452 acc 0.424 f1 0.336\n",
            "[W3] ANN Epoch 11 | train_loss 0.5303 acc 0.754 f1 0.752 || val_loss 1.2442 acc 0.418 f1 0.336\n",
            "[W3] ANN Epoch 12 | train_loss 0.5048 acc 0.776 f1 0.775 || val_loss 1.2684 acc 0.444 f1 0.361\n",
            "[W3] ANN Epoch 13 | train_loss 0.4582 acc 0.799 f1 0.798 || val_loss 1.3387 acc 0.403 f1 0.339\n",
            "[W3] ANN Epoch 14 | train_loss 0.4728 acc 0.784 f1 0.783 || val_loss 1.3529 acc 0.412 f1 0.354\n",
            "[W3] ANN Epoch 15 | train_loss 0.4468 acc 0.803 f1 0.802 || val_loss 1.3874 acc 0.407 f1 0.323\n",
            "[W3] ANN Epoch 16 | train_loss 0.4034 acc 0.819 f1 0.818 || val_loss 1.4411 acc 0.418 f1 0.337\n",
            "[W3] ANN Epoch 17 | train_loss 0.4191 acc 0.823 f1 0.823 || val_loss 1.4178 acc 0.430 f1 0.345\n",
            "[W3] ANN Epoch 18 | train_loss 0.4003 acc 0.827 f1 0.827 || val_loss 1.4982 acc 0.414 f1 0.342\n",
            "[W3] ANN Epoch 19 | train_loss 0.3711 acc 0.846 f1 0.845 || val_loss 1.4519 acc 0.420 f1 0.337\n",
            "[W3] ANN Epoch 20 | train_loss 0.3594 acc 0.847 f1 0.847 || val_loss 1.5199 acc 0.440 f1 0.358\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=91\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0815 acc 0.383 f1 0.385 || val_loss 1.0292 acc 0.447 f1 0.329\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9567 acc 0.535 f1 0.532 || val_loss 1.0492 acc 0.395 f1 0.276\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8282 acc 0.607 f1 0.603 || val_loss 1.0956 acc 0.374 f1 0.309\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7306 acc 0.655 f1 0.653 || val_loss 1.1763 acc 0.372 f1 0.321\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6462 acc 0.697 f1 0.694 || val_loss 1.2040 acc 0.403 f1 0.320\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5834 acc 0.735 f1 0.735 || val_loss 1.2639 acc 0.399 f1 0.327\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5043 acc 0.773 f1 0.772 || val_loss 1.3063 acc 0.416 f1 0.332\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4605 acc 0.796 f1 0.796 || val_loss 1.4730 acc 0.405 f1 0.326\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4007 acc 0.828 f1 0.828 || val_loss 1.5392 acc 0.389 f1 0.315\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3625 acc 0.843 f1 0.842 || val_loss 1.6743 acc 0.379 f1 0.304\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3205 acc 0.867 f1 0.867 || val_loss 1.7468 acc 0.407 f1 0.328\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2955 acc 0.880 f1 0.879 || val_loss 1.8030 acc 0.434 f1 0.343\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2529 acc 0.905 f1 0.905 || val_loss 1.9243 acc 0.401 f1 0.328\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2185 acc 0.923 f1 0.922 || val_loss 2.0994 acc 0.399 f1 0.326\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1816 acc 0.940 f1 0.940 || val_loss 2.2290 acc 0.368 f1 0.302\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1804 acc 0.932 f1 0.932 || val_loss 2.2518 acc 0.422 f1 0.334\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1421 acc 0.952 f1 0.952 || val_loss 2.4355 acc 0.385 f1 0.298\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1413 acc 0.949 f1 0.949 || val_loss 2.5122 acc 0.405 f1 0.323\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1133 acc 0.965 f1 0.965 || val_loss 2.5890 acc 0.383 f1 0.305\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.0899 acc 0.972 f1 0.972 || val_loss 2.6506 acc 0.397 f1 0.321\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=91\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0982 acc 0.352 f1 0.323 || val_loss 1.1125 acc 0.292 f1 0.287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0829 acc 0.403 f1 0.393 || val_loss 1.1031 acc 0.337 f1 0.313\n",
            "[W3] RNN Epoch 03 | train_loss 1.0668 acc 0.435 f1 0.432 || val_loss 1.1060 acc 0.323 f1 0.300\n",
            "[W3] RNN Epoch 04 | train_loss 1.0478 acc 0.463 f1 0.460 || val_loss 1.0952 acc 0.354 f1 0.318\n",
            "[W3] RNN Epoch 05 | train_loss 1.0295 acc 0.473 f1 0.467 || val_loss 1.0801 acc 0.370 f1 0.333\n",
            "[W3] RNN Epoch 06 | train_loss 1.0100 acc 0.498 f1 0.494 || val_loss 1.0970 acc 0.350 f1 0.320\n",
            "[W3] RNN Epoch 07 | train_loss 0.9866 acc 0.522 f1 0.516 || val_loss 1.0987 acc 0.340 f1 0.311\n",
            "[W3] RNN Epoch 08 | train_loss 0.9717 acc 0.532 f1 0.526 || val_loss 1.0962 acc 0.352 f1 0.319\n",
            "[W3] RNN Epoch 09 | train_loss 0.9423 acc 0.545 f1 0.538 || val_loss 1.0960 acc 0.346 f1 0.314\n",
            "[W3] RNN Epoch 10 | train_loss 0.9266 acc 0.556 f1 0.547 || val_loss 1.1034 acc 0.344 f1 0.317\n",
            "[W3] RNN Epoch 11 | train_loss 0.9040 acc 0.575 f1 0.566 || val_loss 1.0816 acc 0.364 f1 0.321\n",
            "[W3] RNN Epoch 12 | train_loss 0.8734 acc 0.584 f1 0.574 || val_loss 1.1240 acc 0.354 f1 0.318\n",
            "[W3] RNN Epoch 13 | train_loss 0.8586 acc 0.602 f1 0.596 || val_loss 1.1022 acc 0.370 f1 0.330\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=91\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0995 acc 0.343 f1 0.285 || val_loss 1.1031 acc 0.286 f1 0.268\n",
            "[W3] GRU Epoch 02 | train_loss 1.0879 acc 0.394 f1 0.390 || val_loss 1.0979 acc 0.288 f1 0.271\n",
            "[W3] GRU Epoch 03 | train_loss 1.0782 acc 0.407 f1 0.402 || val_loss 1.0921 acc 0.323 f1 0.301\n",
            "[W3] GRU Epoch 04 | train_loss 1.0636 acc 0.453 f1 0.447 || val_loss 1.0973 acc 0.327 f1 0.304\n",
            "[W3] GRU Epoch 05 | train_loss 1.0369 acc 0.489 f1 0.481 || val_loss 1.0685 acc 0.368 f1 0.312\n",
            "[W3] GRU Epoch 06 | train_loss 0.9813 acc 0.529 f1 0.520 || val_loss 1.0646 acc 0.368 f1 0.298\n",
            "[W3] GRU Epoch 07 | train_loss 0.9083 acc 0.559 f1 0.552 || val_loss 1.0783 acc 0.348 f1 0.274\n",
            "[W3] GRU Epoch 08 | train_loss 0.8392 acc 0.591 f1 0.584 || val_loss 1.0986 acc 0.368 f1 0.298\n",
            "[W3] GRU Epoch 09 | train_loss 0.7998 acc 0.614 f1 0.607 || val_loss 1.1257 acc 0.346 f1 0.281\n",
            "[W3] GRU Epoch 10 | train_loss 0.7503 acc 0.639 f1 0.633 || val_loss 1.1780 acc 0.360 f1 0.291\n",
            "[W3] GRU Epoch 11 | train_loss 0.7192 acc 0.647 f1 0.642 || val_loss 1.2394 acc 0.337 f1 0.280\n",
            "[W3] GRU Epoch 12 | train_loss 0.6816 acc 0.670 f1 0.666 || val_loss 1.2426 acc 0.362 f1 0.298\n",
            "[W3] GRU Epoch 13 | train_loss 0.6491 acc 0.688 f1 0.684 || val_loss 1.2380 acc 0.372 f1 0.295\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=91\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1008 acc 0.336 f1 0.208 || val_loss 1.0994 acc 0.354 f1 0.247\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0937 acc 0.379 f1 0.357 || val_loss 1.1009 acc 0.315 f1 0.291\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0861 acc 0.404 f1 0.379 || val_loss 1.0964 acc 0.325 f1 0.298\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0718 acc 0.438 f1 0.430 || val_loss 1.0989 acc 0.315 f1 0.294\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0410 acc 0.469 f1 0.463 || val_loss 1.0948 acc 0.358 f1 0.325\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9621 acc 0.534 f1 0.522 || val_loss 1.0644 acc 0.368 f1 0.313\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8843 acc 0.568 f1 0.557 || val_loss 1.1092 acc 0.368 f1 0.319\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8186 acc 0.600 f1 0.594 || val_loss 1.1576 acc 0.350 f1 0.290\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7747 acc 0.620 f1 0.613 || val_loss 1.1655 acc 0.372 f1 0.319\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7319 acc 0.639 f1 0.632 || val_loss 1.1779 acc 0.381 f1 0.299\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7081 acc 0.661 f1 0.655 || val_loss 1.2139 acc 0.379 f1 0.310\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6701 acc 0.676 f1 0.672 || val_loss 1.2429 acc 0.379 f1 0.314\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6378 acc 0.695 f1 0.691 || val_loss 1.2676 acc 0.389 f1 0.317\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  91%| | 91/100 [43:22<04:20, 28.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=92 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=92\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1266 acc 0.406 f1 0.404 || val_loss 1.1370 acc 0.317 f1 0.300\n",
            "[W3] ANN Epoch 02 | train_loss 0.9767 acc 0.502 f1 0.493 || val_loss 1.1491 acc 0.311 f1 0.287\n",
            "[W3] ANN Epoch 03 | train_loss 0.8604 acc 0.587 f1 0.579 || val_loss 1.1357 acc 0.358 f1 0.317\n",
            "[W3] ANN Epoch 04 | train_loss 0.7895 acc 0.624 f1 0.618 || val_loss 1.1241 acc 0.366 f1 0.311\n",
            "[W3] ANN Epoch 05 | train_loss 0.7220 acc 0.651 f1 0.647 || val_loss 1.1184 acc 0.399 f1 0.331\n",
            "[W3] ANN Epoch 06 | train_loss 0.6677 acc 0.684 f1 0.680 || val_loss 1.1352 acc 0.418 f1 0.341\n",
            "[W3] ANN Epoch 07 | train_loss 0.6292 acc 0.699 f1 0.696 || val_loss 1.1889 acc 0.409 f1 0.343\n",
            "[W3] ANN Epoch 08 | train_loss 0.5975 acc 0.726 f1 0.724 || val_loss 1.1932 acc 0.416 f1 0.335\n",
            "[W3] ANN Epoch 09 | train_loss 0.5597 acc 0.738 f1 0.735 || val_loss 1.2191 acc 0.407 f1 0.328\n",
            "[W3] ANN Epoch 10 | train_loss 0.5483 acc 0.745 f1 0.744 || val_loss 1.2280 acc 0.409 f1 0.330\n",
            "[W3] ANN Epoch 11 | train_loss 0.5271 acc 0.764 f1 0.764 || val_loss 1.2481 acc 0.436 f1 0.355\n",
            "[W3] ANN Epoch 12 | train_loss 0.5017 acc 0.774 f1 0.774 || val_loss 1.2871 acc 0.424 f1 0.341\n",
            "[W3] ANN Epoch 13 | train_loss 0.4799 acc 0.787 f1 0.786 || val_loss 1.3236 acc 0.405 f1 0.316\n",
            "[W3] ANN Epoch 14 | train_loss 0.4390 acc 0.807 f1 0.807 || val_loss 1.3355 acc 0.434 f1 0.343\n",
            "[W3] ANN Epoch 15 | train_loss 0.4363 acc 0.813 f1 0.813 || val_loss 1.3682 acc 0.440 f1 0.363\n",
            "[W3] ANN Epoch 16 | train_loss 0.4223 acc 0.814 f1 0.815 || val_loss 1.3960 acc 0.407 f1 0.323\n",
            "[W3] ANN Epoch 17 | train_loss 0.3819 acc 0.837 f1 0.836 || val_loss 1.4315 acc 0.420 f1 0.343\n",
            "[W3] ANN Epoch 18 | train_loss 0.4064 acc 0.823 f1 0.823 || val_loss 1.4506 acc 0.434 f1 0.354\n",
            "[W3] ANN Epoch 19 | train_loss 0.3895 acc 0.834 f1 0.833 || val_loss 1.4345 acc 0.420 f1 0.337\n",
            "[W3] ANN Epoch 20 | train_loss 0.3715 acc 0.845 f1 0.845 || val_loss 1.4996 acc 0.420 f1 0.336\n",
            "[W3] ANN Epoch 21 | train_loss 0.3592 acc 0.845 f1 0.845 || val_loss 1.4876 acc 0.416 f1 0.329\n",
            "[W3] ANN Epoch 22 | train_loss 0.3495 acc 0.853 f1 0.853 || val_loss 1.5184 acc 0.436 f1 0.338\n",
            "[W3] ANN Epoch 23 | train_loss 0.3329 acc 0.863 f1 0.863 || val_loss 1.5238 acc 0.420 f1 0.347\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=92\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0803 acc 0.399 f1 0.400 || val_loss 1.0423 acc 0.412 f1 0.316\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9545 acc 0.539 f1 0.534 || val_loss 1.0538 acc 0.395 f1 0.309\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8172 acc 0.615 f1 0.612 || val_loss 1.1566 acc 0.364 f1 0.307\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7185 acc 0.669 f1 0.666 || val_loss 1.1974 acc 0.368 f1 0.314\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6361 acc 0.716 f1 0.714 || val_loss 1.2891 acc 0.368 f1 0.310\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5693 acc 0.739 f1 0.737 || val_loss 1.2936 acc 0.389 f1 0.324\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5099 acc 0.774 f1 0.774 || val_loss 1.4463 acc 0.381 f1 0.304\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4354 acc 0.821 f1 0.821 || val_loss 1.4752 acc 0.391 f1 0.303\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4035 acc 0.835 f1 0.835 || val_loss 1.6391 acc 0.366 f1 0.310\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3518 acc 0.858 f1 0.858 || val_loss 1.6241 acc 0.409 f1 0.308\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3079 acc 0.883 f1 0.883 || val_loss 1.8080 acc 0.387 f1 0.313\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2657 acc 0.895 f1 0.895 || val_loss 1.8652 acc 0.391 f1 0.305\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2329 acc 0.911 f1 0.911 || val_loss 2.0243 acc 0.387 f1 0.306\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.1906 acc 0.932 f1 0.932 || val_loss 2.0950 acc 0.418 f1 0.330\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1656 acc 0.942 f1 0.942 || val_loss 2.2904 acc 0.414 f1 0.324\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1554 acc 0.945 f1 0.945 || val_loss 2.3688 acc 0.401 f1 0.334\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1335 acc 0.959 f1 0.959 || val_loss 2.4863 acc 0.414 f1 0.328\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1259 acc 0.957 f1 0.957 || val_loss 2.5923 acc 0.405 f1 0.318\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1120 acc 0.962 f1 0.962 || val_loss 2.6075 acc 0.374 f1 0.295\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.0983 acc 0.967 f1 0.967 || val_loss 2.6581 acc 0.397 f1 0.325\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.0828 acc 0.975 f1 0.975 || val_loss 2.7839 acc 0.393 f1 0.318\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.0841 acc 0.969 f1 0.969 || val_loss 2.8013 acc 0.412 f1 0.321\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.0722 acc 0.975 f1 0.975 || val_loss 2.9139 acc 0.405 f1 0.340\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.0681 acc 0.976 f1 0.976 || val_loss 3.0881 acc 0.397 f1 0.304\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.0613 acc 0.979 f1 0.979 || val_loss 3.1403 acc 0.374 f1 0.302\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.0614 acc 0.980 f1 0.980 || val_loss 3.1448 acc 0.403 f1 0.314\n",
            "[W3] CNN1D Epoch 27 | train_loss 0.0552 acc 0.982 f1 0.982 || val_loss 3.1169 acc 0.393 f1 0.302\n",
            "[W3] CNN1D Epoch 28 | train_loss 0.0451 acc 0.985 f1 0.985 || val_loss 3.0896 acc 0.428 f1 0.342\n",
            "[W3] CNN1D Epoch 29 | train_loss 0.0487 acc 0.984 f1 0.984 || val_loss 3.1900 acc 0.407 f1 0.313\n",
            "[W3] CNN1D Epoch 30 | train_loss 0.0406 acc 0.987 f1 0.987 || val_loss 3.3928 acc 0.409 f1 0.320\n",
            "[W3] CNN1D Epoch 31 | train_loss 0.0491 acc 0.983 f1 0.983 || val_loss 3.3327 acc 0.405 f1 0.340\n",
            "[W3] CNN1D Epoch 32 | train_loss 0.0575 acc 0.982 f1 0.982 || val_loss 3.3461 acc 0.426 f1 0.332\n",
            "[W3] CNN1D Epoch 33 | train_loss 0.0576 acc 0.984 f1 0.984 || val_loss 3.2868 acc 0.393 f1 0.306\n",
            "[W3] CNN1D Epoch 34 | train_loss 0.0430 acc 0.988 f1 0.988 || val_loss 3.2459 acc 0.430 f1 0.328\n",
            "[W3] CNN1D Epoch 35 | train_loss 0.0343 acc 0.989 f1 0.989 || val_loss 3.4294 acc 0.420 f1 0.325\n",
            "[W3] CNN1D Epoch 36 | train_loss 0.0321 acc 0.992 f1 0.992 || val_loss 3.3361 acc 0.444 f1 0.360\n",
            "[W3] CNN1D Epoch 37 | train_loss 0.0261 acc 0.994 f1 0.994 || val_loss 3.4878 acc 0.418 f1 0.336\n",
            "[W3] CNN1D Epoch 38 | train_loss 0.0243 acc 0.993 f1 0.993 || val_loss 3.6426 acc 0.412 f1 0.315\n",
            "[W3] CNN1D Epoch 39 | train_loss 0.0263 acc 0.993 f1 0.993 || val_loss 3.6009 acc 0.409 f1 0.343\n",
            "[W3] CNN1D Epoch 40 | train_loss 0.0311 acc 0.988 f1 0.988 || val_loss 3.6566 acc 0.414 f1 0.314\n",
            "[W3] CNN1D Epoch 41 | train_loss 0.0404 acc 0.984 f1 0.984 || val_loss 3.6560 acc 0.409 f1 0.330\n",
            "[W3] CNN1D Epoch 42 | train_loss 0.0494 acc 0.984 f1 0.984 || val_loss 3.5945 acc 0.412 f1 0.327\n",
            "[W3] CNN1D Epoch 43 | train_loss 0.0486 acc 0.980 f1 0.980 || val_loss 3.6306 acc 0.399 f1 0.335\n",
            "[W3] CNN1D Epoch 44 | train_loss 0.0429 acc 0.985 f1 0.985 || val_loss 3.5522 acc 0.420 f1 0.325\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=92\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0988 acc 0.355 f1 0.311 || val_loss 1.0985 acc 0.335 f1 0.313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0824 acc 0.409 f1 0.400 || val_loss 1.0869 acc 0.342 f1 0.292\n",
            "[W3] RNN Epoch 03 | train_loss 1.0666 acc 0.438 f1 0.426 || val_loss 1.0899 acc 0.315 f1 0.292\n",
            "[W3] RNN Epoch 04 | train_loss 1.0466 acc 0.463 f1 0.457 || val_loss 1.0895 acc 0.327 f1 0.303\n",
            "[W3] RNN Epoch 05 | train_loss 1.0263 acc 0.483 f1 0.475 || val_loss 1.0689 acc 0.352 f1 0.314\n",
            "[W3] RNN Epoch 06 | train_loss 1.0014 acc 0.504 f1 0.494 || val_loss 1.0676 acc 0.364 f1 0.323\n",
            "[W3] RNN Epoch 07 | train_loss 0.9762 acc 0.525 f1 0.520 || val_loss 1.0639 acc 0.368 f1 0.331\n",
            "[W3] RNN Epoch 08 | train_loss 0.9464 acc 0.549 f1 0.541 || val_loss 1.0563 acc 0.397 f1 0.337\n",
            "[W3] RNN Epoch 09 | train_loss 0.9175 acc 0.567 f1 0.556 || val_loss 1.0773 acc 0.387 f1 0.348\n",
            "[W3] RNN Epoch 10 | train_loss 0.8957 acc 0.582 f1 0.573 || val_loss 1.0744 acc 0.399 f1 0.358\n",
            "[W3] RNN Epoch 11 | train_loss 0.8677 acc 0.587 f1 0.576 || val_loss 1.0993 acc 0.383 f1 0.352\n",
            "[W3] RNN Epoch 12 | train_loss 0.8434 acc 0.600 f1 0.590 || val_loss 1.0997 acc 0.385 f1 0.349\n",
            "[W3] RNN Epoch 13 | train_loss 0.8202 acc 0.622 f1 0.614 || val_loss 1.1092 acc 0.403 f1 0.353\n",
            "[W3] RNN Epoch 14 | train_loss 0.8003 acc 0.625 f1 0.615 || val_loss 1.1108 acc 0.389 f1 0.338\n",
            "[W3] RNN Epoch 15 | train_loss 0.7777 acc 0.643 f1 0.634 || val_loss 1.1084 acc 0.430 f1 0.375\n",
            "[W3] RNN Epoch 16 | train_loss 0.7484 acc 0.655 f1 0.645 || val_loss 1.1129 acc 0.414 f1 0.373\n",
            "[W3] RNN Epoch 17 | train_loss 0.7251 acc 0.664 f1 0.657 || val_loss 1.1418 acc 0.403 f1 0.348\n",
            "[W3] RNN Epoch 18 | train_loss 0.7010 acc 0.678 f1 0.671 || val_loss 1.1421 acc 0.426 f1 0.376\n",
            "[W3] RNN Epoch 19 | train_loss 0.6903 acc 0.672 f1 0.665 || val_loss 1.1422 acc 0.442 f1 0.380\n",
            "[W3] RNN Epoch 20 | train_loss 0.6623 acc 0.682 f1 0.676 || val_loss 1.1612 acc 0.428 f1 0.361\n",
            "[W3] RNN Epoch 21 | train_loss 0.6421 acc 0.695 f1 0.688 || val_loss 1.1848 acc 0.420 f1 0.371\n",
            "[W3] RNN Epoch 22 | train_loss 0.6231 acc 0.706 f1 0.700 || val_loss 1.1838 acc 0.432 f1 0.356\n",
            "[W3] RNN Epoch 23 | train_loss 0.6063 acc 0.699 f1 0.694 || val_loss 1.1987 acc 0.434 f1 0.366\n",
            "[W3] RNN Epoch 24 | train_loss 0.5894 acc 0.719 f1 0.713 || val_loss 1.2088 acc 0.432 f1 0.360\n",
            "[W3] RNN Epoch 25 | train_loss 0.5748 acc 0.725 f1 0.721 || val_loss 1.2177 acc 0.438 f1 0.354\n",
            "[W3] RNN Epoch 26 | train_loss 0.5590 acc 0.738 f1 0.734 || val_loss 1.2461 acc 0.434 f1 0.358\n",
            "[W3] RNN Epoch 27 | train_loss 0.5443 acc 0.738 f1 0.735 || val_loss 1.2715 acc 0.432 f1 0.361\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=92\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0988 acc 0.326 f1 0.293 || val_loss 1.0860 acc 0.422 f1 0.315\n",
            "[W3] GRU Epoch 02 | train_loss 1.0898 acc 0.397 f1 0.396 || val_loss 1.0913 acc 0.370 f1 0.321\n",
            "[W3] GRU Epoch 03 | train_loss 1.0791 acc 0.424 f1 0.420 || val_loss 1.1012 acc 0.335 f1 0.316\n",
            "[W3] GRU Epoch 04 | train_loss 1.0615 acc 0.464 f1 0.454 || val_loss 1.0894 acc 0.331 f1 0.304\n",
            "[W3] GRU Epoch 05 | train_loss 1.0326 acc 0.488 f1 0.481 || val_loss 1.0916 acc 0.354 f1 0.326\n",
            "[W3] GRU Epoch 06 | train_loss 0.9763 acc 0.527 f1 0.516 || val_loss 1.0849 acc 0.356 f1 0.317\n",
            "[W3] GRU Epoch 07 | train_loss 0.9034 acc 0.567 f1 0.559 || val_loss 1.0720 acc 0.403 f1 0.334\n",
            "[W3] GRU Epoch 08 | train_loss 0.8357 acc 0.605 f1 0.601 || val_loss 1.1185 acc 0.387 f1 0.337\n",
            "[W3] GRU Epoch 09 | train_loss 0.7856 acc 0.617 f1 0.612 || val_loss 1.1552 acc 0.393 f1 0.332\n",
            "[W3] GRU Epoch 10 | train_loss 0.7517 acc 0.644 f1 0.640 || val_loss 1.1462 acc 0.397 f1 0.334\n",
            "[W3] GRU Epoch 11 | train_loss 0.7096 acc 0.660 f1 0.658 || val_loss 1.1740 acc 0.389 f1 0.324\n",
            "[W3] GRU Epoch 12 | train_loss 0.6772 acc 0.672 f1 0.669 || val_loss 1.1994 acc 0.403 f1 0.339\n",
            "[W3] GRU Epoch 13 | train_loss 0.6505 acc 0.678 f1 0.675 || val_loss 1.2137 acc 0.401 f1 0.333\n",
            "[W3] GRU Epoch 14 | train_loss 0.6261 acc 0.701 f1 0.699 || val_loss 1.2744 acc 0.403 f1 0.339\n",
            "[W3] GRU Epoch 15 | train_loss 0.5950 acc 0.721 f1 0.720 || val_loss 1.2725 acc 0.407 f1 0.333\n",
            "[W3] GRU Epoch 16 | train_loss 0.5667 acc 0.728 f1 0.728 || val_loss 1.3354 acc 0.393 f1 0.324\n",
            "[W3] GRU Epoch 17 | train_loss 0.5454 acc 0.738 f1 0.737 || val_loss 1.3310 acc 0.403 f1 0.329\n",
            "[W3] GRU Epoch 18 | train_loss 0.5268 acc 0.751 f1 0.750 || val_loss 1.3747 acc 0.395 f1 0.320\n",
            "[W3] GRU Epoch 19 | train_loss 0.5020 acc 0.757 f1 0.757 || val_loss 1.4539 acc 0.399 f1 0.324\n",
            "[W3] GRU Epoch 20 | train_loss 0.4745 acc 0.772 f1 0.772 || val_loss 1.4774 acc 0.389 f1 0.311\n",
            "[W3] GRU Epoch 21 | train_loss 0.4685 acc 0.784 f1 0.784 || val_loss 1.5014 acc 0.405 f1 0.332\n",
            "[W3] GRU Epoch 22 | train_loss 0.4391 acc 0.798 f1 0.798 || val_loss 1.5790 acc 0.383 f1 0.310\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=92\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1042 acc 0.334 f1 0.170 || val_loss 1.0770 acc 0.459 f1 0.210\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0957 acc 0.343 f1 0.275 || val_loss 1.0982 acc 0.346 f1 0.294\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0894 acc 0.398 f1 0.387 || val_loss 1.1035 acc 0.311 f1 0.292\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0799 acc 0.412 f1 0.401 || val_loss 1.0992 acc 0.337 f1 0.314\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0589 acc 0.457 f1 0.448 || val_loss 1.0833 acc 0.340 f1 0.287\n",
            "[W3] LSTM Epoch 06 | train_loss 1.0020 acc 0.511 f1 0.499 || val_loss 1.0820 acc 0.377 f1 0.320\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8930 acc 0.562 f1 0.553 || val_loss 1.0712 acc 0.383 f1 0.299\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8204 acc 0.601 f1 0.595 || val_loss 1.1351 acc 0.342 f1 0.289\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7702 acc 0.618 f1 0.615 || val_loss 1.1692 acc 0.362 f1 0.307\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7378 acc 0.639 f1 0.635 || val_loss 1.2395 acc 0.364 f1 0.313\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7073 acc 0.656 f1 0.652 || val_loss 1.1984 acc 0.374 f1 0.304\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6666 acc 0.681 f1 0.679 || val_loss 1.2273 acc 0.385 f1 0.321\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6426 acc 0.685 f1 0.683 || val_loss 1.2576 acc 0.412 f1 0.345\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6186 acc 0.713 f1 0.710 || val_loss 1.2985 acc 0.372 f1 0.313\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5866 acc 0.722 f1 0.721 || val_loss 1.2984 acc 0.401 f1 0.336\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5669 acc 0.729 f1 0.727 || val_loss 1.3445 acc 0.430 f1 0.355\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5377 acc 0.755 f1 0.754 || val_loss 1.3535 acc 0.414 f1 0.338\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5128 acc 0.767 f1 0.766 || val_loss 1.3775 acc 0.436 f1 0.355\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4806 acc 0.783 f1 0.783 || val_loss 1.4328 acc 0.416 f1 0.337\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4716 acc 0.781 f1 0.781 || val_loss 1.4374 acc 0.418 f1 0.326\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4466 acc 0.794 f1 0.794 || val_loss 1.4879 acc 0.414 f1 0.329\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4258 acc 0.801 f1 0.800 || val_loss 1.5718 acc 0.422 f1 0.343\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4086 acc 0.810 f1 0.810 || val_loss 1.5857 acc 0.424 f1 0.341\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3868 acc 0.820 f1 0.820 || val_loss 1.6705 acc 0.414 f1 0.338\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  92%|| 92/100 [44:05<04:25, 33.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=93 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=93\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1401 acc 0.402 f1 0.400 || val_loss 1.1371 acc 0.319 f1 0.292\n",
            "[W3] ANN Epoch 02 | train_loss 0.9774 acc 0.505 f1 0.498 || val_loss 1.1295 acc 0.333 f1 0.299\n",
            "[W3] ANN Epoch 03 | train_loss 0.8808 acc 0.582 f1 0.577 || val_loss 1.1363 acc 0.352 f1 0.305\n",
            "[W3] ANN Epoch 04 | train_loss 0.7842 acc 0.624 f1 0.620 || val_loss 1.1235 acc 0.362 f1 0.303\n",
            "[W3] ANN Epoch 05 | train_loss 0.7288 acc 0.650 f1 0.648 || val_loss 1.1258 acc 0.393 f1 0.325\n",
            "[W3] ANN Epoch 06 | train_loss 0.6887 acc 0.682 f1 0.680 || val_loss 1.1189 acc 0.412 f1 0.347\n",
            "[W3] ANN Epoch 07 | train_loss 0.6306 acc 0.704 f1 0.702 || val_loss 1.1449 acc 0.401 f1 0.322\n",
            "[W3] ANN Epoch 08 | train_loss 0.6018 acc 0.724 f1 0.723 || val_loss 1.1250 acc 0.442 f1 0.345\n",
            "[W3] ANN Epoch 09 | train_loss 0.5556 acc 0.747 f1 0.747 || val_loss 1.1632 acc 0.432 f1 0.338\n",
            "[W3] ANN Epoch 10 | train_loss 0.5312 acc 0.756 f1 0.756 || val_loss 1.2444 acc 0.430 f1 0.338\n",
            "[W3] ANN Epoch 11 | train_loss 0.5093 acc 0.768 f1 0.767 || val_loss 1.2317 acc 0.414 f1 0.321\n",
            "[W3] ANN Epoch 12 | train_loss 0.4805 acc 0.791 f1 0.791 || val_loss 1.2627 acc 0.440 f1 0.349\n",
            "[W3] ANN Epoch 13 | train_loss 0.4497 acc 0.796 f1 0.795 || val_loss 1.2824 acc 0.432 f1 0.360\n",
            "[W3] ANN Epoch 14 | train_loss 0.4278 acc 0.811 f1 0.811 || val_loss 1.3154 acc 0.428 f1 0.344\n",
            "[W3] ANN Epoch 15 | train_loss 0.4125 acc 0.815 f1 0.815 || val_loss 1.4015 acc 0.442 f1 0.351\n",
            "[W3] ANN Epoch 16 | train_loss 0.4424 acc 0.821 f1 0.820 || val_loss 1.3920 acc 0.424 f1 0.326\n",
            "[W3] ANN Epoch 17 | train_loss 0.4020 acc 0.835 f1 0.835 || val_loss 1.3674 acc 0.438 f1 0.348\n",
            "[W3] ANN Epoch 18 | train_loss 0.3692 acc 0.851 f1 0.851 || val_loss 1.4520 acc 0.401 f1 0.317\n",
            "[W3] ANN Epoch 19 | train_loss 0.3670 acc 0.848 f1 0.848 || val_loss 1.4591 acc 0.422 f1 0.319\n",
            "[W3] ANN Epoch 20 | train_loss 0.3401 acc 0.858 f1 0.858 || val_loss 1.4723 acc 0.436 f1 0.334\n",
            "[W3] ANN Epoch 21 | train_loss 0.3506 acc 0.859 f1 0.859 || val_loss 1.5254 acc 0.453 f1 0.358\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=93\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0786 acc 0.380 f1 0.379 || val_loss 1.0450 acc 0.403 f1 0.283\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9485 acc 0.543 f1 0.539 || val_loss 1.0520 acc 0.387 f1 0.306\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8187 acc 0.605 f1 0.602 || val_loss 1.1448 acc 0.387 f1 0.324\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7112 acc 0.662 f1 0.659 || val_loss 1.1885 acc 0.407 f1 0.318\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6284 acc 0.715 f1 0.712 || val_loss 1.3038 acc 0.383 f1 0.329\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5636 acc 0.738 f1 0.736 || val_loss 1.3260 acc 0.409 f1 0.343\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.4949 acc 0.781 f1 0.781 || val_loss 1.4156 acc 0.412 f1 0.344\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4410 acc 0.806 f1 0.805 || val_loss 1.5447 acc 0.409 f1 0.335\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.3917 acc 0.831 f1 0.830 || val_loss 1.5519 acc 0.397 f1 0.330\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3472 acc 0.857 f1 0.857 || val_loss 1.6616 acc 0.424 f1 0.331\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3122 acc 0.874 f1 0.874 || val_loss 1.7228 acc 0.391 f1 0.317\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2613 acc 0.899 f1 0.899 || val_loss 1.7950 acc 0.424 f1 0.331\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2388 acc 0.910 f1 0.910 || val_loss 1.9716 acc 0.383 f1 0.309\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2157 acc 0.919 f1 0.919 || val_loss 1.9319 acc 0.385 f1 0.307\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1646 acc 0.941 f1 0.941 || val_loss 2.1177 acc 0.395 f1 0.320\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=93\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0945 acc 0.379 f1 0.337 || val_loss 1.0942 acc 0.315 f1 0.287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0753 acc 0.434 f1 0.430 || val_loss 1.0952 acc 0.292 f1 0.272\n",
            "[W3] RNN Epoch 03 | train_loss 1.0553 acc 0.463 f1 0.450 || val_loss 1.0878 acc 0.327 f1 0.303\n",
            "[W3] RNN Epoch 04 | train_loss 1.0266 acc 0.489 f1 0.481 || val_loss 1.0797 acc 0.350 f1 0.325\n",
            "[W3] RNN Epoch 05 | train_loss 0.9987 acc 0.506 f1 0.496 || val_loss 1.0758 acc 0.379 f1 0.352\n",
            "[W3] RNN Epoch 06 | train_loss 0.9769 acc 0.528 f1 0.517 || val_loss 1.0875 acc 0.368 f1 0.346\n",
            "[W3] RNN Epoch 07 | train_loss 0.9457 acc 0.545 f1 0.534 || val_loss 1.0796 acc 0.356 f1 0.315\n",
            "[W3] RNN Epoch 08 | train_loss 0.9219 acc 0.555 f1 0.545 || val_loss 1.1026 acc 0.368 f1 0.331\n",
            "[W3] RNN Epoch 09 | train_loss 0.9032 acc 0.565 f1 0.554 || val_loss 1.0986 acc 0.360 f1 0.323\n",
            "[W3] RNN Epoch 10 | train_loss 0.8796 acc 0.576 f1 0.565 || val_loss 1.1039 acc 0.352 f1 0.309\n",
            "[W3] RNN Epoch 11 | train_loss 0.8607 acc 0.594 f1 0.582 || val_loss 1.1118 acc 0.364 f1 0.312\n",
            "[W3] RNN Epoch 12 | train_loss 0.8360 acc 0.595 f1 0.584 || val_loss 1.1153 acc 0.377 f1 0.330\n",
            "[W3] RNN Epoch 13 | train_loss 0.8165 acc 0.613 f1 0.601 || val_loss 1.1336 acc 0.364 f1 0.315\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=93\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0974 acc 0.336 f1 0.297 || val_loss 1.0881 acc 0.426 f1 0.324\n",
            "[W3] GRU Epoch 02 | train_loss 1.0889 acc 0.395 f1 0.395 || val_loss 1.0863 acc 0.377 f1 0.333\n",
            "[W3] GRU Epoch 03 | train_loss 1.0775 acc 0.429 f1 0.429 || val_loss 1.0958 acc 0.313 f1 0.297\n",
            "[W3] GRU Epoch 04 | train_loss 1.0588 acc 0.466 f1 0.459 || val_loss 1.1101 acc 0.313 f1 0.304\n",
            "[W3] GRU Epoch 05 | train_loss 1.0227 acc 0.502 f1 0.493 || val_loss 1.0879 acc 0.346 f1 0.312\n",
            "[W3] GRU Epoch 06 | train_loss 0.9577 acc 0.542 f1 0.535 || val_loss 1.0817 acc 0.385 f1 0.323\n",
            "[W3] GRU Epoch 07 | train_loss 0.8886 acc 0.557 f1 0.550 || val_loss 1.1191 acc 0.389 f1 0.339\n",
            "[W3] GRU Epoch 08 | train_loss 0.8207 acc 0.598 f1 0.589 || val_loss 1.1435 acc 0.389 f1 0.333\n",
            "[W3] GRU Epoch 09 | train_loss 0.7861 acc 0.616 f1 0.612 || val_loss 1.1041 acc 0.407 f1 0.334\n",
            "[W3] GRU Epoch 10 | train_loss 0.7424 acc 0.651 f1 0.647 || val_loss 1.1328 acc 0.393 f1 0.320\n",
            "[W3] GRU Epoch 11 | train_loss 0.7107 acc 0.657 f1 0.653 || val_loss 1.1336 acc 0.409 f1 0.326\n",
            "[W3] GRU Epoch 12 | train_loss 0.6792 acc 0.682 f1 0.680 || val_loss 1.2329 acc 0.391 f1 0.335\n",
            "[W3] GRU Epoch 13 | train_loss 0.6563 acc 0.691 f1 0.687 || val_loss 1.1562 acc 0.424 f1 0.336\n",
            "[W3] GRU Epoch 14 | train_loss 0.6232 acc 0.699 f1 0.696 || val_loss 1.1836 acc 0.430 f1 0.335\n",
            "[W3] GRU Epoch 15 | train_loss 0.5981 acc 0.716 f1 0.713 || val_loss 1.2243 acc 0.409 f1 0.335\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=93\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0985 acc 0.335 f1 0.241 || val_loss 1.0963 acc 0.374 f1 0.258\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0929 acc 0.380 f1 0.345 || val_loss 1.1006 acc 0.294 f1 0.288\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0825 acc 0.414 f1 0.397 || val_loss 1.0784 acc 0.337 f1 0.309\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0641 acc 0.442 f1 0.429 || val_loss 1.0841 acc 0.327 f1 0.308\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0192 acc 0.489 f1 0.472 || val_loss 1.0853 acc 0.350 f1 0.320\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9266 acc 0.548 f1 0.534 || val_loss 1.1339 acc 0.337 f1 0.298\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8487 acc 0.587 f1 0.580 || val_loss 1.1799 acc 0.340 f1 0.302\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7994 acc 0.607 f1 0.599 || val_loss 1.1902 acc 0.348 f1 0.298\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7538 acc 0.624 f1 0.618 || val_loss 1.1834 acc 0.354 f1 0.299\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7266 acc 0.645 f1 0.639 || val_loss 1.1979 acc 0.370 f1 0.315\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6928 acc 0.659 f1 0.655 || val_loss 1.1872 acc 0.391 f1 0.300\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6610 acc 0.671 f1 0.665 || val_loss 1.2399 acc 0.383 f1 0.302\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6377 acc 0.691 f1 0.688 || val_loss 1.2379 acc 0.397 f1 0.301\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  93%|| 93/100 [44:29<03:33, 30.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=94 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=94\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1358 acc 0.403 f1 0.404 || val_loss 1.1243 acc 0.321 f1 0.302\n",
            "[W3] ANN Epoch 02 | train_loss 0.9582 acc 0.512 f1 0.504 || val_loss 1.1444 acc 0.333 f1 0.309\n",
            "[W3] ANN Epoch 03 | train_loss 0.8750 acc 0.565 f1 0.555 || val_loss 1.1192 acc 0.366 f1 0.329\n",
            "[W3] ANN Epoch 04 | train_loss 0.7783 acc 0.621 f1 0.614 || val_loss 1.1238 acc 0.362 f1 0.320\n",
            "[W3] ANN Epoch 05 | train_loss 0.7294 acc 0.646 f1 0.640 || val_loss 1.1090 acc 0.405 f1 0.347\n",
            "[W3] ANN Epoch 06 | train_loss 0.6639 acc 0.678 f1 0.675 || val_loss 1.1161 acc 0.424 f1 0.373\n",
            "[W3] ANN Epoch 07 | train_loss 0.6383 acc 0.702 f1 0.699 || val_loss 1.0950 acc 0.447 f1 0.389\n",
            "[W3] ANN Epoch 08 | train_loss 0.5756 acc 0.726 f1 0.725 || val_loss 1.1436 acc 0.440 f1 0.380\n",
            "[W3] ANN Epoch 09 | train_loss 0.5444 acc 0.740 f1 0.739 || val_loss 1.1619 acc 0.412 f1 0.361\n",
            "[W3] ANN Epoch 10 | train_loss 0.5173 acc 0.766 f1 0.765 || val_loss 1.1913 acc 0.426 f1 0.368\n",
            "[W3] ANN Epoch 11 | train_loss 0.5188 acc 0.757 f1 0.755 || val_loss 1.2012 acc 0.444 f1 0.382\n",
            "[W3] ANN Epoch 12 | train_loss 0.4970 acc 0.777 f1 0.777 || val_loss 1.2085 acc 0.414 f1 0.358\n",
            "[W3] ANN Epoch 13 | train_loss 0.4732 acc 0.788 f1 0.787 || val_loss 1.2497 acc 0.438 f1 0.390\n",
            "[W3] ANN Epoch 14 | train_loss 0.4520 acc 0.798 f1 0.797 || val_loss 1.2797 acc 0.449 f1 0.389\n",
            "[W3] ANN Epoch 15 | train_loss 0.4268 acc 0.804 f1 0.805 || val_loss 1.2779 acc 0.447 f1 0.391\n",
            "[W3] ANN Epoch 16 | train_loss 0.4197 acc 0.817 f1 0.816 || val_loss 1.3166 acc 0.436 f1 0.360\n",
            "[W3] ANN Epoch 17 | train_loss 0.4011 acc 0.835 f1 0.835 || val_loss 1.3253 acc 0.475 f1 0.408\n",
            "[W3] ANN Epoch 18 | train_loss 0.3647 acc 0.843 f1 0.843 || val_loss 1.3993 acc 0.434 f1 0.362\n",
            "[W3] ANN Epoch 19 | train_loss 0.3760 acc 0.842 f1 0.841 || val_loss 1.4114 acc 0.436 f1 0.383\n",
            "[W3] ANN Epoch 20 | train_loss 0.3849 acc 0.836 f1 0.836 || val_loss 1.3884 acc 0.471 f1 0.413\n",
            "[W3] ANN Epoch 21 | train_loss 0.3538 acc 0.852 f1 0.852 || val_loss 1.4061 acc 0.447 f1 0.382\n",
            "[W3] ANN Epoch 22 | train_loss 0.3295 acc 0.865 f1 0.864 || val_loss 1.4052 acc 0.484 f1 0.397\n",
            "[W3] ANN Epoch 23 | train_loss 0.3283 acc 0.858 f1 0.858 || val_loss 1.4558 acc 0.455 f1 0.397\n",
            "[W3] ANN Epoch 24 | train_loss 0.2904 acc 0.885 f1 0.885 || val_loss 1.4917 acc 0.461 f1 0.408\n",
            "[W3] ANN Epoch 25 | train_loss 0.2937 acc 0.877 f1 0.876 || val_loss 1.5127 acc 0.453 f1 0.404\n",
            "[W3] ANN Epoch 26 | train_loss 0.3067 acc 0.875 f1 0.875 || val_loss 1.5404 acc 0.440 f1 0.374\n",
            "[W3] ANN Epoch 27 | train_loss 0.2898 acc 0.881 f1 0.881 || val_loss 1.5591 acc 0.444 f1 0.383\n",
            "[W3] ANN Epoch 28 | train_loss 0.2606 acc 0.901 f1 0.901 || val_loss 1.5273 acc 0.442 f1 0.388\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=94\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0805 acc 0.379 f1 0.379 || val_loss 1.0369 acc 0.422 f1 0.335\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9498 acc 0.542 f1 0.536 || val_loss 1.0241 acc 0.389 f1 0.309\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8227 acc 0.608 f1 0.604 || val_loss 1.1100 acc 0.377 f1 0.314\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7221 acc 0.660 f1 0.657 || val_loss 1.1354 acc 0.387 f1 0.316\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6507 acc 0.700 f1 0.697 || val_loss 1.1782 acc 0.412 f1 0.329\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5709 acc 0.737 f1 0.736 || val_loss 1.2411 acc 0.374 f1 0.312\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5219 acc 0.767 f1 0.765 || val_loss 1.3027 acc 0.399 f1 0.330\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4597 acc 0.795 f1 0.794 || val_loss 1.3783 acc 0.364 f1 0.301\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4042 acc 0.826 f1 0.826 || val_loss 1.4338 acc 0.409 f1 0.347\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3614 acc 0.843 f1 0.843 || val_loss 1.5466 acc 0.401 f1 0.341\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3165 acc 0.875 f1 0.874 || val_loss 1.6229 acc 0.401 f1 0.336\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2752 acc 0.888 f1 0.888 || val_loss 1.6810 acc 0.385 f1 0.317\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2596 acc 0.903 f1 0.903 || val_loss 1.7376 acc 0.383 f1 0.312\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2183 acc 0.918 f1 0.918 || val_loss 1.9028 acc 0.389 f1 0.319\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1948 acc 0.929 f1 0.929 || val_loss 1.9984 acc 0.397 f1 0.321\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1546 acc 0.944 f1 0.944 || val_loss 2.0876 acc 0.426 f1 0.347\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1339 acc 0.958 f1 0.958 || val_loss 2.1763 acc 0.434 f1 0.365\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1161 acc 0.962 f1 0.962 || val_loss 2.2526 acc 0.416 f1 0.338\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.0978 acc 0.971 f1 0.971 || val_loss 2.3489 acc 0.430 f1 0.365\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.0878 acc 0.971 f1 0.971 || val_loss 2.5115 acc 0.420 f1 0.344\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.0797 acc 0.974 f1 0.974 || val_loss 2.5853 acc 0.418 f1 0.341\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.0958 acc 0.969 f1 0.969 || val_loss 2.5718 acc 0.430 f1 0.349\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.0903 acc 0.968 f1 0.968 || val_loss 2.5159 acc 0.395 f1 0.320\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.0714 acc 0.976 f1 0.976 || val_loss 2.5335 acc 0.449 f1 0.376\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.0698 acc 0.979 f1 0.979 || val_loss 2.7865 acc 0.391 f1 0.333\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.0648 acc 0.977 f1 0.977 || val_loss 2.7050 acc 0.412 f1 0.336\n",
            "[W3] CNN1D Epoch 27 | train_loss 0.0595 acc 0.978 f1 0.978 || val_loss 2.7206 acc 0.403 f1 0.334\n",
            "[W3] CNN1D Epoch 28 | train_loss 0.0573 acc 0.981 f1 0.981 || val_loss 2.7920 acc 0.430 f1 0.350\n",
            "[W3] CNN1D Epoch 29 | train_loss 0.0449 acc 0.986 f1 0.986 || val_loss 2.9545 acc 0.428 f1 0.358\n",
            "[W3] CNN1D Epoch 30 | train_loss 0.0441 acc 0.988 f1 0.988 || val_loss 3.0091 acc 0.424 f1 0.329\n",
            "[W3] CNN1D Epoch 31 | train_loss 0.0435 acc 0.985 f1 0.985 || val_loss 3.0427 acc 0.409 f1 0.351\n",
            "[W3] CNN1D Epoch 32 | train_loss 0.0387 acc 0.987 f1 0.987 || val_loss 3.0596 acc 0.422 f1 0.339\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=94\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0972 acc 0.358 f1 0.328 || val_loss 1.0946 acc 0.397 f1 0.346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0808 acc 0.424 f1 0.404 || val_loss 1.0858 acc 0.379 f1 0.339\n",
            "[W3] RNN Epoch 03 | train_loss 1.0642 acc 0.464 f1 0.458 || val_loss 1.0755 acc 0.389 f1 0.346\n",
            "[W3] RNN Epoch 04 | train_loss 1.0422 acc 0.464 f1 0.457 || val_loss 1.0823 acc 0.354 f1 0.325\n",
            "[W3] RNN Epoch 05 | train_loss 1.0205 acc 0.483 f1 0.472 || val_loss 1.0947 acc 0.333 f1 0.312\n",
            "[W3] RNN Epoch 06 | train_loss 0.9955 acc 0.500 f1 0.486 || val_loss 1.0626 acc 0.364 f1 0.323\n",
            "[W3] RNN Epoch 07 | train_loss 0.9696 acc 0.532 f1 0.525 || val_loss 1.0753 acc 0.366 f1 0.319\n",
            "[W3] RNN Epoch 08 | train_loss 0.9439 acc 0.540 f1 0.529 || val_loss 1.0748 acc 0.364 f1 0.319\n",
            "[W3] RNN Epoch 09 | train_loss 0.9115 acc 0.568 f1 0.557 || val_loss 1.0917 acc 0.344 f1 0.312\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=94\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1024 acc 0.327 f1 0.279 || val_loss 1.0916 acc 0.374 f1 0.337\n",
            "[W3] GRU Epoch 02 | train_loss 1.0902 acc 0.386 f1 0.375 || val_loss 1.1026 acc 0.274 f1 0.266\n",
            "[W3] GRU Epoch 03 | train_loss 1.0808 acc 0.423 f1 0.414 || val_loss 1.0954 acc 0.325 f1 0.302\n",
            "[W3] GRU Epoch 04 | train_loss 1.0633 acc 0.450 f1 0.444 || val_loss 1.0999 acc 0.302 f1 0.283\n",
            "[W3] GRU Epoch 05 | train_loss 1.0311 acc 0.491 f1 0.481 || val_loss 1.0961 acc 0.325 f1 0.298\n",
            "[W3] GRU Epoch 06 | train_loss 0.9574 acc 0.532 f1 0.523 || val_loss 1.1075 acc 0.333 f1 0.295\n",
            "[W3] GRU Epoch 07 | train_loss 0.8795 acc 0.574 f1 0.567 || val_loss 1.0900 acc 0.356 f1 0.302\n",
            "[W3] GRU Epoch 08 | train_loss 0.8223 acc 0.607 f1 0.602 || val_loss 1.1014 acc 0.358 f1 0.297\n",
            "[W3] GRU Epoch 09 | train_loss 0.7706 acc 0.631 f1 0.626 || val_loss 1.0917 acc 0.374 f1 0.304\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=94\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0999 acc 0.341 f1 0.281 || val_loss 1.1056 acc 0.245 f1 0.217\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0947 acc 0.384 f1 0.382 || val_loss 1.0968 acc 0.364 f1 0.334\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0871 acc 0.410 f1 0.410 || val_loss 1.0927 acc 0.331 f1 0.309\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0764 acc 0.429 f1 0.422 || val_loss 1.0780 acc 0.364 f1 0.329\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0517 acc 0.455 f1 0.450 || val_loss 1.0787 acc 0.356 f1 0.323\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9900 acc 0.525 f1 0.513 || val_loss 1.0618 acc 0.381 f1 0.328\n",
            "[W3] LSTM Epoch 07 | train_loss 0.9073 acc 0.564 f1 0.555 || val_loss 1.1147 acc 0.342 f1 0.304\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8364 acc 0.598 f1 0.593 || val_loss 1.0986 acc 0.370 f1 0.289\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7802 acc 0.625 f1 0.620 || val_loss 1.1315 acc 0.356 f1 0.279\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7328 acc 0.643 f1 0.638 || val_loss 1.1830 acc 0.366 f1 0.304\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  94%|| 94/100 [44:59<03:02, 30.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=95 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=95\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1634 acc 0.385 f1 0.385 || val_loss 1.1311 acc 0.329 f1 0.305\n",
            "[W3] ANN Epoch 02 | train_loss 0.9689 acc 0.512 f1 0.508 || val_loss 1.1472 acc 0.319 f1 0.283\n",
            "[W3] ANN Epoch 03 | train_loss 0.8657 acc 0.581 f1 0.577 || val_loss 1.1128 acc 0.350 f1 0.297\n",
            "[W3] ANN Epoch 04 | train_loss 0.7743 acc 0.631 f1 0.626 || val_loss 1.1200 acc 0.368 f1 0.306\n",
            "[W3] ANN Epoch 05 | train_loss 0.7241 acc 0.661 f1 0.658 || val_loss 1.1294 acc 0.383 f1 0.319\n",
            "[W3] ANN Epoch 06 | train_loss 0.6720 acc 0.680 f1 0.679 || val_loss 1.1478 acc 0.377 f1 0.308\n",
            "[W3] ANN Epoch 07 | train_loss 0.6202 acc 0.713 f1 0.711 || val_loss 1.1378 acc 0.395 f1 0.319\n",
            "[W3] ANN Epoch 08 | train_loss 0.5800 acc 0.725 f1 0.723 || val_loss 1.1790 acc 0.416 f1 0.339\n",
            "[W3] ANN Epoch 09 | train_loss 0.5574 acc 0.737 f1 0.737 || val_loss 1.2176 acc 0.399 f1 0.322\n",
            "[W3] ANN Epoch 10 | train_loss 0.5473 acc 0.740 f1 0.739 || val_loss 1.1943 acc 0.440 f1 0.364\n",
            "[W3] ANN Epoch 11 | train_loss 0.5070 acc 0.762 f1 0.762 || val_loss 1.2525 acc 0.432 f1 0.352\n",
            "[W3] ANN Epoch 12 | train_loss 0.4826 acc 0.785 f1 0.784 || val_loss 1.2009 acc 0.465 f1 0.383\n",
            "[W3] ANN Epoch 13 | train_loss 0.4712 acc 0.796 f1 0.796 || val_loss 1.2776 acc 0.438 f1 0.351\n",
            "[W3] ANN Epoch 14 | train_loss 0.4407 acc 0.803 f1 0.803 || val_loss 1.3190 acc 0.420 f1 0.343\n",
            "[W3] ANN Epoch 15 | train_loss 0.4224 acc 0.814 f1 0.814 || val_loss 1.3925 acc 0.432 f1 0.341\n",
            "[W3] ANN Epoch 16 | train_loss 0.4264 acc 0.820 f1 0.819 || val_loss 1.3934 acc 0.432 f1 0.352\n",
            "[W3] ANN Epoch 17 | train_loss 0.4097 acc 0.824 f1 0.823 || val_loss 1.3856 acc 0.432 f1 0.341\n",
            "[W3] ANN Epoch 18 | train_loss 0.4000 acc 0.831 f1 0.831 || val_loss 1.3522 acc 0.434 f1 0.337\n",
            "[W3] ANN Epoch 19 | train_loss 0.3634 acc 0.849 f1 0.850 || val_loss 1.4097 acc 0.434 f1 0.358\n",
            "[W3] ANN Epoch 20 | train_loss 0.3527 acc 0.854 f1 0.854 || val_loss 1.4140 acc 0.465 f1 0.381\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=95\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0690 acc 0.402 f1 0.403 || val_loss 1.0200 acc 0.453 f1 0.336\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9114 acc 0.563 f1 0.558 || val_loss 1.0544 acc 0.424 f1 0.350\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.7981 acc 0.610 f1 0.606 || val_loss 1.1362 acc 0.403 f1 0.346\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7008 acc 0.663 f1 0.658 || val_loss 1.1451 acc 0.440 f1 0.366\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6275 acc 0.712 f1 0.710 || val_loss 1.2542 acc 0.416 f1 0.356\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5594 acc 0.734 f1 0.732 || val_loss 1.2728 acc 0.412 f1 0.342\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5133 acc 0.771 f1 0.770 || val_loss 1.3313 acc 0.444 f1 0.382\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4555 acc 0.799 f1 0.799 || val_loss 1.4088 acc 0.432 f1 0.378\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4099 acc 0.819 f1 0.819 || val_loss 1.4751 acc 0.426 f1 0.359\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3849 acc 0.829 f1 0.828 || val_loss 1.4554 acc 0.455 f1 0.363\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3322 acc 0.865 f1 0.865 || val_loss 1.5665 acc 0.440 f1 0.353\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2849 acc 0.887 f1 0.887 || val_loss 1.6554 acc 0.428 f1 0.349\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2644 acc 0.895 f1 0.895 || val_loss 1.8018 acc 0.422 f1 0.352\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2407 acc 0.908 f1 0.908 || val_loss 1.8295 acc 0.412 f1 0.346\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.1903 acc 0.934 f1 0.934 || val_loss 1.9002 acc 0.449 f1 0.369\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=95\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0998 acc 0.340 f1 0.324 || val_loss 1.1027 acc 0.313 f1 0.307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0819 acc 0.418 f1 0.407 || val_loss 1.0930 acc 0.366 f1 0.346\n",
            "[W3] RNN Epoch 03 | train_loss 1.0669 acc 0.438 f1 0.432 || val_loss 1.0876 acc 0.366 f1 0.352\n",
            "[W3] RNN Epoch 04 | train_loss 1.0499 acc 0.456 f1 0.450 || val_loss 1.0869 acc 0.358 f1 0.339\n",
            "[W3] RNN Epoch 05 | train_loss 1.0343 acc 0.474 f1 0.470 || val_loss 1.0796 acc 0.352 f1 0.329\n",
            "[W3] RNN Epoch 06 | train_loss 1.0178 acc 0.493 f1 0.487 || val_loss 1.0916 acc 0.362 f1 0.341\n",
            "[W3] RNN Epoch 07 | train_loss 0.9924 acc 0.510 f1 0.503 || val_loss 1.0698 acc 0.374 f1 0.348\n",
            "[W3] RNN Epoch 08 | train_loss 0.9683 acc 0.540 f1 0.534 || val_loss 1.0804 acc 0.381 f1 0.353\n",
            "[W3] RNN Epoch 09 | train_loss 0.9414 acc 0.560 f1 0.552 || val_loss 1.0866 acc 0.346 f1 0.321\n",
            "[W3] RNN Epoch 10 | train_loss 0.9241 acc 0.560 f1 0.555 || val_loss 1.0864 acc 0.381 f1 0.353\n",
            "[W3] RNN Epoch 11 | train_loss 0.8980 acc 0.581 f1 0.574 || val_loss 1.0857 acc 0.389 f1 0.366\n",
            "[W3] RNN Epoch 12 | train_loss 0.8726 acc 0.600 f1 0.592 || val_loss 1.0782 acc 0.399 f1 0.369\n",
            "[W3] RNN Epoch 13 | train_loss 0.8545 acc 0.612 f1 0.605 || val_loss 1.0851 acc 0.389 f1 0.348\n",
            "[W3] RNN Epoch 14 | train_loss 0.8215 acc 0.634 f1 0.629 || val_loss 1.0804 acc 0.399 f1 0.357\n",
            "[W3] RNN Epoch 15 | train_loss 0.7958 acc 0.642 f1 0.636 || val_loss 1.1097 acc 0.391 f1 0.360\n",
            "[W3] RNN Epoch 16 | train_loss 0.7774 acc 0.645 f1 0.638 || val_loss 1.1133 acc 0.403 f1 0.371\n",
            "[W3] RNN Epoch 17 | train_loss 0.7549 acc 0.657 f1 0.650 || val_loss 1.0964 acc 0.389 f1 0.346\n",
            "[W3] RNN Epoch 18 | train_loss 0.7217 acc 0.675 f1 0.668 || val_loss 1.1202 acc 0.407 f1 0.358\n",
            "[W3] RNN Epoch 19 | train_loss 0.7085 acc 0.685 f1 0.679 || val_loss 1.1268 acc 0.409 f1 0.366\n",
            "[W3] RNN Epoch 20 | train_loss 0.6812 acc 0.690 f1 0.685 || val_loss 1.1483 acc 0.393 f1 0.341\n",
            "[W3] RNN Epoch 21 | train_loss 0.6433 acc 0.710 f1 0.706 || val_loss 1.1325 acc 0.434 f1 0.362\n",
            "[W3] RNN Epoch 22 | train_loss 0.6393 acc 0.710 f1 0.707 || val_loss 1.1816 acc 0.420 f1 0.368\n",
            "[W3] RNN Epoch 23 | train_loss 0.6151 acc 0.731 f1 0.728 || val_loss 1.1912 acc 0.414 f1 0.362\n",
            "[W3] RNN Epoch 24 | train_loss 0.5903 acc 0.733 f1 0.728 || val_loss 1.1740 acc 0.440 f1 0.362\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=95\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0982 acc 0.368 f1 0.292 || val_loss 1.1052 acc 0.284 f1 0.268\n",
            "[W3] GRU Epoch 02 | train_loss 1.0883 acc 0.402 f1 0.396 || val_loss 1.0941 acc 0.362 f1 0.330\n",
            "[W3] GRU Epoch 03 | train_loss 1.0767 acc 0.425 f1 0.425 || val_loss 1.0885 acc 0.346 f1 0.313\n",
            "[W3] GRU Epoch 04 | train_loss 1.0623 acc 0.452 f1 0.446 || val_loss 1.1015 acc 0.329 f1 0.316\n",
            "[W3] GRU Epoch 05 | train_loss 1.0404 acc 0.468 f1 0.459 || val_loss 1.0927 acc 0.350 f1 0.318\n",
            "[W3] GRU Epoch 06 | train_loss 0.9979 acc 0.519 f1 0.515 || val_loss 1.1053 acc 0.340 f1 0.313\n",
            "[W3] GRU Epoch 07 | train_loss 0.9261 acc 0.549 f1 0.541 || val_loss 1.1541 acc 0.323 f1 0.292\n",
            "[W3] GRU Epoch 08 | train_loss 0.8464 acc 0.597 f1 0.590 || val_loss 1.1378 acc 0.358 f1 0.310\n",
            "[W3] GRU Epoch 09 | train_loss 0.7976 acc 0.613 f1 0.608 || val_loss 1.2229 acc 0.354 f1 0.308\n",
            "[W3] GRU Epoch 10 | train_loss 0.7421 acc 0.641 f1 0.635 || val_loss 1.1752 acc 0.383 f1 0.327\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=95\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1013 acc 0.338 f1 0.184 || val_loss 1.0978 acc 0.416 f1 0.263\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0936 acc 0.387 f1 0.371 || val_loss 1.0973 acc 0.307 f1 0.297\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0869 acc 0.398 f1 0.393 || val_loss 1.0963 acc 0.302 f1 0.295\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0712 acc 0.428 f1 0.420 || val_loss 1.0941 acc 0.311 f1 0.296\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0322 acc 0.479 f1 0.467 || val_loss 1.1278 acc 0.282 f1 0.273\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9421 acc 0.540 f1 0.530 || val_loss 1.1222 acc 0.333 f1 0.304\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8621 acc 0.576 f1 0.569 || val_loss 1.0934 acc 0.374 f1 0.291\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8024 acc 0.606 f1 0.601 || val_loss 1.1067 acc 0.370 f1 0.302\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7565 acc 0.624 f1 0.619 || val_loss 1.1583 acc 0.362 f1 0.304\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7240 acc 0.645 f1 0.641 || val_loss 1.1692 acc 0.366 f1 0.279\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6944 acc 0.665 f1 0.662 || val_loss 1.2526 acc 0.352 f1 0.292\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6619 acc 0.685 f1 0.681 || val_loss 1.2286 acc 0.370 f1 0.301\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6251 acc 0.699 f1 0.695 || val_loss 1.2436 acc 0.399 f1 0.313\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6015 acc 0.724 f1 0.721 || val_loss 1.2737 acc 0.395 f1 0.304\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5685 acc 0.730 f1 0.729 || val_loss 1.3305 acc 0.372 f1 0.294\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5425 acc 0.743 f1 0.741 || val_loss 1.3716 acc 0.401 f1 0.325\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5191 acc 0.753 f1 0.751 || val_loss 1.4145 acc 0.377 f1 0.307\n",
            "[W3] LSTM Epoch 18 | train_loss 0.4926 acc 0.778 f1 0.777 || val_loss 1.4724 acc 0.389 f1 0.311\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4724 acc 0.779 f1 0.778 || val_loss 1.5200 acc 0.403 f1 0.316\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4416 acc 0.802 f1 0.801 || val_loss 1.5848 acc 0.387 f1 0.305\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4274 acc 0.814 f1 0.814 || val_loss 1.6382 acc 0.399 f1 0.323\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4067 acc 0.818 f1 0.818 || val_loss 1.6705 acc 0.395 f1 0.314\n",
            "[W3] LSTM Epoch 23 | train_loss 0.3798 acc 0.832 f1 0.832 || val_loss 1.7419 acc 0.393 f1 0.317\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3622 acc 0.838 f1 0.838 || val_loss 1.8197 acc 0.387 f1 0.309\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  95%|| 95/100 [45:27<02:28, 29.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=96 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=96\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1348 acc 0.406 f1 0.406 || val_loss 1.1235 acc 0.329 f1 0.300\n",
            "[W3] ANN Epoch 02 | train_loss 0.9624 acc 0.526 f1 0.523 || val_loss 1.1369 acc 0.337 f1 0.301\n",
            "[W3] ANN Epoch 03 | train_loss 0.8879 acc 0.565 f1 0.557 || val_loss 1.1490 acc 0.358 f1 0.317\n",
            "[W3] ANN Epoch 04 | train_loss 0.7948 acc 0.621 f1 0.613 || val_loss 1.1504 acc 0.385 f1 0.337\n",
            "[W3] ANN Epoch 05 | train_loss 0.7331 acc 0.657 f1 0.653 || val_loss 1.1452 acc 0.391 f1 0.331\n",
            "[W3] ANN Epoch 06 | train_loss 0.6836 acc 0.686 f1 0.684 || val_loss 1.1523 acc 0.407 f1 0.338\n",
            "[W3] ANN Epoch 07 | train_loss 0.6419 acc 0.699 f1 0.697 || val_loss 1.1909 acc 0.405 f1 0.341\n",
            "[W3] ANN Epoch 08 | train_loss 0.6185 acc 0.705 f1 0.703 || val_loss 1.1460 acc 0.418 f1 0.354\n",
            "[W3] ANN Epoch 09 | train_loss 0.5709 acc 0.742 f1 0.741 || val_loss 1.2040 acc 0.422 f1 0.343\n",
            "[W3] ANN Epoch 10 | train_loss 0.5577 acc 0.745 f1 0.745 || val_loss 1.2426 acc 0.432 f1 0.365\n",
            "[W3] ANN Epoch 11 | train_loss 0.5110 acc 0.773 f1 0.773 || val_loss 1.3024 acc 0.424 f1 0.335\n",
            "[W3] ANN Epoch 12 | train_loss 0.4708 acc 0.794 f1 0.794 || val_loss 1.2730 acc 0.428 f1 0.344\n",
            "[W3] ANN Epoch 13 | train_loss 0.4913 acc 0.780 f1 0.779 || val_loss 1.2930 acc 0.444 f1 0.344\n",
            "[W3] ANN Epoch 14 | train_loss 0.4444 acc 0.808 f1 0.808 || val_loss 1.3031 acc 0.447 f1 0.352\n",
            "[W3] ANN Epoch 15 | train_loss 0.4333 acc 0.806 f1 0.805 || val_loss 1.3820 acc 0.428 f1 0.336\n",
            "[W3] ANN Epoch 16 | train_loss 0.4209 acc 0.815 f1 0.814 || val_loss 1.3780 acc 0.442 f1 0.341\n",
            "[W3] ANN Epoch 17 | train_loss 0.4006 acc 0.832 f1 0.832 || val_loss 1.4065 acc 0.442 f1 0.347\n",
            "[W3] ANN Epoch 18 | train_loss 0.3919 acc 0.838 f1 0.838 || val_loss 1.4078 acc 0.453 f1 0.361\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=96\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0677 acc 0.408 f1 0.409 || val_loss 1.0113 acc 0.442 f1 0.320\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9240 acc 0.546 f1 0.540 || val_loss 1.0394 acc 0.432 f1 0.345\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8028 acc 0.613 f1 0.610 || val_loss 1.1048 acc 0.407 f1 0.331\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7088 acc 0.662 f1 0.660 || val_loss 1.1962 acc 0.395 f1 0.332\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6413 acc 0.709 f1 0.708 || val_loss 1.1990 acc 0.422 f1 0.353\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5695 acc 0.740 f1 0.738 || val_loss 1.2018 acc 0.434 f1 0.348\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5037 acc 0.782 f1 0.782 || val_loss 1.3354 acc 0.418 f1 0.345\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4522 acc 0.802 f1 0.802 || val_loss 1.3932 acc 0.393 f1 0.322\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4164 acc 0.817 f1 0.816 || val_loss 1.4241 acc 0.428 f1 0.352\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3722 acc 0.840 f1 0.839 || val_loss 1.5303 acc 0.453 f1 0.346\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3319 acc 0.863 f1 0.863 || val_loss 1.5693 acc 0.401 f1 0.329\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3005 acc 0.880 f1 0.880 || val_loss 1.6913 acc 0.414 f1 0.333\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2758 acc 0.889 f1 0.889 || val_loss 1.6803 acc 0.430 f1 0.350\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=96\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 01 | train_loss 1.0968 acc 0.361 f1 0.354 || val_loss 1.0936 acc 0.354 f1 0.324\n",
            "[W3] RNN Epoch 02 | train_loss 1.0812 acc 0.421 f1 0.419 || val_loss 1.0881 acc 0.342 f1 0.313\n",
            "[W3] RNN Epoch 03 | train_loss 1.0635 acc 0.422 f1 0.409 || val_loss 1.0868 acc 0.337 f1 0.309\n",
            "[W3] RNN Epoch 04 | train_loss 1.0423 acc 0.459 f1 0.452 || val_loss 1.1115 acc 0.331 f1 0.318\n",
            "[W3] RNN Epoch 05 | train_loss 1.0204 acc 0.491 f1 0.482 || val_loss 1.0885 acc 0.352 f1 0.306\n",
            "[W3] RNN Epoch 06 | train_loss 0.9945 acc 0.510 f1 0.502 || val_loss 1.0981 acc 0.358 f1 0.329\n",
            "[W3] RNN Epoch 07 | train_loss 0.9610 acc 0.523 f1 0.514 || val_loss 1.0850 acc 0.374 f1 0.330\n",
            "[W3] RNN Epoch 08 | train_loss 0.9406 acc 0.535 f1 0.525 || val_loss 1.0847 acc 0.370 f1 0.327\n",
            "[W3] RNN Epoch 09 | train_loss 0.9077 acc 0.567 f1 0.558 || val_loss 1.0903 acc 0.348 f1 0.294\n",
            "[W3] RNN Epoch 10 | train_loss 0.8826 acc 0.594 f1 0.586 || val_loss 1.1012 acc 0.364 f1 0.312\n",
            "[W3] RNN Epoch 11 | train_loss 0.8674 acc 0.587 f1 0.578 || val_loss 1.1077 acc 0.366 f1 0.306\n",
            "[W3] RNN Epoch 12 | train_loss 0.8342 acc 0.597 f1 0.586 || val_loss 1.1182 acc 0.360 f1 0.306\n",
            "[W3] RNN Epoch 13 | train_loss 0.8068 acc 0.617 f1 0.607 || val_loss 1.1653 acc 0.340 f1 0.300\n",
            "[W3] RNN Epoch 14 | train_loss 0.8006 acc 0.616 f1 0.608 || val_loss 1.1522 acc 0.368 f1 0.315\n",
            "[W3] RNN Epoch 15 | train_loss 0.7703 acc 0.633 f1 0.625 || val_loss 1.1617 acc 0.370 f1 0.308\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=96\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1009 acc 0.339 f1 0.240 || val_loss 1.0965 acc 0.391 f1 0.294\n",
            "[W3] GRU Epoch 02 | train_loss 1.0908 acc 0.388 f1 0.386 || val_loss 1.0897 acc 0.374 f1 0.339\n",
            "[W3] GRU Epoch 03 | train_loss 1.0806 acc 0.415 f1 0.414 || val_loss 1.0977 acc 0.305 f1 0.291\n",
            "[W3] GRU Epoch 04 | train_loss 1.0672 acc 0.428 f1 0.421 || val_loss 1.0944 acc 0.315 f1 0.298\n",
            "[W3] GRU Epoch 05 | train_loss 1.0422 acc 0.472 f1 0.464 || val_loss 1.0964 acc 0.319 f1 0.300\n",
            "[W3] GRU Epoch 06 | train_loss 1.0002 acc 0.514 f1 0.506 || val_loss 1.0768 acc 0.348 f1 0.302\n",
            "[W3] GRU Epoch 07 | train_loss 0.9343 acc 0.554 f1 0.546 || val_loss 1.1423 acc 0.335 f1 0.308\n",
            "[W3] GRU Epoch 08 | train_loss 0.8656 acc 0.582 f1 0.574 || val_loss 1.1422 acc 0.346 f1 0.306\n",
            "[W3] GRU Epoch 09 | train_loss 0.8111 acc 0.608 f1 0.602 || val_loss 1.1235 acc 0.366 f1 0.309\n",
            "[W3] GRU Epoch 10 | train_loss 0.7697 acc 0.625 f1 0.619 || val_loss 1.1651 acc 0.364 f1 0.311\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=96\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1001 acc 0.330 f1 0.218 || val_loss 1.0958 acc 0.412 f1 0.319\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0936 acc 0.374 f1 0.358 || val_loss 1.0943 acc 0.348 f1 0.300\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0856 acc 0.417 f1 0.415 || val_loss 1.0924 acc 0.321 f1 0.298\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0673 acc 0.450 f1 0.444 || val_loss 1.0951 acc 0.294 f1 0.274\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0239 acc 0.489 f1 0.479 || val_loss 1.1221 acc 0.290 f1 0.271\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9289 acc 0.539 f1 0.528 || val_loss 1.1463 acc 0.325 f1 0.297\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8545 acc 0.563 f1 0.557 || val_loss 1.0918 acc 0.362 f1 0.295\n",
            "[W3] LSTM Epoch 08 | train_loss 0.7900 acc 0.603 f1 0.598 || val_loss 1.1553 acc 0.344 f1 0.295\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7420 acc 0.639 f1 0.634 || val_loss 1.1822 acc 0.344 f1 0.292\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  96%|| 96/100 [45:48<01:48, 27.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=97 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=97\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1246 acc 0.399 f1 0.397 || val_loss 1.1497 acc 0.296 f1 0.286\n",
            "[W3] ANN Epoch 02 | train_loss 0.9767 acc 0.514 f1 0.506 || val_loss 1.1253 acc 0.342 f1 0.317\n",
            "[W3] ANN Epoch 03 | train_loss 0.8770 acc 0.563 f1 0.555 || val_loss 1.0926 acc 0.374 f1 0.333\n",
            "[W3] ANN Epoch 04 | train_loss 0.7868 acc 0.615 f1 0.609 || val_loss 1.1202 acc 0.364 f1 0.314\n",
            "[W3] ANN Epoch 05 | train_loss 0.7261 acc 0.647 f1 0.642 || val_loss 1.1220 acc 0.387 f1 0.325\n",
            "[W3] ANN Epoch 06 | train_loss 0.6737 acc 0.680 f1 0.676 || val_loss 1.1369 acc 0.397 f1 0.337\n",
            "[W3] ANN Epoch 07 | train_loss 0.6319 acc 0.702 f1 0.699 || val_loss 1.1294 acc 0.409 f1 0.348\n",
            "[W3] ANN Epoch 08 | train_loss 0.5962 acc 0.713 f1 0.711 || val_loss 1.1336 acc 0.420 f1 0.342\n",
            "[W3] ANN Epoch 09 | train_loss 0.5680 acc 0.736 f1 0.735 || val_loss 1.1511 acc 0.428 f1 0.348\n",
            "[W3] ANN Epoch 10 | train_loss 0.5317 acc 0.756 f1 0.755 || val_loss 1.1796 acc 0.434 f1 0.353\n",
            "[W3] ANN Epoch 11 | train_loss 0.5040 acc 0.770 f1 0.769 || val_loss 1.2501 acc 0.405 f1 0.349\n",
            "[W3] ANN Epoch 12 | train_loss 0.4875 acc 0.785 f1 0.785 || val_loss 1.2524 acc 0.409 f1 0.335\n",
            "[W3] ANN Epoch 13 | train_loss 0.4616 acc 0.793 f1 0.792 || val_loss 1.2681 acc 0.414 f1 0.327\n",
            "[W3] ANN Epoch 14 | train_loss 0.4747 acc 0.789 f1 0.788 || val_loss 1.2890 acc 0.405 f1 0.326\n",
            "[W3] ANN Epoch 15 | train_loss 0.4381 acc 0.806 f1 0.806 || val_loss 1.3110 acc 0.434 f1 0.341\n",
            "[W3] ANN Epoch 16 | train_loss 0.4210 acc 0.812 f1 0.812 || val_loss 1.3176 acc 0.409 f1 0.330\n",
            "[W3] ANN Epoch 17 | train_loss 0.4038 acc 0.826 f1 0.826 || val_loss 1.3474 acc 0.430 f1 0.340\n",
            "[W3] ANN Epoch 18 | train_loss 0.3924 acc 0.834 f1 0.833 || val_loss 1.3829 acc 0.401 f1 0.328\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=97\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0800 acc 0.382 f1 0.384 || val_loss 1.0421 acc 0.422 f1 0.329\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9487 acc 0.527 f1 0.519 || val_loss 1.0104 acc 0.430 f1 0.324\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8121 acc 0.613 f1 0.609 || val_loss 1.0827 acc 0.397 f1 0.330\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7126 acc 0.662 f1 0.658 || val_loss 1.1070 acc 0.397 f1 0.321\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6317 acc 0.709 f1 0.706 || val_loss 1.1690 acc 0.405 f1 0.318\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5644 acc 0.731 f1 0.730 || val_loss 1.2244 acc 0.381 f1 0.317\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5117 acc 0.761 f1 0.759 || val_loss 1.2728 acc 0.412 f1 0.325\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4740 acc 0.789 f1 0.788 || val_loss 1.3653 acc 0.379 f1 0.308\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4134 acc 0.824 f1 0.824 || val_loss 1.4392 acc 0.391 f1 0.306\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3718 acc 0.846 f1 0.846 || val_loss 1.4306 acc 0.409 f1 0.322\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3402 acc 0.858 f1 0.858 || val_loss 1.5947 acc 0.409 f1 0.351\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.2902 acc 0.882 f1 0.882 || val_loss 1.6447 acc 0.416 f1 0.328\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2819 acc 0.892 f1 0.892 || val_loss 1.6785 acc 0.403 f1 0.344\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2412 acc 0.911 f1 0.910 || val_loss 1.8205 acc 0.436 f1 0.348\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2143 acc 0.919 f1 0.920 || val_loss 1.8607 acc 0.418 f1 0.347\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1779 acc 0.930 f1 0.930 || val_loss 1.9499 acc 0.405 f1 0.327\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1656 acc 0.939 f1 0.939 || val_loss 2.0483 acc 0.428 f1 0.343\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1598 acc 0.945 f1 0.945 || val_loss 2.1338 acc 0.440 f1 0.363\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1308 acc 0.959 f1 0.959 || val_loss 2.2639 acc 0.428 f1 0.335\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1227 acc 0.958 f1 0.958 || val_loss 2.2479 acc 0.407 f1 0.338\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.1058 acc 0.964 f1 0.964 || val_loss 2.4214 acc 0.422 f1 0.364\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1017 acc 0.968 f1 0.968 || val_loss 2.4260 acc 0.434 f1 0.362\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.0844 acc 0.971 f1 0.971 || val_loss 2.4374 acc 0.434 f1 0.348\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.0769 acc 0.976 f1 0.976 || val_loss 2.6678 acc 0.418 f1 0.335\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.0732 acc 0.977 f1 0.977 || val_loss 2.7805 acc 0.430 f1 0.326\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.0861 acc 0.967 f1 0.967 || val_loss 2.5209 acc 0.438 f1 0.351\n",
            "[W3] CNN1D Epoch 27 | train_loss 0.0711 acc 0.977 f1 0.977 || val_loss 2.8118 acc 0.422 f1 0.349\n",
            "[W3] CNN1D Epoch 28 | train_loss 0.0573 acc 0.983 f1 0.983 || val_loss 2.7840 acc 0.424 f1 0.341\n",
            "[W3] CNN1D Epoch 29 | train_loss 0.0597 acc 0.980 f1 0.980 || val_loss 2.7723 acc 0.428 f1 0.344\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=97\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1004 acc 0.340 f1 0.269 || val_loss 1.0977 acc 0.362 f1 0.293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0847 acc 0.399 f1 0.397 || val_loss 1.1004 acc 0.356 f1 0.328\n",
            "[W3] RNN Epoch 03 | train_loss 1.0699 acc 0.426 f1 0.424 || val_loss 1.1049 acc 0.309 f1 0.291\n",
            "[W3] RNN Epoch 04 | train_loss 1.0560 acc 0.434 f1 0.431 || val_loss 1.0958 acc 0.335 f1 0.308\n",
            "[W3] RNN Epoch 05 | train_loss 1.0400 acc 0.463 f1 0.462 || val_loss 1.0904 acc 0.331 f1 0.300\n",
            "[W3] RNN Epoch 06 | train_loss 1.0199 acc 0.488 f1 0.485 || val_loss 1.0716 acc 0.358 f1 0.318\n",
            "[W3] RNN Epoch 07 | train_loss 0.9950 acc 0.511 f1 0.509 || val_loss 1.0857 acc 0.350 f1 0.310\n",
            "[W3] RNN Epoch 08 | train_loss 0.9706 acc 0.530 f1 0.524 || val_loss 1.1061 acc 0.333 f1 0.307\n",
            "[W3] RNN Epoch 09 | train_loss 0.9407 acc 0.549 f1 0.542 || val_loss 1.1069 acc 0.350 f1 0.322\n",
            "[W3] RNN Epoch 10 | train_loss 0.9186 acc 0.565 f1 0.558 || val_loss 1.1147 acc 0.337 f1 0.308\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=97\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1004 acc 0.322 f1 0.257 || val_loss 1.0800 acc 0.422 f1 0.290\n",
            "[W3] GRU Epoch 02 | train_loss 1.0907 acc 0.368 f1 0.325 || val_loss 1.0866 acc 0.379 f1 0.312\n",
            "[W3] GRU Epoch 03 | train_loss 1.0788 acc 0.417 f1 0.416 || val_loss 1.0869 acc 0.307 f1 0.275\n",
            "[W3] GRU Epoch 04 | train_loss 1.0590 acc 0.461 f1 0.459 || val_loss 1.0910 acc 0.307 f1 0.283\n",
            "[W3] GRU Epoch 05 | train_loss 1.0269 acc 0.486 f1 0.481 || val_loss 1.0873 acc 0.323 f1 0.295\n",
            "[W3] GRU Epoch 06 | train_loss 0.9786 acc 0.524 f1 0.515 || val_loss 1.0941 acc 0.329 f1 0.290\n",
            "[W3] GRU Epoch 07 | train_loss 0.9119 acc 0.567 f1 0.558 || val_loss 1.1122 acc 0.352 f1 0.295\n",
            "[W3] GRU Epoch 08 | train_loss 0.8464 acc 0.592 f1 0.584 || val_loss 1.1303 acc 0.374 f1 0.323\n",
            "[W3] GRU Epoch 09 | train_loss 0.7981 acc 0.615 f1 0.609 || val_loss 1.1194 acc 0.368 f1 0.290\n",
            "[W3] GRU Epoch 10 | train_loss 0.7472 acc 0.632 f1 0.627 || val_loss 1.1639 acc 0.360 f1 0.290\n",
            "[W3] GRU Epoch 11 | train_loss 0.7135 acc 0.656 f1 0.649 || val_loss 1.1886 acc 0.368 f1 0.296\n",
            "[W3] GRU Epoch 12 | train_loss 0.6639 acc 0.679 f1 0.674 || val_loss 1.2135 acc 0.366 f1 0.283\n",
            "[W3] GRU Epoch 13 | train_loss 0.6382 acc 0.683 f1 0.680 || val_loss 1.2593 acc 0.387 f1 0.309\n",
            "[W3] GRU Epoch 14 | train_loss 0.6145 acc 0.703 f1 0.700 || val_loss 1.2842 acc 0.364 f1 0.292\n",
            "[W3] GRU Epoch 15 | train_loss 0.5758 acc 0.719 f1 0.716 || val_loss 1.3220 acc 0.364 f1 0.288\n",
            "[W3] GRU Epoch 16 | train_loss 0.5471 acc 0.741 f1 0.738 || val_loss 1.3787 acc 0.381 f1 0.305\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=97\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0991 acc 0.344 f1 0.291 || val_loss 1.0985 acc 0.319 f1 0.303\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0936 acc 0.391 f1 0.385 || val_loss 1.0984 acc 0.319 f1 0.308\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0868 acc 0.415 f1 0.413 || val_loss 1.1022 acc 0.302 f1 0.294\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0733 acc 0.437 f1 0.428 || val_loss 1.0909 acc 0.317 f1 0.295\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0430 acc 0.464 f1 0.455 || val_loss 1.1097 acc 0.307 f1 0.284\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9715 acc 0.527 f1 0.517 || val_loss 1.1108 acc 0.325 f1 0.289\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8753 acc 0.576 f1 0.567 || val_loss 1.0947 acc 0.374 f1 0.306\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8075 acc 0.596 f1 0.589 || val_loss 1.0960 acc 0.385 f1 0.316\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7546 acc 0.631 f1 0.627 || val_loss 1.1231 acc 0.393 f1 0.314\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7097 acc 0.647 f1 0.642 || val_loss 1.1532 acc 0.387 f1 0.304\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6726 acc 0.672 f1 0.669 || val_loss 1.2245 acc 0.379 f1 0.315\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6419 acc 0.689 f1 0.685 || val_loss 1.2098 acc 0.399 f1 0.313\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6123 acc 0.698 f1 0.695 || val_loss 1.2244 acc 0.405 f1 0.327\n",
            "[W3] LSTM Epoch 14 | train_loss 0.5902 acc 0.717 f1 0.714 || val_loss 1.2425 acc 0.412 f1 0.327\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5655 acc 0.716 f1 0.713 || val_loss 1.2969 acc 0.399 f1 0.327\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5317 acc 0.739 f1 0.737 || val_loss 1.3393 acc 0.393 f1 0.317\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5099 acc 0.742 f1 0.740 || val_loss 1.4325 acc 0.385 f1 0.319\n",
            "[W3] LSTM Epoch 18 | train_loss 0.4915 acc 0.760 f1 0.758 || val_loss 1.4603 acc 0.405 f1 0.326\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4736 acc 0.767 f1 0.766 || val_loss 1.4709 acc 0.407 f1 0.334\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4518 acc 0.783 f1 0.783 || val_loss 1.5148 acc 0.416 f1 0.335\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4339 acc 0.797 f1 0.796 || val_loss 1.5817 acc 0.412 f1 0.333\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4185 acc 0.802 f1 0.802 || val_loss 1.5999 acc 0.405 f1 0.329\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4059 acc 0.807 f1 0.807 || val_loss 1.6242 acc 0.407 f1 0.334\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3857 acc 0.821 f1 0.821 || val_loss 1.6749 acc 0.430 f1 0.354\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3686 acc 0.829 f1 0.829 || val_loss 1.7103 acc 0.428 f1 0.353\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3497 acc 0.845 f1 0.845 || val_loss 1.7713 acc 0.428 f1 0.349\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3444 acc 0.849 f1 0.849 || val_loss 1.7995 acc 0.424 f1 0.341\n",
            "[W3] LSTM Epoch 28 | train_loss 0.3245 acc 0.857 f1 0.857 || val_loss 1.8900 acc 0.416 f1 0.347\n",
            "[W3] LSTM Epoch 29 | train_loss 0.3109 acc 0.863 f1 0.863 || val_loss 1.9333 acc 0.424 f1 0.352\n",
            "[W3] LSTM Epoch 30 | train_loss 0.2930 acc 0.873 f1 0.873 || val_loss 1.9597 acc 0.447 f1 0.371\n",
            "[W3] LSTM Epoch 31 | train_loss 0.2710 acc 0.881 f1 0.881 || val_loss 2.0194 acc 0.438 f1 0.369\n",
            "[W3] LSTM Epoch 32 | train_loss 0.2629 acc 0.886 f1 0.886 || val_loss 2.0943 acc 0.428 f1 0.364\n",
            "[W3] LSTM Epoch 33 | train_loss 0.2399 acc 0.905 f1 0.905 || val_loss 2.1301 acc 0.451 f1 0.378\n",
            "[W3] LSTM Epoch 34 | train_loss 0.2267 acc 0.908 f1 0.908 || val_loss 2.2084 acc 0.422 f1 0.355\n",
            "[W3] LSTM Epoch 35 | train_loss 0.2063 acc 0.919 f1 0.919 || val_loss 2.1999 acc 0.432 f1 0.355\n",
            "[W3] LSTM Epoch 36 | train_loss 0.1909 acc 0.928 f1 0.928 || val_loss 2.2442 acc 0.440 f1 0.366\n",
            "[W3] LSTM Epoch 37 | train_loss 0.1762 acc 0.933 f1 0.933 || val_loss 2.3468 acc 0.434 f1 0.357\n",
            "[W3] LSTM Epoch 38 | train_loss 0.1654 acc 0.941 f1 0.941 || val_loss 2.3653 acc 0.449 f1 0.369\n",
            "[W3] LSTM Epoch 39 | train_loss 0.1420 acc 0.950 f1 0.950 || val_loss 2.4939 acc 0.447 f1 0.371\n",
            "[W3] LSTM Epoch 40 | train_loss 0.1246 acc 0.955 f1 0.955 || val_loss 2.6098 acc 0.428 f1 0.357\n",
            "[W3] LSTM Epoch 41 | train_loss 0.1069 acc 0.970 f1 0.970 || val_loss 2.6856 acc 0.428 f1 0.356\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  97%|| 97/100 [46:24<01:29, 29.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=98 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=98\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1321 acc 0.391 f1 0.389 || val_loss 1.1549 acc 0.290 f1 0.274\n",
            "[W3] ANN Epoch 02 | train_loss 0.9661 acc 0.521 f1 0.512 || val_loss 1.1423 acc 0.342 f1 0.316\n",
            "[W3] ANN Epoch 03 | train_loss 0.8623 acc 0.581 f1 0.573 || val_loss 1.1260 acc 0.356 f1 0.307\n",
            "[W3] ANN Epoch 04 | train_loss 0.7997 acc 0.622 f1 0.616 || val_loss 1.1303 acc 0.379 f1 0.326\n",
            "[W3] ANN Epoch 05 | train_loss 0.7149 acc 0.663 f1 0.660 || val_loss 1.1264 acc 0.389 f1 0.335\n",
            "[W3] ANN Epoch 06 | train_loss 0.6865 acc 0.672 f1 0.669 || val_loss 1.1411 acc 0.399 f1 0.333\n",
            "[W3] ANN Epoch 07 | train_loss 0.6311 acc 0.694 f1 0.691 || val_loss 1.1422 acc 0.409 f1 0.329\n",
            "[W3] ANN Epoch 08 | train_loss 0.6077 acc 0.709 f1 0.709 || val_loss 1.1767 acc 0.405 f1 0.323\n",
            "[W3] ANN Epoch 09 | train_loss 0.5589 acc 0.735 f1 0.733 || val_loss 1.2446 acc 0.397 f1 0.323\n",
            "[W3] ANN Epoch 10 | train_loss 0.5625 acc 0.747 f1 0.747 || val_loss 1.2154 acc 0.426 f1 0.343\n",
            "[W3] ANN Epoch 11 | train_loss 0.5152 acc 0.760 f1 0.759 || val_loss 1.2638 acc 0.403 f1 0.326\n",
            "[W3] ANN Epoch 12 | train_loss 0.4931 acc 0.786 f1 0.785 || val_loss 1.2240 acc 0.430 f1 0.345\n",
            "[W3] ANN Epoch 13 | train_loss 0.4755 acc 0.788 f1 0.787 || val_loss 1.2528 acc 0.465 f1 0.385\n",
            "[W3] ANN Epoch 14 | train_loss 0.4519 acc 0.800 f1 0.799 || val_loss 1.3141 acc 0.444 f1 0.356\n",
            "[W3] ANN Epoch 15 | train_loss 0.4440 acc 0.804 f1 0.803 || val_loss 1.3058 acc 0.453 f1 0.377\n",
            "[W3] ANN Epoch 16 | train_loss 0.4445 acc 0.803 f1 0.803 || val_loss 1.3715 acc 0.428 f1 0.349\n",
            "[W3] ANN Epoch 17 | train_loss 0.4082 acc 0.816 f1 0.816 || val_loss 1.3453 acc 0.432 f1 0.346\n",
            "[W3] ANN Epoch 18 | train_loss 0.3955 acc 0.828 f1 0.828 || val_loss 1.4067 acc 0.444 f1 0.356\n",
            "[W3] ANN Epoch 19 | train_loss 0.3684 acc 0.849 f1 0.849 || val_loss 1.4414 acc 0.442 f1 0.360\n",
            "[W3] ANN Epoch 20 | train_loss 0.3646 acc 0.842 f1 0.842 || val_loss 1.4458 acc 0.436 f1 0.355\n",
            "[W3] ANN Epoch 21 | train_loss 0.3768 acc 0.842 f1 0.841 || val_loss 1.4596 acc 0.449 f1 0.364\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=98\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0833 acc 0.399 f1 0.392 || val_loss 1.0349 acc 0.455 f1 0.367\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9673 acc 0.533 f1 0.531 || val_loss 1.0132 acc 0.447 f1 0.376\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8192 acc 0.624 f1 0.619 || val_loss 1.0963 acc 0.379 f1 0.319\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7216 acc 0.661 f1 0.659 || val_loss 1.1004 acc 0.393 f1 0.331\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6326 acc 0.702 f1 0.701 || val_loss 1.1592 acc 0.422 f1 0.342\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5732 acc 0.747 f1 0.746 || val_loss 1.2160 acc 0.393 f1 0.330\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5061 acc 0.766 f1 0.766 || val_loss 1.2577 acc 0.401 f1 0.347\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4522 acc 0.810 f1 0.810 || val_loss 1.3712 acc 0.407 f1 0.342\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.3985 acc 0.828 f1 0.827 || val_loss 1.4158 acc 0.418 f1 0.333\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3568 acc 0.846 f1 0.846 || val_loss 1.5010 acc 0.383 f1 0.321\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=98\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0992 acc 0.332 f1 0.288 || val_loss 1.0866 acc 0.414 f1 0.320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0840 acc 0.417 f1 0.415 || val_loss 1.0951 acc 0.350 f1 0.313\n",
            "[W3] RNN Epoch 03 | train_loss 1.0674 acc 0.448 f1 0.446 || val_loss 1.0951 acc 0.344 f1 0.303\n",
            "[W3] RNN Epoch 04 | train_loss 1.0501 acc 0.464 f1 0.459 || val_loss 1.1030 acc 0.311 f1 0.291\n",
            "[W3] RNN Epoch 05 | train_loss 1.0264 acc 0.493 f1 0.490 || val_loss 1.0887 acc 0.350 f1 0.315\n",
            "[W3] RNN Epoch 06 | train_loss 1.0031 acc 0.503 f1 0.495 || val_loss 1.0818 acc 0.379 f1 0.331\n",
            "[W3] RNN Epoch 07 | train_loss 0.9745 acc 0.526 f1 0.519 || val_loss 1.0802 acc 0.370 f1 0.330\n",
            "[W3] RNN Epoch 08 | train_loss 0.9450 acc 0.546 f1 0.539 || val_loss 1.0783 acc 0.370 f1 0.328\n",
            "[W3] RNN Epoch 09 | train_loss 0.9142 acc 0.564 f1 0.553 || val_loss 1.0717 acc 0.360 f1 0.312\n",
            "[W3] RNN Epoch 10 | train_loss 0.8905 acc 0.578 f1 0.570 || val_loss 1.0575 acc 0.424 f1 0.354\n",
            "[W3] RNN Epoch 11 | train_loss 0.8618 acc 0.591 f1 0.584 || val_loss 1.0896 acc 0.389 f1 0.344\n",
            "[W3] RNN Epoch 12 | train_loss 0.8419 acc 0.604 f1 0.595 || val_loss 1.0924 acc 0.399 f1 0.345\n",
            "[W3] RNN Epoch 13 | train_loss 0.8159 acc 0.624 f1 0.616 || val_loss 1.1213 acc 0.368 f1 0.325\n",
            "[W3] RNN Epoch 14 | train_loss 0.7875 acc 0.629 f1 0.620 || val_loss 1.1273 acc 0.370 f1 0.328\n",
            "[W3] RNN Epoch 15 | train_loss 0.7702 acc 0.635 f1 0.626 || val_loss 1.1397 acc 0.379 f1 0.333\n",
            "[W3] RNN Epoch 16 | train_loss 0.7486 acc 0.647 f1 0.639 || val_loss 1.1326 acc 0.385 f1 0.331\n",
            "[W3] RNN Epoch 17 | train_loss 0.7340 acc 0.652 f1 0.645 || val_loss 1.1431 acc 0.377 f1 0.331\n",
            "[W3] RNN Epoch 18 | train_loss 0.6991 acc 0.678 f1 0.671 || val_loss 1.1499 acc 0.358 f1 0.305\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=98\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.0995 acc 0.337 f1 0.256 || val_loss 1.0980 acc 0.348 f1 0.266\n",
            "[W3] GRU Epoch 02 | train_loss 1.0919 acc 0.389 f1 0.368 || val_loss 1.0981 acc 0.323 f1 0.293\n",
            "[W3] GRU Epoch 03 | train_loss 1.0825 acc 0.414 f1 0.404 || val_loss 1.0980 acc 0.300 f1 0.291\n",
            "[W3] GRU Epoch 04 | train_loss 1.0662 acc 0.439 f1 0.434 || val_loss 1.0819 acc 0.327 f1 0.305\n",
            "[W3] GRU Epoch 05 | train_loss 1.0395 acc 0.476 f1 0.467 || val_loss 1.0702 acc 0.354 f1 0.320\n",
            "[W3] GRU Epoch 06 | train_loss 0.9899 acc 0.510 f1 0.502 || val_loss 1.1023 acc 0.325 f1 0.303\n",
            "[W3] GRU Epoch 07 | train_loss 0.9104 acc 0.565 f1 0.557 || val_loss 1.1106 acc 0.352 f1 0.314\n",
            "[W3] GRU Epoch 08 | train_loss 0.8470 acc 0.582 f1 0.575 || val_loss 1.1483 acc 0.342 f1 0.305\n",
            "[W3] GRU Epoch 09 | train_loss 0.7856 acc 0.610 f1 0.605 || val_loss 1.1864 acc 0.356 f1 0.320\n",
            "[W3] GRU Epoch 10 | train_loss 0.7453 acc 0.644 f1 0.639 || val_loss 1.1795 acc 0.377 f1 0.319\n",
            "[W3] GRU Epoch 11 | train_loss 0.7015 acc 0.660 f1 0.656 || val_loss 1.2201 acc 0.374 f1 0.325\n",
            "[W3] GRU Epoch 12 | train_loss 0.6758 acc 0.663 f1 0.659 || val_loss 1.2212 acc 0.397 f1 0.342\n",
            "[W3] GRU Epoch 13 | train_loss 0.6345 acc 0.690 f1 0.687 || val_loss 1.2355 acc 0.383 f1 0.316\n",
            "[W3] GRU Epoch 14 | train_loss 0.6074 acc 0.705 f1 0.702 || val_loss 1.2461 acc 0.409 f1 0.331\n",
            "[W3] GRU Epoch 15 | train_loss 0.5751 acc 0.718 f1 0.716 || val_loss 1.2749 acc 0.412 f1 0.336\n",
            "[W3] GRU Epoch 16 | train_loss 0.5512 acc 0.733 f1 0.731 || val_loss 1.3103 acc 0.399 f1 0.323\n",
            "[W3] GRU Epoch 17 | train_loss 0.5321 acc 0.734 f1 0.732 || val_loss 1.3710 acc 0.387 f1 0.324\n",
            "[W3] GRU Epoch 18 | train_loss 0.5078 acc 0.756 f1 0.754 || val_loss 1.4202 acc 0.399 f1 0.332\n",
            "[W3] GRU Epoch 19 | train_loss 0.4806 acc 0.771 f1 0.770 || val_loss 1.4157 acc 0.412 f1 0.337\n",
            "[W3] GRU Epoch 20 | train_loss 0.4615 acc 0.786 f1 0.785 || val_loss 1.5144 acc 0.397 f1 0.331\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=98\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0984 acc 0.343 f1 0.216 || val_loss 1.0937 acc 0.403 f1 0.308\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0929 acc 0.397 f1 0.397 || val_loss 1.0931 acc 0.348 f1 0.315\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0857 acc 0.418 f1 0.409 || val_loss 1.0866 acc 0.342 f1 0.319\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0657 acc 0.448 f1 0.443 || val_loss 1.0884 acc 0.300 f1 0.291\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0319 acc 0.472 f1 0.458 || val_loss 1.1127 acc 0.286 f1 0.273\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9590 acc 0.529 f1 0.513 || val_loss 1.0858 acc 0.323 f1 0.288\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8797 acc 0.571 f1 0.563 || val_loss 1.1039 acc 0.346 f1 0.283\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8174 acc 0.591 f1 0.584 || val_loss 1.1524 acc 0.360 f1 0.306\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7713 acc 0.623 f1 0.618 || val_loss 1.1458 acc 0.397 f1 0.323\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7386 acc 0.636 f1 0.632 || val_loss 1.1294 acc 0.407 f1 0.318\n",
            "[W3] LSTM Epoch 11 | train_loss 0.6998 acc 0.659 f1 0.654 || val_loss 1.1780 acc 0.412 f1 0.317\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6727 acc 0.669 f1 0.664 || val_loss 1.2170 acc 0.407 f1 0.325\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6416 acc 0.680 f1 0.677 || val_loss 1.2439 acc 0.393 f1 0.315\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6034 acc 0.697 f1 0.694 || val_loss 1.2726 acc 0.385 f1 0.309\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5806 acc 0.719 f1 0.717 || val_loss 1.2931 acc 0.387 f1 0.300\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5515 acc 0.738 f1 0.735 || val_loss 1.3237 acc 0.405 f1 0.304\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5234 acc 0.748 f1 0.747 || val_loss 1.3998 acc 0.383 f1 0.307\n",
            "[W3] LSTM Epoch 18 | train_loss 0.4941 acc 0.772 f1 0.771 || val_loss 1.4176 acc 0.407 f1 0.329\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4704 acc 0.778 f1 0.777 || val_loss 1.4903 acc 0.383 f1 0.313\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4494 acc 0.788 f1 0.788 || val_loss 1.5260 acc 0.414 f1 0.332\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4287 acc 0.798 f1 0.797 || val_loss 1.5737 acc 0.409 f1 0.330\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4061 acc 0.815 f1 0.815 || val_loss 1.6340 acc 0.403 f1 0.330\n",
            "[W3] LSTM Epoch 23 | train_loss 0.3859 acc 0.836 f1 0.836 || val_loss 1.7258 acc 0.393 f1 0.324\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3681 acc 0.831 f1 0.831 || val_loss 1.7550 acc 0.393 f1 0.317\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3522 acc 0.849 f1 0.849 || val_loss 1.8085 acc 0.391 f1 0.317\n",
            "[W3] LSTM Epoch 26 | train_loss 0.3312 acc 0.855 f1 0.855 || val_loss 1.8618 acc 0.407 f1 0.328\n",
            "[W3] LSTM Epoch 27 | train_loss 0.3137 acc 0.862 f1 0.862 || val_loss 1.9293 acc 0.409 f1 0.330\n",
            "[W3] LSTM Epoch 28 | train_loss 0.2914 acc 0.882 f1 0.882 || val_loss 2.0199 acc 0.407 f1 0.324\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  98%|| 98/100 [46:53<00:58, 29.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=99 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=99\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1377 acc 0.383 f1 0.384 || val_loss 1.1086 acc 0.358 f1 0.320\n",
            "[W3] ANN Epoch 02 | train_loss 0.9682 acc 0.515 f1 0.511 || val_loss 1.0969 acc 0.407 f1 0.361\n",
            "[W3] ANN Epoch 03 | train_loss 0.8644 acc 0.592 f1 0.588 || val_loss 1.0984 acc 0.412 f1 0.352\n",
            "[W3] ANN Epoch 04 | train_loss 0.7950 acc 0.617 f1 0.613 || val_loss 1.1138 acc 0.393 f1 0.326\n",
            "[W3] ANN Epoch 05 | train_loss 0.7201 acc 0.661 f1 0.659 || val_loss 1.1105 acc 0.409 f1 0.343\n",
            "[W3] ANN Epoch 06 | train_loss 0.6529 acc 0.706 f1 0.705 || val_loss 1.1334 acc 0.391 f1 0.315\n",
            "[W3] ANN Epoch 07 | train_loss 0.6175 acc 0.719 f1 0.718 || val_loss 1.1546 acc 0.416 f1 0.345\n",
            "[W3] ANN Epoch 08 | train_loss 0.5957 acc 0.729 f1 0.726 || val_loss 1.1329 acc 0.428 f1 0.348\n",
            "[W3] ANN Epoch 09 | train_loss 0.5805 acc 0.734 f1 0.734 || val_loss 1.1572 acc 0.428 f1 0.344\n",
            "[W3] ANN Epoch 10 | train_loss 0.5331 acc 0.761 f1 0.761 || val_loss 1.2102 acc 0.401 f1 0.324\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=99\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0702 acc 0.400 f1 0.401 || val_loss 1.0083 acc 0.407 f1 0.309\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9295 acc 0.540 f1 0.535 || val_loss 1.0328 acc 0.405 f1 0.297\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.7984 acc 0.596 f1 0.592 || val_loss 1.1185 acc 0.391 f1 0.325\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.6962 acc 0.660 f1 0.658 || val_loss 1.1723 acc 0.370 f1 0.310\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6402 acc 0.689 f1 0.686 || val_loss 1.2311 acc 0.364 f1 0.311\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5748 acc 0.718 f1 0.715 || val_loss 1.2843 acc 0.379 f1 0.308\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5129 acc 0.755 f1 0.753 || val_loss 1.3754 acc 0.399 f1 0.317\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4708 acc 0.782 f1 0.781 || val_loss 1.4504 acc 0.379 f1 0.309\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.4382 acc 0.795 f1 0.795 || val_loss 1.4818 acc 0.383 f1 0.310\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3927 acc 0.823 f1 0.823 || val_loss 1.6553 acc 0.399 f1 0.327\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.3561 acc 0.844 f1 0.844 || val_loss 1.7023 acc 0.403 f1 0.329\n",
            "[W3] CNN1D Epoch 12 | train_loss 0.3205 acc 0.866 f1 0.866 || val_loss 1.8150 acc 0.387 f1 0.303\n",
            "[W3] CNN1D Epoch 13 | train_loss 0.2866 acc 0.887 f1 0.887 || val_loss 1.9532 acc 0.377 f1 0.314\n",
            "[W3] CNN1D Epoch 14 | train_loss 0.2631 acc 0.894 f1 0.894 || val_loss 1.9721 acc 0.430 f1 0.340\n",
            "[W3] CNN1D Epoch 15 | train_loss 0.2241 acc 0.915 f1 0.915 || val_loss 2.0560 acc 0.391 f1 0.303\n",
            "[W3] CNN1D Epoch 16 | train_loss 0.1964 acc 0.928 f1 0.928 || val_loss 2.1931 acc 0.416 f1 0.324\n",
            "[W3] CNN1D Epoch 17 | train_loss 0.1615 acc 0.947 f1 0.947 || val_loss 2.3593 acc 0.397 f1 0.309\n",
            "[W3] CNN1D Epoch 18 | train_loss 0.1487 acc 0.951 f1 0.951 || val_loss 2.4181 acc 0.416 f1 0.310\n",
            "[W3] CNN1D Epoch 19 | train_loss 0.1352 acc 0.951 f1 0.951 || val_loss 2.4694 acc 0.434 f1 0.348\n",
            "[W3] CNN1D Epoch 20 | train_loss 0.1150 acc 0.962 f1 0.962 || val_loss 2.5580 acc 0.430 f1 0.337\n",
            "[W3] CNN1D Epoch 21 | train_loss 0.0947 acc 0.967 f1 0.967 || val_loss 2.6659 acc 0.397 f1 0.312\n",
            "[W3] CNN1D Epoch 22 | train_loss 0.1058 acc 0.965 f1 0.965 || val_loss 2.7749 acc 0.395 f1 0.296\n",
            "[W3] CNN1D Epoch 23 | train_loss 0.0945 acc 0.968 f1 0.968 || val_loss 2.7033 acc 0.424 f1 0.312\n",
            "[W3] CNN1D Epoch 24 | train_loss 0.0687 acc 0.978 f1 0.978 || val_loss 2.9228 acc 0.412 f1 0.312\n",
            "[W3] CNN1D Epoch 25 | train_loss 0.0754 acc 0.974 f1 0.974 || val_loss 2.8911 acc 0.409 f1 0.315\n",
            "[W3] CNN1D Epoch 26 | train_loss 0.0689 acc 0.980 f1 0.980 || val_loss 3.0741 acc 0.409 f1 0.312\n",
            "[W3] CNN1D Epoch 27 | train_loss 0.0735 acc 0.976 f1 0.976 || val_loss 3.1889 acc 0.426 f1 0.308\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=99\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.0995 acc 0.346 f1 0.318 || val_loss 1.0932 acc 0.337 f1 0.301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0821 acc 0.395 f1 0.382 || val_loss 1.0884 acc 0.333 f1 0.301\n",
            "[W3] RNN Epoch 03 | train_loss 1.0679 acc 0.428 f1 0.422 || val_loss 1.1145 acc 0.298 f1 0.285\n",
            "[W3] RNN Epoch 04 | train_loss 1.0569 acc 0.437 f1 0.424 || val_loss 1.0895 acc 0.317 f1 0.281\n",
            "[W3] RNN Epoch 05 | train_loss 1.0404 acc 0.456 f1 0.453 || val_loss 1.0835 acc 0.327 f1 0.293\n",
            "[W3] RNN Epoch 06 | train_loss 1.0189 acc 0.486 f1 0.479 || val_loss 1.0910 acc 0.337 f1 0.310\n",
            "[W3] RNN Epoch 07 | train_loss 0.9965 acc 0.506 f1 0.502 || val_loss 1.0959 acc 0.333 f1 0.307\n",
            "[W3] RNN Epoch 08 | train_loss 0.9741 acc 0.520 f1 0.512 || val_loss 1.0962 acc 0.337 f1 0.300\n",
            "[W3] RNN Epoch 09 | train_loss 0.9398 acc 0.555 f1 0.546 || val_loss 1.0959 acc 0.342 f1 0.306\n",
            "[W3] RNN Epoch 10 | train_loss 0.9120 acc 0.569 f1 0.561 || val_loss 1.0960 acc 0.352 f1 0.313\n",
            "[W3] RNN Epoch 11 | train_loss 0.8925 acc 0.585 f1 0.578 || val_loss 1.0865 acc 0.383 f1 0.336\n",
            "[W3] RNN Epoch 12 | train_loss 0.8627 acc 0.599 f1 0.591 || val_loss 1.1089 acc 0.383 f1 0.336\n",
            "[W3] RNN Epoch 13 | train_loss 0.8436 acc 0.600 f1 0.592 || val_loss 1.0863 acc 0.393 f1 0.337\n",
            "[W3] RNN Epoch 14 | train_loss 0.8047 acc 0.625 f1 0.616 || val_loss 1.1223 acc 0.372 f1 0.335\n",
            "[W3] RNN Epoch 15 | train_loss 0.7944 acc 0.623 f1 0.616 || val_loss 1.1048 acc 0.403 f1 0.352\n",
            "[W3] RNN Epoch 16 | train_loss 0.7490 acc 0.643 f1 0.636 || val_loss 1.1403 acc 0.387 f1 0.341\n",
            "[W3] RNN Epoch 17 | train_loss 0.7268 acc 0.663 f1 0.656 || val_loss 1.1447 acc 0.391 f1 0.333\n",
            "[W3] RNN Epoch 18 | train_loss 0.7157 acc 0.660 f1 0.653 || val_loss 1.1453 acc 0.397 f1 0.351\n",
            "[W3] RNN Epoch 19 | train_loss 0.6847 acc 0.675 f1 0.668 || val_loss 1.1622 acc 0.395 f1 0.346\n",
            "[W3] RNN Epoch 20 | train_loss 0.6615 acc 0.685 f1 0.680 || val_loss 1.1713 acc 0.401 f1 0.324\n",
            "[W3] RNN Epoch 21 | train_loss 0.6313 acc 0.704 f1 0.699 || val_loss 1.1853 acc 0.385 f1 0.326\n",
            "[W3] RNN Epoch 22 | train_loss 0.6146 acc 0.708 f1 0.703 || val_loss 1.2120 acc 0.381 f1 0.305\n",
            "[W3] RNN Epoch 23 | train_loss 0.6032 acc 0.718 f1 0.712 || val_loss 1.2160 acc 0.399 f1 0.331\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=99\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1021 acc 0.336 f1 0.196 || val_loss 1.1121 acc 0.218 f1 0.196\n",
            "[W3] GRU Epoch 02 | train_loss 1.0903 acc 0.403 f1 0.388 || val_loss 1.0921 acc 0.381 f1 0.355\n",
            "[W3] GRU Epoch 03 | train_loss 1.0791 acc 0.428 f1 0.419 || val_loss 1.0741 acc 0.401 f1 0.349\n",
            "[W3] GRU Epoch 04 | train_loss 1.0615 acc 0.443 f1 0.443 || val_loss 1.0731 acc 0.366 f1 0.336\n",
            "[W3] GRU Epoch 05 | train_loss 1.0391 acc 0.473 f1 0.470 || val_loss 1.0685 acc 0.350 f1 0.314\n",
            "[W3] GRU Epoch 06 | train_loss 1.0057 acc 0.507 f1 0.499 || val_loss 1.0512 acc 0.385 f1 0.342\n",
            "[W3] GRU Epoch 07 | train_loss 0.9567 acc 0.537 f1 0.526 || val_loss 1.0684 acc 0.354 f1 0.322\n",
            "[W3] GRU Epoch 08 | train_loss 0.8837 acc 0.585 f1 0.576 || val_loss 1.1065 acc 0.360 f1 0.330\n",
            "[W3] GRU Epoch 09 | train_loss 0.8150 acc 0.604 f1 0.596 || val_loss 1.0751 acc 0.374 f1 0.302\n",
            "[W3] GRU Epoch 10 | train_loss 0.7599 acc 0.634 f1 0.629 || val_loss 1.1483 acc 0.379 f1 0.338\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=99\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.0989 acc 0.348 f1 0.332 || val_loss 1.1022 acc 0.290 f1 0.285\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0936 acc 0.389 f1 0.376 || val_loss 1.1041 acc 0.276 f1 0.272\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0854 acc 0.400 f1 0.392 || val_loss 1.1001 acc 0.300 f1 0.290\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0676 acc 0.427 f1 0.409 || val_loss 1.0975 acc 0.317 f1 0.309\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0340 acc 0.479 f1 0.468 || val_loss 1.0742 acc 0.329 f1 0.297\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9672 acc 0.531 f1 0.521 || val_loss 1.0893 acc 0.348 f1 0.283\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8670 acc 0.582 f1 0.576 || val_loss 1.0918 acc 0.393 f1 0.314\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8101 acc 0.604 f1 0.599 || val_loss 1.1062 acc 0.401 f1 0.336\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7662 acc 0.624 f1 0.618 || val_loss 1.1234 acc 0.414 f1 0.334\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7313 acc 0.655 f1 0.652 || val_loss 1.1358 acc 0.418 f1 0.336\n",
            "[W3] LSTM Epoch 11 | train_loss 0.7060 acc 0.657 f1 0.653 || val_loss 1.1535 acc 0.418 f1 0.345\n",
            "[W3] LSTM Epoch 12 | train_loss 0.6662 acc 0.681 f1 0.678 || val_loss 1.1932 acc 0.412 f1 0.333\n",
            "[W3] LSTM Epoch 13 | train_loss 0.6442 acc 0.687 f1 0.684 || val_loss 1.2059 acc 0.416 f1 0.335\n",
            "[W3] LSTM Epoch 14 | train_loss 0.6157 acc 0.703 f1 0.701 || val_loss 1.2431 acc 0.407 f1 0.329\n",
            "[W3] LSTM Epoch 15 | train_loss 0.5899 acc 0.710 f1 0.709 || val_loss 1.2802 acc 0.395 f1 0.318\n",
            "[W3] LSTM Epoch 16 | train_loss 0.5611 acc 0.729 f1 0.728 || val_loss 1.3462 acc 0.414 f1 0.343\n",
            "[W3] LSTM Epoch 17 | train_loss 0.5430 acc 0.730 f1 0.729 || val_loss 1.3511 acc 0.426 f1 0.348\n",
            "[W3] LSTM Epoch 18 | train_loss 0.5191 acc 0.748 f1 0.747 || val_loss 1.3797 acc 0.418 f1 0.335\n",
            "[W3] LSTM Epoch 19 | train_loss 0.4901 acc 0.772 f1 0.771 || val_loss 1.4128 acc 0.428 f1 0.338\n",
            "[W3] LSTM Epoch 20 | train_loss 0.4726 acc 0.781 f1 0.780 || val_loss 1.4832 acc 0.424 f1 0.340\n",
            "[W3] LSTM Epoch 21 | train_loss 0.4501 acc 0.786 f1 0.786 || val_loss 1.5141 acc 0.414 f1 0.338\n",
            "[W3] LSTM Epoch 22 | train_loss 0.4374 acc 0.799 f1 0.799 || val_loss 1.5734 acc 0.399 f1 0.324\n",
            "[W3] LSTM Epoch 23 | train_loss 0.4138 acc 0.804 f1 0.804 || val_loss 1.6286 acc 0.395 f1 0.315\n",
            "[W3] LSTM Epoch 24 | train_loss 0.3980 acc 0.818 f1 0.818 || val_loss 1.6850 acc 0.401 f1 0.325\n",
            "[W3] LSTM Epoch 25 | train_loss 0.3841 acc 0.825 f1 0.825 || val_loss 1.7262 acc 0.389 f1 0.316\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLOSO participants:  99%|| 99/100 [47:23<00:29, 29.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSO: Test subject=100 | Train subjects=81 | Val subjects=18\n",
            "============================================================\n",
            "Running Window=3 Model=ANN Test subject=100\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] ANN Epoch 01 | train_loss 1.1462 acc 0.403 f1 0.400 || val_loss 1.1805 acc 0.272 f1 0.268\n",
            "[W3] ANN Epoch 02 | train_loss 0.9741 acc 0.524 f1 0.515 || val_loss 1.1796 acc 0.286 f1 0.275\n",
            "[W3] ANN Epoch 03 | train_loss 0.8840 acc 0.571 f1 0.561 || val_loss 1.1488 acc 0.337 f1 0.303\n",
            "[W3] ANN Epoch 04 | train_loss 0.8067 acc 0.614 f1 0.606 || val_loss 1.1405 acc 0.348 f1 0.303\n",
            "[W3] ANN Epoch 05 | train_loss 0.7367 acc 0.650 f1 0.645 || val_loss 1.1048 acc 0.370 f1 0.317\n",
            "[W3] ANN Epoch 06 | train_loss 0.6878 acc 0.679 f1 0.676 || val_loss 1.1153 acc 0.395 f1 0.324\n",
            "[W3] ANN Epoch 07 | train_loss 0.6309 acc 0.694 f1 0.691 || val_loss 1.1633 acc 0.422 f1 0.357\n",
            "[W3] ANN Epoch 08 | train_loss 0.5890 acc 0.725 f1 0.723 || val_loss 1.1715 acc 0.424 f1 0.349\n",
            "[W3] ANN Epoch 09 | train_loss 0.5795 acc 0.723 f1 0.722 || val_loss 1.2048 acc 0.422 f1 0.339\n",
            "[W3] ANN Epoch 10 | train_loss 0.5504 acc 0.741 f1 0.740 || val_loss 1.1962 acc 0.436 f1 0.359\n",
            "[W3] ANN Epoch 11 | train_loss 0.5384 acc 0.747 f1 0.745 || val_loss 1.2236 acc 0.414 f1 0.336\n",
            "[W3] ANN Epoch 12 | train_loss 0.4860 acc 0.787 f1 0.786 || val_loss 1.2414 acc 0.451 f1 0.375\n",
            "[W3] ANN Epoch 13 | train_loss 0.4529 acc 0.806 f1 0.805 || val_loss 1.3328 acc 0.391 f1 0.328\n",
            "[W3] ANN Epoch 14 | train_loss 0.4650 acc 0.803 f1 0.803 || val_loss 1.3038 acc 0.434 f1 0.370\n",
            "[W3] ANN Epoch 15 | train_loss 0.4275 acc 0.818 f1 0.817 || val_loss 1.3316 acc 0.432 f1 0.366\n",
            "[W3] ANN Epoch 16 | train_loss 0.4415 acc 0.808 f1 0.808 || val_loss 1.3029 acc 0.432 f1 0.357\n",
            "[W3] ANN Epoch 17 | train_loss 0.4053 acc 0.826 f1 0.826 || val_loss 1.3363 acc 0.457 f1 0.385\n",
            "[W3] ANN Epoch 18 | train_loss 0.3772 acc 0.844 f1 0.844 || val_loss 1.3687 acc 0.449 f1 0.383\n",
            "[W3] ANN Epoch 19 | train_loss 0.3702 acc 0.848 f1 0.848 || val_loss 1.4300 acc 0.430 f1 0.367\n",
            "[W3] ANN Epoch 20 | train_loss 0.3641 acc 0.851 f1 0.850 || val_loss 1.4469 acc 0.442 f1 0.374\n",
            "[W3] ANN Epoch 21 | train_loss 0.3707 acc 0.848 f1 0.848 || val_loss 1.4077 acc 0.449 f1 0.374\n",
            "[W3] ANN Epoch 22 | train_loss 0.3459 acc 0.861 f1 0.860 || val_loss 1.4781 acc 0.438 f1 0.364\n",
            "[W3] ANN Epoch 23 | train_loss 0.3124 acc 0.873 f1 0.872 || val_loss 1.5134 acc 0.447 f1 0.365\n",
            "[W3] ANN Epoch 24 | train_loss 0.2998 acc 0.880 f1 0.880 || val_loss 1.5747 acc 0.430 f1 0.353\n",
            "[W3] ANN Epoch 25 | train_loss 0.3212 acc 0.868 f1 0.868 || val_loss 1.6035 acc 0.424 f1 0.355\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=CNN1D Test subject=100\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] CNN1D Epoch 01 | train_loss 1.0657 acc 0.420 f1 0.420 || val_loss 1.0177 acc 0.420 f1 0.312\n",
            "[W3] CNN1D Epoch 02 | train_loss 0.9172 acc 0.558 f1 0.554 || val_loss 1.0413 acc 0.409 f1 0.326\n",
            "[W3] CNN1D Epoch 03 | train_loss 0.8010 acc 0.611 f1 0.609 || val_loss 1.1089 acc 0.416 f1 0.346\n",
            "[W3] CNN1D Epoch 04 | train_loss 0.7105 acc 0.672 f1 0.670 || val_loss 1.1495 acc 0.412 f1 0.323\n",
            "[W3] CNN1D Epoch 05 | train_loss 0.6262 acc 0.705 f1 0.703 || val_loss 1.2473 acc 0.377 f1 0.310\n",
            "[W3] CNN1D Epoch 06 | train_loss 0.5604 acc 0.742 f1 0.742 || val_loss 1.2843 acc 0.403 f1 0.330\n",
            "[W3] CNN1D Epoch 07 | train_loss 0.5006 acc 0.771 f1 0.770 || val_loss 1.3577 acc 0.412 f1 0.327\n",
            "[W3] CNN1D Epoch 08 | train_loss 0.4390 acc 0.814 f1 0.814 || val_loss 1.4175 acc 0.393 f1 0.318\n",
            "[W3] CNN1D Epoch 09 | train_loss 0.3933 acc 0.835 f1 0.834 || val_loss 1.5552 acc 0.409 f1 0.316\n",
            "[W3] CNN1D Epoch 10 | train_loss 0.3385 acc 0.867 f1 0.867 || val_loss 1.6268 acc 0.387 f1 0.309\n",
            "[W3] CNN1D Epoch 11 | train_loss 0.2960 acc 0.882 f1 0.882 || val_loss 1.7078 acc 0.407 f1 0.333\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=RNN Test subject=100\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n",
            "[W3] RNN Epoch 01 | train_loss 1.1010 acc 0.335 f1 0.291 || val_loss 1.0869 acc 0.420 f1 0.366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] RNN Epoch 02 | train_loss 1.0845 acc 0.406 f1 0.398 || val_loss 1.1022 acc 0.321 f1 0.308\n",
            "[W3] RNN Epoch 03 | train_loss 1.0693 acc 0.436 f1 0.431 || val_loss 1.1129 acc 0.307 f1 0.295\n",
            "[W3] RNN Epoch 04 | train_loss 1.0533 acc 0.443 f1 0.435 || val_loss 1.0958 acc 0.329 f1 0.308\n",
            "[W3] RNN Epoch 05 | train_loss 1.0331 acc 0.474 f1 0.471 || val_loss 1.0866 acc 0.358 f1 0.324\n",
            "[W3] RNN Epoch 06 | train_loss 1.0123 acc 0.489 f1 0.487 || val_loss 1.0871 acc 0.356 f1 0.319\n",
            "[W3] RNN Epoch 07 | train_loss 0.9830 acc 0.515 f1 0.508 || val_loss 1.1070 acc 0.335 f1 0.314\n",
            "[W3] RNN Epoch 08 | train_loss 0.9586 acc 0.529 f1 0.522 || val_loss 1.0814 acc 0.364 f1 0.323\n",
            "[W3] RNN Epoch 09 | train_loss 0.9336 acc 0.558 f1 0.551 || val_loss 1.1083 acc 0.342 f1 0.313\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=GRU Test subject=100\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] GRU Epoch 01 | train_loss 1.1031 acc 0.342 f1 0.227 || val_loss 1.1106 acc 0.276 f1 0.261\n",
            "[W3] GRU Epoch 02 | train_loss 1.0899 acc 0.401 f1 0.398 || val_loss 1.0961 acc 0.335 f1 0.313\n",
            "[W3] GRU Epoch 03 | train_loss 1.0791 acc 0.421 f1 0.416 || val_loss 1.0973 acc 0.335 f1 0.318\n",
            "[W3] GRU Epoch 04 | train_loss 1.0630 acc 0.445 f1 0.443 || val_loss 1.1047 acc 0.319 f1 0.307\n",
            "[W3] GRU Epoch 05 | train_loss 1.0327 acc 0.476 f1 0.470 || val_loss 1.1036 acc 0.321 f1 0.306\n",
            "[W3] GRU Epoch 06 | train_loss 0.9900 acc 0.519 f1 0.509 || val_loss 1.0642 acc 0.377 f1 0.318\n",
            "[W3] GRU Epoch 07 | train_loss 0.9253 acc 0.561 f1 0.555 || val_loss 1.1260 acc 0.319 f1 0.300\n",
            "[W3] GRU Epoch 08 | train_loss 0.8496 acc 0.593 f1 0.587 || val_loss 1.1309 acc 0.352 f1 0.316\n",
            "[W3] GRU Epoch 09 | train_loss 0.7919 acc 0.613 f1 0.606 || val_loss 1.1173 acc 0.385 f1 0.326\n",
            "[W3] GRU Epoch 10 | train_loss 0.7456 acc 0.644 f1 0.639 || val_loss 1.1220 acc 0.403 f1 0.349\n",
            "[W3] GRU Epoch 11 | train_loss 0.7124 acc 0.659 f1 0.655 || val_loss 1.1791 acc 0.401 f1 0.348\n",
            "[W3] GRU Epoch 12 | train_loss 0.6729 acc 0.673 f1 0.669 || val_loss 1.1488 acc 0.420 f1 0.347\n",
            "[W3] GRU Epoch 13 | train_loss 0.6443 acc 0.691 f1 0.688 || val_loss 1.1861 acc 0.401 f1 0.337\n",
            "[W3] GRU Epoch 14 | train_loss 0.6104 acc 0.710 f1 0.707 || val_loss 1.2051 acc 0.414 f1 0.346\n",
            "[W3] GRU Epoch 15 | train_loss 0.5909 acc 0.712 f1 0.710 || val_loss 1.2189 acc 0.426 f1 0.352\n",
            "[W3] GRU Epoch 16 | train_loss 0.5539 acc 0.738 f1 0.736 || val_loss 1.2756 acc 0.424 f1 0.340\n",
            "[W3] GRU Epoch 17 | train_loss 0.5415 acc 0.743 f1 0.741 || val_loss 1.2775 acc 0.432 f1 0.351\n",
            "[W3] GRU Epoch 18 | train_loss 0.5136 acc 0.748 f1 0.747 || val_loss 1.3020 acc 0.436 f1 0.355\n",
            "[W3] GRU Epoch 19 | train_loss 0.4936 acc 0.764 f1 0.763 || val_loss 1.3561 acc 0.428 f1 0.349\n",
            "[W3] GRU Epoch 20 | train_loss 0.4696 acc 0.771 f1 0.770 || val_loss 1.3815 acc 0.436 f1 0.350\n",
            "[W3] GRU Epoch 21 | train_loss 0.4467 acc 0.792 f1 0.791 || val_loss 1.4423 acc 0.403 f1 0.331\n",
            "[W3] GRU Epoch 22 | train_loss 0.4325 acc 0.802 f1 0.801 || val_loss 1.4584 acc 0.422 f1 0.334\n",
            "[W3] GRU Epoch 23 | train_loss 0.4114 acc 0.803 f1 0.803 || val_loss 1.5132 acc 0.405 f1 0.332\n",
            "[W3] GRU Epoch 24 | train_loss 0.4026 acc 0.813 f1 0.813 || val_loss 1.5343 acc 0.432 f1 0.347\n",
            "[W3] GRU Epoch 25 | train_loss 0.3865 acc 0.820 f1 0.819 || val_loss 1.5793 acc 0.416 f1 0.335\n",
            "[W3] GRU Epoch 26 | train_loss 0.3713 acc 0.827 f1 0.827 || val_loss 1.5998 acc 0.432 f1 0.346\n",
            "Early stopping.\n",
            "============================================================\n",
            "Running Window=3 Model=LSTM Test subject=100\n",
            "Class distribution BEFORE SMOTE: Counter({np.int64(2): 981, np.int64(1): 929, np.int64(0): 277})\n",
            "Class distribution AFTER SMOTE: Counter({np.int64(1): 981, np.int64(2): 981, np.int64(0): 981})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W3] LSTM Epoch 01 | train_loss 1.1019 acc 0.335 f1 0.174 || val_loss 1.0909 acc 0.465 f1 0.215\n",
            "[W3] LSTM Epoch 02 | train_loss 1.0947 acc 0.366 f1 0.348 || val_loss 1.0967 acc 0.377 f1 0.340\n",
            "[W3] LSTM Epoch 03 | train_loss 1.0887 acc 0.412 f1 0.409 || val_loss 1.1032 acc 0.323 f1 0.304\n",
            "[W3] LSTM Epoch 04 | train_loss 1.0779 acc 0.434 f1 0.432 || val_loss 1.0921 acc 0.350 f1 0.319\n",
            "[W3] LSTM Epoch 05 | train_loss 1.0491 acc 0.467 f1 0.463 || val_loss 1.1119 acc 0.298 f1 0.288\n",
            "[W3] LSTM Epoch 06 | train_loss 0.9723 acc 0.527 f1 0.514 || val_loss 1.0775 acc 0.389 f1 0.314\n",
            "[W3] LSTM Epoch 07 | train_loss 0.8810 acc 0.572 f1 0.566 || val_loss 1.1098 acc 0.364 f1 0.299\n",
            "[W3] LSTM Epoch 08 | train_loss 0.8195 acc 0.611 f1 0.607 || val_loss 1.1650 acc 0.385 f1 0.326\n",
            "[W3] LSTM Epoch 09 | train_loss 0.7784 acc 0.632 f1 0.627 || val_loss 1.1506 acc 0.401 f1 0.329\n",
            "[W3] LSTM Epoch 10 | train_loss 0.7286 acc 0.654 f1 0.652 || val_loss 1.2175 acc 0.377 f1 0.310\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LOSO participants: 100%|| 100/100 [47:47<00:00, 28.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved LOSO results to LOSO_outputs1/results_loso.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Path to LOSO results CSV\n",
        "INPUT_FILE = Path(\"LOSO_outputs1/results_loso.csv\")\n",
        "OUTPUT_FILE = Path(\"LOSO_outputs1/results_loso_avg.csv\")\n",
        "\n",
        "# Load LOSO results\n",
        "if not INPUT_FILE.exists():\n",
        "    raise FileNotFoundError(f\"{INPUT_FILE} not found!\")\n",
        "\n",
        "df = pd.read_csv(INPUT_FILE)\n",
        "\n",
        "# Columns to average\n",
        "metrics = ['test_loss', 'test_acc', 'test_macro_f1']\n",
        "\n",
        "# Group by model and compute mean and standard deviation\n",
        "avg_df = df.groupby('model')[metrics].agg(['mean', 'std']).reset_index()\n",
        "\n",
        "# Flatten MultiIndex columns\n",
        "avg_df.columns = ['_'.join(col).strip('_') for col in avg_df.columns.values]\n",
        "\n",
        "# Save to CSV\n",
        "avg_df.to_csv(OUTPUT_FILE, index=False)\n",
        "print(f\"Saved LOSO averages per model to {OUTPUT_FILE}\")\n",
        "\n",
        "# Optional: print summary\n",
        "print(\"\\nAverage LOSO performance per model:\")\n",
        "print(avg_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYWcG5GW_aNo",
        "outputId": "860897a4-69dd-45e5-f00b-8bbddab17aac"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved LOSO averages per model to LOSO_outputs1/results_loso_avg.csv\n",
            "\n",
            "Average LOSO performance per model:\n",
            "   model  test_loss_mean  test_loss_std  test_acc_mean  test_acc_std  \\\n",
            "0    ANN        1.366389       0.304318       0.390741      0.094518   \n",
            "1  CNN1D        1.531317       0.606295       0.400000      0.091785   \n",
            "2    GRU        1.230809       0.369776       0.381111      0.093918   \n",
            "3   LSTM        1.299862       0.397285       0.371481      0.088941   \n",
            "4    RNN        1.183072       0.247508       0.362222      0.089116   \n",
            "\n",
            "   test_macro_f1_mean  test_macro_f1_std  \n",
            "0            0.303756           0.087204  \n",
            "1            0.320086           0.086321  \n",
            "2            0.315348           0.086713  \n",
            "3            0.306517           0.079003  \n",
            "4            0.308841           0.087930  \n"
          ]
        }
      ]
    }
  ]
}